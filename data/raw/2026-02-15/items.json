[
  {
    "id": "c16b69a1be247646",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "GPT-5.2 derives a new result in theoretical physics",
    "url": "https://openai.com/index/new-result-theoretical-physics",
    "summary": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
    "published": "Fri, 13 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c5ef81aef2a2ccc1",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
    "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt",
    "summary": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT to help organizations defend against prompt injection and AI-driven data exfiltration.",
    "published": "Fri, 13 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9d95a891a81b27c3",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Beyond rate limits: scaling access to Codex and Sora",
    "url": "https://openai.com/index/beyond-rate-limits",
    "summary": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
    "published": "Fri, 13 Feb 2026 09:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c10ff8c0943e4e07",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Scaling social science research",
    "url": "https://openai.com/index/scaling-social-science-research",
    "summary": "GABRIEL is a new open-source toolkit from OpenAI that uses GPT to turn qualitative text and images into quantitative data, helping social scientists analyze research at scale.",
    "published": "Fri, 13 Feb 2026 09:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e090493a0ff267ce",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Introducing GPT-5.3-Codex-Spark",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex-spark",
    "summary": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
    "published": "Thu, 12 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c35812ccca3ce7be",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Harness engineering: leveraging Codex in an agent-first world",
    "url": "https://openai.com/index/harness-engineering",
    "summary": "By Ryan Lopopolo, Member of the Technical Staff",
    "published": "Wed, 11 Feb 2026 09:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "4e730871994fa83c",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Testing ads in ChatGPT",
    "url": "https://openai.com/index/testing-ads-in-chatgpt",
    "summary": "OpenAI begins testing ads in ChatGPT to support free access, with clear labeling, answer independence, strong privacy protections, and user control.",
    "published": "Mon, 09 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "75bb2edf937a96ea",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Bringing ChatGPT to GenAI.mil",
    "url": "https://openai.com/index/bringing-chatgpt-to-genaimil",
    "summary": "OpenAI for Government announces the deployment of a custom ChatGPT on GenAI.mil, bringing secure, safety-forward AI to U.S. defense teams.",
    "published": "Mon, 09 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ebf24cf04fa0982c",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Making AI work for everyone, everywhere: our approach to localization",
    "url": "https://openai.com/index/our-approach-to-localization",
    "summary": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
    "published": "Fri, 06 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c82ec79fbf9f9804",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "GPT-5 lowers the cost of cell-free protein synthesis",
    "url": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
    "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
    "published": "Thu, 05 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "4e829743aa85afb8",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Introducing Trusted Access for Cyber",
    "url": "https://openai.com/index/trusted-access-for-cyber",
    "summary": "OpenAI introduces Trusted Access for Cyber, a trust-based framework that expands access to frontier cyber capabilities while strengthening safeguards against misuse.",
    "published": "Thu, 05 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ae167fa7c076bdf8",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Introducing OpenAI Frontier",
    "url": "https://openai.com/index/introducing-openai-frontier",
    "summary": "OpenAI Frontier is an enterprise platform for building, deploying, and managing AI agents with shared context, onboarding, permissions, and governance.",
    "published": "Thu, 05 Feb 2026 06:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7bd3c882dcf7e77a",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Introducing GPT-5.3-Codex",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex",
    "summary": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
    "published": "Thu, 05 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "6b46382980c79e4a",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Navigating health questions with ChatGPT",
    "url": "https://openai.com/index/navigating-health-questions",
    "summary": "A family shares how ChatGPT helped them prepare for critical cancer treatment decisions for their son alongside expert guidance from his doctors.",
    "published": "Thu, 05 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b2f960344448d2d5",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "GPT-5.3-Codex System Card",
    "url": "https://openai.com/index/gpt-5-3-codex-system-card",
    "summary": "GPT‑5.3-Codex is the most capable agentic coding model to date, combining the frontier coding performance of GPT‑5.2-Codex with the reasoning and professional knowledge capabilities of GPT‑5.2.",
    "published": "Thu, 05 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "cf0ef71eaded1866",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Unlocking the Codex harness: how we built the App Server",
    "url": "https://openai.com/index/unlocking-the-codex-harness",
    "summary": "Learn how to embed the Codex agent using the Codex App Server, a bidirectional JSON-RPC API powering streaming progress, tool use, approvals, and diffs.",
    "published": "Wed, 04 Feb 2026 13:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f68e038d8548a39e",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "VfL Wolfsburg turns ChatGPT into a club-wide capability",
    "url": "https://openai.com/index/vfl-wolfsburg",
    "summary": "By focusing on people, not pilots, the Bundesliga club is scaling efficiency, creativity, and knowledge—without losing its football identity.",
    "published": "Wed, 04 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "26e2757a88ef178e",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "The Sora feed philosophy",
    "url": "https://openai.com/index/sora-feed-philosophy",
    "summary": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
    "published": "Tue, 03 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "974dde3d72e6bf95",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Snowflake and OpenAI partner to bring frontier intelligence to enterprise data",
    "url": "https://openai.com/index/snowflake-partnership",
    "summary": "OpenAI and Snowflake partner in a $200M agreement to bring frontier intelligence into enterprise data, enabling AI agents and insights directly in Snowflake.",
    "published": "Mon, 02 Feb 2026 06:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c2e00d7c926c0f28",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Introducing the Codex app",
    "url": "https://openai.com/index/introducing-the-codex-app",
    "summary": "Introducing the Codex app for macOS—a command center for AI coding and software development with multiple agents, parallel workflows, and long-running tasks.",
    "published": "Mon, 02 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a6861025e0f1a141",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Inside OpenAI’s in-house data agent",
    "url": "https://openai.com/index/inside-our-in-house-data-agent",
    "summary": "How OpenAI built an in-house AI data agent that uses GPT-5, Codex, and memory to reason over massive datasets and deliver reliable insights in minutes.",
    "published": "Thu, 29 Jan 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "cacb53a143831134",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Taisei Corporation shapes the next generation of talent with ChatGPT",
    "url": "https://openai.com/index/taisei",
    "summary": "Taisei Corporation uses ChatGPT Enterprise to support HR-led talent development and scale generative AI across its global construction business.",
    "published": "Thu, 29 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "8af76a98aaedba17",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT",
    "url": "https://openai.com/index/retiring-gpt-4o-and-older-models",
    "summary": "On February 13, 2026, alongside the previously announced retirement⁠ of GPT‑5 (Instant, Thinking, and Pro), we will retire GPT‑4o, GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini from ChatGPT. In the API, there are no changes at this time.",
    "published": "Thu, 29 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "92ce58d187a17ee3",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "The next chapter for AI in the EU",
    "url": "https://openai.com/index/the-next-chapter-for-ai-in-the-eu",
    "summary": "OpenAI launches the EU Economic Blueprint 2.0 with new data, partnerships, and initiatives to accelerate AI adoption, skills, and growth across Europe.",
    "published": "Wed, 28 Jan 2026 01:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b2fec04c8337daa6",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "EMEA Youth & Wellbeing Grant",
    "url": "https://openai.com/index/emea-youth-and-wellbeing-grant",
    "summary": "Apply for the EMEA Youth & Wellbeing Grant, a €500,000 program funding NGOs and researchers advancing youth safety and wellbeing in the age of AI.",
    "published": "Wed, 28 Jan 2026 01:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "443337d9f1fab6df",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Keeping your data safe when an AI agent clicks a link",
    "url": "https://openai.com/index/ai-agent-link-safety",
    "summary": "Learn how OpenAI protects user data when AI agents open links, preventing URL-based data exfiltration and prompt injection with built-in safeguards.",
    "published": "Wed, 28 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "3d62d157a9fb33c0",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "PVH reimagines the future of fashion with OpenAI",
    "url": "https://openai.com/index/pvh-future-of-fashion",
    "summary": "PVH Corp., parent company of Calvin Klein and Tommy Hilfiger, is adopting ChatGPT Enterprise to bring AI into fashion design, supply chain, and consumer engagement.",
    "published": "Tue, 27 Jan 2026 06:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "2b9a95ae28f5d889",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Introducing Prism",
    "url": "https://openai.com/index/introducing-prism",
    "summary": "Prism is a free LaTeX-native workspace with GPT-5.2 built in, helping researchers write, collaborate, and reason in one place.",
    "published": "Tue, 27 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c6bbb381c4c62cfe",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Powering tax donations with AI powered personalized recommendations",
    "url": "https://openai.com/index/trustbank",
    "summary": "TRUSTBANK partnered with Recursive to build Choice AI using OpenAI models, delivering personalized, conversational recommendations that simplify Furusato Nozei gift discovery. A multi-agent system helps donors navigate thousands of options and find gifts that match their preferences.",
    "published": "Tue, 27 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "113ca482302d82d2",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "How Indeed uses AI to help evolve the job search",
    "url": "https://openai.com/index/indeed-maggie-hulce",
    "summary": "Indeed’s CRO Maggie Hulce shares how AI is transforming job search, recruiting, and talent acquisition for employers and job seekers.",
    "published": "Mon, 26 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b61d2d12af8f8b49",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Unrolling the Codex agent loop",
    "url": "https://openai.com/index/unrolling-the-codex-agent-loop",
    "summary": "A technical deep dive into the Codex agent loop, explaining how Codex CLI orchestrates models, tools, prompts, and performance using the Responses API.",
    "published": "Fri, 23 Jan 2026 12:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b518ddec0acdec8d",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Scaling PostgreSQL to power 800 million ChatGPT users",
    "url": "https://openai.com/index/scaling-postgresql",
    "summary": "An inside look at how OpenAI scaled PostgreSQL to millions of queries per second using replicas, caching, rate limiting, and workload isolation.",
    "published": "Thu, 22 Jan 2026 12:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "06b92625ece2ea5c",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Inside Praktika's conversational approach to language learning",
    "url": "https://openai.com/index/praktika",
    "summary": "How Praktika uses GPT-4.1 and GPT-5.2 to build adaptive AI tutors that personalize lessons, track progress, and help learners achieve real-world language fluency",
    "published": "Thu, 22 Jan 2026 05:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "51f698ecb8840e49",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Inside GPT-5 for Work: How Businesses Use GPT-5",
    "url": "https://openai.com/business/guides-and-resources/chatgpt-usage-and-adoption-patterns-at-work",
    "summary": "A data-driven report on how workers across industries use ChatGPT—covering adoption trends, top tasks, departmental patterns, and the future of AI at work.",
    "published": "Thu, 22 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7e790b947fa69980",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "How Higgsfield turns simple ideas into cinematic social videos",
    "url": "https://openai.com/index/higgsfield",
    "summary": "Discover how Higgsfield gives creators cinematic, social-first video output from simple inputs using OpenAI GPT-4.1, GPT-5, and Sora 2.",
    "published": "Wed, 21 Jan 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "fa2fed92141bd20c",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Introducing Edu for Countries",
    "url": "https://openai.com/index/edu-for-countries",
    "summary": "Edu for Countries is a new OpenAI initiative helping governments use AI to modernize education systems and build future-ready workforces.",
    "published": "Wed, 21 Jan 2026 01:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "469f9a6719278f63",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "How countries can end the capability overhang",
    "url": "https://openai.com/index/how-countries-can-end-the-capability-overhang",
    "summary": "Our latest report reveals stark differences in advanced AI adoption across countries and outlines new initiatives to help nations capture productivity gains from AI.",
    "published": "Wed, 21 Jan 2026 01:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ed986a99514cf288",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Horizon 1000: Advancing AI for primary healthcare",
    "url": "https://openai.com/index/horizon-1000",
    "summary": "OpenAI and the Gates Foundation launch Horizon 1000, a $50M pilot advancing AI capabilities for healthcare in Africa. The initiative aims to reach 1,000 clinics by 2028.",
    "published": "Tue, 20 Jan 2026 21:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "20b1e4b4fbc6fb96",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Stargate Community",
    "url": "https://openai.com/index/stargate-community",
    "summary": "Stargate Community plans detail a community-first approach to AI infrastructure, using locally tailored plans shaped by community input, energy needs, and workforce priorities.",
    "published": "Tue, 20 Jan 2026 19:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "3aa5db1070347228",
    "source": "openai_blog",
    "source_weight": 1.2,
    "title": "Cisco and OpenAI redefine enterprise engineering with AI agents",
    "url": "https://openai.com/index/cisco",
    "summary": "Cisco and OpenAI redefine enterprise engineering with Codex, an AI software agent embedded in workflows to speed builds, automate defect fixes, and enable AI-native development.",
    "published": "Tue, 20 Jan 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f09c45ee226de24a",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Chris Liddell Appointed Anthropic Board",
    "url": "https://www.anthropic.com/news/chris-liddell-appointed-anthropic-board",
    "summary": "",
    "published": "2026-02-13T16:21:14.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b91259f7d1a90da4",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Codepath Partnership",
    "url": "https://www.anthropic.com/news/anthropic-codepath-partnership",
    "summary": "",
    "published": "2026-02-13T16:19:50.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a76c02cb3f5a6457",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude For Financial Services",
    "url": "https://www.anthropic.com/news/claude-for-financial-services",
    "summary": "",
    "published": "2026-02-13T15:35:49.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "61bc1bd4729cda9c",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Raises 30 Billion Series G Funding 380 Billion Post Money Valuation",
    "url": "https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation",
    "summary": "",
    "published": "2026-02-12T21:43:18.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e2fb706f0e744611",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Donate Public First Action",
    "url": "https://www.anthropic.com/news/donate-public-first-action",
    "summary": "",
    "published": "2026-02-12T14:45:37.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "dc134a75ef857e94",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Healthcare Life Sciences",
    "url": "https://www.anthropic.com/news/healthcare-life-sciences",
    "summary": "",
    "published": "2026-02-11T22:31:33.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d90c001d10ad2d99",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Covering Electricity Price Increases",
    "url": "https://www.anthropic.com/news/covering-electricity-price-increases",
    "summary": "",
    "published": "2026-02-11T21:08:31.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "42710d92908034f2",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude Opus 4 6",
    "url": "https://www.anthropic.com/news/claude-opus-4-6",
    "summary": "",
    "published": "2026-02-10T19:07:24.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "92f2a5d3a3a7ce4c",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude For Nonprofits",
    "url": "https://www.anthropic.com/news/claude-for-nonprofits",
    "summary": "",
    "published": "2026-02-09T15:51:38.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "adc962dc0eb8fb5f",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude Is A Space To Think",
    "url": "https://www.anthropic.com/news/claude-is-a-space-to-think",
    "summary": "",
    "published": "2026-02-04T17:29:42.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "87e9dc91cff0bded",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Protecting Well Being Of Users",
    "url": "https://www.anthropic.com/news/protecting-well-being-of-users",
    "summary": "",
    "published": "2026-02-03T22:04:42.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "da97f24efa77bffc",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Apple Xcode Claude Agent Sdk",
    "url": "https://www.anthropic.com/news/apple-xcode-claude-agent-sdk",
    "summary": "",
    "published": "2026-02-03T18:06:03.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "22ae838c5b8585d0",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Partners With Allen Institute And Howard Hughes Medical Institute",
    "url": "https://www.anthropic.com/news/anthropic-partners-with-allen-institute-and-howard-hughes-medical-institute",
    "summary": "",
    "published": "2026-02-02T14:22:01.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "1698fab210cfb462",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Servicenow Anthropic Claude",
    "url": "https://www.anthropic.com/news/servicenow-anthropic-claude",
    "summary": "",
    "published": "2026-01-28T22:27:37.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "6c270de312aa4702",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Gov Uk Partnership",
    "url": "https://www.anthropic.com/news/gov-UK-partnership",
    "summary": "",
    "published": "2026-01-27T17:47:33.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d1090b0c93d0ddc9",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claudes Constitution",
    "url": "https://www.anthropic.com/news/claudes-constitution",
    "summary": "",
    "published": "2026-01-21T18:26:10.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "5adc396ab77c76e2",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Teach For All",
    "url": "https://www.anthropic.com/news/anthropic-teach-for-all",
    "summary": "",
    "published": "2026-01-21T17:36:00.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "effb3780b4481f1c",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude New Constitution",
    "url": "https://www.anthropic.com/news/claude-new-constitution",
    "summary": "",
    "published": "2026-01-21T16:34:47.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "1ad8b4f49514b15c",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Mariano Florentino Long Term Benefit Trust",
    "url": "https://www.anthropic.com/news/mariano-florentino-long-term-benefit-trust",
    "summary": "",
    "published": "2026-01-20T15:09:28.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a625cc82b9c13a98",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Appoints Irina Ghose As Managing Director Of India",
    "url": "https://www.anthropic.com/news/anthropic-appoints-irina-ghose-as-managing-director-of-india",
    "summary": "",
    "published": "2026-01-16T03:28:16.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "bae48fe420c0151d",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Accelerating Scientific Research",
    "url": "https://www.anthropic.com/news/accelerating-scientific-research",
    "summary": "",
    "published": "2026-01-15T22:49:21.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "414ab5999dc1d268",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Introducing Anthropic Labs",
    "url": "https://www.anthropic.com/news/introducing-anthropic-labs",
    "summary": "",
    "published": "2026-01-13T22:46:14.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0d91496d44e61262",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Prompting Long Context",
    "url": "https://www.anthropic.com/news/prompting-long-context",
    "summary": "",
    "published": "2026-01-06T15:23:18.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ca06aa5da94ea9da",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Compliance Framework Sb53",
    "url": "https://www.anthropic.com/news/compliance-framework-SB53",
    "summary": "",
    "published": "2025-12-19T21:05:06.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "58514db118b48352",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Genesis Mission Partnership",
    "url": "https://www.anthropic.com/news/genesis-mission-partnership",
    "summary": "",
    "published": "2025-12-18T23:06:17.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a9ab1bcc95b77ca2",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "How People Use Claude For Support Advice And Companionship",
    "url": "https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship",
    "summary": "",
    "published": "2025-12-12T01:47:15.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "fea9572459fb3562",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Donating The Model Context Protocol And Establishing Of The Agentic Ai Foundation",
    "url": "https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation",
    "summary": "",
    "published": "2025-12-11T18:14:55.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0e449fb1ae69d06a",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Political Even Handedness",
    "url": "https://www.anthropic.com/news/political-even-handedness",
    "summary": "",
    "published": "2025-12-10T00:18:44.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "3ab707c82df25f07",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Accenture Partnership",
    "url": "https://www.anthropic.com/news/anthropic-accenture-partnership",
    "summary": "",
    "published": "2025-12-09T17:55:19.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "cca01521c3a186d1",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Economic Index Insights From Claude Sonnet 3 7",
    "url": "https://www.anthropic.com/news/anthropic-economic-index-insights-from-claude-sonnet-3-7",
    "summary": "",
    "published": "2025-12-07T14:53:39.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0ca73ca8a8d841c6",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "The Anthropic Economic Index",
    "url": "https://www.anthropic.com/news/the-anthropic-economic-index",
    "summary": "",
    "published": "2025-12-07T14:53:27.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a39a5fb15249a533",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Snowflake Anthropic Expanded Partnership",
    "url": "https://www.anthropic.com/news/snowflake-anthropic-expanded-partnership",
    "summary": "",
    "published": "2025-12-03T21:13:50.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "40cd6a2be347c3bf",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Signs Cms Health Tech Ecosystem Pledge To Advance Healthcare Interoperability",
    "url": "https://www.anthropic.com/news/anthropic-signs-cms-health-tech-ecosystem-pledge-to-advance-healthcare-interoperability",
    "summary": "",
    "published": "2025-12-02T23:19:08.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d39c75c646da99d7",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude Opus 4 5",
    "url": "https://www.anthropic.com/news/claude-opus-4-5",
    "summary": "",
    "published": "2025-12-02T22:16:48.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0e5f5f5ac0abd2dc",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Acquires Bun As Claude Code Reaches Usd1B Milestone",
    "url": "https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone",
    "summary": "",
    "published": "2025-12-02T18:17:32.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f5beac66a9d5b4d3",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Education Report How Educators Use Claude",
    "url": "https://www.anthropic.com/news/anthropic-education-report-how-educators-use-claude",
    "summary": "",
    "published": "2025-11-22T20:40:06.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b42cc076232e9883",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude Sonnet 4 5",
    "url": "https://www.anthropic.com/news/claude-sonnet-4-5",
    "summary": "",
    "published": "2025-11-20T16:26:28.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "70dc6d57fbc42f80",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Thoughts On America S Ai Action Plan",
    "url": "https://www.anthropic.com/news/thoughts-on-america-s-ai-action-plan",
    "summary": "",
    "published": "2025-11-20T16:26:24.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f87abd86265a3a5d",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Raises Series F At Usd183B Post Money Valuation",
    "url": "https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation",
    "summary": "",
    "published": "2025-11-20T16:26:11.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "65078d3793b46c6f",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude Haiku 4 5",
    "url": "https://www.anthropic.com/news/claude-haiku-4-5",
    "summary": "",
    "published": "2025-11-20T16:25:50.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "65b08ecd50d2a05b",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Microsoft Nvidia Anthropic Announce Strategic Partnerships",
    "url": "https://www.anthropic.com/news/microsoft-nvidia-anthropic-announce-strategic-partnerships",
    "summary": "",
    "published": "2025-11-18T18:17:07.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "4b11d6fcc5cde12e",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude In Microsoft Foundry",
    "url": "https://www.anthropic.com/news/claude-in-microsoft-foundry",
    "summary": "",
    "published": "2025-11-18T16:52:14.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7416f8b164e9987c",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Rwandan Government Partnership Ai Education",
    "url": "https://www.anthropic.com/news/rwandan-government-partnership-ai-education",
    "summary": "",
    "published": "2025-11-17T13:29:53.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "3943a3ed768fbeee",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Disrupting Ai Espionage",
    "url": "https://www.anthropic.com/news/disrupting-AI-espionage",
    "summary": "",
    "published": "2025-11-14T11:25:56.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a835a61ba42351d0",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Maryland Partnership",
    "url": "https://www.anthropic.com/news/maryland-partnership",
    "summary": "",
    "published": "2025-11-13T17:57:34.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "88dc42a407df8994",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "New Offices In Paris And Munich Expand European Presence",
    "url": "https://www.anthropic.com/news/new-offices-in-paris-and-munich-expand-european-presence",
    "summary": "",
    "published": "2025-11-12T18:10:23.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e8193ce5e3db5513",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic Invests 50 Billion In American Ai Infrastructure",
    "url": "https://www.anthropic.com/news/anthropic-invests-50-billion-in-american-ai-infrastructure",
    "summary": "",
    "published": "2025-11-12T16:56:05.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "15084be5e0440817",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Cognizant Partnership",
    "url": "https://www.anthropic.com/news/cognizant-partnership",
    "summary": "",
    "published": "2025-11-06T20:57:55.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9e268111a73dd856",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Economic Futures Uk Europe",
    "url": "https://www.anthropic.com/news/economic-futures-uk-europe",
    "summary": "",
    "published": "2025-11-05T07:57:22.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "26eb897cc98a6111",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Anthropic And Iceland Announce One Of The World S First National Ai Education Pilots",
    "url": "https://www.anthropic.com/news/anthropic-and-iceland-announce-one-of-the-world-s-first-national-ai-education-pilots",
    "summary": "",
    "published": "2025-11-04T07:36:15.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "da4441c2e4c27664",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Opening Our Tokyo Office",
    "url": "https://www.anthropic.com/news/opening-our-tokyo-office",
    "summary": "",
    "published": "2025-10-29T21:42:18.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "59d3b01ed7721abe",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Advancing Claude For Financial Services",
    "url": "https://www.anthropic.com/news/advancing-claude-for-financial-services",
    "summary": "",
    "published": "2025-10-27T19:47:26.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "fe873d94ab95b6f0",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Seoul Becomes Third Anthropic Office In Asia Pacific",
    "url": "https://www.anthropic.com/news/seoul-becomes-third-anthropic-office-in-asia-pacific",
    "summary": "",
    "published": "2025-10-23T23:11:18.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c31d7a98e1c0abbc",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Expanding Our Use Of Google Cloud Tpus And Services",
    "url": "https://www.anthropic.com/news/expanding-our-use-of-google-cloud-tpus-and-services",
    "summary": "",
    "published": "2025-10-23T20:34:59.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "eebced64aedd3b8c",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Statement Dario Amodei American Ai Leadership",
    "url": "https://www.anthropic.com/news/statement-dario-amodei-american-ai-leadership",
    "summary": "",
    "published": "2025-10-21T14:30:53.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b48533d3b12ff543",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Claude For Life Sciences",
    "url": "https://www.anthropic.com/news/claude-for-life-sciences",
    "summary": "",
    "published": "2025-10-21T02:53:49.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "fb02ab0e33ecb870",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Salesforce Anthropic Expanded Partnership",
    "url": "https://www.anthropic.com/news/salesforce-anthropic-expanded-partnership",
    "summary": "",
    "published": "2025-10-14T12:02:03.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e3d23a17207a0019",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Expanding Global Operations To India",
    "url": "https://www.anthropic.com/news/expanding-global-operations-to-india",
    "summary": "",
    "published": "2025-10-08T00:58:04.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "6217bc3645bac073",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Rahul Patil Joins Anthropic",
    "url": "https://www.anthropic.com/news/rahul-patil-joins-anthropic",
    "summary": "",
    "published": "2025-10-07T19:17:27.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d77e7220341d1aa0",
    "source": "anthropic_newsroom",
    "source_weight": 1.2,
    "title": "Deloitte Anthropic Partnership",
    "url": "https://www.anthropic.com/news/deloitte-anthropic-partnership",
    "summary": "",
    "published": "2025-10-06T20:38:02.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9f6a0346a31359dc",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Building C Compiler",
    "url": "https://www.anthropic.com/engineering/building-c-compiler",
    "summary": "",
    "published": "2026-02-05T19:38:29.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "912cf25fe3586ccd",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Infrastructure Noise",
    "url": "https://www.anthropic.com/engineering/infrastructure-noise",
    "summary": "",
    "published": "2026-02-05T17:42:29.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d689d52beed2dd22",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Demystifying Evals For Ai Agents",
    "url": "https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents",
    "summary": "",
    "published": "2026-02-04T18:45:28.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "1ec70862d85c1973",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Claude Code Best Practices",
    "url": "https://www.anthropic.com/engineering/claude-code-best-practices",
    "summary": "",
    "published": "2026-01-26T23:24:56.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "bef6fafffc802698",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Ai Resistant Technical Evaluations",
    "url": "https://www.anthropic.com/engineering/AI-resistant-technical-evaluations",
    "summary": "",
    "published": "2026-01-22T01:16:20.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e2b90dd8fabfd8f2",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Contextual Retrieval",
    "url": "https://www.anthropic.com/engineering/contextual-retrieval",
    "summary": "",
    "published": "2026-01-07T15:00:44.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "512a12b7ae75ff77",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Effective Context Engineering For Ai Agents",
    "url": "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
    "summary": "",
    "published": "2026-01-06T15:31:12.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "88027f7315d59a33",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Building Effective Agents",
    "url": "https://www.anthropic.com/engineering/building-effective-agents",
    "summary": "",
    "published": "2026-01-06T15:24:44.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "4bffd18b76c47d13",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Multi Agent Research System",
    "url": "https://www.anthropic.com/engineering/multi-agent-research-system",
    "summary": "",
    "published": "2026-01-06T15:09:32.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "13d851a818b9d01a",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Writing Tools For Agents",
    "url": "https://www.anthropic.com/engineering/writing-tools-for-agents",
    "summary": "",
    "published": "2026-01-06T15:05:23.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9c6963896a79a683",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Claude Think Tool",
    "url": "https://www.anthropic.com/engineering/claude-think-tool",
    "summary": "",
    "published": "2025-12-15T18:02:25.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "317293c01f1d2a57",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "A Postmortem Of Three Recent Issues",
    "url": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
    "summary": "",
    "published": "2025-12-14T20:27:33.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a5a54faf2dba6740",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Effective Harnesses For Long Running Agents",
    "url": "https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents",
    "summary": "",
    "published": "2025-11-26T18:07:21.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "befa65b8a744151e",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Advanced Tool Use",
    "url": "https://www.anthropic.com/engineering/advanced-tool-use",
    "summary": "",
    "published": "2025-11-25T16:53:53.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "8761099c9d017166",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Code Execution With Mcp",
    "url": "https://www.anthropic.com/engineering/code-execution-with-mcp",
    "summary": "",
    "published": "2025-11-04T22:06:04.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "cf1f68524d89bd5b",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Claude Code Sandboxing",
    "url": "https://www.anthropic.com/engineering/claude-code-sandboxing",
    "summary": "",
    "published": "2025-11-03T21:01:45.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "93f054b6e0ffd331",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Desktop Extensions",
    "url": "https://www.anthropic.com/engineering/desktop-extensions",
    "summary": "",
    "published": "2025-09-12T00:43:28.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "96ce3fe2047170df",
    "source": "anthropic_engineering",
    "source_weight": 1.2,
    "title": "Swe Bench Sonnet",
    "url": "https://www.anthropic.com/engineering/swe-bench-sonnet",
    "summary": "",
    "published": "2025-03-19T20:43:04.000Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a8257b8b88f12ec0",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Custom Kernels for All from Codex and Claude",
    "url": "https://huggingface.co/blog/custom-cuda-kernels-agent-skills",
    "summary": "",
    "published": "Fri, 13 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "504929088957b0e2",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
    "url": "https://huggingface.co/blog/openenv-turing",
    "summary": "",
    "published": "Thu, 12 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "fccbd276447697a6",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Transformers.js v4 Preview: Now Available on NPM!",
    "url": "https://huggingface.co/blog/transformersjs-v4",
    "summary": "",
    "published": "Mon, 09 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d2d3810992df7825",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing SyGra Studio",
    "url": "https://huggingface.co/blog/ServiceNow-AI/sygra-studio",
    "summary": "",
    "published": "Thu, 05 Feb 2026 16:52:28 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d9c84300073d05b6",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Nemotron ColEmbed V2: Raising the Bar for Multimodal Retrieval with ViDoRe V3’s Top Model",
    "url": "https://huggingface.co/blog/nvidia/nemotron-colembed-v2",
    "summary": "",
    "published": "Wed, 04 Feb 2026 15:00:40 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "fe9da202a4704ea8",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Community Evals: Because we're done trusting black-box leaderboards over the community",
    "url": "https://huggingface.co/blog/community-evals",
    "summary": "",
    "published": "Wed, 04 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "91102c029a7fd92f",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "H Company's new Holo2 model takes the lead in UI Localization",
    "url": "https://huggingface.co/blog/Hcompany/introducing-holo2-235b-a22b",
    "summary": "",
    "published": "Tue, 03 Feb 2026 17:40:14 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f634b4223a6db742",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "The Future of the Global Open-Source AI Ecosystem: From DeepSeek to AI+",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-3",
    "summary": "",
    "published": "Tue, 03 Feb 2026 15:03:19 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "90c07e0cfebb61a7",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Training Design for Text-to-Image Models: Lessons from Ablations",
    "url": "https://huggingface.co/blog/Photoroom/prx-part2",
    "summary": "",
    "published": "Tue, 03 Feb 2026 11:25:53 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "29d122eeb5bb0bc2",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing Daggr: Chain apps programmatically, inspect visually",
    "url": "https://huggingface.co/blog/daggr",
    "summary": "",
    "published": "Thu, 29 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9bd6b937a4204960",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "We Got Claude to Build CUDA Kernels and teach open models!",
    "url": "https://huggingface.co/blog/upskill",
    "summary": "",
    "published": "Wed, 28 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ccef47d5befa3bba",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Architectural Choices in China's Open-Source AI Ecosystem: Building Beyond DeepSeek",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2",
    "summary": "",
    "published": "Tue, 27 Jan 2026 15:01:45 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "bf182a437ebdeb73",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Alyah ⭐️: Toward Robust Evaluation of Emirati Dialect Capabilities in Arabic LLMs",
    "url": "https://huggingface.co/blog/tiiuae/emirati-benchmarks",
    "summary": "",
    "published": "Tue, 27 Jan 2026 10:26:42 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "5c42dd2e17d30e84",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective",
    "url": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
    "summary": "",
    "published": "Tue, 27 Jan 2026 01:53:15 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7ce7b359b6cedf88",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality",
    "url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face",
    "summary": "",
    "published": "Wed, 21 Jan 2026 06:25:31 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "34a5690dbeed7dab",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "One Year Since the “DeepSeek Moment”",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment",
    "summary": "",
    "published": "Tue, 20 Jan 2026 15:02:10 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "77a6435a098b2d8e",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Differential Transformer V2",
    "url": "https://huggingface.co/blog/microsoft/diff-attn-v2",
    "summary": "",
    "published": "Tue, 20 Jan 2026 03:20:57 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "00a016ffc13d0f85",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing Waypoint-1: Real-time interactive video diffusion from Overworld",
    "url": "https://huggingface.co/blog/waypoint-1",
    "summary": "",
    "published": "Tue, 20 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "941eaaebf3b7a1cf",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Open Responses: What you need to know",
    "url": "https://huggingface.co/blog/open-responses",
    "summary": "",
    "published": "Thu, 15 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e62eb3fb8a1bc4ed",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI",
    "url": "https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning",
    "summary": "",
    "published": "Mon, 05 Jan 2026 22:56:51 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "52e841e7968c5f43",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture",
    "url": "https://huggingface.co/blog/tiiuae/falcon-h1-arabic",
    "summary": "",
    "published": "Mon, 05 Jan 2026 09:16:51 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b74e82e624fdd0f0",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "NVIDIA brings agents to life with DGX Spark and Reachy Mini",
    "url": "https://huggingface.co/blog/nvidia-reachy-mini",
    "summary": "",
    "published": "Mon, 05 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "808f5b4c07f3795e",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
    "url": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
    "summary": "",
    "published": "Tue, 23 Dec 2025 14:07:35 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b8d1f7c0db043f25",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
    "url": "https://huggingface.co/blog/tokenizers",
    "summary": "",
    "published": "Thu, 18 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "551dda269267aa50",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator",
    "url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe",
    "summary": "",
    "published": "Wed, 17 Dec 2025 13:22:18 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f7c6514fd6f76ace",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents",
    "url": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face",
    "summary": "",
    "published": "Mon, 15 Dec 2025 16:01:04 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "6bee81a8f68bf4d2",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "New in llama.cpp: Model Management",
    "url": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp",
    "summary": "",
    "published": "Thu, 11 Dec 2025 15:47:44 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e8a902cfc34f94d4",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Codex is Open Sourcing AI models",
    "url": "https://huggingface.co/blog/hf-skills-training-codex",
    "summary": "",
    "published": "Thu, 11 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "518b88720d0a5f21",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face",
    "url": "https://huggingface.co/blog/swift-huggingface",
    "summary": "",
    "published": "Fri, 05 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "467e7644a71ef33c",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "DeepMath: A lightweight math reasoning Agent with smolagents",
    "url": "https://huggingface.co/blog/intel-deepmath",
    "summary": "",
    "published": "Thu, 04 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e96f257938927a3c",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "We Got Claude to Fine-Tune an Open Source LLM",
    "url": "https://huggingface.co/blog/hf-skills-training",
    "summary": "",
    "published": "Thu, 04 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ec318a42062d586a",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
    "url": "https://huggingface.co/blog/transformers-v5",
    "summary": "",
    "published": "Mon, 01 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "daef2cca15dcfd10",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Diffusers welcomes FLUX-2",
    "url": "https://huggingface.co/blog/flux-2",
    "summary": "",
    "published": "Tue, 25 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "973a146a0013d1b8",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Continuous batching from first principles",
    "url": "https://huggingface.co/blog/continuous_batching",
    "summary": "",
    "published": "Tue, 25 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "eca5b1aa2a72ec13",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Building Deep Research: How we Achieved State of the Art",
    "url": "https://huggingface.co/blog/Tavily/tavily-deep-research",
    "summary": "",
    "published": "Mon, 24 Nov 2025 17:40:14 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c10899941a3e4bec",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "OVHcloud on Hugging Face Inference Providers 🔥",
    "url": "https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud",
    "summary": "",
    "published": "Mon, 24 Nov 2025 16:08:47 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "dbae085ad582536d",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "20x Faster TRL Fine-tuning with RapidFire AI",
    "url": "https://huggingface.co/blog/rapidfireai",
    "summary": "",
    "published": "Fri, 21 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7573916c05eb2ab7",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks",
    "url": "https://huggingface.co/blog/open-asr-leaderboard",
    "summary": "",
    "published": "Fri, 21 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "40249b55353db3e5",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms",
    "url": "https://huggingface.co/blog/anylanguagemodel",
    "summary": "",
    "published": "Thu, 20 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "41d7ff537a2d5ced",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models",
    "url": "https://huggingface.co/blog/ServiceNow-AI/apriel-h1",
    "summary": "",
    "published": "Wed, 19 Nov 2025 05:19:07 GMT",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d4ea9f8c41e92794",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Code, Compute and Connection: Inside the Inaugural NVIDIA AI Day São Paulo",
    "url": "https://blogs.nvidia.com/blog/ai-day-sao-paulo/",
    "summary": "The worldwide tour of NVIDIA AI Days — bringing together AI enthusiasts, developers, researchers and startups — made its latest stop in São Paulo, Brazil.",
    "published": "Thu, 12 Feb 2026 22:00:58 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0530f2ee25a8efae",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Leading Inference Providers Cut AI Costs by up to 10x With Open Source Models on NVIDIA Blackwell",
    "url": "https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/",
    "summary": "A diagnostic insight in healthcare. A character’s dialogue in an interactive game. An autonomous resolution from a customer service agent. Each of these AI-powered interactions is built on the same unit of intelligence: a token. Scaling these AI interactions requires businesses to consider whether they can afford more tokens. The answer lies in better tokenomics\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 12 Feb 2026 16:00:46 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "bac9f0ac7445edad",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "NVIDIA DGX Spark Powers Big Projects in Higher Education",
    "url": "https://blogs.nvidia.com/blog/dgx-spark-higher-education/",
    "summary": "At leading institutions across the globe, the NVIDIA DGX Spark desktop supercomputer is bringing data‑center‑class AI to lab benches, faculty offices and students’ systems. There’s even a DGX Spark hard at work in the South Pole, at the IceCube Neutrino Observatory run by the University of Wisconsin-Madison. The compact supercomputer’s petaflop‑class performance enables local deployment\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/dgx-spark-higher-education/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 12 Feb 2026 15:00:23 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b0bbdaed56ce378b",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "GeForce NOW Turns Screens Into a Gaming Machine",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-amazon-fire-tv-app/",
    "summary": "The GeForce NOW sixth-anniversary festivities roll on this February, continuing a monthlong celebration of NVIDIA’s cloud gaming service. This week brings even more reasons to join the party, as GeForce NOW launches on a new platform with support for Amazon Fire TV devices, and eight new games to keep the streaming going strong. The new\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-amazon-fire-tv-app/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 12 Feb 2026 14:00:58 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "8010462db1e78b23",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "GeForce NOW Celebrates Six Years of Streaming With 24 Games in February",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-feb-2026-games-list/",
    "summary": "Break out the cake and green sprinkles — GeForce NOW is turning six. Since launch, members have streamed over 1 billion hours, and the party’s just getting started. Throughout February, members can look forward to new games, fresh ways to play across more devices and even more ways to bring RTX power to every screen\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-feb-2026-games-list/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 05 Feb 2026 14:00:53 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "993f080f34b17b79",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Nemotron Labs: How AI Agents Are Turning Documents Into Real-Time Business Intelligence",
    "url": "https://blogs.nvidia.com/blog/ai-agents-intelligent-document-processing/",
    "summary": "Businesses today face the challenge of uncovering valuable insights buried within a wide variety of documents — including reports, presentations, PDFs, web pages and spreadsheets.",
    "published": "Wed, 04 Feb 2026 16:00:36 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7cb2e5a4bd8e8e51",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Everything Will Be Represented in a Virtual Twin, NVIDIA CEO Jensen Huang Says at 3DEXPERIENCE World",
    "url": "https://blogs.nvidia.com/blog/huang-3dexperience-2026/",
    "summary": "NVIDIA founder and CEO Jensen Huang and Dassault Systèmes CEO Pascal Daloz announced a partnership to build a shared industrial AI architecture, merging virtual twins with physics-based AI to redefine the future of design, engineering and manufacturing.",
    "published": "Tue, 03 Feb 2026 22:14:51 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7fcd2a2c35056be2",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Mercedes-Benz Unveils New S-Class Built on NVIDIA DRIVE AV, Which Enables an L4-Ready Architecture",
    "url": "https://blogs.nvidia.com/blog/mercedes-benz-l4-s-class-drive-av-platform/",
    "summary": "Mercedes-Benz is marking 140 years of automotive innovation with a new S-Class built for the AI era, bringing together automotive safety and NVIDIA’s advanced autonomous driving platform to enable a level 4-ready architecture designed for trust. The new S-Class with MB.OS, which will be equipped with the NVIDIA DRIVE Hyperion architecture and full-stack NVIDIA DRIVE\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/mercedes-benz-l4-s-class-drive-av-platform/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 29 Jan 2026 18:00:31 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "23548019f4b64ecc",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Into the Omniverse: Physical AI Open Models and Frameworks Advance Robots and Autonomous Systems",
    "url": "https://blogs.nvidia.com/blog/physical-ai-open-models-robot-autonomous-systems-omniverse/",
    "summary": "Open source has become essential for driving innovation in robotics and autonomy. By providing access to critical infrastructure — from simulation frameworks to AI models — NVIDIA is enabling collaborative development that accelerates the path to safer, more capable autonomous systems.",
    "published": "Thu, 29 Jan 2026 17:00:21 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "cd990fc767cf9d3d",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "GeForce NOW Brings GeForce RTX Gaming to Linux PCs",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-linux/",
    "summary": "Get ready to game — the native GeForce NOW app for Linux PCs is now available in beta, letting Linux desktops tap directly into GeForce RTX performance from the cloud. Alongside the expansion comes ten new games, including The Bard’s Tale IV: Director’s Cut and The Bard’s Tale Trilogy for a leveled-up gaming weekend. And\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-linux/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 29 Jan 2026 14:00:49 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7a29dda9cde09e1e",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Accelerating Science: A Blueprint for a Renewed National Quantum Initiative",
    "url": "https://blogs.nvidia.com/blog/national-quantum-initiative/",
    "summary": "Quantum technologies are rapidly emerging as foundational capabilities for economic competitiveness, national security and scientific leadership in the 21st century. Sustained U.S. leadership in quantum information science is critical to ensuring that breakthroughs in computing, sensing, networking and materials translate into secure technologies and industries, a skilled domestic workforce and long-term strategic advantage. To secure\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/national-quantum-initiative/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Wed, 28 Jan 2026 18:17:53 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7a6926c10a8e13bc",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "NVIDIA Launches Earth-2 Family of Open Models — the World’s First Fully Open, Accelerated Set of Models and Tools for AI Weather",
    "url": "https://blogs.nvidia.com/blog/nvidia-earth-2-open-models/",
    "summary": "At the American Meteorological Society’s Annual Meeting, NVIDIA today unveiled a new NVIDIA Earth-2 family of open models, libraries and frameworks for weather and climate AI, offering the world’s first fully open, production-ready weather AI software stack.",
    "published": "Mon, 26 Jan 2026 14:00:53 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "484509bdb7334e78",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "NVIDIA DRIVE AV Raises the Bar for Vehicle Safety as Mercedes-Benz CLA Earns Top Euro NCAP Award",
    "url": "https://blogs.nvidia.com/blog/drive-av-mercedes-benz-cla-euro-ncap-safety-award/",
    "summary": "AI-powered driver assistance technologies are becoming standard equipment, fundamentally changing how vehicle safety is assessed and validated. The recent recognition of the Mercedes-Benz CLA as Euro NCAP&#8217;s Best Performer of 2025 underscores this shift, as the vehicle combines traditional passive safety features with NVIDIA DRIVE AV software to achieve the highest overall safety score of\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/drive-av-mercedes-benz-cla-euro-ncap-safety-award/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 22 Jan 2026 18:21:49 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b8b6e98e2c15bfbe",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "How to Get Started With Visual Generative AI on NVIDIA RTX PCs",
    "url": "https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/",
    "summary": "AI-powered content generation is now embedded in everyday tools like Adobe and Canva, with a slew of agencies and studios incorporating the technology into their workflows. Image models now deliver photorealistic results consistently, video models can generate long and coherent clips, and both can follow creative directions. Creators are increasingly running these workflows locally on\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 22 Jan 2026 14:00:57 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e21d720a13f10d82",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "From Pilot to Profit: Survey Reveals the Financial Services Industry Is Doubling Down on AI Investment and Open Source",
    "url": "https://blogs.nvidia.com/blog/ai-in-financial-services-survey-2026/",
    "summary": "AI has taken center stage in financial services, automating the research and execution behind algorithmic trading and helping banks more accurately detect fraud and money laundering — all while improving risk management practices and expediting document processing. The sixth annual “NVIDIA State of AI in Financial Services” report, based on a survey of more than\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/ai-in-financial-services-survey-2026/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 22 Jan 2026 14:00:54 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "4303f629b9fa2240",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Flight Controls Are Cleared for Takeoff on GeForce NOW",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-flight-controls/",
    "summary": "The wait is over, pilots. Flight control support — one of the most community-requested features for GeForce NOW — is live starting today, following its announcement at CES earlier this month. Virtual captains can now bring dedicated flight gear into the cloud and feel every roll, yaw and throttle change with even more precision, starting\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-flight-controls/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 22 Jan 2026 14:00:53 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "95ae2111e3d3e387",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "‘Largest Infrastructure Buildout in Human History’: Jensen Huang on AI’s ‘Five-Layer Cake’ at Davos",
    "url": "https://blogs.nvidia.com/blog/davos-wef-blackrock-ceo-larry-fink-jensen-huang/",
    "summary": "AI is becoming the foundation of the “largest infrastructure buildout in human history,” spanning energy and computing infrastructure, AI models and applications, NVIDIA founder and CEO Jensen Huang said during a World Economic Forum discussion with BlackRock CEO Larry Fink.",
    "published": "Wed, 21 Jan 2026 12:50:16 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "6291a6f878a116e7",
    "source": "nvidia_blog",
    "source_weight": 1.0,
    "title": "Survive the Quarantine Zone and More With Devolver Digital Games on GeForce NOW",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-quarantine-zone/",
    "summary": "NVIDIA kicked off the year at CES, where the crowd buzzed about the latest gaming announcements — including the native GeForce NOW app for Linux and Amazon Fire TV sticks, community-requested flight-control support and a stacked AAA lineup for the year. Check out what the creators of the YouTube channels Cloud Gaming Battle and Anytime\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-quarantine-zone/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 15 Jan 2026 14:00:35 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "2dba7ccd26b39904",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Gemini 3 Deep Think: Advancing science, research and engineering",
    "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/",
    "summary": "Gemini 3 Deep Think logo",
    "published": "Thu, 12 Feb 2026 16:13:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "3bf9086fd27aeeb5",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "9 fun questions to try asking Google Photos",
    "url": "https://blog.google/products-and-platforms/products/photos/ask-button-ask-photos-tips/",
    "summary": "A collage of outdoor images, a blue icon that say \"Ask Photos,\" and examples of Ask Photos prompts.",
    "published": "Tue, 10 Feb 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "65afd187aa2ff455",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Helping kids and teens learn and grow online on Safer Internet Day",
    "url": "https://blog.google/innovation-and-ai/technology/safety-security/safer-internet-day-2026-kids-teens/",
    "summary": "User profile on smartphone connected to security, media, and settings icons.",
    "published": "Tue, 10 Feb 2026 02:30:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "e5dc30bcbd963b92",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Natively Adaptive Interfaces: A new framework for AI accessibility",
    "url": "https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/",
    "summary": "A collage of four images, the first of a woman with curly hair in front of a silver laptop, the second of the same woman and a man with short black hair speaking on a stairwell, the third of a the same man with glasses, and an aerial image of NTID",
    "published": "Thu, 05 Feb 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0b7dc60d8af90d37",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "How Google Cloud is helping Team USA elevate their tricks with AI",
    "url": "https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/",
    "summary": "A woman outdoors in the snow looks at a tablet. A half pipe is behind her.",
    "published": "Thu, 05 Feb 2026 16:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "26a63f74613e6671",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Watch our new Gemini ad ahead of football’s biggest weekend",
    "url": "https://blog.google/company-news/inside-google/company-announcements/gemini-ad-new-home/",
    "summary": "A toddler in a blue and yellow striped shirt sits on a kitchen counter eating a red apple. Text in the corner reads: 'New Home, Google Gemini SB Commercial’",
    "published": "Thu, 05 Feb 2026 14:30:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "2dede2dbdc9c1702",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "The latest AI news we announced in January",
    "url": "https://blog.google/innovation-and-ai/products/google-ai-updates-january-2026/",
    "summary": "mp4 showing a carousel of images including a card reading \"Help that's made for you\"",
    "published": "Wed, 04 Feb 2026 16:55:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0fc6449cf17e4c48",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "How we’re helping preserve the genetic information of endangered species with AI",
    "url": "https://blog.google/innovation-and-ai/technology/ai/ai-to-preserve-endangered-species/",
    "summary": "A four-part vertical collage showing a cotton-top tamarin, an ibex, a golden lion tamarin, and a penguin.",
    "published": "Mon, 02 Feb 2026 18:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9233762a8b5309dd",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Advancing AI benchmarking with Game Arena",
    "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
    "summary": "An illustration of a King and Ace playing card, a wolf's head, two chess pieces, a poker chip, and other abstract shapes on a white background.1",
    "published": "Mon, 02 Feb 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "6f71c08f77142fcb",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Project Genie: Experimenting with infinite, interactive worlds",
    "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/",
    "summary": "Text reads Introducing Project Genie",
    "published": "Thu, 29 Jan 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "29936bb569042f97",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Hear more about interactive world models in our latest podcast.",
    "url": "https://blog.google/innovation-and-ai/technology/ai/release-notes-podcast-project-genie/",
    "summary": "Project Genie: Create and explore worlds",
    "published": "Thu, 29 Jan 2026 15:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9b9db1424f88a8b6",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Google AI Plus is now available everywhere our AI plans are available, including the U.S.",
    "url": "https://blog.google/products-and-platforms/products/google-one/google-ai-plus-availability/",
    "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Plus_Hero_Visual_2096.max-600x600.format-webp_4ffr2GI.webp\" />We’re launching Google AI Plus in 35 new countries and territories including the US, making it available everywhere Google AI plans are available.",
    "published": "Tue, 27 Jan 2026 18:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a4e42dbf35ad9723",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Just ask anything: a seamless new Search experience",
    "url": "https://blog.google/products-and-platforms/products/search/ai-mode-ai-overviews-updates/",
    "summary": "A centered, elongated oval shape resembling a search bar with the text \"Ask anything\" inside it.",
    "published": "Tue, 27 Jan 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f6ce5d131c6814c1",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "In our latest podcast, hear how the “Smoke Jumpers” team brings Gemini to billions of people.",
    "url": "https://blog.google/products-and-platforms/products/gemini/release-notes-podcast-smokejumpers/",
    "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/thumbnails_EP24_002_ccRelease_N.max-600x600.format-webp.webp\" />Bringing Gemini to billions of users requires a massive, coordinated infrastructure effort. In the latest episode of the Google AI: Release Notes podcast, host Logan Kil…",
    "published": "Tue, 27 Jan 2026 10:28:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a5a028261628d3f1",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "How animators and AI researchers made ‘Dear Upstairs Neighbors’",
    "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/dear-upstairs-neighbors/",
    "summary": "A movie poster for a film titled “Dear Upstairs Neighbors”, hand-painted in a vivid expressionist style, featuring an exasperated cartoon woman clutching her ears, surrounded by neon-colored images of noisy things like howling dogs and stomping shoes",
    "published": "Mon, 26 Jan 2026 18:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7e4f2a7329c01802",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Personal Intelligence in AI Mode in Search: Help that's uniquely yours",
    "url": "https://blog.google/products-and-platforms/products/search/personal-intelligence-ai-mode-search/",
    "summary": "Screenshot of the AI Mode page with the text, \"Hi Lukas what's on your mind?\"",
    "published": "Thu, 22 Jan 2026 16:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "49017c07cf07fbd5",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Building a community-led future for AI in film with Sundance Institute",
    "url": "https://blog.google/company-news/outreach-and-initiatives/google-org/sundance-institute-ai-education/",
    "summary": "Illustration of a cinematographer filming with a professional video camera mounted on a tripod, positioned next to a studio light and a film clapboard.",
    "published": "Tue, 20 Jan 2026 20:30:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "03759defcafc7565",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "How Nano Banana got its name",
    "url": "https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/",
    "summary": "A Nano Banana-generated image showing yellow bananas spelling out 'Nano Banana' on a bed of additional yellow bananas",
    "published": "Thu, 15 Jan 2026 16:06:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "53d957920fd917e1",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Learners and educators are AI’s new “super users”",
    "url": "https://blog.google/products-and-platforms/products/education/our-life-with-ai-2025/",
    "summary": "Teacher using a computer with students",
    "published": "Thu, 15 Jan 2026 11:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "de750dee6aa7002a",
    "source": "google_ai_blog",
    "source_weight": 1.0,
    "title": "Introducing Community Benchmarks on Kaggle",
    "url": "https://blog.google/innovation-and-ai/technology/developers-tools/kaggle-community-benchmarks/",
    "summary": "A drawing of three people working on laptops, with one person's screen showing \"Kaggle Benchmark Results\" with \"Gemini XXXX\" and a large \"PASS\" checkmark.1",
    "published": "Wed, 14 Jan 2026 14:00:00 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "18ed65b18f8661b8",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Customize AI agent browsing with proxies, profiles, and extensions in Amazon Bedrock AgentCore Browser",
    "url": "https://aws.amazon.com/blogs/machine-learning/customize-ai-agent-browsing-with-proxies-profiles-and-extensions-in-amazon-bedrock-agentcore-browser/",
    "summary": "Today, we are announcing three new capabilities that address these requirements: proxy configuration, browser profiles, and browser extensions. Together, these features give you fine-grained control over how your AI agents interact with the web. This post will walk through each capability with configuration examples and practical use cases to help you get started.",
    "published": "Fri, 13 Feb 2026 22:57:34 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "caf0f5862fc0931a",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "AI meets HR: Transforming talent acquisition with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/ai-meets-hr-transforming-talent-acquisition-with-amazon-bedrock/",
    "summary": "In this post, we show how to create an AI-powered recruitment system using Amazon Bedrock, Amazon Bedrock Knowledge Bases, AWS Lambda, and other AWS services to enhance job description creation, candidate communication, and interview preparation while maintaining human oversight.",
    "published": "Thu, 12 Feb 2026 20:18:58 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ce4da08a3b944062",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Build long-running MCP servers on Amazon Bedrock AgentCore with Strands Agents integration",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-long-running-mcp-servers-on-amazon-bedrock-agentcore-with-strands-agents-integration/",
    "summary": "In this post, we provide you with a comprehensive approach to achieve this. First, we introduce a context message strategy that maintains continuous communication between servers and clients during extended operations. Next, we develop an asynchronous task management framework that allows your AI agents to initiate long-running processes without blocking other operations. Finally, we demonstrate how to bring these strategies together with Amazon Bedrock AgentCore and Strands Agents to build production-ready AI agents that can handle complex, time-intensive operations reliably.",
    "published": "Thu, 12 Feb 2026 20:16:20 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "31398854086f7b11",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "NVIDIA Nemotron 3 Nano 30B MoE model is now available in Amazon SageMaker JumpStart",
    "url": "https://aws.amazon.com/blogs/machine-learning/nvidia-nemotron-3-nano-30b-is-now-available-in-amazon-sagemaker-jumpstart/",
    "summary": "Today we’re excited to announce that the NVIDIA Nemotron 3 Nano 30B model with &nbsp;3B active parameters is now generally available in the Amazon SageMaker JumpStart model catalog. You can accelerate innovation and deliver tangible business value with Nemotron 3 Nano on Amazon Web Services (AWS) without having to manage model deployment complexities. You can power your generative AI applications with Nemotron capabilities using the managed deployment capabilities offered by SageMaker JumpStart.",
    "published": "Wed, 11 Feb 2026 19:38:47 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9f25ae5d3e4cfddc",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Mastering Amazon Bedrock throttling and service availability: A comprehensive guide",
    "url": "https://aws.amazon.com/blogs/machine-learning/mastering-amazon-bedrock-throttling-and-service-availability-a-comprehensive-guide/",
    "summary": "This post shows you how to implement robust error handling strategies that can help improve application reliability and user experience when using Amazon Bedrock. We'll dive deep into strategies for optimizing performances for the application with these errors. Whether this is for a fairly new application or matured AI application, in this post you will be able to find the practical guidelines to operate with on these errors.",
    "published": "Wed, 11 Feb 2026 15:52:54 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f7ecc60851a906e9",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Swann provides Generative AI to millions of IoT Devices using Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/swann-provides-generative-ai-to-millions-of-iot-devices-using-amazon-bedrock/",
    "summary": "This post shows you how to implement intelligent notification filtering using Amazon Bedrock and its gen-AI capabilities. You'll learn model selection strategies, cost optimization techniques, and architectural patterns for deploying gen-AI at IoT scale, based on Swann Communications deployment across millions of devices.",
    "published": "Wed, 11 Feb 2026 15:48:15 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "05f8faf1ec331d76",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "How LinqAlpha assesses investment theses using Devil’s Advocate on Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-linqalpha-assesses-investment-theses-using-devils-advocate-on-amazon-bedrock/",
    "summary": "LinqAlpha is a Boston-based multi-agent AI system built specifically for institutional investors. The system supports and streamlines agentic workflows across company screening, primer generation, stock price catalyst mapping, and now, pressure-testing investment ideas through a new AI agent called Devil’s Advocate. In this post, we share how LinqAlpha uses Amazon Bedrock to build and scale Devil’s Advocate.",
    "published": "Wed, 11 Feb 2026 15:45:30 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "3ca09597f05bfb41",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "How Amazon uses Amazon Nova models to automate operational readiness testing for new fulfillment centers",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-amazon-uses-amazon-nova-models-to-automate-operational-readiness-testing-for-new-fulfillment-centers/",
    "summary": "In this post, we discuss how&nbsp;Amazon Nova&nbsp;in&nbsp;Amazon Bedrock&nbsp;can be used to implement an AI-powered image recognition solution that automates the detection and validation of module components, significantly reducing manual verification efforts and improving accuracy.",
    "published": "Tue, 10 Feb 2026 18:34:09 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "25156d2be403a27d",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Iberdrola enhances IT operations using Amazon Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/iberdrola-enhances-it-operations-using-amazon-bedrock-agentcore/",
    "summary": "Iberdrola, one of the world’s largest utility companies, has embraced cutting-edge AI technology to revolutionize its IT operations in ServiceNow. Through its partnership with AWS, Iberdrola implemented different agentic architectures using Amazon Bedrock AgentCore, targeting three key areas: optimizing change request validation in the draft phase, enriching incident management with contextual intelligence, and simplifying change model selection using conversational AI. These innovations reduce bottlenecks, help teams accelerate ticket resolution, and deliver consistent and high-quality data handling throughout the organization.",
    "published": "Tue, 10 Feb 2026 18:31:57 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "51378656ffa229b1",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Building real-time voice assistants with Amazon Nova Sonic compared to cascading architectures",
    "url": "https://aws.amazon.com/blogs/machine-learning/building-real-time-voice-assistants-with-amazon-nova-sonic-compared-to-cascading-architectures/",
    "summary": "Amazon Nova Sonic&nbsp;delivers real-time, human-like voice conversations through the bidirectional streaming interface. In this post, you learn how Amazon Nova Sonic can solve some of the challenges faced by cascaded approaches, simplify building voice AI agents, and provide natural conversational capabilities. We also provide guidance on when to choose each approach to help you make informed decisions for your voice AI projects.",
    "published": "Tue, 10 Feb 2026 18:29:05 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a075dbc7d77bca68",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Automated Reasoning checks rewriting chatbot reference implementation",
    "url": "https://aws.amazon.com/blogs/machine-learning/automated-reasoning-checks-rewriting-chatbot-reference-implementation/",
    "summary": "This blog post dives deeper into the implementation architecture for the Automated Reasoning checks rewriting chatbot.",
    "published": "Mon, 09 Feb 2026 19:34:05 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "22c42c0a18e7784d",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Scale LLM fine-tuning with Hugging Face and Amazon SageMaker AI",
    "url": "https://aws.amazon.com/blogs/machine-learning/scale-llm-fine-tuning-with-hugging-face-and-amazon-sagemaker-ai/",
    "summary": "In this post, we show how this integrated approach transforms enterprise LLM fine-tuning from a complex, resource-intensive challenge into a streamlined, scalable solution for achieving better model performance in domain-specific applications.",
    "published": "Mon, 09 Feb 2026 16:48:46 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "5e64b0122203008f",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "New Relic transforms productivity with generative AI on AWS",
    "url": "https://aws.amazon.com/blogs/machine-learning/new-relic-transforms-productivity-with-generative-ai-on-aws/",
    "summary": "Working with the Generative AI Innovation Center, New Relic NOVA (New Relic Omnipresence Virtual Assistant) evolved from a knowledge assistant into a comprehensive productivity engine. We explore the technical architecture, development journey, and key lessons learned in building an enterprise-grade AI solution that delivers measurable productivity gains at scale.",
    "published": "Mon, 09 Feb 2026 16:45:16 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a98185712084d329",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Accelerate agentic application development with a full-stack starter template for Amazon Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/accelerate-agentic-application-development-with-a-full-stack-starter-template-for-amazon-bedrock-agentcore/",
    "summary": "In this post, you will learn how to deploy Fullstack AgentCore Solution Template (FAST) to your Amazon Web Services (AWS) account, understand its architecture, and see how to extend it for your requirements. You will learn how to build your own agent while FAST handles authentication, infrastructure as code (IaC), deployment pipelines, and service integration.",
    "published": "Mon, 09 Feb 2026 16:40:58 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "da0df0d660b4dc54",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Agent-to-agent collaboration: Using Amazon Nova 2 Lite and Amazon Nova Act for multi-agent systems",
    "url": "https://aws.amazon.com/blogs/machine-learning/agent-to-agent-collaboration-using-amazon-nova-2-lite-and-amazon-nova-act-for-multi-agent-systems/",
    "summary": "This post walks through how agent-to-agent collaboration on Amazon Bedrock works in practice, using Amazon Nova 2 Lite for planning and Amazon Nova Act for browser interaction, to turn a fragile single-agent setup into a predictable multi-agent system.",
    "published": "Mon, 09 Feb 2026 16:00:28 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "79370d2495557126",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Structured outputs on Amazon Bedrock: Schema-compliant AI responses",
    "url": "https://aws.amazon.com/blogs/machine-learning/structured-outputs-on-amazon-bedrock-schema-compliant-ai-responses/",
    "summary": "Today, we're announcing structured outputs on Amazon Bedrock—a capability that fundamentally transforms how you can obtain validated JSON responses from foundation models through constrained decoding for schema compliance. In this post, we explore the challenges of traditional JSON generation and how structured outputs solves them. We cover the two core mechanisms—JSON Schema output format and strict tool use—along with implementation details, best practices, and practical code examples.",
    "published": "Fri, 06 Feb 2026 20:12:14 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "157be0691604e576",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Manage Amazon SageMaker HyperPod clusters using the HyperPod CLI and SDK",
    "url": "https://aws.amazon.com/blogs/machine-learning/manage-amazon-sagemaker-hyperpod-clusters-using-the-hyperpod-cli-and-sdk/",
    "summary": "In this post, we demonstrate how to use the CLI and the SDK to create and manage SageMaker HyperPod clusters in your AWS account. We walk through a practical example and dive deeper into the user workflow and parameter choices.",
    "published": "Fri, 06 Feb 2026 19:27:45 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "c1e45689c97e4666",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "Evaluate generative AI models with an Amazon Nova rubric-based LLM judge on Amazon SageMaker AI (Part 2)",
    "url": "https://aws.amazon.com/blogs/machine-learning/evaluate-generative-ai-models-with-an-amazon-nova-rubric-based-llm-judge-on-amazon-sagemaker-ai-part-2/",
    "summary": "In this post, we explore the Amazon Nova rubric-based judge feature: what a rubric-based judge is, how the judge is trained, what metrics to consider, and how to calibrate the judge. We chare notebook code of the Amazon Nova rubric-based LLM-as-a-judge methodology to evaluate and compare the outputs of two different LLMs using SageMaker training jobs.",
    "published": "Fri, 06 Feb 2026 16:29:45 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "3e9f373e384ab4b1",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "How Associa transforms document classification with the GenAI IDP Accelerator and Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-associa-transforms-document-classification-with-the-genai-idp-accelerator-and-amazon-bedrock/",
    "summary": "Associa collaborated with the AWS Generative AI Innovation Center to build a generative AI-powered document classification system aligning with Associa’s long-term vision of using generative AI to achieve operational efficiencies in document management. The solution automatically categorizes incoming documents with high accuracy, processes documents efficiently, and provides substantial cost savings while maintaining operational excellence. The document classification system, developed using the Generative AI Intelligent Document Processing (GenAI IDP) Accelerator, is designed to integrate seamlessly into existing workflows. It revolutionizes how employees interact with document management systems by reducing the time spent on manual classification tasks.",
    "published": "Thu, 05 Feb 2026 20:41:52 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "39f36f2af84fbdf3",
    "source": "aws_ml_blog",
    "source_weight": 1.0,
    "title": "A practical guide to Amazon Nova Multimodal Embeddings",
    "url": "https://aws.amazon.com/blogs/machine-learning/a-practical-guide-to-amazon-nova-multimodal-embeddings/",
    "summary": "In this post, you will learn how to configure and use Amazon Nova Multimodal Embeddings for media asset search systems, product discovery experiences, and document retrieval applications.",
    "published": "Thu, 05 Feb 2026 20:35:34 +0000",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "6e2225d549ed5ae2",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.16.0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0",
    "summary": "<h1>vLLM v0.16.0</h1>\n<h2>Highlights</h2>\n<p>This release features 440 commits from 203 contributors (7 new)!</p>\n<ul>\n<li><strong>PyTorch 2.10 upgrade</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>). This is a breaking change for environment dependency.</li>\n<li><strong>Async scheduling + Pipeline Parallelism</strong> is now fully supported, delivering <strong>30.8% E2E throughput improvement</strong> and <strong>31.8% TPOT improvement</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>).</li>\n<li><strong>Realtime API</strong>: A new WebSocket-based Realtime API enables streaming audio interactions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>), building on the Voxtral realtime infrastructure.</li>\n<li><strong>RLHF workflow improvements</strong>: Native NCCL-based weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise weight reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), and engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li><strong>Unified Parallel Drafting</strong> for speculative decoding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), plus spec decode now works with structured outputs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>) and penalty application in Model Runner V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>).</li>\n<li><strong>Major XPU platform overhaul</strong>: Deprecated IPEX in favor of vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>), adding MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), and FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>) support.</li>\n</ul>\n<h3>Model Support</h3>\n<ul>\n<li>New architectures: GLM-OCR with MTP (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33005\">#33005</a>), Qwen3-ASR (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33312\">#33312</a>), DeepSeek-OCR-2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33165\">#33165</a>), Intern-S1-Pro (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33636\">#33636</a>), MiniCPM-o 4.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33431\">#33431</a>), openPangu7B-VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32449\">#32449</a>), NemotronHPuzzle heterogeneous (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32549\">#32549</a>), MusicFlamingo (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32696\">#32696</a>), FunAudioChat (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/2\">#2</a>), ColBERT late interaction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33686\">#33686</a>), voyage-4-nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33720\">#33720</a>), GLM-5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34124\">#34124</a>).</li>\n<li>Speculative decoding: EAGLE3 for Hunyuan/HunyuanVL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33035\">#33035</a>), AFMoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33111\">#33111</a>), Mistral3 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a>).</li>\n<li>LoRA expansion: Gemma3 vision components (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32764\">#32764</a>), Nemotron-H MTP models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32265\">#32265</a>), Qwen3 output embedding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29816\">#29816</a>). Optimized fused MoE-LoRA kernel indexing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32770\">#32770</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32774\">#32774</a>), unpermute-aware fused MoE LoRA path (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32655\">#32655</a>), reduced kernel overhead for fewer active LoRAs with multiple CUDA graphs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32005\">#32005</a>).</li>\n<li>Features: Qwen3-Omni transcription (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29828\">#29828</a>), Mistral Large 3 with FlashInfer MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33174\">#33174</a>), LFM2 SigLIP2 intermediate encoder layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33370\">#33370</a>), Qwen3-Omni/GLM-4.xV MRoPE positioning fixes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33010\">#33010</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33039\">#33039</a>), embedding input for disabled modalities (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32493\">#32493</a>).</li>\n<li>Performance: GLM-4.7-GPTQ decode and MTP acceptance rate regression fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a>), DeepSeek V3.2 fast detokenization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33855\">#33855</a>), DeepSeek V3.2 tokenizer fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33832\">#33832</a>), GLM-5 MTP accuracy fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34385\">#34385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li>Async scheduling + Pipeline Parallelism: Full support with 30.8% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>), optimized spec decode + async scheduling with 1.5% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33612\">#33612</a>), deadlock fix for torchrun PP broadcast (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33701\">#33701</a>).</li>\n<li>Speculative decoding: Unified Parallel Drafting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), structured output support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>), penalty application in MRV2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>), skip softmax for all-greedy rejection sampling (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32852\">#32852</a>), correctness fix for spec tokens with prefill chunks (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33652\">#33652</a>).</li>\n<li>RLHF: Native NCCL weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li>Helion kernel framework: ConfigManager (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32740\">#32740</a>), kernel wrapper (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32964\">#32964</a>), kernel registry (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33203\">#33203</a>).</li>\n<li>PluggableLayer: Applied to linear layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33152\">#33152</a>) and Mamba layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33660\">#33660</a>).</li>\n<li>Batch invariance: Disable Cascade Attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32561\">#32561</a>), enable Triton attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33688\">#33688</a>).</li>\n<li>Performance: Grammar bitmask H2D copy on separate stream (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33059\">#33059</a>), zero-copy GQA for multimodal and CPU (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33732\">#33732</a>), early-reject oversized MM requests (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33502\">#33502</a>), CPU memory leak fix from Request reference cycle in prefix caching (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34183\">#34183</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<ul>\n<li><strong>NVIDIA</strong>: FlashInfer TRTLLM BF16 MoE integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32954\">#32954</a>), SM100 INT4 W4A16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32437\">#32437</a>), SM121 (DGX Spark) CUTLASS support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a>), MNNVL protocol for GB series (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33540\">#33540</a>), FlashInfer MLA concat optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31171\">#31171</a>), GDN attention layout optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33291\">#33291</a>), DeepGEMM FP8 MLA performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33568\">#33568</a>), wvSplitK_fp8 performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33527\">#33527</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33493\">#33493</a>), B200 MoE configs for Nemotron Nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32804\">#32804</a>), Super B200 TP2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33510\">#33510</a>), GLM 4.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32958\">#32958</a>), Mamba selective scan tuning for B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32873\">#32873</a>). Fix: DeepSeek R1 CUTLASS MLA on B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33637\">#33637</a>), QK Norm+RoPE fusion on B200+FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a>), CUTLASS FP8 blockwise on SM103a (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32224\">#32224</a>).</li>\n<li><strong>AMD ROCm</strong>: QWEN3-NEXT FP8 tunings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32042\">#32042</a>), AITER attention backend for Qwen3-Next (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32492\">#32492</a>), fused_add_rmsnorm_pad for GPT-OSS (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30976\">#30976</a>), Qwen3-Omni startup fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33077\">#33077</a>).</li>\n<li><strong>Intel XPU</strong>: Platform overhaul - deprecated IPEX, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>). New: unquantized MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>).</li>\n<li><strong>ARM CPU</strong>: KleidiAI INT4 dynamic quant with BF16 activations (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33122\">#33122</a>), NEON BFMMLA BF16 paged attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32263\">#32263</a>), vectorization backend optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30329\">#30329</a>), attention dispatch by head_dim alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32161\">#32161</a>).</li>\n<li><strong>IBM Z</strong>: BF16 kernel type for s390x (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33788\">#33788</a>).</li>\n<li><strong>torch.compile</strong>: Stop compiling identical artifacts (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34003\">#34003</a>), MoE cold start optimization option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33735\">#33735</a>), fix 32-bit indexing assumption (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33113\">#33113</a>), attention fusion pass fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33945\">#33945</a>).</li>\n<li><strong>Performance</strong>: Chat completion streaming optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33782\">#33782</a>), ORJSONResponse for faster API responses (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33548\">#33548</a>), MoE permute optimization for CUTLASS FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32892\">#32892</a>), shared/routed overlap for latent MoE on Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32790\">#32790</a>), FlashInfer autotune control flag (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34006\">#34006</a>).</li>\n</ul>\n<h3>Large Scale Serving</h3>\n<ul>\n<li>Disaggregated serving: Mooncake connector rework with bootstrap server (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31034\">#31034</a>), cross-layer KV cache layout at NIXL Connector V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33339\">#33339</a>), delay freeing blocks for aborted async loads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32255\">#32255</a>), async double-free fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33377\">#33377</a>), Ray multi-replica single-instance fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a>).</li>\n<li>EPLB: Capture logical experts with router replay (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33013\">#33013</a>), DP metadata fix for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32739\">#32739</a>).</li>\n<li>Metrics: KV offloading connector metrics (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/27942\">#27942</a>), labeled prompt token metrics for P/D disaggregation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33290\">#33290</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li>New: FP8 block quant for CompressedTensorsW8A16Fp8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33280\">#33280</a>), ModelOpt MXFP8 for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33786\">#33786</a>), NVFP4/FP8 on Turing GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33076\">#33076</a>), TP &gt; 4 for FP4 Gemm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31099\">#31099</a>).</li>\n<li>Bugfixes: FP8 online quantization memory fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31914\">#31914</a>), asymmetric W4A16 (ConchLinear) for CT (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33200\">#33200</a>), DeepSeek V3.2 NVFP4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33932\">#33932</a>), LoRA FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33879\">#33879</a>), quantized Falcon-H1 model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32728\">#32728</a>), quantized Mamba TP with n_groups=1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33257\">#33257</a>), CPU W8A8 with bias (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33582\">#33582</a>), CPU W8A8 3D input support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33727\">#33727</a>).</li>\n<li><strong>Deprecation</strong>: Removed BitBlas (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Realtime API</strong>: WebSocket-based streaming API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>) with Voxtral realtime support.</li>\n<li><strong>Responses API</strong>: Sampling parameters (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32609\">#32609</a>), return token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33212\">#33212</a>), return prompt token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33378\">#33378</a>), parser implementation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32712\">#32712</a>).</li>\n<li>Pooling API: Request schema consensus for ScoreRequest (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33060\">#33060</a>) and final standardization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31127\">#31127</a>).</li>\n<li>Tool calling: Fix multi-turn tool call ID preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32768\">#32768</a>), fix indexing double-counting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33141\">#33141</a>), GLM-4 incremental string streaming (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33218\">#33218</a>), DSV3.2 fast detokenization fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33964\">#33964</a>), MCP tools non-streaming fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32762\">#32762</a>).</li>\n<li>Structured outputs: Performance optimization with reasoning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33557\">#33557</a>), guidance vocab size fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33509\">#33509</a>).</li>\n<li>CLI: <code>--disable-access-log-for-endpoints</code> option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30011\">#30011</a>).</li>\n<li>UX: Nested configs in YAML files (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33193\">#33193</a>), GGUF <code>repo_id:quant_type</code> syntax (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33371\">#33371</a>), DeepSeek ReasoningParser with thinking enabled by default (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33221\">#33221</a>), remove noisy CT warning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33273\">#33273</a>), early tokenization validation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31366\">#31366</a>), reasoning_content backward compatibility (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33635\">#33635</a>), only include Authorization header when OPENAI_API_KEY is set (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33488\">#33488</a>).</li>\n<li>Features: run_batch transcription/translation support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33934\">#33934</a>), /server_info collect_env (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33246\">#33246</a>), OTEL tracing during model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31162\">#31162</a>), clear MM and encoder cache (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33452\">#33452</a>), HF Hub LoRA resolver (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/20320\">#20320</a>).</li>\n<li>Scoring: Fix multi-document scoring returning single result (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33837\">#33837</a>).</li>\n</ul>\n<h3>Security</h3>\n<ul>\n<li>Patch protobuf for <a href=\"https://github.com/advisories/GHSA-7gcm-g887-7qv7\" title=\"CVE-2026-0994\">CVE-2026-0994</a> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34253\">#34253</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li><strong>PyTorch 2.10</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>) - breaking change for environment dependency.</li>\n<li>huggingface-hub updates for Transformers v5 preparation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33473\">#33473</a>).</li>\n<li>Transformers v5 compatibility fixes across multiple models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33977\">#33977</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33683\">#33683</a>).</li>\n</ul>\n<h3>Deprecation &amp; Breaking Changes</h3>\n<ul>\n<li>Removed BitBlas quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n<li>Removed deprecated <code>reasoning_content</code> message field (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33402\">#33402</a>).</li>\n<li>Removed deprecated pooling items (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33477\">#33477</a>).</li>\n<li>Removed deprecated <code>VLLM_ALL2ALL_BACKEND</code> environment variable (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33535\">#33535</a>).</li>\n<li>Deprecated IPEX for XPU, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/aabbccddwasd\">@aabbccddwasd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Code4me2\">@Code4me2</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikchifo\">@ikchifo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jiangwu300\">@jiangwu300</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/pjs102793\">@pjs102793</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33963\">#33963</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sleepcoo\">@sleepcoo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33978\">#33978</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TundeAtSN\">@TundeAtSN</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a></li>\n</ul>",
    "published": "2026-02-13T06:13:20Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "2cfc1c73d3671883",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.16.0rc3: [Bugfix] Fix MTP accuracy for GLM-5 (#34385)",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc3",
    "summary": "<p>Signed-off-by: mgoin <a href=\"mailto:mgoin64@gmail.com\">mgoin64@gmail.com</a><br />\n(cherry picked from commit <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/commit/ec12d39d44739bee408ec1473acc09e75daf1a5d\"><tt>ec12d39</tt></a>)</p>",
    "published": "2026-02-12T04:54:27Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9ae1e737b9ab2d0e",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.16.0rc2: Patch protobuf for CVE-2026-0994 (#34253)",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc2",
    "summary": "<p>Signed-off-by: Seiji Eicher <a href=\"mailto:seiji@anyscale.com\">seiji@anyscale.com</a><br />\nCo-authored-by: Kevin H. Luu <a href=\"mailto:khluu000@gmail.com\">khluu000@gmail.com</a><br />\n(cherry picked from commit <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/commit/5045d5c9831a3a4a423a409ccea521d299a43a9a\"><tt>5045d5c</tt></a>)</p>",
    "published": "2026-02-11T10:33:40Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "bf4ad96790c2c64d",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.16.0rc1",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc1",
    "summary": "<p>[Frontend][last/5] Make pooling entrypoints request schema consensus.…</p>",
    "published": "2026-02-09T06:42:38Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ab7b6c6a13f1ac1c",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.15.2rc0: [Bugfix] Disable TRTLLM attention when KV transfer is enabled (#33192)",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.2rc0",
    "summary": "<p>Signed-off-by: Zhanqiu Hu <a href=\"mailto:zh338@cornell.edu\">zh338@cornell.edu</a></p>",
    "published": "2026-02-05T00:49:18Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "82c84921d9c73a9e",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.15.1",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1",
    "summary": "<p>v0.15.1 is a patch release with security fixes, RTX Blackwell GPU fixes support, and bug fixes.</p>\n<h2>Security</h2>\n<ul>\n<li><strong><a href=\"https://github.com/advisories/GHSA-6mq8-rvhq-8wgg\" title=\"CVE-2025-69223\">CVE-2025-69223</a></strong>: Updated aiohttp dependency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33621\">#33621</a>)</li>\n<li><strong><a href=\"https://github.com/advisories/GHSA-7gcm-g887-7qv7\" title=\"CVE-2026-0994\">CVE-2026-0994</a></strong>: Updated Protobuf dependency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33619\">#33619</a>)</li>\n</ul>\n<h2>Highlights</h2>\n<h3>Bugfix Hardware Support</h3>\n<ul>\n<li><strong>RTX Blackwell (SM120)</strong>: Fixed NVFP4 MoE kernel support for RTX Blackwell workstation GPUs. Previously, NVFP4 MoE models would fail to load on these GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33417\">#33417</a>)</li>\n<li><strong>FP8 kernel selection</strong>: Fixed FP8 CUTLASS group GEMM to properly fall back to Triton kernels on SM120 GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33285\">#33285</a>)</li>\n</ul>\n<h3>Model Support</h3>\n<ul>\n<li><strong>Step-3.5-Flash</strong>: New model support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33523\">#33523</a>)</li>\n</ul>\n<h3>Bugfix Model Support</h3>\n<ul>\n<li><strong>Qwen3-VL-Reranker</strong>: Fixed model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33298\">#33298</a>)</li>\n<li><strong>Whisper</strong>: Fixed FlashAttention2 with full CUDA graphs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33360\">#33360</a>)</li>\n</ul>\n<h3>Performance</h3>\n<ul>\n<li><strong>torch.compile cold-start</strong>: Fixed regression that increased cold-start compilation time (Llama3-70B: ~88s → ~22s) (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33441\">#33441</a>)</li>\n<li><strong>MoE forward pass</strong>: Optimized by caching layer name computation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33184\">#33184</a>)</li>\n</ul>\n<h3>Bug Fixes</h3>\n<ul>\n<li>Fixed prefix cache hit rate of 0% with GPT-OSS style hybrid attention models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33524\">#33524</a>)</li>\n<li>Enabled Triton MoE backend for FP8 per-tensor dynamic quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33300\">#33300</a>)</li>\n<li>Disabled unsupported Renormalize routing methods for TRTLLM per-tensor FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33620\">#33620</a>)</li>\n<li>Fixed speculative decoding metrics crash when no tokens generated (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33729\">#33729</a>)</li>\n<li>Disabled fast MoE cold start optimization with speculative decoding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33624\">#33624</a>)</li>\n<li>Fixed ROCm skinny GEMM dispatch logic (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33366\">#33366</a>)</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li>Pinned LMCache &gt;= v0.3.9 for API compatibility (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33440\">#33440</a>)</li>\n</ul>\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/zaristei2\">@zaristei2</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33621\">#33621</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/compare/v0.15.0...v0.15.1\"><tt>v0.15.0...v0.15.1</tt></a></p>",
    "published": "2026-02-05T01:01:39Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d9c13dbb510e62f5",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.15.1rc1",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1rc1",
    "summary": "<p>[BugFix][Spec Decoding] Fix negative accepted tokens metric crash (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/3\">#3</a>…</p>",
    "published": "2026-02-04T01:28:32Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "131daa0d67d1bfbe",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.15.1rc0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1rc0",
    "summary": "<p>[torch.compile] Don't do the fast moe cold start optimization if ther…</p>",
    "published": "2026-02-03T08:07:18Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "68b7491da1db14bf",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.16.0rc0: [Docs] Adding links and intro to Speculators and LLM Compressor (#32849)",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc0",
    "summary": "<p>Signed-off-by: Aidan Reilly <a href=\"mailto:aireilly@redhat.com\">aireilly@redhat.com</a><br />\nSigned-off-by: Harry Mellor <a href=\"mailto:19981378+hmellor@users.noreply.github.com\">19981378+hmellor@users.noreply.github.com</a><br />\nCo-authored-by: Harry Mellor <a href=\"mailto:19981378+hmellor@users.noreply.github.com\">19981378+hmellor@users.noreply.github.com</a></p>",
    "published": "2026-01-29T22:12:35Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "877d9d2f64c35601",
    "source": "vllm_releases",
    "source_weight": 1.2,
    "title": "v0.15.0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.0",
    "summary": "<h2>Highlights</h2>\n<p>This release features 335 commits from 158 contributors (39 new)!</p>\n<h3>Model Support</h3>\n<ul>\n<li><strong>New architectures</strong>: Kimi-K2.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33131\">#33131</a>), Molmo2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30997\">#30997</a>), Step3vl 10B (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32329\">#32329</a>), Step1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32511\">#32511</a>), GLM-Lite (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31386\">#31386</a>), Eagle2.5-8B VLM (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32456\">#32456</a>).</li>\n<li><strong>LoRA expansion</strong>: Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30802\">#30802</a>), InternVL2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32397\">#32397</a>), MiniMax M2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32763\">#32763</a>).</li>\n<li><strong>Speculative decoding</strong>: EAGLE3 for Pixtral/LlavaForConditionalGeneration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32542\">#32542</a>), Qwen3 VL MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32048\">#32048</a>), draft model support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/24322\">#24322</a>).</li>\n<li><strong>Embeddings</strong>: BGE-M3 sparse embeddings and ColBERT embeddings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/14526\">#14526</a>).</li>\n<li><strong>Model enhancements</strong>: Voxtral streaming architecture (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32861\">#32861</a>), SharedFusedMoE for Qwen3MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32082\">#32082</a>), dynamic resolution for Nemotron Nano VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32121\">#32121</a>), Molmo2 vision backbone quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32385\">#32385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li><strong>Async scheduling + Pipeline Parallelism</strong>: <code>--async-scheduling</code> now works with pipeline parallelism (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32359\">#32359</a>).</li>\n<li><strong>Mamba prefix caching</strong>: Block-aligned prefix caching for Mamba/hybrid models with <code>--enable-prefix-caching --mamba-cache-mode align</code>. Achieves ~2x speedup by caching Mamba states directly (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30877\">#30877</a>).</li>\n<li><strong>Session-based streaming input</strong>: New incremental input support for interactive workloads like ASR. Accepts async generators producing <code>StreamingInput</code> objects while maintaining KV cache alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28973\">#28973</a>).</li>\n<li><strong>Model Runner V2</strong>: VLM support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32546\">#32546</a>), architecture improvements.</li>\n<li><strong>LoRA</strong>: Inplace loading for memory efficiency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31326\">#31326</a>).</li>\n<li><strong>AOT compilation</strong>: torch.compile inductor artifacts support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/25205\">#25205</a>).</li>\n<li><strong>Performance</strong>: KV cache offloading redundant load prevention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29087\">#29087</a>), FlashAttn attention/cache update separation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/25954\">#25954</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<h4>NVIDIA</h4>\n<ul>\n<li><strong>Blackwell defaults</strong>: FlashInfer MLA is now the default MLA backend on Blackwell, with TRTLLM as default prefill (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32615\">#32615</a>).</li>\n<li><strong>MoE performance</strong>: 1.2-2% E2E throughput improvement via grouped topk kernel fusion (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32058\">#32058</a>), NVFP4 small-batch decoding improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30885\">#30885</a>), faster cold start for MoEs with torch.compile (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32805\">#32805</a>).</li>\n<li><strong>FP4 kernel optimization</strong>: Up to 65% faster FP4 quantization on Blackwell (SM100F) using 256-bit loads, ~4% E2E throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32520\">#32520</a>).</li>\n<li><strong>Kernel improvements</strong>: topk_sigmoid kernel for MoE routing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31246\">#31246</a>), atomics reduce counting for SplitK skinny GEMMs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29843\">#29843</a>), fused cat+quant for FP8 KV cache in MLA (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32950\">#32950</a>).</li>\n<li><strong>torch.compile</strong>: SiluAndMul and QuantFP8 CustomOp compilation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32806\">#32806</a>), Triton prefill attention performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32403\">#32403</a>).</li>\n</ul>\n<h4>AMD ROCm</h4>\n<ul>\n<li><strong>MoRI EP</strong>: High-performance all2all backend for Expert Parallel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28664\">#28664</a>).</li>\n<li><strong>Attention improvements</strong>: Shuffle KV cache layout and assembly paged attention kernel for AiterFlashAttentionBackend (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29887\">#29887</a>).</li>\n<li><strong>FP4 support</strong>: MLA projection GEMMs with dynamic quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32238\">#32238</a>).</li>\n<li><strong>Consumer GPU support</strong>: Flash Attention Triton backend on RDNA3/RDNA4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32944\">#32944</a>).</li>\n</ul>\n<h4>Other Platforms</h4>\n<ul>\n<li><strong>TPU</strong>: Pipeline parallelism support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28506\">#28506</a>), backend option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32438\">#32438</a>).</li>\n<li><strong>Intel XPU</strong>: AgRsAll2AllManager for distributed communication (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32654\">#32654</a>).</li>\n<li><strong>CPU</strong>: NUMA-aware acceleration for TP/DP inference on ARM (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32792\">#32792</a>), PyTorch 2.10 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32869\">#32869</a>).</li>\n<li><strong>Whisper</strong>: torch.compile support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30385\">#30385</a>).</li>\n<li><strong>WSL</strong>: Platform compatibility fix for Windows Subsystem for Linux (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32749\">#32749</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li><strong>MXFP4</strong>: W4A16 support for compressed-tensors MoE models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32285\">#32285</a>).</li>\n<li><strong>Non-gated MoE</strong>: Quantization support with Marlin, NVFP4 CUTLASS, FP8, INT8, and compressed-tensors (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32257\">#32257</a>).</li>\n<li><strong>Intel</strong>: Quantization Toolkit integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31716\">#31716</a>).</li>\n<li><strong>FP8 KV cache</strong>: Per-tensor and per-attention-head quantization via llmcompressor (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30141\">#30141</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Responses API</strong>: Partial message generation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32100\">#32100</a>), <code>include_stop_str_in_output</code> tuning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32383\">#32383</a>), <code>prompt_cache_key</code> support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32824\">#32824</a>).</li>\n<li><strong>OpenAI API</strong>: <code>skip_special_tokens</code> configuration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32345\">#32345</a>).</li>\n<li><strong>Score endpoint</strong>: Flexible input formats with <code>data_1</code>/<code>data_2</code> and <code>queries</code>/<code>documents</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32577\">#32577</a>).</li>\n<li><strong>Render endpoints</strong>: New endpoints for prompt preprocessing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32473\">#32473</a>).</li>\n<li><strong>Whisper API</strong>: <code>avg_logprob</code> and <code>compression_ratio</code> in verbose_json segments (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31059\">#31059</a>).</li>\n<li><strong>Security</strong>: FIPS 140-3 compliant hash option for enterprise/government users (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32386\">#32386</a>), <code>--ssl-ciphers</code> CLI argument (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30937\">#30937</a>).</li>\n<li><strong>UX improvements</strong>: Auto <code>api_server_count</code> based on <code>dp_size</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32525\">#32525</a>), wheel variant auto-detection during install (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32948\">#32948</a>), custom profiler URI schemes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32393\">#32393</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li>FlashInfer v0.6.1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30993\">#30993</a>)</li>\n<li>Transformers 4.57.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32287\">#32287</a>)</li>\n<li>PyTorch 2.10 for CPU backend (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32869\">#32869</a>)</li>\n<li>DeepGEMM newer version (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32479\">#32479</a>)</li>\n</ul>\n<h3>Breaking Changes &amp; Deprecations</h3>\n<ul>\n<li><strong>Metrics</strong>: Removed deprecated <code>vllm:time_per_output_token_seconds</code> metric - use <code>vllm:inter_token_latency_seconds</code> instead (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32661\">#32661</a>).</li>\n<li><strong>Environment variables</strong>: Removed deprecated environment variables (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32812\">#32812</a>).</li>\n<li><strong>Quantization</strong>: DeepSpeedFp8 removed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32679\">#32679</a>), RTN removed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32697\">#32697</a>), HQQ deprecated (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32681\">#32681</a>).</li>\n</ul>\n<h3>Bug Fixes</h3>\n<ul>\n<li><strong>Speculative decoding</strong>: Eagle draft_model_config fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31753\">#31753</a>).</li>\n<li><strong>DeepSeek</strong>: DeepSeek-V3.1 + DeepGEMM incompatible scale shapes fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32361\">#32361</a>).</li>\n<li><strong>Distributed</strong>: DP+MoE inference fix via CpuCommunicator (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31867\">#31867</a>), P/D with non-MoE DP fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33037\">#33037</a>).</li>\n<li><strong>EPLB</strong>: Possible deadlock fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32418\">#32418</a>).</li>\n<li><strong>NIXL</strong>: UCX memory leak fix by exporting UCX_MEM_MMAP_HOOK_MODE=none (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32181\">#32181</a>).</li>\n<li><strong>Structured output</strong>: Outlines byte fallback handling fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31391\">#31391</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/YunzhuLu\">@YunzhuLu</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32126\">#32126</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/emricksini-h\">@emricksini-h</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30784\">#30784</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/dsfaccini\">@dsfaccini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32289\">#32289</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ofirzaf\">@ofirzaf</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32312\">#32312</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/seekskyworld\">@seekskyworld</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32321\">#32321</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/brian033\">@brian033</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31715\">#31715</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TomerBN-Nvidia\">@TomerBN-Nvidia</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32257\">#32257</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/vanshilshah97\">@vanshilshah97</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32448\">#32448</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/George-Polya\">@George-Polya</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32385\">#32385</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/T1mn\">@T1mn</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32411\">#32411</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/mritunjaysharma394\">@mritunjaysharma394</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31492\">#31492</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/randzero\">@randzero</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32511\">#32511</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/DemingCheng\">@DemingCheng</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32556\">#32556</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/iboiko-habana\">@iboiko-habana</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32471\">#32471</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/honglyua-il\">@honglyua-il</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32462\">#32462</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/hyeongyun0916\">@hyeongyun0916</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32473\">#32473</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/DanielMe\">@DanielMe</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32560\">#32560</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/netanel-haber\">@netanel-haber</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32121\">#32121</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/longregen\">@longregen</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28784\">#28784</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jasonyanwenl\">@jasonyanwenl</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32749\">#32749</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Wauplin\">@Wauplin</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32788\">#32788</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikaadil\">@ikaadil</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32775\">#32775</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/alexsun07\">@alexsun07</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28664\">#28664</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/liranschour\">@liranschour</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30207\">#30207</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/AuYang261\">@AuYang261</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32844\">#32844</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/diviramon\">@diviramon</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32393\">#32393</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/RishabhSaini\">@RishabhSaini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32884\">#32884</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/MatteoFari\">@MatteoFari</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32397\">#32397</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/peakcrosser7\">@peakcrosser7</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30877\">#30877</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/orionr\">@orionr</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30443\">#30443</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/marksverdhei\">@marksverdhei</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32614\">#32614</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/joninco\">@joninco</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32935\">#32935</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/monajafi-amd\">@monajafi-amd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32944\">#32944</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ruizcrp\">@ruizcrp</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32988\">#32988</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sjhddh\">@sjhddh</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32983\">#32983</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/HirokenOvo\">@HirokenOvo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32646\">#32646</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Chenhao-Guan\">@Chenhao-Guan</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32763\">#32763</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/joshuadeng\">@joshuadeng</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28973\">#28973</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ZhanqiuHu\">@ZhanqiuHu</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33016\">#33016</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/compare/v0.14.1...v0.15.0\"><tt>v0.14.1...v0.15.0</tt></a></p>",
    "published": "2026-01-29T10:21:01Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b67a55266dc20649",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.65.0 corresponding to NGC container 26.01",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.65.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<div class=\"markdown-alert markdown-alert-note\"><p class=\"markdown-alert-title\"><svg class=\"octicon octicon-info mr-2\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg>Note</p><p>Windows support is deprecated, latest assets build for windows could be found in is release <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.51.0\">2.51.0 / 25.01</a></p>\n</div>\n  <details>\n    <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Exposed HTTP errors for non-JSON format response.</p>\n</li>\n<li>\n<p>Perf Analyzer dependencies were made optional when installing tritonclient via pip.</p>\n</li>\n<li>\n<p>Added support for <code>nv_inference_first_response_histogram_ms</code> metric when running models in coupled (aka non-decoupled) mode.</p>\n</li>\n<li>\n<p>Fixed an issue where a crashed Python backend stub process was not correctly detected leading to failed inference requests.</p>\n</li>\n<li>\n<p>Fixed an issue where a malicious HTTP request could exhaust all available system memory leading to a process crash or denial of service.</p>\n</li>\n<li>\n<p>Fixed an issue where loading a TensorRT engine larger than 30GB could cause out of memory errors despite sufficient memory to load the engine being available.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Known Issues</h2>\n<ul>\n<li>\n<p>If you are using the vllm backend to run models that accept video inputs, then do not upgrade to Triton 26.01 and wait for Triton 26.02</p>\n</li>\n<li>\n<p>Triton python package uses outdated dependency <code>starlette</code> package version.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.65.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r26.01#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n  </details>\n  <details>\n    <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.65.0/tritonserver2.65.0-igpu.tar\"><code>tritonserver2.65.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> [<code>2.10.0a0+a36e1d3](https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html), **Python** </code>3.12` and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.65.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n  </details>\n  <details>\n    <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.65.0/tritonserver2.65.0-agx.tar\"><code>tritonserver2.65.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.1</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+a36e1d3</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:26.01-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n  </details>\n<details>\n    <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.65.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.1.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n</details>\n<details>\n  <h2>ManyLinux Assets (early access)</h2>\n<p>This release was compiled with AlmaLinux 8.9 based out of <code>manylinux_2_28</code> and can be used on RHEL8  and later versions.<br />\nSee the included README.md for complete details about installation, verification, and support.<br />\nThis release supports CUDA 13, TensorRT 10.14.1.48, Onnx Runtime 1.23.2, PyTorch 2.10.0a0+b4e4ee8, Python 3.12 and supports ensembles.<br />\nSome optional backend features such as the PyTorch backend's TorchTRT extension are not currently supported.</p>\n</details>",
    "published": "2026-02-11T22:21:05Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "1ef2c05d1093cdc8",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.64.0 corresponding to NGC container 25.12",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.64.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n  <details>\n    <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Fixed an issue with Triton Server’s Sagemaker Service which could result in a server crash resulting from a race condition, caused by unprotected access to the list of models.</p>\n</li>\n<li>\n<p>Extended the set of accelerated PyTorch libraries included with the Triton PyTorch backend.</p>\n</li>\n<li>\n<p>Upgraded Triton Client’s Golang dependencies to latest stable versions to resolve known issues with the previous version of the dependencies.</p>\n</li>\n<li>\n<p>The OpenAI-compatible frontend has transitioned from beta to a stable release.</p>\n</li>\n<li>\n<p>Added <code>echo</code> request parameter for TensorRT-LLM and Python backends to OpenAI-compatible API frontend <code>v1/completions</code> endpoint.</p>\n</li>\n<li>\n<p>Enabled OpenAI-compatible API frontend multi-LoRA support for TensorRT-LLM backend.</p>\n</li>\n<li>\n<p>Backends can now implement the new  <code>TRITONBACKEND_ModelInstanceReady</code> function to report accurate model readiness status.</p>\n</li>\n<li>\n<p>Updated the Python backend to accurately report model readiness.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Known Issues</h2>\n<ul>\n<li>\n<p>The error <code>'__init__(): incompatible function arguments</code> may occur when using TensorRT-LLM backend python models. To suppress the error temporarily, set input tensor <code>stream</code> with a boolean value explicitly in the request header.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.64.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.12#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n  </details>\n  <details>\n    <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.64.0/tritonserver2.64.0-igpu.tar\"><code>tritonserver2.64.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b4e4ee8</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.64.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n  </details>\n  <details>\n    <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.64.0/tritonserver2.64.0-agx.tar\"><code>tritonserver2.64.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.1</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b4e4ee8</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.12-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.64.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.1.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n  </details>\n  <details>\n    <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.12, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n  </details>\n<details>\n  <h2>ManyLinux Assets (early access)</h2>\n<p>This release was compiled with AlmaLinux 8.9 based out of <code>manylinux_2_28</code> and can be used on RHEL8  and later versions.<br />\nSee the included README.md for complete details about installation, verification, and support.<br />\nThis release supports CUDA 13, TensorRT 10.14.1.48, Onnx Runtime 1.23.2, PyTorch 2.10.0a0+b4e4ee8, Python 3.12 and supports ensembles.<br />\nSome optional backend features such as the PyTorch backend's TorchTRT extension are not currently supported.</p>\n</details>",
    "published": "2026-02-11T22:17:34Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "f26808a6663f56c4",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.63.0 corresponding to NGC container 25.11",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.63.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Enabled endpoint <code>v1/embeddings</code> for vLLM backend in OpenAI-compatible API server.</p>\n</li>\n<li>\n<p>Enabled <code>echo</code> parameter for TensorRT-LLM and Python backends in OpenAI-compatible API server.</p>\n</li>\n<li>\n<p>Improved error handling in OpenAI-compatible API server by providing more specific and OpenAI-compliant error codes.</p>\n</li>\n<li>\n<p>Upgraded the version of starlette used by OpenAI frontend.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.63.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.10#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.63.0/tritonserver2.63.0-igpu.tar\"><code>tritonserver2.63.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b558c98</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.63.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.63.0/tritonserver2.63.0-agx.tar\"><code>tritonserver2.63.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b558c98</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.11-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.63.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>",
    "published": "2026-01-30T17:03:42Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9c10e7e1095fc2e0",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.62.0 corresponding to NGC container 25.10",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.62.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Fixed a server crash issue caused by specially crafted request messages sent to <code>/v2/models/&lt;model_name&gt;/infer</code>.</p>\n</li>\n<li>\n<p>Fixed a server crash issue caused by incorrect handling of malformed HTTP requests.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Triton python package uses outdated dependency <code>starlette</code> package version.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>Enabling vLLM metrics during inferences causes the engine to crash.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka ‘model.pt’). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.62.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.10#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.62.0/tritonserver2.62.0-igpu.tar\"><code>tritonserver2.62.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.1</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+145a3a7</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.62.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.62.0/tritonserver2.62.0-agx.tar\"><code>tritonserver2.62.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.3.9</code>, <strong>Onnx Runtime</strong> <code>1.23.1</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.10-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.06 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.62.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.06. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2026-01-30T16:56:51Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "7cd095f5f55dba53",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.61.0 corresponding to NGC container 25.09",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.61.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Static key authentication for OpenAI Frontend APIs</p>\n</li>\n<li>\n<p>Prevented models outside Triton’s repository being loaded from OpenAI Frontend.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.61.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.09#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.61.0/tritonserver2.61.0-igpu.tar\"><code>tritonserver2.61.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.09/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.61.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.61.0/tritonserver2.61.0-agx.tar\"><code>tritonserver2.61.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.3.9</code>, <strong>Onnx Runtime</strong> <code>1.23.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.09-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.06 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.61.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.06. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-10-07T22:10:06Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0b6403585fa5c6b3",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.60.0 corresponding to NGC container 25.08",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.60.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Added CUDA 13 support.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Triton ONNX Runtime Backend build uses <a href=\"https://github.com/microsoft/onnxruntime/commit/1d1712fdafb9e61b2d6d033c4433c1033395d7e7\">microsoft/onnxruntime/commit/1d1712fdaf</a> and may have some limitations on DGX Spark hardware which will be addressed in future versions.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy</p>\n</li>\n<li>\n<p>CuPy does not support CUDA 13 at the time of writing. Issues may be encountered when using CuPy before it officially supports CUDA 13, see <a href=\"https://github.com/triton-inference-server/server/tree/r25.08/python/openai#pre-requisites\">https://github.com/triton-inference-server/server/tree/r25.08/python/openai#pre-requisites</a> for more details</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.60.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.08#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.60.0/tritonserver2.60.0-igpu.tar\"><code>tritonserver2.60.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.0+1d1712fdaf</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a+34c6371d24</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.07/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.60.0/tritonserver2.60.0-agx.tar\"><code>tritonserver2.60.0-agx.tar</code></a>.</p>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Jetson AGX release for 25.08, requires DCGM version 4 to be installed in order to use GPU metrics.<br />\nPlease use following command to install DCGM 4:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>curl -o /tmp/cuda-keyring.deb \\\n         https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb \\\n     &amp;&amp; apt install /tmp/cuda-keyring.deb \\\n     &amp;&amp; rm /tmp/cuda-keyring.deb \\\n     &amp;&amp; apt update \\\n     &amp;&amp; apt install --yes --no-install-recommends \\\n                  datacenter-gpu-manager-4-core=1:4.4.0-1\n</code></pre></div>\n</blockquote>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.2.6</code>, <strong>Onnx Runtime</strong> <code>1.23.0+1d1712fdaf</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+34c6371</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.08-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.60.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.21.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-08-26T22:15:33Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "671420a2a130cda5",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.59.1 corresponding to NGC container 25.07",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.59.1",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Fixed vulnerabilities in the Triton Inference Server.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>There was no python wheels packages released as part of 25.07 release</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.59.1_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.07, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.59.1/tritonserver2.59.1-igpu.tar\"><code>tritonserver2.59.1-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.6</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.07/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.04-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.04-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.59.1/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.20.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.10.0.31</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-07-29T21:50:01Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d62c6c9a740a083f",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.59.0 corresponding to NGC container 25.06",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.59.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Improved ensemble model performance in scenarios that allow out-of-order responses by increasing maximum throughput and reducing latency.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.59.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.06, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.59.0/tritonserver2.59.0-igpu.tar\"><code>tritonserver2.59.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.6</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.06/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.04-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.04-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.59.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.20.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.10.0.31</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-06-30T22:54:06Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "ebc920b10350277a",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.58.0 corresponding to NGC container 25.05",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.58.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Optional “execution_context_allocation_strategy” parameter in the TensorRT backend configuration allows selection of memory allocation behavior.</li>\n<li>Support Tool calling functionality with Llama 3 and Mistral models in OpenAI frontend.</li>\n<li>Improvements around memory allocation and various bug fixes.</li>\n<li>GenAI-Perf now offers a new configuration file alongside the command line.</li>\n<li>GenAI-Perf now collects GPU metrics from /metrics endpoint exposed by DCGM Exporter.</li>\n<li>GenAI-Perf supports new Power, Utilization, Ecc, Errors and PCie metrics.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM backend for 25.05 might be unstable with the vLLM V1 architecture. We recommend switching to V0 for this release, by setting <code>VLLM_USE_V1</code> environment variable to 0. However, users should be aware that vLLM's V0 API is affected by vulnerabilities.</p>\n</li>\n<li>\n<p>vLLM containers include vllm version 0.8.4 which is affected by vulnerabilities.<br />\nWorkarounds:<br />\nPrior to the fix, your options include:</p>\n<ul>\n<li>Do not expose the vLLM host to a network where any untrusted connections may reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that port used is random.</li>\n</ul>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.58.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.05, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.58.0/tritonserver2.58.0-igpu.tar\"><code>tritonserver2.58.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.10.0.31</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.5</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.05/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.58.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.03 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.03-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.03-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.58.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.03. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.19.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.9.0.34</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-06-30T22:54:03Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "d171b80d5d104921",
    "source": "triton_releases",
    "source_weight": 1.1,
    "title": "Release 2.57.0 corresponding to NGC container 25.04",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.57.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Exposed gRPC infer thread count as a server option.</li>\n<li>Improved server stability during the gRPC client cancellation.</li>\n<li>Improved server stability in tracing mode.</li>\n<li>Added BLS decoupled request cancellation in the Python Backend</li>\n<li>GenAI-Perf now offers a new configuration file alongside the command line.</li>\n<li>GenAI-Perf now supports the Huggingface TGI generated endpoint.</li>\n<li>GenAI-Perf added a Token per second per user (TPS/user) metric.</li>\n<li>GenAI-Perf metric parsing speed was increased by 60%.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM backend for 25.04 might be unstable with the vLLM V1 architecture. We recommend switching to V0 for this release, by setting <code>VLLM_USE_V1</code> environment variable to 0. However, users should be aware that vLLM's V0 API is affected by vulnerabilities.</p>\n</li>\n<li>\n<p>vLLM containers include vllm version 0.8.1 which is affected by new vulnerabilities.<br />\nWorkarounds:<br />\nPrior to the fix, your options include:</p>\n<ul>\n<li>Do not expose the vLLM host to a network where any untrusted connections may reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that port used is random.</li>\n</ul>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.57.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.04#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.04, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.57.0/tritonserver2.57.0-igpu.tar\"><code>tritonserver2.57.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.9.0.34</code>, <strong>Onnx Runtime</strong> <code>1.21.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.7.0a0+79aa17489c.nv25.4</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.04/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.57.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.03 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.03-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.03-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.57.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.03. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.18.2</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.9.0.34</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-06-30T22:54:00Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "8bed79565182bbc6",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.14",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.14",
    "summary": "<h1>Release Notes</h1>\n<h2>[2026-02-10]</h2>\n<h3>llama-index-callbacks-wandb [0.4.2]</h3>\n<ul>\n<li>Fix potential crashes and improve security defaults in core components (<a href=\"https://github.com/run-llama/llama_index/pull/20610\">#20610</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.14]</h3>\n<ul>\n<li>fix: catch pydantic ValidationError in VectorStoreQueryOutputParser (<a href=\"https://github.com/run-llama/llama_index/pull/20450\">#20450</a>)</li>\n<li>fix: distinguish empty string from None in MediaResource.hash (<a href=\"https://github.com/run-llama/llama_index/pull/20451\">#20451</a>)</li>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n<li>fix(core): fallback to bundled nltk cache if env var missing (<a href=\"https://github.com/run-llama/llama_index/pull/20528\">#20528</a>)</li>\n<li>feat(callbacks): add TokenBudgetHandler for cost governance (<a href=\"https://github.com/run-llama/llama_index/pull/20546\">#20546</a>)</li>\n<li>fix(core):handled a edge case in truncate_text function (<a href=\"https://github.com/run-llama/llama_index/pull/20551\">#20551</a>)</li>\n<li>fix(core):fix in types Thread passing None when target is None instead of copy_context().run (<a href=\"https://github.com/run-llama/llama_index/pull/20553\">#20553</a>)</li>\n<li>chore: bump llama-index lockfile, and minor test tweaks (<a href=\"https://github.com/run-llama/llama_index/pull/20556\">#20556</a>)</li>\n<li>Compatibility for workflows context changes (<a href=\"https://github.com/run-llama/llama_index/pull/20557\">#20557</a>)</li>\n<li>test(core): fix cache dir path test for Windows compatibility (<a href=\"https://github.com/run-llama/llama_index/pull/20566\">#20566</a>)</li>\n<li>fix(tests): enforce utf-8 encoding in json reader tests for windows compatibility (<a href=\"https://github.com/run-llama/llama_index/pull/20576\">#20576</a>)</li>\n<li>Fix BM25Retriever mapping in upgrade tool / 修复升级工具中的 BM25Retriever 映射 (<a href=\"https://github.com/run-llama/llama_index/pull/20582\">#20582</a>)</li>\n<li>fix(agent): handle empty LLM responses with retry logic and add test cases (<a href=\"https://github.com/run-llama/llama_index/pull/20596\">#20596</a>)</li>\n<li>fix: add show_progress parameter to run_transformations to prevent unexpected keyword argument error (<a href=\"https://github.com/run-llama/llama_index/pull/20608\">#20608</a>)</li>\n<li>Fix potential crashes and improve security defaults in core components (<a href=\"https://github.com/run-llama/llama_index/pull/20610\">#20610</a>)</li>\n<li>Add core 3.14 tests (<a href=\"https://github.com/run-llama/llama_index/pull/20619\">#20619</a>)</li>\n</ul>\n<h3>llama-index-embeddings-cohere [0.7.0]</h3>\n<ul>\n<li>fix(embeddings-cohere): add retry logic with tenacity (<a href=\"https://github.com/run-llama/llama_index/pull/20592\">#20592</a>)</li>\n</ul>\n<h3>llama-index-embeddings-google-genai [0.3.2]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n</ul>\n<h3>llama-index-embeddings-siliconflow [0.3.2]</h3>\n<ul>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n</ul>\n<h3>llama-index-embeddings-upstage [0.5.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-graph-stores-falkordb [0.4.2]</h3>\n<ul>\n<li>fix(falkordb): Fix MENTIONS relationship creation with triplet_source_id (<a href=\"https://github.com/run-llama/llama_index/pull/20650\">#20650</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.8]</h3>\n<ul>\n<li>chore: Update cacheable Anthropic models (<a href=\"https://github.com/run-llama/llama_index/pull/20581\">#20581</a>)</li>\n<li>chore: add support for opus 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20635\">#20635</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.8]</h3>\n<ul>\n<li>fix bedrock converse empty tool config issue (<a href=\"https://github.com/run-llama/llama_index/pull/20571\">#20571</a>)</li>\n<li>fix(llms-bedrock-converse): improve bedrock converse retry handling (<a href=\"https://github.com/run-llama/llama_index/pull/20590\">#20590</a>)</li>\n<li>feat(bedrock-converse): Add support for Claude Opus 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20637\">#20637</a>)</li>\n<li>Add support for adaptive thinking in Bedrock (<a href=\"https://github.com/run-llama/llama_index/pull/20659\">#20659</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-llms-cohere [0.7.1]</h3>\n<ul>\n<li>Feat: add custom base_url support to Cohere LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20534\">#20534</a>)</li>\n<li>fix(llms-cohere): handle additional error types in retry logic (<a href=\"https://github.com/run-llama/llama_index/pull/20591\">#20591</a>)</li>\n</ul>\n<h3>llama-index-llms-dashscope [0.5.2]</h3>\n<ul>\n<li>fix(dashscope): remove empty tool_calls from assistant messages (<a href=\"https://github.com/run-llama/llama_index/pull/20535\">#20535</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.8.7]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n<li>fix(decorator):adds logic to llm_retry_decorator for async methods. (<a href=\"https://github.com/run-llama/llama_index/pull/20588\">#20588</a>)</li>\n<li>Fix/google genai cleanup (<a href=\"https://github.com/run-llama/llama_index/pull/20607\">#20607</a>)</li>\n<li>fix(google-genai): skip model meta fetch when not needed (<a href=\"https://github.com/run-llama/llama_index/pull/20639\">#20639</a>)</li>\n</ul>\n<h3>llama-index-llms-huggingface-api [0.6.2]</h3>\n<ul>\n<li>Update sensible default provider for huggingface inference api (<a href=\"https://github.com/run-llama/llama_index/pull/20589\">#20589</a>)</li>\n</ul>\n<h3>llama-index-llms-langchain [0.7.1]</h3>\n<ul>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.18]</h3>\n<ul>\n<li>OpenAI response fix (<a href=\"https://github.com/run-llama/llama_index/pull/20538\">#20538</a>)</li>\n<li>feat: Add support for gpt-5.2-chat model (<a href=\"https://github.com/run-llama/llama_index/pull/20549\">#20549</a>)</li>\n<li>fix(openai): make image_url detail optional in message dict (<a href=\"https://github.com/run-llama/llama_index/pull/20609\">#20609</a>)</li>\n<li>Add new reasoning types (<a href=\"https://github.com/run-llama/llama_index/pull/20612\">#20612</a>)</li>\n<li>fix(openai): exclude unsupported params for all reasoning models (<a href=\"https://github.com/run-llama/llama_index/pull/20627\">#20627</a>)</li>\n</ul>\n<h3>llama-index-llms-openai-like [0.6.0]</h3>\n<ul>\n<li>make transformers an optional dependency for openai-like (<a href=\"https://github.com/run-llama/llama_index/pull/20580\">#20580</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.4]</h3>\n<ul>\n<li>make transformers an optional dependency for openai-like (<a href=\"https://github.com/run-llama/llama_index/pull/20580\">#20580</a>)</li>\n</ul>\n<h3>llama-index-llms-siliconflow [0.4.3]</h3>\n<ul>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.7.0]</h3>\n<ul>\n<li>add new upstage model(solar-pro3) (<a href=\"https://github.com/run-llama/llama_index/pull/20544\">#20544</a>)</li>\n</ul>\n<h3>llama-index-llms-vllm [0.6.2]</h3>\n<ul>\n<li>feat: add openai-like server mode for VllmServer (<a href=\"https://github.com/run-llama/llama_index/pull/20537\">#20537</a>)</li>\n</ul>\n<h3>llama-index-memory-bedrock-agentcore [0.1.2]</h3>\n<ul>\n<li>Add event and memory record deletion methods in bedrock-agentcorememory (<a href=\"https://github.com/run-llama/llama_index/pull/20428\">#20428</a>)</li>\n<li>chore(deps): update llama-index-core dependency lock to include 0.14.x (<a href=\"https://github.com/run-llama/llama_index/pull/20483\">#20483</a>)</li>\n</ul>\n<h3>llama-index-memory-mem0 [1.0.0]</h3>\n<ul>\n<li>fix: mem0 integration cleanup + refactor (<a href=\"https://github.com/run-llama/llama_index/pull/20532\">#20532</a>)</li>\n</ul>\n<h3>llama-index-node-parser-chonkie [0.1.1]</h3>\n<ul>\n<li>feat: add chonkie integration (<a href=\"https://github.com/run-llama/llama_index/pull/20622\">#20622</a>)</li>\n<li>update readme (<a href=\"https://github.com/run-llama/llama_index/pull/20656\">#20656</a>)</li>\n</ul>\n<h3>llama-index-node-parser-docling [0.4.2]</h3>\n<ul>\n<li>fix: catch pydantic ValidationError in VectorStoreQueryOutputParser (<a href=\"https://github.com/run-llama/llama_index/pull/20450\">#20450</a>)</li>\n</ul>\n<h3>llama-index-packs-code-hierarchy [0.6.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-gmail-openai-agent [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-multidoc-autoretrieval [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-panel-chatbot [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-recursive-retriever [0.7.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-packs-resume-screener [0.9.3]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-retry-engine-weaviate [0.5.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-streamlit-chatbot [0.5.2]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-sub-question-weaviate [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-timescale-vector-autoretrieval [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-cohere-rerank [0.6.0]</h3>\n<ul>\n<li>fix(cohere-rerank): add retry logic and tenacity dependency to cohere rerank (<a href=\"https://github.com/run-llama/llama_index/pull/20593\">#20593</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-nvidia-rerank [0.5.4]</h3>\n<ul>\n<li>fix(nvidia-rerank): fix initialization logic for on-prem auth (<a href=\"https://github.com/run-llama/llama_index/pull/20560\">#20560</a>)</li>\n<li>fix(nvidia-rerank): correct private attribute reference (<a href=\"https://github.com/run-llama/llama_index/pull/20570\">#20570</a>)</li>\n<li>fix(nvidia-rerank): Fix POST request url for locally hosted NIM rerankers (<a href=\"https://github.com/run-llama/llama_index/pull/20579\">#20579</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-tei-rerank [0.4.2]</h3>\n<ul>\n<li>fix(tei-rerank): use index field from API response for correct score … (<a href=\"https://github.com/run-llama/llama_index/pull/20599\">#20599</a>)</li>\n<li>test(tei-rerank): add test coverage for rerank retry coverage (<a href=\"https://github.com/run-llama/llama_index/pull/20600\">#20600</a>)</li>\n</ul>\n<h3>llama-index-protocols-ag-ui [0.2.4]</h3>\n<ul>\n<li>fix: avoid ValueError in ag-ui message conversion for multi-block ChatMessages (<a href=\"https://github.com/run-llama/llama_index/pull/20648\">#20648</a>)</li>\n</ul>\n<h3>llama-index-readers-datasets [0.1.0]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-readers-microsoft-sharepoint [0.7.0]</h3>\n<ul>\n<li>Sharepoint page support events (<a href=\"https://github.com/run-llama/llama_index/pull/20572\">#20572</a>)</li>\n</ul>\n<h3>llama-index-readers-obsidian [0.6.1]</h3>\n<ul>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.6]</h3>\n<ul>\n<li>feat: implement partial_params support to McpToolSpec (<a href=\"https://github.com/run-llama/llama_index/pull/20554\">#20554</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp-discovery [0.1.0]</h3>\n<ul>\n<li>Add llama-index-tools-mcp-discovery integration (<a href=\"https://github.com/run-llama/llama_index/pull/20502\">#20502</a>)</li>\n</ul>\n<h3>llama-index-tools-moss [0.1.0]</h3>\n<ul>\n<li>feat(tools): add Moss search engine integration (<a href=\"https://github.com/run-llama/llama_index/pull/20615\">#20615</a>)</li>\n</ul>\n<h3>llama-index-tools-seltz [0.1.0]</h3>\n<ul>\n<li>feat(tools): add Seltz web knowledge tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20626\">#20626</a>)</li>\n</ul>\n<h3>llama-index-tools-typecast [0.1.0]</h3>\n<ul>\n<li>Migrate Typecast tool to V2 API for voices endpoints (<a href=\"https://github.com/run-llama/llama_index/pull/20548\">#20548</a>)</li>\n</ul>\n<h3>llama-index-tools-wolfram-alpha [0.5.0]</h3>\n<ul>\n<li>feat(wolfram-alpha): switch to LLM API with bearer auth (<a href=\"https://github.com/run-llama/llama_index/pull/20586\">#20586</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-clickhouse [0.6.2]</h3>\n<ul>\n<li>fix(clickhouse): Add drop_existing_table parameter to prevent data loss (<a href=\"https://github.com/run-llama/llama_index/pull/20651\">#20651</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.6]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.1]</h3>\n<ul>\n<li>Update MongoDB vector store tests to use newer model (<a href=\"https://github.com/run-llama/llama_index/pull/20515\">#20515</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-oceanbase [0.4.0]</h3>\n<ul>\n<li>feat(oceanbase): add sparse/fulltext/hybrid search (<a href=\"https://github.com/run-llama/llama_index/pull/20524\">#20524</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [1.0.0]</h3>\n<ul>\n<li>Changed OpenSearch engine default from deprecated <code>nmslib</code> to <code>faiss</code> (<a href=\"https://github.com/run-llama/llama_index/pull/20507\">#20507</a>)</li>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.3]</h3>\n<ul>\n<li>fix(postgres): disable bitmap scan for vector queries (<a href=\"https://github.com/run-llama/llama_index/pull/20514\">#20514</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-yugabytedb [0.5.4]</h3>\n<ul>\n<li>Add YugabyteDB as a Vector Store (<a href=\"https://github.com/run-llama/llama_index/pull/20559\">#20559</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-voice-agents-gemini-live [0.2.2]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n</ul>",
    "published": "2026-02-10T23:08:46Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "4472a8eadbb3b8d2",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.13",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.13",
    "summary": "<h1>Release Notes</h1>\n<h2>[2026-01-21]</h2>\n<h3>llama-index-core [0.14.13]</h3>\n<ul>\n<li>feat: add early_stopping_method parameter to agent workflows (<a href=\"https://github.com/run-llama/llama_index/pull/20389\">#20389</a>)</li>\n<li>feat: Add token-based code splitting support to CodeSplitter (<a href=\"https://github.com/run-llama/llama_index/pull/20438\">#20438</a>)</li>\n<li>Add RayIngestionPipeline integration for distributed data ingestion (<a href=\"https://github.com/run-llama/llama_index/pull/20443\">#20443</a>)</li>\n<li>Added the multi-modal version of the Condensed Conversation &amp; Context… (<a href=\"https://github.com/run-llama/llama_index/pull/20446\">#20446</a>)</li>\n<li>Replace ChatMemoryBuffer with Memory (<a href=\"https://github.com/run-llama/llama_index/pull/20458\">#20458</a>)</li>\n<li>fix(bug):Raise value error on when input is empty list in mean_agg instead of returning float (<a href=\"https://github.com/run-llama/llama_index/pull/20466\">#20466</a>)</li>\n<li>fix: The classmethod of ReActChatFormatter should use cls instead of the class name (<a href=\"https://github.com/run-llama/llama_index/pull/20475\">#20475</a>)</li>\n<li>feat: add configurable empty response message to synthesizers (<a href=\"https://github.com/run-llama/llama_index/pull/20503\">#20503</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.3]</h3>\n<ul>\n<li>Enable use of ARNs for Bedrock Embedding Models (<a href=\"https://github.com/run-llama/llama_index/pull/20435\">#20435</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.6]</h3>\n<ul>\n<li>Improved Ollama batch embedding (<a href=\"https://github.com/run-llama/llama_index/pull/20447\">#20447</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.3]</h3>\n<ul>\n<li>Adding voyage-4 models (<a href=\"https://github.com/run-llama/llama_index/pull/20497\">#20497</a>)</li>\n</ul>\n<h3>llama-index-ingestion-ray [0.1.0]</h3>\n<ul>\n<li>Add RayIngestionPipeline integration for distributed data ingestion (<a href=\"https://github.com/run-llama/llama_index/pull/20443\">#20443</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.6]</h3>\n<ul>\n<li>feat: enhance structured predict methods for anthropic (<a href=\"https://github.com/run-llama/llama_index/pull/20440\">#20440</a>)</li>\n<li>fix: preserve input_tokens in Anthropic stream_chat responses (<a href=\"https://github.com/run-llama/llama_index/pull/20512\">#20512</a>)</li>\n</ul>\n<h3>llama-index-llms-apertis [0.1.0]</h3>\n<ul>\n<li>Add Apertis LLM integration with example notebook (<a href=\"https://github.com/run-llama/llama_index/pull/20436\">#20436</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.4]</h3>\n<ul>\n<li>chore(bedrock-converse): Remove extraneous thinking_delta kwarg from ChatMessage (<a href=\"https://github.com/run-llama/llama_index/pull/20455\">#20455</a>)</li>\n</ul>\n<h3>llama-index-llms-gemini [0.6.2]</h3>\n<ul>\n<li>chore: deprecate llama-index-llms-gemini (<a href=\"https://github.com/run-llama/llama_index/pull/20511\">#20511</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.13]</h3>\n<ul>\n<li>Sanitize OpenAI structured output JSON schema name for generic Pydantic models (<a href=\"https://github.com/run-llama/llama_index/pull/20452\">#20452</a>)</li>\n<li>chore: vbump openai (<a href=\"https://github.com/run-llama/llama_index/pull/20482\">#20482</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.3]</h3>\n<ul>\n<li>Feature/openrouter provider routing support (<a href=\"https://github.com/run-llama/llama_index/pull/20431\">#20431</a>)</li>\n</ul>\n<h3>llama-index-packs-recursive-retriever [0.7.1]</h3>\n<ul>\n<li>security: remove exposed OpenAI API keys from notebook outputs (<a href=\"https://github.com/run-llama/llama_index/pull/20474\">#20474</a>)</li>\n</ul>\n<h3>llama-index-packs-sentence-window-retriever [0.5.1]</h3>\n<ul>\n<li>security: remove exposed OpenAI API keys from notebook outputs (<a href=\"https://github.com/run-llama/llama_index/pull/20474\">#20474</a>)</li>\n</ul>\n<h3>llama-index-readers-datasets [0.1.0]</h3>\n<ul>\n<li>Add HuggingFace datasets reader integration (<a href=\"https://github.com/run-llama/llama_index/pull/20468\">#20468</a>)</li>\n</ul>\n<h3>llama-index-readers-patentsview [1.0.0]</h3>\n<ul>\n<li>Patentsview reader api changes (<a href=\"https://github.com/run-llama/llama_index/pull/20481\">#20481</a>)</li>\n</ul>\n<h3>llama-index-retrievers-you [1.0.0]</h3>\n<ul>\n<li>Revamp YouRetriever integration (<a href=\"https://github.com/run-llama/llama_index/pull/20493\">#20493</a>)</li>\n</ul>\n<h3>llama-index-tools-parallel-web-systems [0.1.0]</h3>\n<ul>\n<li>feat: added Parallel Web System tools (<a href=\"https://github.com/run-llama/llama_index/pull/20442\">#20442</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-alibabacloud-mysql [0.1.0]</h3>\n<ul>\n<li>Feature/alibaba mysql vector integration (<a href=\"https://github.com/run-llama/llama_index/pull/20396\">#20396</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.6]</h3>\n<ul>\n<li>Feat milvus partition names (<a href=\"https://github.com/run-llama/llama_index/pull/20445\">#20445</a>)</li>\n<li>improve(llama-index-vector-stores-milvus): Changed the partition parameter to <code>milvus_partition_name</code> in add/delete. (<a href=\"https://github.com/run-llama/llama_index/pull/20460\">#20460</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.1]</h3>\n<ul>\n<li>INTPYTHON-863 Fix mongodb async integration (<a href=\"https://github.com/run-llama/llama_index/pull/20444\">#20444</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-neo4jvector [0.5.2]</h3>\n<ul>\n<li>Handle missing metadata for neo4j vector store (<a href=\"https://github.com/run-llama/llama_index/pull/20491\">#20491</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [0.6.3]</h3>\n<ul>\n<li>fix (opensearch): add close and aclose methods to vector client (<a href=\"https://github.com/run-llama/llama_index/pull/20463\">#20463</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.9.1]</h3>\n<ul>\n<li>Qdrant search params (<a href=\"https://github.com/run-llama/llama_index/pull/20476\">#20476</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.4]</h3>\n<ul>\n<li>feat(vertexaivectorsearch): add hybrid search support (<a href=\"https://github.com/run-llama/llama_index/pull/20487\">#20487</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-volcenginemysql [0.2.0]</h3>\n<ul>\n<li>feat: Volcengine MySQL vector store integration (<a href=\"https://github.com/run-llama/llama_index/pull/20404\">#20404</a>)</li>\n</ul>",
    "published": "2026-01-21T20:44:52Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "1b059a88e8ce2e31",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.12",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.12",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-30]</h2>\n<h3>llama-index-callbacks-agentops [0.4.1]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.12]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n<li>Improve <code>MockFunctionCallingLLM</code> (<a href=\"https://github.com/run-llama/llama_index/pull/20356\">#20356</a>)</li>\n<li>fix(openai): sanitize generic Pydantic model schema names (<a href=\"https://github.com/run-llama/llama_index/pull/20371\">#20371</a>)</li>\n<li>Element node parser (<a href=\"https://github.com/run-llama/llama_index/pull/20399\">#20399</a>)</li>\n<li>improve llama dev logging (<a href=\"https://github.com/run-llama/llama_index/pull/20411\">#20411</a>)</li>\n<li>test(node_parser): add unit tests for Java CodeSplitter (<a href=\"https://github.com/run-llama/llama_index/pull/20423\">#20423</a>)</li>\n<li>fix: crash in log_vector_store_query_result when result.ids is None (<a href=\"https://github.com/run-llama/llama_index/pull/20427\">#20427</a>)</li>\n</ul>\n<h3>llama-index-embeddings-litellm [0.4.1]</h3>\n<ul>\n<li>Add docstring to LiteLLM embedding class (<a href=\"https://github.com/run-llama/llama_index/pull/20336\">#20336</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.5]</h3>\n<ul>\n<li>feat(llama-index-embeddings-ollama): Add keep_alive parameter (<a href=\"https://github.com/run-llama/llama_index/pull/20395\">#20395</a>)</li>\n<li>docs: improve Ollama embeddings README with comprehensive documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20414\">#20414</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.2]</h3>\n<ul>\n<li>Voyage multimodal 35 (<a href=\"https://github.com/run-llama/llama_index/pull/20398\">#20398</a>)</li>\n</ul>\n<h3>llama-index-graph-stores-nebula [0.5.1]</h3>\n<ul>\n<li>feat(nebula): add MENTIONS edge to property graph store (<a href=\"https://github.com/run-llama/llama_index/pull/20401\">#20401</a>)</li>\n</ul>\n<h3>llama-index-llms-aibadgr [0.1.0]</h3>\n<ul>\n<li>feat(llama-index-llms-aibadgr): Add AI Badgr OpenAI‑compatible LLM integration (<a href=\"https://github.com/run-llama/llama_index/pull/20365\">#20365</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.4]</h3>\n<ul>\n<li>add back haiku-3 support (<a href=\"https://github.com/run-llama/llama_index/pull/20408\">#20408</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.3]</h3>\n<ul>\n<li>fix: bedrock converse thinking block issue (<a href=\"https://github.com/run-llama/llama_index/pull/20355\">#20355</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.8.3]</h3>\n<ul>\n<li>Switch use_file_api to Flexible file_mode; Improve File Upload Handling &amp; Bump google-genai to v1.52.0 (<a href=\"https://github.com/run-llama/llama_index/pull/20347\">#20347</a>)</li>\n<li>Fix missing role from Google-GenAI (<a href=\"https://github.com/run-llama/llama_index/pull/20357\">#20357</a>)</li>\n<li>Add signature index fix (<a href=\"https://github.com/run-llama/llama_index/pull/20362\">#20362</a>)</li>\n<li>Add positional thought signature for thoughts (<a href=\"https://github.com/run-llama/llama_index/pull/20418\">#20418</a>)</li>\n</ul>\n<h3>llama-index-llms-ollama [0.9.1]</h3>\n<ul>\n<li>feature: pydantic no longer complains if you pass 'low', 'medium', 'h… (<a href=\"https://github.com/run-llama/llama_index/pull/20394\">#20394</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.12]</h3>\n<ul>\n<li>fix: Handle tools=None in OpenAIResponses._get_model_kwargs (<a href=\"https://github.com/run-llama/llama_index/pull/20358\">#20358</a>)</li>\n<li>feat: add support for gpt-5.2 and 5.2 pro (<a href=\"https://github.com/run-llama/llama_index/pull/20361\">#20361</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.6.1]</h3>\n<ul>\n<li>fix(confluence): support Python 3.14 (<a href=\"https://github.com/run-llama/llama_index/pull/20370\">#20370</a>)</li>\n</ul>\n<h3>llama-index-readers-file [0.5.6]</h3>\n<ul>\n<li>Loosen constraint on <code>pandas</code> version (<a href=\"https://github.com/run-llama/llama_index/pull/20387\">#20387</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>chore(deps): bump urllib3 from 2.5.0 to 2.6.0 in /llama-index-integrations/readers/llama-index-readers-service-now in the pip group across 1 directory (<a href=\"https://github.com/run-llama/llama_index/pull/20341\">#20341</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.5]</h3>\n<ul>\n<li>fix: pass timeout parameters to transport clients in BasicMCPClient (<a href=\"https://github.com/run-llama/llama_index/pull/20340\">#20340</a>)</li>\n<li>feature: Permit to pass a custom httpx.AsyncClient when creating a BasicMcpClient (<a href=\"https://github.com/run-llama/llama_index/pull/20368\">#20368</a>)</li>\n</ul>\n<h3>llama-index-tools-typecast [0.1.0]</h3>\n<ul>\n<li>feat: add Typecast tool integration with text to speech features (<a href=\"https://github.com/run-llama/llama_index/pull/20343\">#20343</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azurepostgresql [0.2.0]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-chroma [0.5.5]</h3>\n<ul>\n<li>Fix chroma nested metadata filters (<a href=\"https://github.com/run-llama/llama_index/pull/20424\">#20424</a>)</li>\n<li>fix(chroma): support multimodal results (<a href=\"https://github.com/run-llama/llama_index/pull/20426\">#20426</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-couchbase [0.6.0]</h3>\n<ul>\n<li>Update FTS &amp; GSI reference docs for Couchbase vector-store (<a href=\"https://github.com/run-llama/llama_index/pull/20346\">#20346</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-faiss [0.5.2]</h3>\n<ul>\n<li>fix(faiss): pass numpy array instead of int to add_with_ids (<a href=\"https://github.com/run-llama/llama_index/pull/20384\">#20384</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-lancedb [0.4.4]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n<li>fix(vector_stores/lancedb): add missing '&lt;' filter operator (<a href=\"https://github.com/run-llama/llama_index/pull/20364\">#20364</a>)</li>\n<li>fix(lancedb): fix metadata filtering logic and list value SQL generation (<a href=\"https://github.com/run-llama/llama_index/pull/20374\">#20374</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.0]</h3>\n<ul>\n<li>Update mongo vector store to initialize without list permissions (<a href=\"https://github.com/run-llama/llama_index/pull/20354\">#20354</a>)</li>\n<li>add mongodb delete index (<a href=\"https://github.com/run-llama/llama_index/pull/20429\">#20429</a>)</li>\n<li>async mongodb atlas support (<a href=\"https://github.com/run-llama/llama_index/pull/20430\">#20430</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-redis [0.6.2]</h3>\n<ul>\n<li>Redis metadata filter fix (<a href=\"https://github.com/run-llama/llama_index/pull/20359\">#20359</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.3]</h3>\n<ul>\n<li>feat(vertex-vector-search): Add Google Vertex AI Vector Search v2.0 support (<a href=\"https://github.com/run-llama/llama_index/pull/20351\">#20351</a>)</li>\n</ul>",
    "published": "2025-12-30T01:07:03Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "12a10411c82007d3",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.10",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.10",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-04]</h2>\n<h3>llama-index-core [0.14.10]</h3>\n<ul>\n<li>feat: add mock function calling llm (<a href=\"https://github.com/run-llama/llama_index/pull/20331\">#20331</a>)</li>\n</ul>\n<h3>llama-index-llms-qianfan [0.4.1]</h3>\n<ul>\n<li>test: fix typo 'reponse' to 'response' in variable names (<a href=\"https://github.com/run-llama/llama_index/pull/20329\">#20329</a>)</li>\n</ul>\n<h3>llama-index-tools-airweave [0.1.0]</h3>\n<ul>\n<li>feat: add Airweave tool integration with advanced search features (<a href=\"https://github.com/run-llama/llama_index/pull/20111\">#20111</a>)</li>\n</ul>\n<h3>llama-index-utils-qianfan [0.4.1]</h3>\n<ul>\n<li>test: fix typo 'reponse' to 'response' in variable names (<a href=\"https://github.com/run-llama/llama_index/pull/20329\">#20329</a>)</li>\n</ul>",
    "published": "2025-12-04T19:46:03Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "76e7d59ba15bdef3",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.9",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.9",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-02]</h2>\n<h3>llama-index-agent-azure [0.2.1]</h3>\n<ul>\n<li>fix: Pin azure-ai-projects version to prevent breaking changes (<a href=\"https://github.com/run-llama/llama_index/pull/20255\">#20255</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.9]</h3>\n<ul>\n<li>MultiModalVectorStoreIndex now returns a multi-modal ContextChatEngine. (<a href=\"https://github.com/run-llama/llama_index/pull/20265\">#20265</a>)</li>\n<li>Ingestion to vector store now ensures that _node-content is readable (<a href=\"https://github.com/run-llama/llama_index/pull/20266\">#20266</a>)</li>\n<li>fix: ensure context is copied with async utils run_async (<a href=\"https://github.com/run-llama/llama_index/pull/20286\">#20286</a>)</li>\n<li>fix(memory): ensure first message in queue is always a user message after flush (<a href=\"https://github.com/run-llama/llama_index/pull/20310\">#20310</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.2]</h3>\n<ul>\n<li>feat(embeddings-bedrock): Add support for Amazon Bedrock Application Inference Profiles (<a href=\"https://github.com/run-llama/llama_index/pull/20267\">#20267</a>)</li>\n<li>fix:(embeddings-bedrock) correct extraction of provider from model_name (<a href=\"https://github.com/run-llama/llama_index/pull/20295\">#20295</a>)</li>\n<li>Bump version of bedrock-embedding (<a href=\"https://github.com/run-llama/llama_index/pull/20304\">#20304</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.1]</h3>\n<ul>\n<li>VoyageAI correction and documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20251\">#20251</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.3]</h3>\n<ul>\n<li>feat: add anthropic opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20306\">#20306</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.2]</h3>\n<ul>\n<li>fix(bedrock-converse): Only use guardrail_stream_processing_mode in streaming functions (<a href=\"https://github.com/run-llama/llama_index/pull/20289\">#20289</a>)</li>\n<li>feat: add anthropic opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20306\">#20306</a>)</li>\n<li>feat(bedrock-converse): Additional support for Claude Opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20317\">#20317</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.7.4]</h3>\n<ul>\n<li>Fix gemini-3 support and gemini function call support (<a href=\"https://github.com/run-llama/llama_index/pull/20315\">#20315</a>)</li>\n</ul>\n<h3>llama-index-llms-helicone [0.1.1]</h3>\n<ul>\n<li>update helicone docs + examples (<a href=\"https://github.com/run-llama/llama_index/pull/20208\">#20208</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.10]</h3>\n<ul>\n<li>Smallest Nit (<a href=\"https://github.com/run-llama/llama_index/pull/20252\">#20252</a>)</li>\n<li>Feat: Add gpt-5.1-chat model support (<a href=\"https://github.com/run-llama/llama_index/pull/20311\">#20311</a>)</li>\n</ul>\n<h3>llama-index-llms-ovhcloud [0.1.0]</h3>\n<ul>\n<li>Add OVHcloud AI Endpoints provider (<a href=\"https://github.com/run-llama/llama_index/pull/20288\">#20288</a>)</li>\n</ul>\n<h3>llama-index-llms-siliconflow [0.4.2]</h3>\n<ul>\n<li>[Bugfix] None check on content in delta in siliconflow LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20327\">#20327</a>)</li>\n</ul>\n<h3>llama-index-node-parser-docling [0.4.2]</h3>\n<ul>\n<li>Relax docling Python constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20322\">#20322</a>)</li>\n</ul>\n<h3>llama-index-packs-resume-screener [0.9.3]</h3>\n<ul>\n<li>feat: Update pypdf to latest version (<a href=\"https://github.com/run-llama/llama_index/pull/20285\">#20285</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-voyageai-rerank [0.4.1]</h3>\n<ul>\n<li>VoyageAI correction and documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20251\">#20251</a>)</li>\n</ul>\n<h3>llama-index-protocols-ag-ui [0.2.3]</h3>\n<ul>\n<li>fix: correct order of ag-ui events to avoid event conflicts (<a href=\"https://github.com/run-llama/llama_index/pull/20296\">#20296</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.6.0]</h3>\n<ul>\n<li>Refactor Confluence integration: Update license to MIT, remove requirements.txt, and implement HtmlTextParser for HTML to Markdown conversion. Update dependencies and tests accordingly. (<a href=\"https://github.com/run-llama/llama_index/pull/20262\">#20262</a>)</li>\n</ul>\n<h3>llama-index-readers-docling [0.4.2]</h3>\n<ul>\n<li>Relax docling Python constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20322\">#20322</a>)</li>\n</ul>\n<h3>llama-index-readers-file [0.5.5]</h3>\n<ul>\n<li>feat: Update pypdf to latest version (<a href=\"https://github.com/run-llama/llama_index/pull/20285\">#20285</a>)</li>\n</ul>\n<h3>llama-index-readers-reddit [0.4.1]</h3>\n<ul>\n<li>Fix typo in README.md for Reddit integration (<a href=\"https://github.com/run-llama/llama_index/pull/20283\">#20283</a>)</li>\n</ul>\n<h3>llama-index-storage-chat-store-postgres [0.3.2]</h3>\n<ul>\n<li>[FIX] Postgres ChatStore automatically prefix table name with \"data_\" (<a href=\"https://github.com/run-llama/llama_index/pull/20241\">#20241</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azureaisearch [0.4.4]</h3>\n<ul>\n<li><code>vector-azureaisearch</code>: check if user agent already in policy before add it to azure client (<a href=\"https://github.com/run-llama/llama_index/pull/20243\">#20243</a>)</li>\n<li>fix(azureaisearch): Add close/aclose methods to fix unclosed client session warnings (<a href=\"https://github.com/run-llama/llama_index/pull/20309\">#20309</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.4]</h3>\n<ul>\n<li>Fix/consistency level param for milvus (<a href=\"https://github.com/run-llama/llama_index/pull/20268\">#20268</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.2]</h3>\n<ul>\n<li>Fix postgresql dispose (<a href=\"https://github.com/run-llama/llama_index/pull/20312\">#20312</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.9.0]</h3>\n<ul>\n<li>fix: Update qdrant-client version constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20280\">#20280</a>)</li>\n<li>Feat: update Qdrant client to 1.16.0 (<a href=\"https://github.com/run-llama/llama_index/pull/20287\">#20287</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.2]</h3>\n<ul>\n<li>fix: update blob path in batch_update_index (<a href=\"https://github.com/run-llama/llama_index/pull/20281\">#20281</a>)</li>\n</ul>\n<h3>llama-index-voice-agents-openai [0.2.2]</h3>\n<ul>\n<li>Smallest Nit (<a href=\"https://github.com/run-llama/llama_index/pull/20252\">#20252</a>)</li>\n</ul>",
    "published": "2025-12-02T21:31:18Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "1b2cfee52de7ca3d",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.8",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.8",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-11-10]</h2>\n<h3>llama-index-core [0.14.8]</h3>\n<ul>\n<li>Fix ReActOutputParser getting stuck when \"Answer:\" contains \"Action:\" (<a href=\"https://github.com/run-llama/llama_index/pull/20098\">#20098</a>)</li>\n<li>Add buffer to image, audio, video and document blocks (<a href=\"https://github.com/run-llama/llama_index/pull/20153\">#20153</a>)</li>\n<li>fix(agent): Handle multi-block ChatMessage in ReActAgent (<a href=\"https://github.com/run-llama/llama_index/pull/20196\">#20196</a>)</li>\n<li>Fix/20209 (<a href=\"https://github.com/run-llama/llama_index/pull/20214\">#20214</a>)</li>\n<li>Preserve Exception in ToolOutput (<a href=\"https://github.com/run-llama/llama_index/pull/20231\">#20231</a>)</li>\n<li>fix weird pydantic warning (<a href=\"https://github.com/run-llama/llama_index/pull/20235\">#20235</a>)</li>\n</ul>\n<h3>llama-index-embeddings-nvidia [0.4.2]</h3>\n<ul>\n<li>docs: Edit pass and update example model (<a href=\"https://github.com/run-llama/llama_index/pull/20198\">#20198</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.4]</h3>\n<ul>\n<li>Added a test case (no code) to check the embedding through an actual connection to a Ollama server (after checking that the ollama server exists) (<a href=\"https://github.com/run-llama/llama_index/pull/20230\">#20230</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.2]</h3>\n<ul>\n<li>feat(llms/anthropic): Add support for RawMessageDeltaEvent in streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20206\">#20206</a>)</li>\n<li>chore: remove unsupported models (<a href=\"https://github.com/run-llama/llama_index/pull/20211\">#20211</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.11.1]</h3>\n<ul>\n<li>feat: integrate bedrock converse with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20099\">#20099</a>)</li>\n<li>feat: Update model name extraction to include 'jp' region prefix and … (<a href=\"https://github.com/run-llama/llama_index/pull/20233\">#20233</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.7.3]</h3>\n<ul>\n<li>feat: google genai integration with tool block (<a href=\"https://github.com/run-llama/llama_index/pull/20096\">#20096</a>)</li>\n<li>fix: non-streaming gemini tool calling (<a href=\"https://github.com/run-llama/llama_index/pull/20207\">#20207</a>)</li>\n<li>Add token usage information in GoogleGenAI chat additional_kwargs (<a href=\"https://github.com/run-llama/llama_index/pull/20219\">#20219</a>)</li>\n<li>bug fix google genai stream_complete (<a href=\"https://github.com/run-llama/llama_index/pull/20220\">#20220</a>)</li>\n</ul>\n<h3>llama-index-llms-nvidia [0.4.4]</h3>\n<ul>\n<li>docs: Edit pass and code example updates (<a href=\"https://github.com/run-llama/llama_index/pull/20200\">#20200</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.8]</h3>\n<ul>\n<li>FixV2: Correct DocumentBlock type for OpenAI from 'input_file' to 'file' (<a href=\"https://github.com/run-llama/llama_index/pull/20203\">#20203</a>)</li>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.6.5]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-packs-streamlit-chatbot [0.5.2]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-packs-voyage-query-engine [0.5.2]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-nvidia-rerank [0.5.1]</h3>\n<ul>\n<li>docs: Edit pass (<a href=\"https://github.com/run-llama/llama_index/pull/20199\">#20199</a>)</li>\n</ul>\n<h3>llama-index-readers-web [0.5.6]</h3>\n<ul>\n<li>feat: Add ScrapyWebReader Integration (<a href=\"https://github.com/run-llama/llama_index/pull/20212\">#20212</a>)</li>\n<li>Update Scrapy dependency to 2.13.3 (<a href=\"https://github.com/run-llama/llama_index/pull/20228\">#20228</a>)</li>\n</ul>\n<h3>llama-index-readers-whisper [0.3.0]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-storage-kvstore-postgres [0.4.3]</h3>\n<ul>\n<li>fix: Ensure schema creation only occurs if it doesn't already exist (<a href=\"https://github.com/run-llama/llama_index/pull/20225\">#20225</a>)</li>\n</ul>\n<h3>llama-index-tools-brightdata [0.2.1]</h3>\n<ul>\n<li>docs: add api key claim instructions (<a href=\"https://github.com/run-llama/llama_index/pull/20204\">#20204</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.3]</h3>\n<ul>\n<li>Added test case for issue 19211. No code change (<a href=\"https://github.com/run-llama/llama_index/pull/20201\">#20201</a>)</li>\n</ul>\n<h3>llama-index-utils-oracleai [0.3.1]</h3>\n<ul>\n<li>Update llama-index-core dependency to 0.12.45 (<a href=\"https://github.com/run-llama/llama_index/pull/20227\">#20227</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-lancedb [0.4.2]</h3>\n<ul>\n<li>fix: FTS index recreation bug on every LanceDB query (<a href=\"https://github.com/run-llama/llama_index/pull/20213\">#20213</a>)</li>\n</ul>",
    "published": "2025-11-10T22:18:42Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "673d75a2997ef7ce",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.7",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.7",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-30]</h2>\n<h3>llama-index-core [0.14.7]</h3>\n<ul>\n<li>Feat/serpex tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20141\">#20141</a>)</li>\n<li>Fix outdated error message about setting LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20157\">#20157</a>)</li>\n<li>Fixing some recently failing tests (<a href=\"https://github.com/run-llama/llama_index/pull/20165\">#20165</a>)</li>\n<li>Fix: update lock to latest workflow and fix issues (<a href=\"https://github.com/run-llama/llama_index/pull/20173\">#20173</a>)</li>\n<li>fix: ensure full docstring is used in FunctionTool (<a href=\"https://github.com/run-llama/llama_index/pull/20175\">#20175</a>)</li>\n<li>fix api docs build (<a href=\"https://github.com/run-llama/llama_index/pull/20180\">#20180</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.0]</h3>\n<ul>\n<li>Updating the VoyageAI integration (<a href=\"https://github.com/run-llama/llama_index/pull/20073\">#20073</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.0]</h3>\n<ul>\n<li>feat: integrate anthropic with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20100\">#20100</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.10.7]</h3>\n<ul>\n<li>feat: Add support for Bedrock Guardrails streamProcessingMode (<a href=\"https://github.com/run-llama/llama_index/pull/20150\">#20150</a>)</li>\n<li>bedrock structured output optional force (<a href=\"https://github.com/run-llama/llama_index/pull/20158\">#20158</a>)</li>\n</ul>\n<h3>llama-index-llms-fireworks [0.4.5]</h3>\n<ul>\n<li>Update FireworksAI models (<a href=\"https://github.com/run-llama/llama_index/pull/20169\">#20169</a>)</li>\n</ul>\n<h3>llama-index-llms-mistralai [0.9.0]</h3>\n<ul>\n<li>feat: mistralai integration with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20103\">#20103</a>)</li>\n</ul>\n<h3>llama-index-llms-ollama [0.9.0]</h3>\n<ul>\n<li>feat: integrate ollama with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20097\">#20097</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.6]</h3>\n<ul>\n<li>Allow setting temp of gpt-5-chat (<a href=\"https://github.com/run-llama/llama_index/pull/20156\">#20156</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.5.0]</h3>\n<ul>\n<li>feat(confluence): make SVG processing optional to fix pycairo install… (<a href=\"https://github.com/run-llama/llama_index/pull/20115\">#20115</a>)</li>\n</ul>\n<h3>llama-index-readers-github [0.9.0]</h3>\n<ul>\n<li>Add GitHub App authentication support (<a href=\"https://github.com/run-llama/llama_index/pull/20106\">#20106</a>)</li>\n</ul>\n<h3>llama-index-retrievers-bedrock [0.5.1]</h3>\n<ul>\n<li>Fixing some recently failing tests (<a href=\"https://github.com/run-llama/llama_index/pull/20165\">#20165</a>)</li>\n</ul>\n<h3>llama-index-tools-serpex [0.1.0]</h3>\n<ul>\n<li>Feat/serpex tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20141\">#20141</a>)</li>\n<li>add missing toml info (<a href=\"https://github.com/run-llama/llama_index/pull/20186\">#20186</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-couchbase [0.6.0]</h3>\n<ul>\n<li>Add Hyperscale and Composite Vector Indexes support for Couchbase vector-store (<a href=\"https://github.com/run-llama/llama_index/pull/20170\">#20170</a>)</li>\n</ul>",
    "published": "2025-10-30T23:58:43Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a4240f028d473960",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.6",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.6",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-26]</h2>\n<h3>llama-index-core [0.14.6]</h3>\n<ul>\n<li>Add allow_parallel_tool_calls for non-streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20117\">#20117</a>)</li>\n<li>Fix invalid use of field-specific metadata (<a href=\"https://github.com/run-llama/llama_index/pull/20122\">#20122</a>)</li>\n<li>update doc for SemanticSplitterNodeParser (<a href=\"https://github.com/run-llama/llama_index/pull/20125\">#20125</a>)</li>\n<li>fix rare cases when sentence splits are larger than chunk size (<a href=\"https://github.com/run-llama/llama_index/pull/20147\">#20147</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.0]</h3>\n<ul>\n<li>Fix BedrockEmbedding to support Cohere v4 response format (<a href=\"https://github.com/run-llama/llama_index/pull/20094\">#20094</a>)</li>\n</ul>\n<h3>llama-index-embeddings-isaacus [0.1.0]</h3>\n<ul>\n<li>feat: Isaacus embeddings integration (<a href=\"https://github.com/run-llama/llama_index/pull/20124\">#20124</a>)</li>\n</ul>\n<h3>llama-index-embeddings-oci-genai [0.4.2]</h3>\n<ul>\n<li>Update OCI GenAI cohere models (<a href=\"https://github.com/run-llama/llama_index/pull/20146\">#20146</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.9.7]</h3>\n<ul>\n<li>Fix double token stream in anthropic llm (<a href=\"https://github.com/run-llama/llama_index/pull/20108\">#20108</a>)</li>\n<li>Ensure anthropic content delta only has user facing response (<a href=\"https://github.com/run-llama/llama_index/pull/20113\">#20113</a>)</li>\n</ul>\n<h3>llama-index-llms-baseten [0.1.7]</h3>\n<ul>\n<li>add GLM (<a href=\"https://github.com/run-llama/llama_index/pull/20121\">#20121</a>)</li>\n</ul>\n<h3>llama-index-llms-helicone [0.1.0]</h3>\n<ul>\n<li>integrate helicone to llama-index (<a href=\"https://github.com/run-llama/llama_index/pull/20131\">#20131</a>)</li>\n</ul>\n<h3>llama-index-llms-oci-genai [0.6.4]</h3>\n<ul>\n<li>Update OCI GenAI cohere models (<a href=\"https://github.com/run-llama/llama_index/pull/20146\">#20146</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.5]</h3>\n<ul>\n<li>chore: openai vbump (<a href=\"https://github.com/run-llama/llama_index/pull/20095\">#20095</a>)</li>\n</ul>\n<h3>llama-index-readers-imdb-review [0.4.2]</h3>\n<ul>\n<li>chore: Update selenium dependency in imdb-review reader (<a href=\"https://github.com/run-llama/llama_index/pull/20105\">#20105</a>)</li>\n</ul>\n<h3>llama-index-retrievers-bedrock [0.5.0]</h3>\n<ul>\n<li>feat(bedrock): add async support for AmazonKnowledgeBasesRetriever (<a href=\"https://github.com/run-llama/llama_index/pull/20114\">#20114</a>)</li>\n</ul>\n<h3>llama-index-retrievers-superlinked [0.1.3]</h3>\n<ul>\n<li>Update README.md (<a href=\"https://github.com/run-llama/llama_index/pull/19829\">#19829</a>)</li>\n</ul>\n<h3>llama-index-storage-kvstore-postgres [0.4.2]</h3>\n<ul>\n<li>fix: Replace raw SQL string interpolation with proper SQLAlchemy parameterized APIs in PostgresKVStore (<a href=\"https://github.com/run-llama/llama_index/pull/20104\">#20104</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.3]</h3>\n<ul>\n<li>Fix BasicMCPClient resource signatures (<a href=\"https://github.com/run-llama/llama_index/pull/20118\">#20118</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.1]</h3>\n<ul>\n<li>Add GIN index support for text array metadata in PostgreSQL vector store (<a href=\"https://github.com/run-llama/llama_index/pull/20130\">#20130</a>)</li>\n</ul>",
    "published": "2025-10-26T03:01:31Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "6b02016212dad53a",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.5",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.5",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-15]</h2>\n<h3>llama-index-core [0.14.5]</h3>\n<ul>\n<li>Remove debug print (<a href=\"https://github.com/run-llama/llama_index/pull/20000\">#20000</a>)</li>\n<li>safely initialize RefDocInfo in Docstore (<a href=\"https://github.com/run-llama/llama_index/pull/20031\">#20031</a>)</li>\n<li>Add progress bar for multiprocess loading (<a href=\"https://github.com/run-llama/llama_index/pull/20048\">#20048</a>)</li>\n<li>Fix duplicate node positions when identical text appears multiple times in document (<a href=\"https://github.com/run-llama/llama_index/pull/20050\">#20050</a>)</li>\n<li>chore: tool call block - part 1 (<a href=\"https://github.com/run-llama/llama_index/pull/20074\">#20074</a>)</li>\n</ul>\n<h3>llama-index-instrumentation [0.4.2]</h3>\n<ul>\n<li>update instrumentation package metadata (<a href=\"https://github.com/run-llama/llama_index/pull/20079\">#20079</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.9.5]</h3>\n<ul>\n<li>✨ feat(anthropic): add prompt caching model validation utilities (<a href=\"https://github.com/run-llama/llama_index/pull/20069\">#20069</a>)</li>\n<li>fix streaming thinking/tool calling with anthropic (<a href=\"https://github.com/run-llama/llama_index/pull/20077\">#20077</a>)</li>\n<li>Add haiku 4.5 support (<a href=\"https://github.com/run-llama/llama_index/pull/20092\">#20092</a>)</li>\n</ul>\n<h3>llama-index-llms-baseten [0.1.6]</h3>\n<ul>\n<li>Baseten provider Kimi K2 0711, Llama 4 Maverick and Llama 4 Scout Model APIs deprecation (<a href=\"https://github.com/run-llama/llama_index/pull/20042\">#20042</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.10.5]</h3>\n<ul>\n<li>feat: List Claude Sonnet 4.5 as a reasoning model (<a href=\"https://github.com/run-llama/llama_index/pull/20022\">#20022</a>)</li>\n<li>feat: Support global cross-region inference profile prefix (<a href=\"https://github.com/run-llama/llama_index/pull/20064\">#20064</a>)</li>\n<li>Update utils.py for opus 4.1 (<a href=\"https://github.com/run-llama/llama_index/pull/20076\">#20076</a>)</li>\n<li>4.1 opus bedrockconverse missing in funcitoncalling models (<a href=\"https://github.com/run-llama/llama_index/pull/20084\">#20084</a>)</li>\n<li>Add haiku 4.5 support (<a href=\"https://github.com/run-llama/llama_index/pull/20092\">#20092</a>)</li>\n</ul>\n<h3>llama-index-llms-fireworks [0.4.4]</h3>\n<ul>\n<li>Add Support for Custom Models in Fireworks LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20023\">#20023</a>)</li>\n<li>fix(llms/fireworks): Cannot use Fireworks Deepseek V3.1-20006 issue (<a href=\"https://github.com/run-llama/llama_index/pull/20028\">#20028</a>)</li>\n</ul>\n<h3>llama-index-llms-oci-genai [0.6.3]</h3>\n<ul>\n<li>Add support for xAI models in OCI GenAI (<a href=\"https://github.com/run-llama/llama_index/pull/20089\">#20089</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.4]</h3>\n<ul>\n<li>Gpt 5 pro addition (<a href=\"https://github.com/run-llama/llama_index/pull/20029\">#20029</a>)</li>\n<li>fix collecting final response with openai responses streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20037\">#20037</a>)</li>\n<li>Add support for GPT-5 models in utils.py (JSON_SCHEMA_MODELS) (<a href=\"https://github.com/run-llama/llama_index/pull/20045\">#20045</a>)</li>\n<li>chore: tool call block - part 1 (<a href=\"https://github.com/run-llama/llama_index/pull/20074\">#20074</a>)</li>\n</ul>\n<h3>llama-index-llms-sglang [0.1.0]</h3>\n<ul>\n<li>Added Sglang llm integration (<a href=\"https://github.com/run-llama/llama_index/pull/20020\">#20020</a>)</li>\n</ul>\n<h3>llama-index-readers-gitlab [0.5.1]</h3>\n<ul>\n<li>feat(gitlab): add pagination params for repository tree and issues (<a href=\"https://github.com/run-llama/llama_index/pull/20052\">#20052</a>)</li>\n</ul>\n<h3>llama-index-readers-json [0.4.2]</h3>\n<ul>\n<li>vbump the JSON reader (<a href=\"https://github.com/run-llama/llama_index/pull/20039\">#20039</a>)</li>\n</ul>\n<h3>llama-index-readers-web [0.5.5]</h3>\n<ul>\n<li>fix: ScrapflyReader Pydantic validation error (<a href=\"https://github.com/run-llama/llama_index/pull/19999\">#19999</a>)</li>\n</ul>\n<h3>llama-index-storage-chat-store-dynamodb [0.4.2]</h3>\n<ul>\n<li>bump dynamodb chat store deps (<a href=\"https://github.com/run-llama/llama_index/pull/20078\">#20078</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.2]</h3>\n<ul>\n<li>🐛 fix(tools/mcp): Fix dict type handling and reference resolution in … (<a href=\"https://github.com/run-llama/llama_index/pull/20082\">#20082</a>)</li>\n</ul>\n<h3>llama-index-tools-signnow [0.1.0]</h3>\n<ul>\n<li>feat(signnow): SignNow mcp tools integration (<a href=\"https://github.com/run-llama/llama_index/pull/20057\">#20057</a>)</li>\n</ul>\n<h3>llama-index-tools-tavily-research [0.4.2]</h3>\n<ul>\n<li>feat: Add Tavily extract function for URL content extraction (<a href=\"https://github.com/run-llama/llama_index/pull/20038\">#20038</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azurepostgresql [0.2.0]</h3>\n<ul>\n<li>Add hybrid search to Azure PostgreSQL integration (<a href=\"https://github.com/run-llama/llama_index/pull/20027\">#20027</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.3]</h3>\n<ul>\n<li>fix: Milvus get_field_kwargs() (<a href=\"https://github.com/run-llama/llama_index/pull/20086\">#20086</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [0.6.2]</h3>\n<ul>\n<li>fix(opensearch): Correct version check for efficient filtering (<a href=\"https://github.com/run-llama/llama_index/pull/20067\">#20067</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.8.6]</h3>\n<ul>\n<li>fix(qdrant): Allow async-only initialization with hybrid search (<a href=\"https://github.com/run-llama/llama_index/pull/20005\">#20005</a>)</li>\n</ul>",
    "published": "2025-10-15T19:10:57Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "0c724a601c66433d",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.4",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.4",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-09-24]</h2>\n<h3>llama-index-core [0.14.4]</h3>\n<ul>\n<li>fix pre-release installs (<a href=\"https://github.com/run-llama/llama_index/pull/20010\">#20010</a>)</li>\n</ul>\n<h3>llama-index-embeddings-anyscale [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-embeddings-baseten [0.1.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-embeddings-fireworks [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-embeddings-opea [0.2.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-embeddings-text-embeddings-inference [0.4.2]</h3>\n<ul>\n<li>Fix authorization header setup logic in text embeddings inference (<a href=\"https://github.com/run-llama/llama_index/pull/19979\">#19979</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.9.3]</h3>\n<ul>\n<li>feat: add anthropic sonnet 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/19977\">#19977</a>)</li>\n</ul>\n<h3>llama-index-llms-anyscale [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-azure-openai [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-baseten [0.1.5]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.9.5]</h3>\n<ul>\n<li>feat: Additional support for Claude Sonnet 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/19980\">#19980</a>)</li>\n</ul>\n<h3>llama-index-llms-deepinfra [0.5.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-everlyai [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-fireworks [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.6.2]</h3>\n<ul>\n<li>Fix for ValueError: ChatMessage contains multiple blocks, use 'ChatMe… (<a href=\"https://github.com/run-llama/llama_index/pull/19954\">#19954</a>)</li>\n</ul>\n<h3>llama-index-llms-keywordsai [1.1.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-localai [0.5.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-mistralai [0.8.2]</h3>\n<ul>\n<li>Update list of MistralAI LLMs (<a href=\"https://github.com/run-llama/llama_index/pull/19981\">#19981</a>)</li>\n</ul>\n<h3>llama-index-llms-monsterapi [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-nvidia [0.4.4]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-ollama [0.7.4]</h3>\n<ul>\n<li>Fix <code>TypeError: unhashable type: 'dict'</code> in Ollama stream chat with tools (<a href=\"https://github.com/run-llama/llama_index/pull/19938\">#19938</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.1]</h3>\n<ul>\n<li>feat(OpenAILike): support structured outputs (<a href=\"https://github.com/run-llama/llama_index/pull/19967\">#19967</a>)</li>\n</ul>\n<h3>llama-index-llms-openai-like [0.5.3]</h3>\n<ul>\n<li>feat(OpenAILike): support structured outputs (<a href=\"https://github.com/run-llama/llama_index/pull/19967\">#19967</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.2]</h3>\n<ul>\n<li>chore(openrouter,anthropic): add py.typed (<a href=\"https://github.com/run-llama/llama_index/pull/19966\">#19966</a>)</li>\n</ul>\n<h3>llama-index-llms-perplexity [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-portkey [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-sarvam [0.2.1]</h3>\n<ul>\n<li>fixed Sarvam Integration and Typos (Fixes <a class=\"issue-link js-issue-link\" href=\"https://github.com/run-llama/llama_index/issues/19931\">#19931</a>) (<a href=\"https://github.com/run-llama/llama_index/pull/19932\">#19932</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.6.4]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-yi [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-memory-bedrock-agentcore [0.1.0]</h3>\n<ul>\n<li>feat: Bedrock AgentCore Memory integration (<a href=\"https://github.com/run-llama/llama_index/pull/19953\">#19953</a>)</li>\n</ul>\n<h3>llama-index-multi-modal-llms-openai [0.6.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.4.4]</h3>\n<ul>\n<li>Fix: Respect cloud parameter when fetching child pages in ConfluenceR… (<a href=\"https://github.com/run-llama/llama_index/pull/19983\">#19983</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>Bug Fix :- Not Able to Fetch Page whose latest is empty or null (<a href=\"https://github.com/run-llama/llama_index/pull/19916\">#19916</a>)</li>\n</ul>\n<h3>llama-index-selectors-notdiamond [0.4.0]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-tools-agentql [1.2.0]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-tools-playwright [0.3.1]</h3>\n<ul>\n<li>chore: fix playwright tests (<a href=\"https://github.com/run-llama/llama_index/pull/19946\">#19946</a>)</li>\n</ul>\n<h3>llama-index-tools-scrapegraph [0.2.2]</h3>\n<ul>\n<li>feat: update scrapegraphai (<a href=\"https://github.com/run-llama/llama_index/pull/19974\">#19974</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-chroma [0.5.3]</h3>\n<ul>\n<li>docs: fix query method docstring in ChromaVectorStore Fixes <a class=\"issue-link js-issue-link\" href=\"https://github.com/run-llama/llama_index/issues/19969\">#19969</a> (<a href=\"https://github.com/run-llama/llama_index/pull/19973\">#19973</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.8.1]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.0]</h3>\n<ul>\n<li>fix index creation in postgres vector store (<a href=\"https://github.com/run-llama/llama_index/pull/19955\">#19955</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-solr [0.1.0]</h3>\n<ul>\n<li>Add ApacheSolrVectorStore Integration (<a href=\"https://github.com/run-llama/llama_index/pull/19933\">#19933</a>)</li>\n</ul>",
    "published": "2025-10-03T17:52:41Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "df23a683c09c1437",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-sdk==0.3.6",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.6",
    "summary": "<p>Changes since sdk==0.3.5</p>\n<ul>\n<li>release(sdk-py): 0.3.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6805\">#6805</a>)</li>\n<li>chore: update to add prune method (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6804\">#6804</a>)</li>\n<li>chore: Re-organize client files. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6787\">#6787</a>)</li>\n</ul>",
    "published": "2026-02-14T19:46:16Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "165fb96a655d42e0",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-sdk==0.3.5",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.5",
    "summary": "<p>Changes since sdk==0.3.4</p>\n<ul>\n<li>chore: server runtime type (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6774\">#6774</a>)</li>\n</ul>",
    "published": "2026-02-10T16:56:28Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "9a2a7007f26a8fa7",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph==1.0.8",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/1.0.8",
    "summary": "<p>Changes since 1.0.7</p>\n<ul>\n<li>release(langgraph): 1.0.8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6757\">#6757</a>)</li>\n<li>chore: shallow copy futures (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6755\">#6755</a>)</li>\n<li>fix: pydantic messages double streaming (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6753\">#6753</a>)</li>\n<li>chore(deps-dev): bump ruff from 0.14.7 to 0.14.11 in /libs/sdk-py (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6673\">#6673</a>)</li>\n<li>chore: Omit lock when using connection pool (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6734\">#6734</a>)</li>\n<li>docs: enhance <code>Runtime</code> and <code>ToolRuntime</code> class descriptions for clarity (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6689\">#6689</a>)</li>\n<li>docs: add clarity to use of <code>thread_id</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6515\">#6515</a>)</li>\n<li>docs: add docstrings to <code>add_node</code> overloads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6514\">#6514</a>)</li>\n<li>docs: update notebook links and add archival notices for examples (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6720\">#6720</a>)</li>\n<li>release(cli): 0.4.12 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6716\">#6716</a>)</li>\n</ul>",
    "published": "2026-02-06T12:31:26Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a08265c21cb9aab8",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-sdk==0.3.4",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.4",
    "summary": "<p>Changes since sdk==0.3.3</p>\n<ul>\n<li>chore: release python sdk (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6754\">#6754</a>)</li>\n<li>feat(sdk-py): add update method for crons client (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6742\">#6742</a>)</li>\n<li>feat(sdk-py): add support for enabling/disabling crons (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6740\">#6740</a>)</li>\n<li>chore(deps-dev): bump ruff from 0.14.7 to 0.14.11 in /libs/sdk-py (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6673\">#6673</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>docs: clarify cron job schedule interpretation in UTC (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6692\">#6692</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-02-06T00:44:26Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "71a7cd16a493bef9",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-checkpoint-postgres==3.0.4",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/checkpointpostgres%3D%3D3.0.4",
    "summary": "<p>Changes since checkpointpostgres==3.0.3</p>\n<ul>\n<li>chore: Omit lock when using connection pool (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6734\">#6734</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-31T00:46:04Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "b2b8ee2468790fbd",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-cli==0.4.12",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/cli%3D%3D0.4.12",
    "summary": "<p>Changes since cli==0.4.11</p>\n<ul>\n<li>release(cli): 0.4.12 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6716\">#6716</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-23T13:34:28Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "a509bbcb9e389f17",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph==1.0.7",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/1.0.7",
    "summary": "<p>Changes since 1.0.6</p>\n<ul>\n<li>release: langgraph and prebuilt 1.0.7 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6712\">#6712</a>)</li>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-22T16:57:59Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "55029e01959ae6f4",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-prebuilt==1.0.7",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/prebuilt%3D%3D1.0.7",
    "summary": "<p>Changes since prebuilt==1.0.6</p>\n<ul>\n<li>release: langgraph and prebuilt 1.0.7 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6712\">#6712</a>)</li>\n<li>feat: support dynamic tool calling via <code>tool</code> override in <code>wrap_model_call</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6711\">#6711</a>)</li>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-22T16:45:37Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "fc8438b25945c3c3",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-checkpoint-sqlite==3.0.3",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/checkpointsqlite%3D%3D3.0.3",
    "summary": "<p>Changes since checkpointsqlite==3.0.2</p>\n<ul>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-19T00:38:58Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  },
  {
    "id": "874fa68e3215adde",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-sdk==0.3.3",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.3",
    "summary": "<p>Changes since sdk==0.3.2</p>\n<ul>\n<li>chore: Better error messages (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6681\">#6681</a>)</li>\n<li>feat(sdk-py): add end-time to crons client (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6674\">#6674</a>)</li>\n</ul>",
    "published": "2026-01-13T00:30:58Z",
    "collected_at": "2026-02-15T03:04:57.496151+00:00"
  }
]