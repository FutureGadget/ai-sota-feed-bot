[
  {
    "id": "5eb1dc4c2b27f35b",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "url": "http://arxiv.org/abs/2602.12281v1",
    "summary": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
    "published": "2026-02-12T18:59:59Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e4aef8221107f2b4",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
    "url": "http://arxiv.org/abs/2602.12279v1",
    "summary": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
    "published": "2026-02-12T18:59:49Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ff6af3576c9906a8",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
    "url": "http://arxiv.org/abs/2602.12278v1",
    "summary": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.",
    "published": "2026-02-12T18:59:35Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8c7ff1a869d5d3eb",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Agentic Test-Time Scaling for WebAgents",
    "url": "http://arxiv.org/abs/2602.12276v1",
    "summary": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.",
    "published": "2026-02-12T18:58:30Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "68a3a1d3bce91643",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Creative Ownership in the Age of AI",
    "url": "http://arxiv.org/abs/2602.12270v1",
    "summary": "Copyright law focuses on whether a new work is \"substantially similar\" to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \\emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.",
    "published": "2026-02-12T18:56:42Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "79ecbbcbd73d6a2a",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use",
    "url": "http://arxiv.org/abs/2602.12268v1",
    "summary": "AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.",
    "published": "2026-02-12T18:55:09Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2af2b0e3e1874c8b",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
    "url": "http://arxiv.org/abs/2602.12259v1",
    "summary": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.",
    "published": "2026-02-12T18:49:27Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "dbb292bf2030238a",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "On the implicit regularization of Langevin dynamics with projected noise",
    "url": "http://arxiv.org/abs/2602.12257v1",
    "summary": "We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.",
    "published": "2026-02-12T18:45:42Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "64844eef3ffe55d0",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication",
    "url": "http://arxiv.org/abs/2602.12251v1",
    "summary": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.",
    "published": "2026-02-12T18:37:23Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "caabe2ff646d06d4",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "\"Sorry, I Didn't Catch That\": How Speech Models Miss What Matters Most",
    "url": "http://arxiv.org/abs/2602.12249v1",
    "summary": "Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.",
    "published": "2026-02-12T18:36:09Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e438c20e03c93af5",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction",
    "url": "http://arxiv.org/abs/2602.12247v1",
    "summary": "Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.",
    "published": "2026-02-12T18:31:37Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7078d34342f06b84",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces",
    "url": "http://arxiv.org/abs/2602.12245v1",
    "summary": "Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.",
    "published": "2026-02-12T18:30:27Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c93523dac4a3835b",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
    "url": "http://arxiv.org/abs/2602.12237v1",
    "summary": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.",
    "published": "2026-02-12T18:16:05Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ec4fed051f047224",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision",
    "url": "http://arxiv.org/abs/2602.12236v1",
    "summary": "Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.",
    "published": "2026-02-12T18:15:32Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9e61b9ab865323bf",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Bandit Learning in Matching Markets with Interviews",
    "url": "http://arxiv.org/abs/2602.12224v1",
    "summary": "Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as \\textit{low-cost hints} that reveal partial preference information to both sides. Our framework departs from existing work by allowing firm-side uncertainty: firms, like agents, may be unsure of their own preferences and can make early hiring mistakes by hiring less preferred agents. To handle this, we extend the firm's action space to allow \\emph{strategic deferral} (choosing not to hire in a round), enabling recovery from suboptimal hires and supporting decentralized learning without coordination. We design novel algorithms for (i) a centralized setting with an omniscient interview allocator and (ii) decentralized settings with two types of firm-side feedback. Across all settings, our algorithms achieve time-independent regret, a substantial improvement over the $O(\\log T)$ regret bounds known for learning stable matchings without interviews. Also, under mild structured markets, decentralized performance matches the centralized counterpart up to polynomial factors in the number of agents and firms.",
    "published": "2026-02-12T18:03:37Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a6603f6b27e793c6",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
    "url": "http://arxiv.org/abs/2602.12222v1",
    "summary": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT",
    "published": "2026-02-12T17:59:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5bf6a2102b28a339",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
    "url": "http://arxiv.org/abs/2602.12218v1",
    "summary": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.",
    "published": "2026-02-12T17:56:07Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "075c459ac9e5611a",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation",
    "url": "http://arxiv.org/abs/2602.12207v1",
    "summary": "Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA's no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.",
    "published": "2026-02-12T17:46:52Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1b171d22b0a6af98",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
    "url": "http://arxiv.org/abs/2602.12205v1",
    "summary": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
    "published": "2026-02-12T17:44:24Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1c2fe5a9234dde43",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education",
    "url": "http://arxiv.org/abs/2602.12196v1",
    "summary": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.",
    "published": "2026-02-12T17:29:03Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "887bc16533cdbcd2",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization",
    "url": "http://arxiv.org/abs/2602.12187v1",
    "summary": "Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.",
    "published": "2026-02-12T17:18:00Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5fbc27d4bc3afba1",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation",
    "url": "http://arxiv.org/abs/2602.12173v1",
    "summary": "Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.",
    "published": "2026-02-12T17:01:49Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "68002895637c0e3e",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation",
    "url": "http://arxiv.org/abs/2602.12172v1",
    "summary": "Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.",
    "published": "2026-02-12T17:00:36Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "865497aa97dd7f50",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Statistical Parsing for Logical Information Retrieval",
    "url": "http://arxiv.org/abs/2602.12170v1",
    "summary": "In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.\n  This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive reasoning (modus tollens) via backward lambda messages, completing Prawitz's simple elimination rules. The engine handles 44/44 test cases spanning 22 reasoning patterns. For semantics, we present a typed logical language with role-labeled predicates, modal quantifiers, and three tiers of expressiveness following Prawitz: first-order quantification, propositions as arguments, and predicate quantification via lambda abstraction. For syntax, we present a typed slot grammar that deterministically compiles sentences to logical form (33/33 correct, zero ambiguity). LLMs handle disambiguation (95% PP attachment accuracy) but cannot produce structured parses directly (12.4% UAS), confirming grammars are necessary. The architecture: LLM preprocesses, grammar parses, LLM reranks, QBBN infers.\n  We argue this reconciles formal semantics with Sutton's \"bitter lesson\" (2019): LLMs eliminate the annotation bottleneck that killed formal NLP, serving as annotator while the QBBN serves as verifier. Code: https://github.com/gregorycoppola/world",
    "published": "2026-02-12T16:57:25Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7ef7d2f770395963",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
    "url": "http://arxiv.org/abs/2602.12164v1",
    "summary": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.",
    "published": "2026-02-12T16:46:00Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8fca98bb113050e2",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting",
    "url": "http://arxiv.org/abs/2602.12159v1",
    "summary": "Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/",
    "published": "2026-02-12T16:41:26Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a9e931b288f7117b",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "dVoting: Fast Voting for dLLMs",
    "url": "http://arxiv.org/abs/2602.12153v1",
    "summary": "Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting",
    "published": "2026-02-12T16:35:05Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3217aee848417fa5",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "GPT-4o Lacks Core Features of Theory of Mind",
    "url": "http://arxiv.org/abs/2602.12150v1",
    "summary": "Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.",
    "published": "2026-02-12T16:33:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a377fc68c9896b85",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2602.12146v1",
    "summary": "Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.",
    "published": "2026-02-12T16:30:55Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d7bbe81a5fed7a89",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "On the Adoption of AI Coding Agents in Open-source Android and iOS Development",
    "url": "http://arxiv.org/abs/2602.12144v1",
    "summary": "AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.",
    "published": "2026-02-12T16:30:29Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "15dc7058e58e8b77",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction",
    "url": "http://arxiv.org/abs/2602.12143v1",
    "summary": "As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model.",
    "published": "2026-02-12T16:30:07Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8e8c5d422b280730",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment",
    "url": "http://arxiv.org/abs/2602.12134v1",
    "summary": "Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.",
    "published": "2026-02-12T16:21:22Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "60b9b703744f9058",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5",
    "url": "http://arxiv.org/abs/2602.12133v1",
    "summary": "This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), PERLA, and Fitzpatrick scales. Neutral prompts produced highly polarized defaults. Both models exhibited a strong \"default white\" bias (>96% of outputs). However, they diverged sharply on gender: Gemini favored female-presenting subjects, while GPT favored male-presenting subjects with lighter skin tones. This research provides a large-scale, comparative audit of state-of-the-art models using an illumination-aware colorimetric methodology, distinguishing aesthetic rendering from underlying pigmentation in synthetic imagery. The study demonstrates that neutral prompts function as diagnostic probes rather than neutral instructions. It offers a robust framework for auditing algorithmic visual culture and challenges the sociolinguistic assumption that unmarked language results in inclusive representation.",
    "published": "2026-02-12T16:21:03Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "43641dd068048b42",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "HLA: Hadamard Linear Attention",
    "url": "http://arxiv.org/abs/2602.12128v1",
    "summary": "The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.\n  We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.",
    "published": "2026-02-12T16:16:47Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9f143285801f0d81",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "url": "http://arxiv.org/abs/2602.12125v1",
    "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
    "published": "2026-02-12T16:14:29Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "34b07b3324d2fa26",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning",
    "url": "http://arxiv.org/abs/2602.12123v1",
    "summary": "Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.\n  Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.\n  Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.",
    "published": "2026-02-12T16:11:29Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "89faee4c7c2b000c",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models",
    "url": "http://arxiv.org/abs/2602.12120v1",
    "summary": "Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gains in annual, data-sparse institutional forecasting under leakage-disciplined covariate construction. We benchmark multiple TSFM families in a zero-shot setting and test a compact, leakage-safe covariate set and introduce the Institutional Operating Conditions Index (IOCI), a transferable 0-100 regime covariate derived from time-stamped documentary evidence available at each forecast origin, alongside Google Trends demand proxies with stabilising feature engineering. Using an expanding-window backtest with strict vintage alignment, covariate-conditioned TSFMs perform on par with classical benchmarks without institution-specific training, with performance differences varying by cohort and model.",
    "published": "2026-02-12T16:10:42Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "011fdfedd452a553",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation on Meteorological Satellite",
    "url": "http://arxiv.org/abs/2602.12117v1",
    "summary": "Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices. Current physics-guided models suffer from linear feature interactions that fail to capture high-order polynomial relationships between TC attributes, leading to inflated model sizes and hardware incompatibility. To overcome these challenges, this study introduces the Kolmogorov-Arnold Network-based Feature Interaction Framework (KAN-FIF), a lightweight multimodal architecture that integrates MLP and CNN layers with spline-parameterized KAN layers. For Maximum Sustained Wind (MSW) prediction, experiments demonstrate that the KAN-FIF framework achieves a $94.8\\%$ reduction in parameters (0.99MB vs 19MB) and $68.7\\%$ faster inference per sample (2.3ms vs 7.35ms) compared to baseline model Phy-CoCo, while maintaining superior accuracy with $32.5\\%$ lower MAE. The offline deployment experiment of the FY-4 series meteorological satellite processor on the Qingyun-1000 development board achieved a 14.41ms per-sample inference latency with the KAN-FIF framework, demonstrating promising feasibility for operational TC monitoring and extending deployability to edge-device AI applications. The code is released at https://github.com/Jinglin-Zhang/KAN-FIF.",
    "published": "2026-02-12T16:07:39Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "01866f9acfa12268",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty",
    "url": "http://arxiv.org/abs/2602.12113v1",
    "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at https://github.com/ZeweiYu1/ARLCP .",
    "published": "2026-02-12T16:04:00Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bf66f4350268e693",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context",
    "url": "http://arxiv.org/abs/2602.12108v1",
    "summary": "In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.",
    "published": "2026-02-12T16:00:01Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e4aef8221107f2b4",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
    "url": "http://arxiv.org/abs/2602.12279v1",
    "summary": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
    "published": "2026-02-12T18:59:49Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a7a4345c41b65656",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage",
    "url": "http://arxiv.org/abs/2602.12274v1",
    "summary": "Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.",
    "published": "2026-02-12T18:58:12Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0f0dfac01402e3a9",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs",
    "url": "http://arxiv.org/abs/2602.12273v1",
    "summary": "We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\\varepsilon$-optimality for the iUzawa-Net, and validate its promising numerical efficiency through nonsmooth elliptic and parabolic optimal control problems. Our techniques offer a versatile framework for designing and analyzing various optimization-informed deep learning approaches to optimal control and other PDE-constrained optimization problems. The proposed learning-to-control approach synergizes model-based optimization algorithms and data-driven deep learning techniques, inheriting the merits of both methodologies.",
    "published": "2026-02-12T18:57:43Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "12180bace4bd6103",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "MonarchRT: Efficient Attention for Real-Time Video Generation",
    "url": "http://arxiv.org/abs/2602.12271v1",
    "summary": "Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.",
    "published": "2026-02-12T18:56:53Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "99d7cac01d38218e",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data",
    "url": "http://arxiv.org/abs/2602.12267v1",
    "summary": "Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO's robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.",
    "published": "2026-02-12T18:54:57Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "57a72ed8051ad7b3",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
    "url": "http://arxiv.org/abs/2602.12262v1",
    "summary": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.",
    "published": "2026-02-12T18:52:35Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2af2b0e3e1874c8b",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
    "url": "http://arxiv.org/abs/2602.12259v1",
    "summary": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.",
    "published": "2026-02-12T18:49:27Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "32d5450e45c6213a",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Is Online Linear Optimization Sufficient for Strategic Robustness?",
    "url": "http://arxiv.org/abs/2602.12253v1",
    "summary": "We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller's manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids.\n  In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\\sqrt{T \\log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\\sqrt{T (\\log K+\\log(T/δ)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].",
    "published": "2026-02-12T18:41:55Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ebc06d8bc5443bf1",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Community Concealment from Unsupervised Graph Learning-Based Clustering",
    "url": "http://arxiv.org/abs/2602.12250v1",
    "summary": "Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.",
    "published": "2026-02-12T18:36:19Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e438c20e03c93af5",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction",
    "url": "http://arxiv.org/abs/2602.12247v1",
    "summary": "Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.",
    "published": "2026-02-12T18:31:37Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7078d34342f06b84",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces",
    "url": "http://arxiv.org/abs/2602.12245v1",
    "summary": "Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.",
    "published": "2026-02-12T18:30:27Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c3a8163196257d7f",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
    "url": "http://arxiv.org/abs/2602.12241v1",
    "summary": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.",
    "published": "2026-02-12T18:20:45Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c93523dac4a3835b",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
    "url": "http://arxiv.org/abs/2602.12237v1",
    "summary": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.",
    "published": "2026-02-12T18:16:05Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9d4bb21d9b2ea81d",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Categorical Flow Maps",
    "url": "http://arxiv.org/abs/2602.12233v1",
    "summary": "We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.",
    "published": "2026-02-12T18:10:46Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f99bb696ff715daf",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser",
    "url": "http://arxiv.org/abs/2602.12229v1",
    "summary": "Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.",
    "published": "2026-02-12T18:06:03Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a6603f6b27e793c6",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
    "url": "http://arxiv.org/abs/2602.12222v1",
    "summary": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT",
    "published": "2026-02-12T17:59:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5bf6a2102b28a339",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
    "url": "http://arxiv.org/abs/2602.12218v1",
    "summary": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.",
    "published": "2026-02-12T17:56:07Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5d326061d753cfde",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction",
    "url": "http://arxiv.org/abs/2602.12204v1",
    "summary": "Hybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \\emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \\textbf{88\\%} of attention operations retrieve information already predictable from the model's hidden state, and this redundancy does \\emph{not} decrease during training. Motivated by this observation, we introduce \\textbf{\\ours{}} (\\textbf{C}onsolidation-based \\textbf{R}outing for \\textbf{A}daptive \\textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \\ours{} exhibits \\emph{decreasing attention utilization} over training, achieving a \\textbf{37.8$\\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \\emph{impossible} without consolidation: any static routing scheme requires $Ω(f \\cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \\ours{} achieves \\textbf{100\\% retrieval accuracy} at 1.6\\% attention compute (vs.\\ 68\\% for baselines), and consolidated patterns transfer to unseen tasks with \\textbf{48--52\\%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($γ= 0.43$ vs.\\ $γ_{\\text{human}} \\approx 0.4$--$0.5$). Code and benchmarks are available at [anonymized].",
    "published": "2026-02-12T17:40:15Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a204d56e17c6d2af",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "WaveFormer: Wavelet Embedding Transformer for Biomedical Signals",
    "url": "http://arxiv.org/abs/2602.12189v1",
    "summary": "Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain information, and positional encoding, where Dynamic Wavelet Positional Encoding (DyWPE) adapts position embeddings to signal-specific temporal structure through mono-channel DWT analysis. We evaluate WaveFormer on eight diverse datasets spanning human activity recognition and brain signal analysis, with sequence lengths ranging from 50 to 3000 timesteps and channel counts from 1 to 144. Experimental results demonstrate that WaveFormer achieves competitive performance through comprehensive frequency-aware processing. Our approach provides a principled framework for incorporating frequency-domain knowledge into transformer-based time series classification.",
    "published": "2026-02-12T17:20:43Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bf376f14b99b5d8a",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Convex Markov Games and Beyond: New Proof of Existence, Characterization and Learning Algorithms for Nash Equilibria",
    "url": "http://arxiv.org/abs/2602.12181v1",
    "summary": "Convex Markov Games (cMGs) were recently introduced as a broad class of multi-agent learning problems that generalize Markov games to settings where strategic agents optimize general utilities beyond additive rewards. While cMGs expand the modeling frontier, their theoretical foundations, particularly the structure of Nash equilibria (NE) and guarantees for learning algorithms, are not yet well understood. In this work, we address these gaps for an extension of cMGs, which we term General Utility Markov Games (GUMGs), capturing new applications requiring coupling between agents' occupancy measures. We prove that in GUMGs, Nash equilibria coincide with the fixed points of projected pseudo-gradient dynamics (i.e., first-order stationary points), enabled by a novel agent-wise gradient domination property. This insight also yields a simple proof of NE existence using Brouwer's fixed-point theorem. We further show the existence of Markov perfect equilibria. Building on this characterization, we establish a policy gradient theorem for GUMGs and design a model-free policy gradient algorithm. For potential GUMGs, we establish iteration complexity guarantees for computing approximate-NE under exact gradients and provide sample complexity bounds in both the generative model and on-policy settings. Our results extend beyond prior work restricted to zero-sum cMGs, providing the first theoretical analysis of common-interest cMGs.",
    "published": "2026-02-12T17:11:20Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ac2da6b94aaeba25",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics",
    "url": "http://arxiv.org/abs/2602.12180v1",
    "summary": "Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.",
    "published": "2026-02-12T17:11:08Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f328d3d7c67d85ba",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Amortized Molecular Optimization via Group Relative Policy Optimization",
    "url": "http://arxiv.org/abs/2602.12162v1",
    "summary": "Molecular design encompasses tasks ranging from de-novo design to structural alteration of given molecules or fragments. For the latter, state-of-the-art methods predominantly function as \"Instance Optimizers'', expending significant compute restarting the search for every input structure. While model-based approaches theoretically offer amortized efficiency by learning a policy transferable to unseen structures, existing methods struggle to generalize. We identify a key failure mode: the high variance arising from the heterogeneous difficulty of distinct starting structures. To address this, we introduce GRXForm, adapting a pre-trained Graph Transformer model that optimizes molecules via sequential atom-and-bond additions. We employ Group Relative Policy Optimization (GRPO) for goal-directed fine-tuning to mitigate variance by normalizing rewards relative to the starting structure. Empirically, GRXForm generalizes to out-of-distribution molecular scaffolds without inference-time oracle calls or refinement, achieving scores in multi-objective optimization competitive with leading instance optimizers.",
    "published": "2026-02-12T16:43:59Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "170235014c4b750d",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "SafeNeuron: Neuron-Level Safety Alignment for Large Language Models",
    "url": "http://arxiv.org/abs/2602.12158v1",
    "summary": "Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities. Furthermore, our layer-wise analysis reveals that safety behaviors are governed by stable and shared internal representations. Overall, SafeNeuron provides an interpretable and robust perspective for model alignment.",
    "published": "2026-02-12T16:40:05Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3217aee848417fa5",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "GPT-4o Lacks Core Features of Theory of Mind",
    "url": "http://arxiv.org/abs/2602.12150v1",
    "summary": "Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.",
    "published": "2026-02-12T16:33:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0ebb199a929bb388",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks",
    "url": "http://arxiv.org/abs/2602.12147v1",
    "summary": "Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at https://huggingface.co/spaces/Real-TSF/TIME-leaderboard.",
    "published": "2026-02-12T16:31:01Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "15dc7058e58e8b77",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction",
    "url": "http://arxiv.org/abs/2602.12143v1",
    "summary": "As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model.",
    "published": "2026-02-12T16:30:07Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "88e298cc70260efe",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Oscillators Are All You Need: Irregular Time Series Modelling via Damped Harmonic Oscillators with Closed-Form Solutions",
    "url": "http://arxiv.org/abs/2602.12139v1",
    "summary": "Transformers excel at time series modelling through attention mechanisms that capture long-term temporal patterns. However, they assume uniform time intervals and therefore struggle with irregular time series. Neural Ordinary Differential Equations (NODEs) effectively handle irregular time series by modelling hidden states as continuously evolving trajectories. ContiFormers arxiv:2402.10635 combine NODEs with Transformers, but inherit the computational bottleneck of the former by using heavy numerical solvers. This bottleneck can be removed by using a closed-form solution for the given dynamical system - but this is known to be intractable in general! We obviate this by replacing NODEs with a novel linear damped harmonic oscillator analogy - which has a known closed-form solution. We model keys and values as damped, driven oscillators and expand the query in a sinusoidal basis up to a suitable number of modes. This analogy naturally captures the query-key coupling that is fundamental to any transformer architecture by modelling attention as a resonance phenomenon. Our closed-form solution eliminates the computational overhead of numerical ODE solvers while preserving expressivity. We prove that this oscillator-based parameterisation maintains the universal approximation property of continuous-time attention; specifically, any discrete attention matrix realisable by ContiFormer's continuous keys can be approximated arbitrarily well by our fixed oscillator modes. Our approach delivers both theoretical guarantees and scalability, achieving state-of-the-art performance on irregular time series benchmarks while being orders of magnitude faster.",
    "published": "2026-02-12T16:27:09Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b6396b47a432f25b",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Towards Personalized Bangla Book Recommendation: A Large-Scale Multi-Entity Book Graph Dataset",
    "url": "http://arxiv.org/abs/2602.12129v1",
    "summary": "Personalized book recommendation in Bangla literature has been constrained by the lack of structured, large-scale, and publicly available datasets. This work introduces RokomariBG, a large-scale, multi-entity heterogeneous book graph dataset designed to support research on personalized recommendation in a low-resource language setting. The dataset comprises 127,302 books, 63,723 users, 16,601 authors, 1,515 categories, 2,757 publishers, and 209,602 reviews, connected through eight relation types and organized as a comprehensive knowledge graph.\n  To demonstrate the utility of the dataset, we provide a systematic benchmarking study on the Top-N recommendation task, evaluating a diverse set of representative recommendation models, including classical collaborative filtering methods, matrix factorization models, content-based approaches, graph neural networks, a hybrid matrix factorization model with side information, and a neural two-tower retrieval architecture. The benchmarking results highlight the importance of leveraging multi-relational structure and textual side information, with neural retrieval models achieving the strongest performance (NDCG@10 = 0.204). Overall, this work establishes a foundational benchmark and a publicly available resource for Bangla book recommendation research, enabling reproducible evaluation and future studies on recommendation in low-resource cultural domains. The dataset and code are publicly available at https://github.com/backlashblitz/Bangla-Book-Recommendation-Dataset",
    "published": "2026-02-12T16:18:55Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9f143285801f0d81",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "url": "http://arxiv.org/abs/2602.12125v1",
    "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
    "published": "2026-02-12T16:14:29Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "20ff53169d96ab48",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Capability-Oriented Training Induced Alignment Risk",
    "url": "http://arxiv.org/abs/2602.12124v1",
    "summary": "While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse \"vulnerability games\", each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow \"tricks\" but generalizable skills; they can be transferred to new tasks and even \"distilled\" from a capable teacher model to other student models through data alone. Our findings reveal that capability-oriented training induced risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves. Code is available at https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk.",
    "published": "2026-02-12T16:13:14Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "34b07b3324d2fa26",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning",
    "url": "http://arxiv.org/abs/2602.12123v1",
    "summary": "Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.\n  Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.\n  Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.",
    "published": "2026-02-12T16:11:29Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "011fdfedd452a553",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation on Meteorological Satellite",
    "url": "http://arxiv.org/abs/2602.12117v1",
    "summary": "Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices. Current physics-guided models suffer from linear feature interactions that fail to capture high-order polynomial relationships between TC attributes, leading to inflated model sizes and hardware incompatibility. To overcome these challenges, this study introduces the Kolmogorov-Arnold Network-based Feature Interaction Framework (KAN-FIF), a lightweight multimodal architecture that integrates MLP and CNN layers with spline-parameterized KAN layers. For Maximum Sustained Wind (MSW) prediction, experiments demonstrate that the KAN-FIF framework achieves a $94.8\\%$ reduction in parameters (0.99MB vs 19MB) and $68.7\\%$ faster inference per sample (2.3ms vs 7.35ms) compared to baseline model Phy-CoCo, while maintaining superior accuracy with $32.5\\%$ lower MAE. The offline deployment experiment of the FY-4 series meteorological satellite processor on the Qingyun-1000 development board achieved a 14.41ms per-sample inference latency with the KAN-FIF framework, demonstrating promising feasibility for operational TC monitoring and extending deployability to edge-device AI applications. The code is released at https://github.com/Jinglin-Zhang/KAN-FIF.",
    "published": "2026-02-12T16:07:39Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7cf1510b435ba2cd",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Few-Shot Design Optimization by Exploiting Auxiliary Information",
    "url": "http://arxiv.org/abs/2602.12112v1",
    "summary": "Many real-world design problems involve optimizing an expensive black-box function $f(x)$, such as hardware design or drug discovery. Bayesian Optimization has emerged as a sample-efficient framework for this problem. However, the basic setting considered by these methods is simplified compared to real-world experimental setups, where experiments often generate a wealth of useful information. We introduce a new setting where an experiment generates high-dimensional auxiliary information $h(x)$ along with the performance measure $f(x)$; moreover, a history of previously solved tasks from the same task family is available for accelerating optimization. A key challenge of our setting is learning how to represent and utilize $h(x)$ for efficiently solving new optimization tasks beyond the task history. We develop a novel approach for this setting based on a neural model which predicts $f(x)$ for unseen designs given a few-shot context containing observations of $h(x)$. We evaluate our method on two challenging domains, robotic hardware design and neural network hyperparameter tuning, and introduce a novel design problem and large-scale benchmark for the former. On both domains, our method utilizes auxiliary feedback effectively to achieve more accurate few-shot prediction and faster optimization of design tasks, significantly outperforming several methods for multi-task optimization.",
    "published": "2026-02-12T16:03:46Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3297e0ea8eb7ef9a",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "On the Complexity of Offline Reinforcement Learning with $Q^\\star$-Approximation and Partial Coverage",
    "url": "http://arxiv.org/abs/2602.12107v1",
    "summary": "We study offline reinforcement learning under $Q^\\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: \"Are $Q^\\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?\"\n  We answer in the negative by establishing an information-theoretic lower bound. Going substantially beyond this, we introduce a general framework that characterizes the intrinsic complexity of a given $Q^\\star$ function class, inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and extends to broader settings. Our decision-estimation decomposition can be combined with a wide range of $Q^\\star$ estimation procedures, modularizing and generalizing existing approaches.\n  Beyond the general framework, we make further contributions: By developing a novel second-order performance difference lemma, we obtain the first $ε^{-2}$ sample complexity under partial coverage for soft $Q$-learning, improving the $ε^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang's (2022) need for additional online interaction when the value gap of $Q^\\star$ is unknown. We also give the first characterization of offline learnability for general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017; Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains unexplored in offline RL except for special cases. Finally, we provide the first analysis for CQL under $Q^\\star$-realizability and Bellman completeness beyond the tabular case.",
    "published": "2026-02-12T15:59:42Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "df042e0e9aa869ad",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Iskra: A System for Inverse Geometry Processing",
    "url": "http://arxiv.org/abs/2602.12105v1",
    "summary": "We propose a system for differentiating through solutions to geometry processing problems. Our system differentiates a broad class of geometric algorithms, exploiting existing fast problem-specific schemes common to geometry processing, including local-global and ADMM solvers. It is compatible with machine learning frameworks, opening doors to new classes of inverse geometry processing applications. We marry the scatter-gather approach to mesh processing with tensor-based workflows and rely on the adjoint method applied to user-specified imperative code to generate an efficient backward pass behind the scenes. We demonstrate our approach by differentiating through mean curvature flow, spectral conformal parameterization, geodesic distance computation, and as-rigid-as-possible deformation, examining usability and performance on these applications. Our system allows practitioners to differentiate through existing geometry processing algorithms without needing to reformulate them, resulting in low implementation effort, fast runtimes, and lower memory requirements than differentiable optimization tools not tailored to geometry processing.",
    "published": "2026-02-12T15:59:06Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "aa0a5ad842e69dd4",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL",
    "url": "http://arxiv.org/abs/2602.12087v1",
    "summary": "Estimating the state of an environment from high-dimensional, multimodal, and noisy observations is a fundamental challenge in reinforcement learning (RL). Traditional approaches rely on probabilistic models to account for the uncertainty, but often require explicit noise assumptions, in turn limiting generalization. In this work, we contribute a novel method to learn a structured latent representation, in which distances between states directly correlate with the minimum number of actions required to transition between them. The proposed metric space formulation provides a geometric interpretation of uncertainty without the need for explicit probabilistic modeling. To achieve this, we introduce a multimodal latent transition model and a sensor fusion mechanism based on inverse distance weighting, allowing for the adaptive integration of multiple sensor modalities without prior knowledge of noise distributions. We empirically validate the approach on a range of multimodal RL tasks, demonstrating improved robustness to sensor noise and superior state estimation compared to baseline methods. Our experiments show enhanced performance of an RL agent via the learned representation, eliminating the need of explicit noise augmentation. The presented results suggest that leveraging transition-aware metric spaces provides a principled and scalable solution for robust state estimation in sequential decision-making.",
    "published": "2026-02-12T15:41:20Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "efb11edc29fd8474",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Empirical Gaussian Processes",
    "url": "http://arxiv.org/abs/2602.12082v1",
    "summary": "Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.",
    "published": "2026-02-12T15:39:08Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "dc6da3d4db78d5aa",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories",
    "url": "http://arxiv.org/abs/2602.12080v1",
    "summary": "Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs. As a result, comprehensive data collection in soccer is largely confined to top-tier competitions, limiting the broader adoption of data-driven analysis in this domain. To address this challenge, this paper proposes PathCRF, a framework for detecting on-ball soccer events using only player tracking data. We model player trajectories as a fully connected dynamic graph and formulate event detection as the problem of selecting exactly one edge corresponding to the current possession state at each time step. To ensure logical consistency of the resulting edge sequence, we employ a Conditional Random Field (CRF) that forbids impossible transitions between consecutive edges. Both emission and transition scores dynamically computed from edge embeddings produced by a Set Attention-based backbone architecture. During inference, the most probable edge sequence is obtained via Viterbi decoding, and events such as ball controls or passes are detected whenever the selected edge changes between adjacent time steps. Experiments show that PathCRF produces accurate, logically consistent possession paths, enabling reliable downstream analyses while substantially reducing the need for manual event annotation. The source code is available at https://github.com/hyunsungkim-ds/pathcrf.git.",
    "published": "2026-02-12T15:37:31Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b4b8f61d726f4d43",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards",
    "url": "http://arxiv.org/abs/2602.12049v1",
    "summary": "Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.",
    "published": "2026-02-12T15:12:59Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ae64d52422ba9d97",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Safety Beyond the Training Data: Robust Out-of-Distribution MPC via Conformalized System Level Synthesis",
    "url": "http://arxiv.org/abs/2602.12047v1",
    "summary": "We present a novel framework for robust out-of-distribution planning and control using conformal prediction (CP) and system level synthesis (SLS), addressing the challenge of ensuring safety and robustness when using learned dynamics models beyond the training data distribution. We first derive high-confidence model error bounds using weighted CP with a learned, state-control-dependent covariance model. These bounds are integrated into an SLS-based robust nonlinear model predictive control (MPC) formulation, which performs constraint tightening over the prediction horizon via volume-optimized forward reachable sets. We provide theoretical guarantees on coverage and robustness under distributional drift, and analyze the impact of data density and trajectory tube size on prediction coverage. Empirically, we demonstrate our method on nonlinear systems of increasing complexity, including a 4D car and a {12D} quadcopter, improving safety and robustness compared to fixed-bound and non-robust baselines, especially outside of the data distribution.",
    "published": "2026-02-12T15:11:44Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8c7ff1a869d5d3eb",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Agentic Test-Time Scaling for WebAgents",
    "url": "http://arxiv.org/abs/2602.12276v1",
    "summary": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.",
    "published": "2026-02-12T18:58:30Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "eaa6f891cd3ab809",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "On-Policy Context Distillation for Language Models",
    "url": "http://arxiv.org/abs/2602.12275v1",
    "summary": "Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.",
    "published": "2026-02-12T18:58:28Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "57a72ed8051ad7b3",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
    "url": "http://arxiv.org/abs/2602.12262v1",
    "summary": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.",
    "published": "2026-02-12T18:52:35Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "64844eef3ffe55d0",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication",
    "url": "http://arxiv.org/abs/2602.12251v1",
    "summary": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.",
    "published": "2026-02-12T18:37:23Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "caabe2ff646d06d4",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "\"Sorry, I Didn't Catch That\": How Speech Models Miss What Matters Most",
    "url": "http://arxiv.org/abs/2602.12249v1",
    "summary": "Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.",
    "published": "2026-02-12T18:36:09Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c3a8163196257d7f",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
    "url": "http://arxiv.org/abs/2602.12241v1",
    "summary": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.",
    "published": "2026-02-12T18:20:45Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c93523dac4a3835b",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
    "url": "http://arxiv.org/abs/2602.12237v1",
    "summary": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.",
    "published": "2026-02-12T18:16:05Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ebba05869bbe229f",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
    "url": "http://arxiv.org/abs/2602.12235v1",
    "summary": "Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define \\emph{token overflow} as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.",
    "published": "2026-02-12T18:15:08Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "86ba838023db2f51",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images",
    "url": "http://arxiv.org/abs/2602.12203v1",
    "summary": "Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.",
    "published": "2026-02-12T17:38:57Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1c2fe5a9234dde43",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education",
    "url": "http://arxiv.org/abs/2602.12196v1",
    "summary": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.",
    "published": "2026-02-12T17:29:03Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c729e15d5736d921",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Query-focused and Memory-aware Reranker for Long Context Processing",
    "url": "http://arxiv.org/abs/2602.12192v1",
    "summary": "Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.",
    "published": "2026-02-12T17:23:38Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "68002895637c0e3e",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation",
    "url": "http://arxiv.org/abs/2602.12172v1",
    "summary": "Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.",
    "published": "2026-02-12T17:00:36Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a9e931b288f7117b",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "dVoting: Fast Voting for dLLMs",
    "url": "http://arxiv.org/abs/2602.12153v1",
    "summary": "Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting",
    "published": "2026-02-12T16:35:05Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3217aee848417fa5",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "GPT-4o Lacks Core Features of Theory of Mind",
    "url": "http://arxiv.org/abs/2602.12150v1",
    "summary": "Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.",
    "published": "2026-02-12T16:33:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a377fc68c9896b85",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2602.12146v1",
    "summary": "Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.",
    "published": "2026-02-12T16:30:55Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "244848e560223042",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes",
    "url": "http://arxiv.org/abs/2602.12137v1",
    "summary": "City councils play a crucial role in local governance, directly influencing citizens' daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ultimately limit the development of computational models. To address this gap, we introduce CitiLink-Minutes, a multilayer dataset of 120 European Portuguese municipal meeting minutes from six municipalities. Unlike prior annotated datasets of parliamentary or video records, CitiLink-Minutes provides multilayer annotations and structured linkage of official written minutes. The dataset contains over one million tokens, with all personal identifiers de-identified. Each minute was manually annotated by two trained annotators and curated by an experienced linguist across three complementary dimensions: (1) metadata, (2) subjects of discussion, and (3) voting outcomes, totaling over 38,000 individual annotations. Released under FAIR principles and accompanied by baseline results on metadata extraction, topic classification, and vote labeling, CitiLink-Minutes demonstrates its potential for downstream NLP and IR tasks, while promoting transparent access to municipal decisions.",
    "published": "2026-02-12T16:22:55Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "57b245c8771937b5",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models",
    "url": "http://arxiv.org/abs/2602.12135v1",
    "summary": "With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark designed to evaluate realistic conversational abilities where prior works fall short. Uniquely, WavBench establishes a tripartite framework: 1) Pro subset, designed to rigorously challenge reasoning-enhanced models with significantly increased difficulty; 2) Basic subset, defining a novel standard for spoken colloquialism that prioritizes \"listenability\" through natural vocabulary, linguistic fluency, and interactive rapport, rather than rigid written accuracy; and 3) Acoustic subset, covering explicit understanding, generation, and implicit dialogue to rigorously evaluate comprehensive paralinguistic capabilities within authentic real-world scenarios. Through evaluating five state-of-the-art models, WavBench offers critical insights into the intersection of complex problem-solving, colloquial delivery, and paralinguistic fidelity, guiding the evolution of robust spoken dialogue models. The benchmark dataset and evaluation toolkit are available at https://naruto-2024.github.io/wavbench.github.io/.",
    "published": "2026-02-12T16:22:11Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "60b9b703744f9058",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5",
    "url": "http://arxiv.org/abs/2602.12133v1",
    "summary": "This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), PERLA, and Fitzpatrick scales. Neutral prompts produced highly polarized defaults. Both models exhibited a strong \"default white\" bias (>96% of outputs). However, they diverged sharply on gender: Gemini favored female-presenting subjects, while GPT favored male-presenting subjects with lighter skin tones. This research provides a large-scale, comparative audit of state-of-the-art models using an illumination-aware colorimetric methodology, distinguishing aesthetic rendering from underlying pigmentation in synthetic imagery. The study demonstrates that neutral prompts function as diagnostic probes rather than neutral instructions. It offers a robust framework for auditing algorithmic visual culture and challenges the sociolinguistic assumption that unmarked language results in inclusive representation.",
    "published": "2026-02-12T16:21:03Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d13ad78588ee97fa",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "A Rule-based Computational Model for Gaidhlig Morphology",
    "url": "http://arxiv.org/abs/2602.12132v1",
    "summary": "Language models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource languages. This paper describes work-in-progress to construct a rule-based model of Gaidhlig morphology using data from Wiktionary, arguing that rule-based systems effectively leverage limited sample data, support greater interpretability, and provide insights useful in the design of teaching materials. The use of SQL for querying the occurrence of different lexical patterns is investigated, and a declarative rule-base is presented that allows Python utilities to derive inflected forms of Gaidhlig words. This functionality could be used to support educational tools that teach or explain language patterns, for example, or to support higher level tools such as rule-based dependency parsers. This approach adds value to the data already present in Wiktionary by adapting it to new use-cases.",
    "published": "2026-02-12T16:20:17Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9f143285801f0d81",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "url": "http://arxiv.org/abs/2602.12125v1",
    "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
    "published": "2026-02-12T16:14:29Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "20ff53169d96ab48",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Capability-Oriented Training Induced Alignment Risk",
    "url": "http://arxiv.org/abs/2602.12124v1",
    "summary": "While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse \"vulnerability games\", each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow \"tricks\" but generalizable skills; they can be transferred to new tasks and even \"distilled\" from a capable teacher model to other student models through data alone. Our findings reveal that capability-oriented training induced risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves. Code is available at https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk.",
    "published": "2026-02-12T16:13:14Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "34b07b3324d2fa26",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning",
    "url": "http://arxiv.org/abs/2602.12123v1",
    "summary": "Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.\n  Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.\n  Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.",
    "published": "2026-02-12T16:11:29Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "862a981a653f47b9",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
    "url": "http://arxiv.org/abs/2602.12116v1",
    "summary": "Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.",
    "published": "2026-02-12T16:07:22Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "01866f9acfa12268",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty",
    "url": "http://arxiv.org/abs/2602.12113v1",
    "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at https://github.com/ZeweiYu1/ARLCP .",
    "published": "2026-02-12T16:04:00Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "27f8d5565b410049",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "url": "http://arxiv.org/abs/2602.12092v1",
    "summary": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
    "published": "2026-02-12T15:43:14Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d5cbf8c5cf4c5e81",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Tiny Recursive Reasoning with Mamba-2 Attention Hybrid",
    "url": "http://arxiv.org/abs/2602.12078v1",
    "summary": "Recent work on recursive reasoning models like TRM demonstrates that tiny networks (7M parameters) can achieve strong performance on abstract reasoning tasks through latent recursion -- iterative refinement in hidden representation space without emitting intermediate tokens. This raises a natural question about operator choice: Mamba-2's state space recurrence is itself a form of iterative refinement, making it a natural candidate for recursive reasoning -- but does introducing Mamba-2 into the recursive scaffold preserve reasoning capability? We investigate this by replacing the Transformer blocks in TRM with Mamba-2 hybrid operators while maintaining parameter parity (6.83M vs 6.86M parameters). On ARC-AGI-1, we find that the hybrid improves pass@2 (the official metric) by +2.0\\% (45.88\\% vs 43.88\\%) and consistently outperforms at higher K values (+4.75\\% at pass@100), whilst maintaining pass@1 parity. This suggests improved candidate coverage -- the model generates correct solutions more reliably -- with similar top-1 selection. Our results validate that Mamba-2 hybrid operators preserve reasoning capability within the recursive scaffold, establishing SSM-based operators as viable candidates in the recursive operator design space and taking a first step towards understanding the best mixing strategies for recursive reasoning.",
    "published": "2026-02-12T15:36:32Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9d0adf3ee62df6a2",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models",
    "url": "http://arxiv.org/abs/2602.12036v1",
    "summary": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.",
    "published": "2026-02-12T15:03:37Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "14215270c6de5680",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Artificial intelligence is creating a new global linguistic hierarchy",
    "url": "http://arxiv.org/abs/2602.12018v1",
    "summary": "Artificial intelligence (AI) has the potential to transform healthcare, education, governance and socioeconomic equity, but its benefits remain concentrated in a small number of languages (Bender, 2019; Blasi et al., 2022; Joshi et al., 2020; Ranathunga and de Silva, 2022; Young, 2015). Language AI - the technologies that underpin widely-used conversational systems such as ChatGPT - could provide major benefits if available in people's native languages, yet most of the world's 7,000+ linguistic communities currently lack access and face persistent digital marginalization. Here we present a global longitudinal analysis of social, economic and infrastructural conditions across languages to assess systemic inequalities in language AI. We first analyze the existence of AI resources for 6003 languages. We find that despite efforts of the community to broaden the reach of language technologies (Bapna et al., 2022; Costa-Jussà et al., 2022), the dominance of a handful of languages is exacerbating disparities on an unprecedented scale, with divides widening exponentially rather than narrowing. Further, we contrast the longitudinal diffusion of AI with that of earlier IT technologies, revealing a distinctive hype-driven pattern of spread. To translate our findings into practical insights and guide prioritization efforts, we introduce the Language AI Readiness Index (EQUATE), which maps the state of technological, socio-economic, and infrastructural prerequisites for AI deployment across languages. The index highlights communities where capacity exists but remains underutilized, and provides a framework for accelerating more equitable diffusion of language AI. Our work contributes to setting the baseline for a transition towards more sustainable and equitable language technologies.",
    "published": "2026-02-12T14:50:44Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "09c8a1b0ae432b46",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "Disentangling Ambiguity from Instability in Large Language Models: A Clinical Text-to-SQL Case Study",
    "url": "http://arxiv.org/abs/2602.12015v1",
    "summary": "Deploying large language models for clinical Text-to-SQL requires distinguishing two qualitatively different causes of output diversity: (i) input ambiguity that should trigger clarification, and (ii) model instability that should trigger human review. We propose CLUES, a framework that models Text-to-SQL as a two-stage process (interpretations --> answers) and decomposes semantic uncertainty into an ambiguity score and an instability score. The instability score is computed via the Schur complement of a bipartite semantic graph matrix. Across AmbigQA/SituatedQA (gold interpretations) and a clinical Text-to-SQL benchmark (known interpretations), CLUES improves failure prediction over state-of-the-art Kernel Language Entropy. In deployment settings, it remains competitive while providing a diagnostic decomposition unavailable from a single score. The resulting uncertainty regimes map to targeted interventions - query refinement for ambiguity, model improvement for instability. The high-ambiguity/high-instability regime contains 51% of errors while covering 25% of queries, enabling efficient triage.",
    "published": "2026-02-12T14:46:20Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "20aeeff4919c05af",
    "source": "arxiv_cs_cl",
    "source_weight": 0.8,
    "title": "LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss",
    "url": "http://arxiv.org/abs/2602.12005v1",
    "summary": "Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \\emph{which tokens an SLM can and should learn} during pretraining, versus \\emph{which ones it should delegate} via a \\texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \\emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \\texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.",
    "published": "2026-02-12T14:37:25Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c16b69a1be247646",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "GPT-5.2 derives a new result in theoretical physics",
    "url": "https://openai.com/index/new-result-theoretical-physics",
    "summary": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
    "published": "Fri, 13 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c5ef81aef2a2ccc1",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
    "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt",
    "summary": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT to help organizations defend against prompt injection and AI-driven data exfiltration.",
    "published": "Fri, 13 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9d95a891a81b27c3",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Beyond rate limits: scaling access to Codex and Sora",
    "url": "https://openai.com/index/beyond-rate-limits",
    "summary": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
    "published": "Fri, 13 Feb 2026 09:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c10ff8c0943e4e07",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Scaling social science research",
    "url": "https://openai.com/index/scaling-social-science-research",
    "summary": "GABRIEL is a new open-source toolkit from OpenAI that uses GPT to turn qualitative text and images into quantitative data, helping social scientists analyze research at scale.",
    "published": "Fri, 13 Feb 2026 09:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e090493a0ff267ce",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing GPT-5.3-Codex-Spark",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex-spark",
    "summary": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
    "published": "Thu, 12 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c35812ccca3ce7be",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Harness engineering: leveraging Codex in an agent-first world",
    "url": "https://openai.com/index/harness-engineering",
    "summary": "By Ryan Lopopolo, Member of the Technical Staff",
    "published": "Wed, 11 Feb 2026 09:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4e730871994fa83c",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Testing ads in ChatGPT",
    "url": "https://openai.com/index/testing-ads-in-chatgpt",
    "summary": "OpenAI begins testing ads in ChatGPT to support free access, with clear labeling, answer independence, strong privacy protections, and user control.",
    "published": "Mon, 09 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "75bb2edf937a96ea",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Bringing ChatGPT to GenAI.mil",
    "url": "https://openai.com/index/bringing-chatgpt-to-genaimil",
    "summary": "OpenAI for Government announces the deployment of a custom ChatGPT on GenAI.mil, bringing secure, safety-forward AI to U.S. defense teams.",
    "published": "Mon, 09 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ebf24cf04fa0982c",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Making AI work for everyone, everywhere: our approach to localization",
    "url": "https://openai.com/index/our-approach-to-localization",
    "summary": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
    "published": "Fri, 06 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c82ec79fbf9f9804",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "GPT-5 lowers the cost of cell-free protein synthesis",
    "url": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
    "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
    "published": "Thu, 05 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4e829743aa85afb8",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing Trusted Access for Cyber",
    "url": "https://openai.com/index/trusted-access-for-cyber",
    "summary": "OpenAI introduces Trusted Access for Cyber, a trust-based framework that expands access to frontier cyber capabilities while strengthening safeguards against misuse.",
    "published": "Thu, 05 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ae167fa7c076bdf8",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing OpenAI Frontier",
    "url": "https://openai.com/index/introducing-openai-frontier",
    "summary": "OpenAI Frontier is an enterprise platform for building, deploying, and managing AI agents with shared context, onboarding, permissions, and governance.",
    "published": "Thu, 05 Feb 2026 06:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6b46382980c79e4a",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Navigating health questions with ChatGPT",
    "url": "https://openai.com/index/navigating-health-questions",
    "summary": "A family shares how ChatGPT helped them prepare for critical cancer treatment decisions for their son alongside expert guidance from his doctors.",
    "published": "Thu, 05 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7bd3c882dcf7e77a",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing GPT-5.3-Codex",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex",
    "summary": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
    "published": "Thu, 05 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b2f960344448d2d5",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "GPT-5.3-Codex System Card",
    "url": "https://openai.com/index/gpt-5-3-codex-system-card",
    "summary": "GPT‑5.3-Codex is the most capable agentic coding model to date, combining the frontier coding performance of GPT‑5.2-Codex with the reasoning and professional knowledge capabilities of GPT‑5.2.",
    "published": "Thu, 05 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cf0ef71eaded1866",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Unlocking the Codex harness: how we built the App Server",
    "url": "https://openai.com/index/unlocking-the-codex-harness",
    "summary": "Learn how to embed the Codex agent using the Codex App Server, a bidirectional JSON-RPC API powering streaming progress, tool use, approvals, and diffs.",
    "published": "Wed, 04 Feb 2026 13:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f68e038d8548a39e",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "VfL Wolfsburg turns ChatGPT into a club-wide capability",
    "url": "https://openai.com/index/vfl-wolfsburg",
    "summary": "By focusing on people, not pilots, the Bundesliga club is scaling efficiency, creativity, and knowledge—without losing its football identity.",
    "published": "Wed, 04 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "26e2757a88ef178e",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "The Sora feed philosophy",
    "url": "https://openai.com/index/sora-feed-philosophy",
    "summary": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
    "published": "Tue, 03 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "974dde3d72e6bf95",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Snowflake and OpenAI partner to bring frontier intelligence to enterprise data",
    "url": "https://openai.com/index/snowflake-partnership",
    "summary": "OpenAI and Snowflake partner in a $200M agreement to bring frontier intelligence into enterprise data, enabling AI agents and insights directly in Snowflake.",
    "published": "Mon, 02 Feb 2026 06:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c2e00d7c926c0f28",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing the Codex app",
    "url": "https://openai.com/index/introducing-the-codex-app",
    "summary": "Introducing the Codex app for macOS—a command center for AI coding and software development with multiple agents, parallel workflows, and long-running tasks.",
    "published": "Mon, 02 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a6861025e0f1a141",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Inside OpenAI’s in-house data agent",
    "url": "https://openai.com/index/inside-our-in-house-data-agent",
    "summary": "How OpenAI built an in-house AI data agent that uses GPT-5, Codex, and memory to reason over massive datasets and deliver reliable insights in minutes.",
    "published": "Thu, 29 Jan 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8af76a98aaedba17",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT",
    "url": "https://openai.com/index/retiring-gpt-4o-and-older-models",
    "summary": "On February 13, 2026, alongside the previously announced retirement⁠ of GPT‑5 (Instant, Thinking, and Pro), we will retire GPT‑4o, GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini from ChatGPT. In the API, there are no changes at this time.",
    "published": "Thu, 29 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cacb53a143831134",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Taisei Corporation shapes the next generation of talent with ChatGPT",
    "url": "https://openai.com/index/taisei",
    "summary": "Taisei Corporation uses ChatGPT Enterprise to support HR-led talent development and scale generative AI across its global construction business.",
    "published": "Thu, 29 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "92ce58d187a17ee3",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "The next chapter for AI in the EU",
    "url": "https://openai.com/index/the-next-chapter-for-ai-in-the-eu",
    "summary": "OpenAI launches the EU Economic Blueprint 2.0 with new data, partnerships, and initiatives to accelerate AI adoption, skills, and growth across Europe.",
    "published": "Wed, 28 Jan 2026 01:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b2fec04c8337daa6",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "EMEA Youth & Wellbeing Grant",
    "url": "https://openai.com/index/emea-youth-and-wellbeing-grant",
    "summary": "Apply for the EMEA Youth & Wellbeing Grant, a €500,000 program funding NGOs and researchers advancing youth safety and wellbeing in the age of AI.",
    "published": "Wed, 28 Jan 2026 01:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "443337d9f1fab6df",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Keeping your data safe when an AI agent clicks a link",
    "url": "https://openai.com/index/ai-agent-link-safety",
    "summary": "Learn how OpenAI protects user data when AI agents open links, preventing URL-based data exfiltration and prompt injection with built-in safeguards.",
    "published": "Wed, 28 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3d62d157a9fb33c0",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "PVH reimagines the future of fashion with OpenAI",
    "url": "https://openai.com/index/pvh-future-of-fashion",
    "summary": "PVH Corp., parent company of Calvin Klein and Tommy Hilfiger, is adopting ChatGPT Enterprise to bring AI into fashion design, supply chain, and consumer engagement.",
    "published": "Tue, 27 Jan 2026 06:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2b9a95ae28f5d889",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing Prism",
    "url": "https://openai.com/index/introducing-prism",
    "summary": "Prism is a free LaTeX-native workspace with GPT-5.2 built in, helping researchers write, collaborate, and reason in one place.",
    "published": "Tue, 27 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c6bbb381c4c62cfe",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Powering tax donations with AI powered personalized recommendations",
    "url": "https://openai.com/index/trustbank",
    "summary": "TRUSTBANK partnered with Recursive to build Choice AI using OpenAI models, delivering personalized, conversational recommendations that simplify Furusato Nozei gift discovery. A multi-agent system helps donors navigate thousands of options and find gifts that match their preferences.",
    "published": "Tue, 27 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "113ca482302d82d2",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "How Indeed uses AI to help evolve the job search",
    "url": "https://openai.com/index/indeed-maggie-hulce",
    "summary": "Indeed’s CRO Maggie Hulce shares how AI is transforming job search, recruiting, and talent acquisition for employers and job seekers.",
    "published": "Mon, 26 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b61d2d12af8f8b49",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Unrolling the Codex agent loop",
    "url": "https://openai.com/index/unrolling-the-codex-agent-loop",
    "summary": "A technical deep dive into the Codex agent loop, explaining how Codex CLI orchestrates models, tools, prompts, and performance using the Responses API.",
    "published": "Fri, 23 Jan 2026 12:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b518ddec0acdec8d",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Scaling PostgreSQL to power 800 million ChatGPT users",
    "url": "https://openai.com/index/scaling-postgresql",
    "summary": "An inside look at how OpenAI scaled PostgreSQL to millions of queries per second using replicas, caching, rate limiting, and workload isolation.",
    "published": "Thu, 22 Jan 2026 12:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "06b92625ece2ea5c",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Inside Praktika's conversational approach to language learning",
    "url": "https://openai.com/index/praktika",
    "summary": "How Praktika uses GPT-4.1 and GPT-5.2 to build adaptive AI tutors that personalize lessons, track progress, and help learners achieve real-world language fluency",
    "published": "Thu, 22 Jan 2026 05:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "51f698ecb8840e49",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Inside GPT-5 for Work: How Businesses Use GPT-5",
    "url": "https://openai.com/business/guides-and-resources/chatgpt-usage-and-adoption-patterns-at-work",
    "summary": "A data-driven report on how workers across industries use ChatGPT—covering adoption trends, top tasks, departmental patterns, and the future of AI at work.",
    "published": "Thu, 22 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7e790b947fa69980",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "How Higgsfield turns simple ideas into cinematic social videos",
    "url": "https://openai.com/index/higgsfield",
    "summary": "Discover how Higgsfield gives creators cinematic, social-first video output from simple inputs using OpenAI GPT-4.1, GPT-5, and Sora 2.",
    "published": "Wed, 21 Jan 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fa2fed92141bd20c",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing Edu for Countries",
    "url": "https://openai.com/index/edu-for-countries",
    "summary": "Edu for Countries is a new OpenAI initiative helping governments use AI to modernize education systems and build future-ready workforces.",
    "published": "Wed, 21 Jan 2026 01:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "469f9a6719278f63",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "How countries can end the capability overhang",
    "url": "https://openai.com/index/how-countries-can-end-the-capability-overhang",
    "summary": "Our latest report reveals stark differences in advanced AI adoption across countries and outlines new initiatives to help nations capture productivity gains from AI.",
    "published": "Wed, 21 Jan 2026 01:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ed986a99514cf288",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Horizon 1000: Advancing AI for primary healthcare",
    "url": "https://openai.com/index/horizon-1000",
    "summary": "OpenAI and the Gates Foundation launch Horizon 1000, a $50M pilot advancing AI capabilities for healthcare in Africa. The initiative aims to reach 1,000 clinics by 2028.",
    "published": "Tue, 20 Jan 2026 21:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "20b1e4b4fbc6fb96",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Stargate Community",
    "url": "https://openai.com/index/stargate-community",
    "summary": "Stargate Community plans detail a community-first approach to AI infrastructure, using locally tailored plans shaped by community input, energy needs, and workforce priorities.",
    "published": "Tue, 20 Jan 2026 19:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3aa5db1070347228",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Cisco and OpenAI redefine enterprise engineering with AI agents",
    "url": "https://openai.com/index/cisco",
    "summary": "Cisco and OpenAI redefine enterprise engineering with Codex, an AI software agent embedded in workflows to speed builds, automate defect fixes, and enable AI-native development.",
    "published": "Tue, 20 Jan 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f09c45ee226de24a",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Chris Liddell Appointed Anthropic Board",
    "url": "https://www.anthropic.com/news/chris-liddell-appointed-anthropic-board",
    "summary": "",
    "published": "2026-02-13T16:21:14.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b91259f7d1a90da4",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Codepath Partnership",
    "url": "https://www.anthropic.com/news/anthropic-codepath-partnership",
    "summary": "",
    "published": "2026-02-13T16:19:50.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a76c02cb3f5a6457",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude For Financial Services",
    "url": "https://www.anthropic.com/news/claude-for-financial-services",
    "summary": "",
    "published": "2026-02-13T15:35:49.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "61bc1bd4729cda9c",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Raises 30 Billion Series G Funding 380 Billion Post Money Valuation",
    "url": "https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation",
    "summary": "",
    "published": "2026-02-12T21:43:18.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e2fb706f0e744611",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Donate Public First Action",
    "url": "https://www.anthropic.com/news/donate-public-first-action",
    "summary": "",
    "published": "2026-02-12T14:45:37.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "dc134a75ef857e94",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Healthcare Life Sciences",
    "url": "https://www.anthropic.com/news/healthcare-life-sciences",
    "summary": "",
    "published": "2026-02-11T22:31:33.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d90c001d10ad2d99",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Covering Electricity Price Increases",
    "url": "https://www.anthropic.com/news/covering-electricity-price-increases",
    "summary": "",
    "published": "2026-02-11T21:08:31.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "42710d92908034f2",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude Opus 4 6",
    "url": "https://www.anthropic.com/news/claude-opus-4-6",
    "summary": "",
    "published": "2026-02-10T19:07:24.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "92f2a5d3a3a7ce4c",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude For Nonprofits",
    "url": "https://www.anthropic.com/news/claude-for-nonprofits",
    "summary": "",
    "published": "2026-02-09T15:51:38.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "adc962dc0eb8fb5f",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude Is A Space To Think",
    "url": "https://www.anthropic.com/news/claude-is-a-space-to-think",
    "summary": "",
    "published": "2026-02-04T17:29:42.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "87e9dc91cff0bded",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Protecting Well Being Of Users",
    "url": "https://www.anthropic.com/news/protecting-well-being-of-users",
    "summary": "",
    "published": "2026-02-03T22:04:42.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "da97f24efa77bffc",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Apple Xcode Claude Agent Sdk",
    "url": "https://www.anthropic.com/news/apple-xcode-claude-agent-sdk",
    "summary": "",
    "published": "2026-02-03T18:06:03.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "22ae838c5b8585d0",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Partners With Allen Institute And Howard Hughes Medical Institute",
    "url": "https://www.anthropic.com/news/anthropic-partners-with-allen-institute-and-howard-hughes-medical-institute",
    "summary": "",
    "published": "2026-02-02T14:22:01.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1698fab210cfb462",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Servicenow Anthropic Claude",
    "url": "https://www.anthropic.com/news/servicenow-anthropic-claude",
    "summary": "",
    "published": "2026-01-28T22:27:37.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6c270de312aa4702",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Gov Uk Partnership",
    "url": "https://www.anthropic.com/news/gov-UK-partnership",
    "summary": "",
    "published": "2026-01-27T17:47:33.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d1090b0c93d0ddc9",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claudes Constitution",
    "url": "https://www.anthropic.com/news/claudes-constitution",
    "summary": "",
    "published": "2026-01-21T18:26:10.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5adc396ab77c76e2",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Teach For All",
    "url": "https://www.anthropic.com/news/anthropic-teach-for-all",
    "summary": "",
    "published": "2026-01-21T17:36:00.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "effb3780b4481f1c",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude New Constitution",
    "url": "https://www.anthropic.com/news/claude-new-constitution",
    "summary": "",
    "published": "2026-01-21T16:34:47.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1ad8b4f49514b15c",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Mariano Florentino Long Term Benefit Trust",
    "url": "https://www.anthropic.com/news/mariano-florentino-long-term-benefit-trust",
    "summary": "",
    "published": "2026-01-20T15:09:28.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a625cc82b9c13a98",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Appoints Irina Ghose As Managing Director Of India",
    "url": "https://www.anthropic.com/news/anthropic-appoints-irina-ghose-as-managing-director-of-india",
    "summary": "",
    "published": "2026-01-16T03:28:16.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bae48fe420c0151d",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Accelerating Scientific Research",
    "url": "https://www.anthropic.com/news/accelerating-scientific-research",
    "summary": "",
    "published": "2026-01-15T22:49:21.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "414ab5999dc1d268",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Introducing Anthropic Labs",
    "url": "https://www.anthropic.com/news/introducing-anthropic-labs",
    "summary": "",
    "published": "2026-01-13T22:46:14.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0d91496d44e61262",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Prompting Long Context",
    "url": "https://www.anthropic.com/news/prompting-long-context",
    "summary": "",
    "published": "2026-01-06T15:23:18.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ca06aa5da94ea9da",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Compliance Framework Sb53",
    "url": "https://www.anthropic.com/news/compliance-framework-SB53",
    "summary": "",
    "published": "2025-12-19T21:05:06.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "58514db118b48352",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Genesis Mission Partnership",
    "url": "https://www.anthropic.com/news/genesis-mission-partnership",
    "summary": "",
    "published": "2025-12-18T23:06:17.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a9ab1bcc95b77ca2",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "How People Use Claude For Support Advice And Companionship",
    "url": "https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship",
    "summary": "",
    "published": "2025-12-12T01:47:15.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fea9572459fb3562",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Donating The Model Context Protocol And Establishing Of The Agentic Ai Foundation",
    "url": "https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation",
    "summary": "",
    "published": "2025-12-11T18:14:55.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0e449fb1ae69d06a",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Political Even Handedness",
    "url": "https://www.anthropic.com/news/political-even-handedness",
    "summary": "",
    "published": "2025-12-10T00:18:44.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3ab707c82df25f07",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Accenture Partnership",
    "url": "https://www.anthropic.com/news/anthropic-accenture-partnership",
    "summary": "",
    "published": "2025-12-09T17:55:19.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cca01521c3a186d1",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Economic Index Insights From Claude Sonnet 3 7",
    "url": "https://www.anthropic.com/news/anthropic-economic-index-insights-from-claude-sonnet-3-7",
    "summary": "",
    "published": "2025-12-07T14:53:39.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0ca73ca8a8d841c6",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "The Anthropic Economic Index",
    "url": "https://www.anthropic.com/news/the-anthropic-economic-index",
    "summary": "",
    "published": "2025-12-07T14:53:27.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a39a5fb15249a533",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Snowflake Anthropic Expanded Partnership",
    "url": "https://www.anthropic.com/news/snowflake-anthropic-expanded-partnership",
    "summary": "",
    "published": "2025-12-03T21:13:50.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "40cd6a2be347c3bf",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Signs Cms Health Tech Ecosystem Pledge To Advance Healthcare Interoperability",
    "url": "https://www.anthropic.com/news/anthropic-signs-cms-health-tech-ecosystem-pledge-to-advance-healthcare-interoperability",
    "summary": "",
    "published": "2025-12-02T23:19:08.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d39c75c646da99d7",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude Opus 4 5",
    "url": "https://www.anthropic.com/news/claude-opus-4-5",
    "summary": "",
    "published": "2025-12-02T22:16:48.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0e5f5f5ac0abd2dc",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Acquires Bun As Claude Code Reaches Usd1B Milestone",
    "url": "https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone",
    "summary": "",
    "published": "2025-12-02T18:17:32.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f5beac66a9d5b4d3",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Education Report How Educators Use Claude",
    "url": "https://www.anthropic.com/news/anthropic-education-report-how-educators-use-claude",
    "summary": "",
    "published": "2025-11-22T20:40:06.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b42cc076232e9883",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude Sonnet 4 5",
    "url": "https://www.anthropic.com/news/claude-sonnet-4-5",
    "summary": "",
    "published": "2025-11-20T16:26:28.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "70dc6d57fbc42f80",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Thoughts On America S Ai Action Plan",
    "url": "https://www.anthropic.com/news/thoughts-on-america-s-ai-action-plan",
    "summary": "",
    "published": "2025-11-20T16:26:24.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f87abd86265a3a5d",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Raises Series F At Usd183B Post Money Valuation",
    "url": "https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation",
    "summary": "",
    "published": "2025-11-20T16:26:11.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "65078d3793b46c6f",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude Haiku 4 5",
    "url": "https://www.anthropic.com/news/claude-haiku-4-5",
    "summary": "",
    "published": "2025-11-20T16:25:50.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "65b08ecd50d2a05b",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Microsoft Nvidia Anthropic Announce Strategic Partnerships",
    "url": "https://www.anthropic.com/news/microsoft-nvidia-anthropic-announce-strategic-partnerships",
    "summary": "",
    "published": "2025-11-18T18:17:07.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4b11d6fcc5cde12e",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude In Microsoft Foundry",
    "url": "https://www.anthropic.com/news/claude-in-microsoft-foundry",
    "summary": "",
    "published": "2025-11-18T16:52:14.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7416f8b164e9987c",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Rwandan Government Partnership Ai Education",
    "url": "https://www.anthropic.com/news/rwandan-government-partnership-ai-education",
    "summary": "",
    "published": "2025-11-17T13:29:53.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3943a3ed768fbeee",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Disrupting Ai Espionage",
    "url": "https://www.anthropic.com/news/disrupting-AI-espionage",
    "summary": "",
    "published": "2025-11-14T11:25:56.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a835a61ba42351d0",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Maryland Partnership",
    "url": "https://www.anthropic.com/news/maryland-partnership",
    "summary": "",
    "published": "2025-11-13T17:57:34.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "88dc42a407df8994",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "New Offices In Paris And Munich Expand European Presence",
    "url": "https://www.anthropic.com/news/new-offices-in-paris-and-munich-expand-european-presence",
    "summary": "",
    "published": "2025-11-12T18:10:23.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e8193ce5e3db5513",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Invests 50 Billion In American Ai Infrastructure",
    "url": "https://www.anthropic.com/news/anthropic-invests-50-billion-in-american-ai-infrastructure",
    "summary": "",
    "published": "2025-11-12T16:56:05.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "15084be5e0440817",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Cognizant Partnership",
    "url": "https://www.anthropic.com/news/cognizant-partnership",
    "summary": "",
    "published": "2025-11-06T20:57:55.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9e268111a73dd856",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Economic Futures Uk Europe",
    "url": "https://www.anthropic.com/news/economic-futures-uk-europe",
    "summary": "",
    "published": "2025-11-05T07:57:22.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "26eb897cc98a6111",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic And Iceland Announce One Of The World S First National Ai Education Pilots",
    "url": "https://www.anthropic.com/news/anthropic-and-iceland-announce-one-of-the-world-s-first-national-ai-education-pilots",
    "summary": "",
    "published": "2025-11-04T07:36:15.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "da4441c2e4c27664",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Opening Our Tokyo Office",
    "url": "https://www.anthropic.com/news/opening-our-tokyo-office",
    "summary": "",
    "published": "2025-10-29T21:42:18.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "59d3b01ed7721abe",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Advancing Claude For Financial Services",
    "url": "https://www.anthropic.com/news/advancing-claude-for-financial-services",
    "summary": "",
    "published": "2025-10-27T19:47:26.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fe873d94ab95b6f0",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Seoul Becomes Third Anthropic Office In Asia Pacific",
    "url": "https://www.anthropic.com/news/seoul-becomes-third-anthropic-office-in-asia-pacific",
    "summary": "",
    "published": "2025-10-23T23:11:18.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c31d7a98e1c0abbc",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Expanding Our Use Of Google Cloud Tpus And Services",
    "url": "https://www.anthropic.com/news/expanding-our-use-of-google-cloud-tpus-and-services",
    "summary": "",
    "published": "2025-10-23T20:34:59.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "eebced64aedd3b8c",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Statement Dario Amodei American Ai Leadership",
    "url": "https://www.anthropic.com/news/statement-dario-amodei-american-ai-leadership",
    "summary": "",
    "published": "2025-10-21T14:30:53.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b48533d3b12ff543",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Claude For Life Sciences",
    "url": "https://www.anthropic.com/news/claude-for-life-sciences",
    "summary": "",
    "published": "2025-10-21T02:53:49.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fb02ab0e33ecb870",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Salesforce Anthropic Expanded Partnership",
    "url": "https://www.anthropic.com/news/salesforce-anthropic-expanded-partnership",
    "summary": "",
    "published": "2025-10-14T12:02:03.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e3d23a17207a0019",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Expanding Global Operations To India",
    "url": "https://www.anthropic.com/news/expanding-global-operations-to-india",
    "summary": "",
    "published": "2025-10-08T00:58:04.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6217bc3645bac073",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Rahul Patil Joins Anthropic",
    "url": "https://www.anthropic.com/news/rahul-patil-joins-anthropic",
    "summary": "",
    "published": "2025-10-07T19:17:27.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d77e7220341d1aa0",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Deloitte Anthropic Partnership",
    "url": "https://www.anthropic.com/news/deloitte-anthropic-partnership",
    "summary": "",
    "published": "2025-10-06T20:38:02.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9f6a0346a31359dc",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Building C Compiler",
    "url": "https://www.anthropic.com/engineering/building-c-compiler",
    "summary": "",
    "published": "2026-02-05T19:38:29.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "912cf25fe3586ccd",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Infrastructure Noise",
    "url": "https://www.anthropic.com/engineering/infrastructure-noise",
    "summary": "",
    "published": "2026-02-05T17:42:29.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d689d52beed2dd22",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Demystifying Evals For Ai Agents",
    "url": "https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents",
    "summary": "",
    "published": "2026-02-04T18:45:28.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1ec70862d85c1973",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Claude Code Best Practices",
    "url": "https://www.anthropic.com/engineering/claude-code-best-practices",
    "summary": "",
    "published": "2026-01-26T23:24:56.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bef6fafffc802698",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Ai Resistant Technical Evaluations",
    "url": "https://www.anthropic.com/engineering/AI-resistant-technical-evaluations",
    "summary": "",
    "published": "2026-01-22T01:16:20.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e2b90dd8fabfd8f2",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Contextual Retrieval",
    "url": "https://www.anthropic.com/engineering/contextual-retrieval",
    "summary": "",
    "published": "2026-01-07T15:00:44.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "512a12b7ae75ff77",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Effective Context Engineering For Ai Agents",
    "url": "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
    "summary": "",
    "published": "2026-01-06T15:31:12.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "88027f7315d59a33",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Building Effective Agents",
    "url": "https://www.anthropic.com/engineering/building-effective-agents",
    "summary": "",
    "published": "2026-01-06T15:24:44.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4bffd18b76c47d13",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Multi Agent Research System",
    "url": "https://www.anthropic.com/engineering/multi-agent-research-system",
    "summary": "",
    "published": "2026-01-06T15:09:32.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "13d851a818b9d01a",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Writing Tools For Agents",
    "url": "https://www.anthropic.com/engineering/writing-tools-for-agents",
    "summary": "",
    "published": "2026-01-06T15:05:23.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9c6963896a79a683",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Claude Think Tool",
    "url": "https://www.anthropic.com/engineering/claude-think-tool",
    "summary": "",
    "published": "2025-12-15T18:02:25.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "317293c01f1d2a57",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "A Postmortem Of Three Recent Issues",
    "url": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
    "summary": "",
    "published": "2025-12-14T20:27:33.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a5a54faf2dba6740",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Effective Harnesses For Long Running Agents",
    "url": "https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents",
    "summary": "",
    "published": "2025-11-26T18:07:21.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "befa65b8a744151e",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Advanced Tool Use",
    "url": "https://www.anthropic.com/engineering/advanced-tool-use",
    "summary": "",
    "published": "2025-11-25T16:53:53.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8761099c9d017166",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Code Execution With Mcp",
    "url": "https://www.anthropic.com/engineering/code-execution-with-mcp",
    "summary": "",
    "published": "2025-11-04T22:06:04.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cf1f68524d89bd5b",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Claude Code Sandboxing",
    "url": "https://www.anthropic.com/engineering/claude-code-sandboxing",
    "summary": "",
    "published": "2025-11-03T21:01:45.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "93f054b6e0ffd331",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Desktop Extensions",
    "url": "https://www.anthropic.com/engineering/desktop-extensions",
    "summary": "",
    "published": "2025-09-12T00:43:28.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "96ce3fe2047170df",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Swe Bench Sonnet",
    "url": "https://www.anthropic.com/engineering/swe-bench-sonnet",
    "summary": "",
    "published": "2025-03-19T20:43:04.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bc475ac3fcd28ab6",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Ai Assistance Coding Skills",
    "url": "https://www.anthropic.com/research/AI-assistance-coding-skills",
    "summary": "",
    "published": "2026-02-05T00:34:35.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a4a5412f81884cb3",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Economic Index Primitives",
    "url": "https://www.anthropic.com/research/economic-index-primitives",
    "summary": "",
    "published": "2026-02-03T15:57:16.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2a08adab75ed6739",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Disempowerment Patterns",
    "url": "https://www.anthropic.com/research/disempowerment-patterns",
    "summary": "",
    "published": "2026-01-28T22:01:01.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "122e113ea46c2238",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Assistant Axis",
    "url": "https://www.anthropic.com/research/assistant-axis",
    "summary": "",
    "published": "2026-01-19T20:47:03.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2c4fcdf9dbaee0a9",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Anthropic Economic Index January 2026 Report",
    "url": "https://www.anthropic.com/research/anthropic-economic-index-january-2026-report",
    "summary": "",
    "published": "2026-01-15T23:15:33.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "13559d5a0b24f9d8",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Anthropic Economic Index September 2025 Report",
    "url": "https://www.anthropic.com/research/anthropic-economic-index-september-2025-report",
    "summary": "",
    "published": "2026-01-15T01:06:36.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "83e037259286a365",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Next Generation Constitutional Classifiers",
    "url": "https://www.anthropic.com/research/next-generation-constitutional-classifiers",
    "summary": "",
    "published": "2026-01-09T21:18:12.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "50ed598c41e44951",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Anthropic Interviewer",
    "url": "https://www.anthropic.com/research/anthropic-interviewer",
    "summary": "",
    "published": "2026-01-07T18:48:36.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8786587032c1ecab",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Bloom",
    "url": "https://www.anthropic.com/research/bloom",
    "summary": "",
    "published": "2025-12-20T17:09:41.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "94085dc263d90941",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Project Vend 2",
    "url": "https://www.anthropic.com/research/project-vend-2",
    "summary": "",
    "published": "2025-12-18T16:17:58.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d3f945bbee7feae5",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Project Vend 1",
    "url": "https://www.anthropic.com/research/project-vend-1",
    "summary": "",
    "published": "2025-12-18T11:02:21.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "13ddab6ab9dcb120",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "How Ai Is Transforming Work At Anthropic",
    "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
    "summary": "",
    "published": "2025-12-09T03:30:54.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2eb2c33da2cc012d",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Estimating Productivity Gains",
    "url": "https://www.anthropic.com/research/estimating-productivity-gains",
    "summary": "",
    "published": "2025-12-07T14:55:39.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "263ff8b812b8ab5f",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Economic Research",
    "url": "https://www.anthropic.com/research/team/economic-research",
    "summary": "",
    "published": "2025-12-05T22:37:18.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cfd2d194360d9c43",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Prompt Injection Defenses",
    "url": "https://www.anthropic.com/research/prompt-injection-defenses",
    "summary": "",
    "published": "2025-11-24T18:51:33.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "20eba96c250ff5cb",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Emergent Misalignment Reward Hacking",
    "url": "https://www.anthropic.com/research/emergent-misalignment-reward-hacking",
    "summary": "",
    "published": "2025-11-21T18:24:54.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3e7b3a8a06f8dcd2",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Societal Impacts",
    "url": "https://www.anthropic.com/research/team/societal-impacts",
    "summary": "",
    "published": "2025-11-20T19:03:43.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0bfa1fec60052996",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Alignment",
    "url": "https://www.anthropic.com/research/team/alignment",
    "summary": "",
    "published": "2025-11-20T19:03:42.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b829cb554218f98f",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Interpretability",
    "url": "https://www.anthropic.com/research/team/interpretability",
    "summary": "",
    "published": "2025-11-20T19:03:41.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a4198c338fe4f0e5",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Project Fetch Robot Dog",
    "url": "https://www.anthropic.com/research/project-fetch-robot-dog",
    "summary": "",
    "published": "2025-11-20T16:17:01.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ee3faf99469d461d",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Constitutional Classifiers",
    "url": "https://www.anthropic.com/research/constitutional-classifiers",
    "summary": "",
    "published": "2025-11-20T16:16:54.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0b1f7507e68b6ed7",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Introspection",
    "url": "https://www.anthropic.com/research/introspection",
    "summary": "",
    "published": "2025-11-20T16:16:23.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1eb8cb42ce3ead18",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Tracing Thoughts Language Model",
    "url": "https://www.anthropic.com/research/tracing-thoughts-language-model",
    "summary": "",
    "published": "2025-11-20T16:16:19.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c728eca7ebe43f47",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Persona Vectors",
    "url": "https://www.anthropic.com/research/persona-vectors",
    "summary": "",
    "published": "2025-11-20T16:16:15.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e18c60273b41c0f6",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Toy Models Of Superposition",
    "url": "https://www.anthropic.com/research/toy-models-of-superposition",
    "summary": "",
    "published": "2025-11-20T16:16:12.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "15742a5ecf56dcc3",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Claude Character",
    "url": "https://www.anthropic.com/research/claude-character",
    "summary": "",
    "published": "2025-11-20T16:15:32.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c434f1a6187d2461",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Alignment Faking",
    "url": "https://www.anthropic.com/research/alignment-faking",
    "summary": "",
    "published": "2025-11-20T16:15:29.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d96e769992161dc2",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Auditing Hidden Objectives",
    "url": "https://www.anthropic.com/research/auditing-hidden-objectives",
    "summary": "",
    "published": "2025-11-20T16:15:19.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "244c3974d2b52ba5",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Reward Tampering",
    "url": "https://www.anthropic.com/research/reward-tampering",
    "summary": "",
    "published": "2025-11-20T16:15:12.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "526608729c2000bc",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Economic Index Geography",
    "url": "https://www.anthropic.com/research/economic-index-geography",
    "summary": "",
    "published": "2025-11-20T16:14:42.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f02b59238818e8e4",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Impact Software Development",
    "url": "https://www.anthropic.com/research/impact-software-development",
    "summary": "",
    "published": "2025-11-20T16:14:37.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d79f9a2f435f0642",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Values Wild",
    "url": "https://www.anthropic.com/research/values-wild",
    "summary": "",
    "published": "2025-11-20T16:14:30.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7467318a80b609e6",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Collective Constitutional Ai Aligning A Language Model With Public Input",
    "url": "https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input",
    "summary": "",
    "published": "2025-11-20T16:14:28.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "88c0a35869b05c08",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Predictability And Surprise In Large Generative Models",
    "url": "https://www.anthropic.com/research/predictability-and-surprise-in-large-generative-models",
    "summary": "",
    "published": "2025-11-20T16:14:26.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b49229b7725670dd",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Deprecation Commitments",
    "url": "https://www.anthropic.com/research/deprecation-commitments",
    "summary": "",
    "published": "2025-11-04T16:36:56.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "351da742628d5e98",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Economic Policy Responses",
    "url": "https://www.anthropic.com/research/economic-policy-responses",
    "summary": "",
    "published": "2025-10-14T17:05:02.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "39ced484f83627c0",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Small Samples Poison",
    "url": "https://www.anthropic.com/research/small-samples-poison",
    "summary": "",
    "published": "2025-10-09T16:01:15.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7b9cd5c2cc7daea5",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Petri Open Source Auditing",
    "url": "https://www.anthropic.com/research/petri-open-source-auditing",
    "summary": "",
    "published": "2025-10-06T17:12:39.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1447b534eb94d637",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Building Ai Cyber Defenders",
    "url": "https://www.anthropic.com/research/building-ai-cyber-defenders",
    "summary": "",
    "published": "2025-10-03T19:33:59.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "605cb9e6f830d25a",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Clio",
    "url": "https://www.anthropic.com/research/clio",
    "summary": "",
    "published": "2025-08-28T15:58:31.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b254a528056f8018",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "End Subset Conversations",
    "url": "https://www.anthropic.com/research/end-subset-conversations",
    "summary": "",
    "published": "2025-08-15T19:36:25.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "de57177a638311ae",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Visible Extended Thinking",
    "url": "https://www.anthropic.com/research/visible-extended-thinking",
    "summary": "",
    "published": "2025-07-23T18:02:08.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7cc31501c76f227f",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Shade Arena Sabotage Monitoring",
    "url": "https://www.anthropic.com/research/shade-arena-sabotage-monitoring",
    "summary": "",
    "published": "2025-06-25T00:45:44.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bc5cacb6e6a74060",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Agentic Misalignment",
    "url": "https://www.anthropic.com/research/agentic-misalignment",
    "summary": "",
    "published": "2025-06-23T08:51:14.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cace14b95b5ca5f9",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Confidential Inference Trusted Vms",
    "url": "https://www.anthropic.com/research/confidential-inference-trusted-vms",
    "summary": "",
    "published": "2025-06-18T16:16:11.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "df94d43091648c60",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Open Source Circuit Tracing",
    "url": "https://www.anthropic.com/research/open-source-circuit-tracing",
    "summary": "",
    "published": "2025-05-29T16:14:52.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "55e47dff8b65e922",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Exploring Model Welfare",
    "url": "https://www.anthropic.com/research/exploring-model-welfare",
    "summary": "",
    "published": "2025-04-24T14:38:39.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "dbf5587630180d16",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Reasoning Models Dont Say Think",
    "url": "https://www.anthropic.com/research/reasoning-models-dont-say-think",
    "summary": "",
    "published": "2025-04-04T09:11:36.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d05ef1a75f3802e3",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Forecasting Rare Behaviors",
    "url": "https://www.anthropic.com/research/forecasting-rare-behaviors",
    "summary": "",
    "published": "2025-02-28T21:57:01.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d0ef259581ab078a",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Crosscoder Model Diffing",
    "url": "https://www.anthropic.com/research/crosscoder-model-diffing",
    "summary": "",
    "published": "2025-02-20T23:53:28.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2eea703adcf8adfe",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Evaluating Ai Systems",
    "url": "https://www.anthropic.com/research/evaluating-ai-systems",
    "summary": "",
    "published": "2024-12-19T19:04:12.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "455e5ff358e13eb4",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Training A Helpful And Harmless Assistant With Reinforcement Learning From Human Feedback",
    "url": "https://www.anthropic.com/research/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback",
    "summary": "",
    "published": "2024-12-19T18:59:50.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e975b9aa59e85668",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Towards Understanding Sycophancy In Language Models",
    "url": "https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models",
    "summary": "",
    "published": "2024-12-19T18:59:42.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6bfa2364af973e81",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Towards Monosemanticity Decomposing Language Models With Dictionary Learning",
    "url": "https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning",
    "summary": "",
    "published": "2024-12-19T18:59:35.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c03fe00bf0cf5a95",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Towards Measuring The Representation Of Subjective Global Opinions In Language Models",
    "url": "https://www.anthropic.com/research/towards-measuring-the-representation-of-subjective-global-opinions-in-language-models",
    "summary": "",
    "published": "2024-12-19T18:59:28.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "77cbffe6503e701a",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "The Capacity For Moral Self Correction In Large Language Models",
    "url": "https://www.anthropic.com/research/the-capacity-for-moral-self-correction-in-large-language-models",
    "summary": "",
    "published": "2024-12-19T18:59:21.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1a73fccb52bf0d56",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Swe Bench Sonnet",
    "url": "https://www.anthropic.com/research/swe-bench-sonnet",
    "summary": "",
    "published": "2024-12-19T18:59:14.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "046dd3f3861840cd",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Superposition Memorization And Double Descent",
    "url": "https://www.anthropic.com/research/superposition-memorization-and-double-descent",
    "summary": "",
    "published": "2024-12-19T18:59:07.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f46aef28c36a5103",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Studying Large Language Model Generalization With Influence Functions",
    "url": "https://www.anthropic.com/research/studying-large-language-model-generalization-with-influence-functions",
    "summary": "",
    "published": "2024-12-19T18:58:59.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e6a6e5bff14768ed",
    "source": "anthropic_research",
    "source_weight": 1.4,
    "title": "Specific Versus General Principles For Constitutional Ai",
    "url": "https://www.anthropic.com/research/specific-versus-general-principles-for-constitutional-ai",
    "summary": "",
    "published": "2024-12-19T18:58:41.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a8257b8b88f12ec0",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Custom Kernels for All from Codex and Claude",
    "url": "https://huggingface.co/blog/custom-cuda-kernels-agent-skills",
    "summary": "",
    "published": "Fri, 13 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "504929088957b0e2",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
    "url": "https://huggingface.co/blog/openenv-turing",
    "summary": "",
    "published": "Thu, 12 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fccbd276447697a6",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Transformers.js v4 Preview: Now Available on NPM!",
    "url": "https://huggingface.co/blog/transformersjs-v4",
    "summary": "",
    "published": "Mon, 09 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d2d3810992df7825",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing SyGra Studio",
    "url": "https://huggingface.co/blog/ServiceNow-AI/sygra-studio",
    "summary": "",
    "published": "Thu, 05 Feb 2026 16:52:28 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d9c84300073d05b6",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Nemotron ColEmbed V2: Raising the Bar for Multimodal Retrieval with ViDoRe V3’s Top Model",
    "url": "https://huggingface.co/blog/nvidia/nemotron-colembed-v2",
    "summary": "",
    "published": "Wed, 04 Feb 2026 15:00:40 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fe9da202a4704ea8",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Community Evals: Because we're done trusting black-box leaderboards over the community",
    "url": "https://huggingface.co/blog/community-evals",
    "summary": "",
    "published": "Wed, 04 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "91102c029a7fd92f",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "H Company's new Holo2 model takes the lead in UI Localization",
    "url": "https://huggingface.co/blog/Hcompany/introducing-holo2-235b-a22b",
    "summary": "",
    "published": "Tue, 03 Feb 2026 17:40:14 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f634b4223a6db742",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "The Future of the Global Open-Source AI Ecosystem: From DeepSeek to AI+",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-3",
    "summary": "",
    "published": "Tue, 03 Feb 2026 15:03:19 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "90c07e0cfebb61a7",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Training Design for Text-to-Image Models: Lessons from Ablations",
    "url": "https://huggingface.co/blog/Photoroom/prx-part2",
    "summary": "",
    "published": "Tue, 03 Feb 2026 11:25:53 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "29d122eeb5bb0bc2",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing Daggr: Chain apps programmatically, inspect visually",
    "url": "https://huggingface.co/blog/daggr",
    "summary": "",
    "published": "Thu, 29 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9bd6b937a4204960",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "We Got Claude to Build CUDA Kernels and teach open models!",
    "url": "https://huggingface.co/blog/upskill",
    "summary": "",
    "published": "Wed, 28 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ccef47d5befa3bba",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Architectural Choices in China's Open-Source AI Ecosystem: Building Beyond DeepSeek",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2",
    "summary": "",
    "published": "Tue, 27 Jan 2026 15:01:45 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bf182a437ebdeb73",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Alyah ⭐️: Toward Robust Evaluation of Emirati Dialect Capabilities in Arabic LLMs",
    "url": "https://huggingface.co/blog/tiiuae/emirati-benchmarks",
    "summary": "",
    "published": "Tue, 27 Jan 2026 10:26:42 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5c42dd2e17d30e84",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective",
    "url": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
    "summary": "",
    "published": "Tue, 27 Jan 2026 01:53:15 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7ce7b359b6cedf88",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality",
    "url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face",
    "summary": "",
    "published": "Wed, 21 Jan 2026 06:25:31 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "34a5690dbeed7dab",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "One Year Since the “DeepSeek Moment”",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment",
    "summary": "",
    "published": "Tue, 20 Jan 2026 15:02:10 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "77a6435a098b2d8e",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Differential Transformer V2",
    "url": "https://huggingface.co/blog/microsoft/diff-attn-v2",
    "summary": "",
    "published": "Tue, 20 Jan 2026 03:20:57 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "00a016ffc13d0f85",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing Waypoint-1: Real-time interactive video diffusion from Overworld",
    "url": "https://huggingface.co/blog/waypoint-1",
    "summary": "",
    "published": "Tue, 20 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "941eaaebf3b7a1cf",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Open Responses: What you need to know",
    "url": "https://huggingface.co/blog/open-responses",
    "summary": "",
    "published": "Thu, 15 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e62eb3fb8a1bc4ed",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI",
    "url": "https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning",
    "summary": "",
    "published": "Mon, 05 Jan 2026 22:56:51 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "52e841e7968c5f43",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture",
    "url": "https://huggingface.co/blog/tiiuae/falcon-h1-arabic",
    "summary": "",
    "published": "Mon, 05 Jan 2026 09:16:51 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b74e82e624fdd0f0",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "NVIDIA brings agents to life with DGX Spark and Reachy Mini",
    "url": "https://huggingface.co/blog/nvidia-reachy-mini",
    "summary": "",
    "published": "Mon, 05 Jan 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "808f5b4c07f3795e",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
    "url": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
    "summary": "",
    "published": "Tue, 23 Dec 2025 14:07:35 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b8d1f7c0db043f25",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
    "url": "https://huggingface.co/blog/tokenizers",
    "summary": "",
    "published": "Thu, 18 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "551dda269267aa50",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator",
    "url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe",
    "summary": "",
    "published": "Wed, 17 Dec 2025 13:22:18 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f7c6514fd6f76ace",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents",
    "url": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face",
    "summary": "",
    "published": "Mon, 15 Dec 2025 16:01:04 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6bee81a8f68bf4d2",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "New in llama.cpp: Model Management",
    "url": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp",
    "summary": "",
    "published": "Thu, 11 Dec 2025 15:47:44 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e8a902cfc34f94d4",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Codex is Open Sourcing AI models",
    "url": "https://huggingface.co/blog/hf-skills-training-codex",
    "summary": "",
    "published": "Thu, 11 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "518b88720d0a5f21",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face",
    "url": "https://huggingface.co/blog/swift-huggingface",
    "summary": "",
    "published": "Fri, 05 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "467e7644a71ef33c",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "DeepMath: A lightweight math reasoning Agent with smolagents",
    "url": "https://huggingface.co/blog/intel-deepmath",
    "summary": "",
    "published": "Thu, 04 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e96f257938927a3c",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "We Got Claude to Fine-Tune an Open Source LLM",
    "url": "https://huggingface.co/blog/hf-skills-training",
    "summary": "",
    "published": "Thu, 04 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ec318a42062d586a",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
    "url": "https://huggingface.co/blog/transformers-v5",
    "summary": "",
    "published": "Mon, 01 Dec 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "daef2cca15dcfd10",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Diffusers welcomes FLUX-2",
    "url": "https://huggingface.co/blog/flux-2",
    "summary": "",
    "published": "Tue, 25 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "973a146a0013d1b8",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Continuous batching from first principles",
    "url": "https://huggingface.co/blog/continuous_batching",
    "summary": "",
    "published": "Tue, 25 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "eca5b1aa2a72ec13",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Building Deep Research: How we Achieved State of the Art",
    "url": "https://huggingface.co/blog/Tavily/tavily-deep-research",
    "summary": "",
    "published": "Mon, 24 Nov 2025 17:40:14 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c10899941a3e4bec",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "OVHcloud on Hugging Face Inference Providers 🔥",
    "url": "https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud",
    "summary": "",
    "published": "Mon, 24 Nov 2025 16:08:47 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "dbae085ad582536d",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "20x Faster TRL Fine-tuning with RapidFire AI",
    "url": "https://huggingface.co/blog/rapidfireai",
    "summary": "",
    "published": "Fri, 21 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7573916c05eb2ab7",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks",
    "url": "https://huggingface.co/blog/open-asr-leaderboard",
    "summary": "",
    "published": "Fri, 21 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "40249b55353db3e5",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms",
    "url": "https://huggingface.co/blog/anylanguagemodel",
    "summary": "",
    "published": "Thu, 20 Nov 2025 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "41d7ff537a2d5ced",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models",
    "url": "https://huggingface.co/blog/ServiceNow-AI/apriel-h1",
    "summary": "",
    "published": "Wed, 19 Nov 2025 05:19:07 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d4ea9f8c41e92794",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Code, Compute and Connection: Inside the Inaugural NVIDIA AI Day São Paulo",
    "url": "https://blogs.nvidia.com/blog/ai-day-sao-paulo/",
    "summary": "The worldwide tour of NVIDIA AI Days — bringing together AI enthusiasts, developers, researchers and startups — made its latest stop in São Paulo, Brazil.",
    "published": "Thu, 12 Feb 2026 22:00:58 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0530f2ee25a8efae",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Leading Inference Providers Cut AI Costs by up to 10x With Open Source Models on NVIDIA Blackwell",
    "url": "https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/",
    "summary": "A diagnostic insight in healthcare. A character’s dialogue in an interactive game. An autonomous resolution from a customer service agent. Each of these AI-powered interactions is built on the same unit of intelligence: a token. Scaling these AI interactions requires businesses to consider whether they can afford more tokens. The answer lies in better tokenomics\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 12 Feb 2026 16:00:46 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bac9f0ac7445edad",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "NVIDIA DGX Spark Powers Big Projects in Higher Education",
    "url": "https://blogs.nvidia.com/blog/dgx-spark-higher-education/",
    "summary": "At leading institutions across the globe, the NVIDIA DGX Spark desktop supercomputer is bringing data‑center‑class AI to lab benches, faculty offices and students’ systems. There’s even a DGX Spark hard at work in the South Pole, at the IceCube Neutrino Observatory run by the University of Wisconsin-Madison. The compact supercomputer’s petaflop‑class performance enables local deployment\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/dgx-spark-higher-education/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 12 Feb 2026 15:00:23 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b0bbdaed56ce378b",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "GeForce NOW Turns Screens Into a Gaming Machine",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-amazon-fire-tv-app/",
    "summary": "The GeForce NOW sixth-anniversary festivities roll on this February, continuing a monthlong celebration of NVIDIA’s cloud gaming service. This week brings even more reasons to join the party, as GeForce NOW launches on a new platform with support for Amazon Fire TV devices, and eight new games to keep the streaming going strong. The new\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-amazon-fire-tv-app/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 12 Feb 2026 14:00:58 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8010462db1e78b23",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "GeForce NOW Celebrates Six Years of Streaming With 24 Games in February",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-feb-2026-games-list/",
    "summary": "Break out the cake and green sprinkles — GeForce NOW is turning six. Since launch, members have streamed over 1 billion hours, and the party’s just getting started. Throughout February, members can look forward to new games, fresh ways to play across more devices and even more ways to bring RTX power to every screen\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-feb-2026-games-list/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 05 Feb 2026 14:00:53 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "993f080f34b17b79",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Nemotron Labs: How AI Agents Are Turning Documents Into Real-Time Business Intelligence",
    "url": "https://blogs.nvidia.com/blog/ai-agents-intelligent-document-processing/",
    "summary": "Businesses today face the challenge of uncovering valuable insights buried within a wide variety of documents — including reports, presentations, PDFs, web pages and spreadsheets.",
    "published": "Wed, 04 Feb 2026 16:00:36 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7cb2e5a4bd8e8e51",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Everything Will Be Represented in a Virtual Twin, NVIDIA CEO Jensen Huang Says at 3DEXPERIENCE World",
    "url": "https://blogs.nvidia.com/blog/huang-3dexperience-2026/",
    "summary": "NVIDIA founder and CEO Jensen Huang and Dassault Systèmes CEO Pascal Daloz announced a partnership to build a shared industrial AI architecture, merging virtual twins with physics-based AI to redefine the future of design, engineering and manufacturing.",
    "published": "Tue, 03 Feb 2026 22:14:51 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7fcd2a2c35056be2",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Mercedes-Benz Unveils New S-Class Built on NVIDIA DRIVE AV, Which Enables an L4-Ready Architecture",
    "url": "https://blogs.nvidia.com/blog/mercedes-benz-l4-s-class-drive-av-platform/",
    "summary": "Mercedes-Benz is marking 140 years of automotive innovation with a new S-Class built for the AI era, bringing together automotive safety and NVIDIA’s advanced autonomous driving platform to enable a level 4-ready architecture designed for trust. The new S-Class with MB.OS, which will be equipped with the NVIDIA DRIVE Hyperion architecture and full-stack NVIDIA DRIVE\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/mercedes-benz-l4-s-class-drive-av-platform/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 29 Jan 2026 18:00:31 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "23548019f4b64ecc",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Into the Omniverse: Physical AI Open Models and Frameworks Advance Robots and Autonomous Systems",
    "url": "https://blogs.nvidia.com/blog/physical-ai-open-models-robot-autonomous-systems-omniverse/",
    "summary": "Open source has become essential for driving innovation in robotics and autonomy. By providing access to critical infrastructure — from simulation frameworks to AI models — NVIDIA is enabling collaborative development that accelerates the path to safer, more capable autonomous systems.",
    "published": "Thu, 29 Jan 2026 17:00:21 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cd990fc767cf9d3d",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "GeForce NOW Brings GeForce RTX Gaming to Linux PCs",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-linux/",
    "summary": "Get ready to game — the native GeForce NOW app for Linux PCs is now available in beta, letting Linux desktops tap directly into GeForce RTX performance from the cloud. Alongside the expansion comes ten new games, including The Bard’s Tale IV: Director’s Cut and The Bard’s Tale Trilogy for a leveled-up gaming weekend. And\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-linux/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 29 Jan 2026 14:00:49 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7a29dda9cde09e1e",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Accelerating Science: A Blueprint for a Renewed National Quantum Initiative",
    "url": "https://blogs.nvidia.com/blog/national-quantum-initiative/",
    "summary": "Quantum technologies are rapidly emerging as foundational capabilities for economic competitiveness, national security and scientific leadership in the 21st century. Sustained U.S. leadership in quantum information science is critical to ensuring that breakthroughs in computing, sensing, networking and materials translate into secure technologies and industries, a skilled domestic workforce and long-term strategic advantage. To secure\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/national-quantum-initiative/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Wed, 28 Jan 2026 18:17:53 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7a6926c10a8e13bc",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "NVIDIA Launches Earth-2 Family of Open Models — the World’s First Fully Open, Accelerated Set of Models and Tools for AI Weather",
    "url": "https://blogs.nvidia.com/blog/nvidia-earth-2-open-models/",
    "summary": "At the American Meteorological Society’s Annual Meeting, NVIDIA today unveiled a new NVIDIA Earth-2 family of open models, libraries and frameworks for weather and climate AI, offering the world’s first fully open, production-ready weather AI software stack.",
    "published": "Mon, 26 Jan 2026 14:00:53 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "484509bdb7334e78",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "NVIDIA DRIVE AV Raises the Bar for Vehicle Safety as Mercedes-Benz CLA Earns Top Euro NCAP Award",
    "url": "https://blogs.nvidia.com/blog/drive-av-mercedes-benz-cla-euro-ncap-safety-award/",
    "summary": "AI-powered driver assistance technologies are becoming standard equipment, fundamentally changing how vehicle safety is assessed and validated. The recent recognition of the Mercedes-Benz CLA as Euro NCAP&#8217;s Best Performer of 2025 underscores this shift, as the vehicle combines traditional passive safety features with NVIDIA DRIVE AV software to achieve the highest overall safety score of\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/drive-av-mercedes-benz-cla-euro-ncap-safety-award/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 22 Jan 2026 18:21:49 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b8b6e98e2c15bfbe",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "How to Get Started With Visual Generative AI on NVIDIA RTX PCs",
    "url": "https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/",
    "summary": "AI-powered content generation is now embedded in everyday tools like Adobe and Canva, with a slew of agencies and studios incorporating the technology into their workflows. Image models now deliver photorealistic results consistently, video models can generate long and coherent clips, and both can follow creative directions. Creators are increasingly running these workflows locally on\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 22 Jan 2026 14:00:57 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e21d720a13f10d82",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "From Pilot to Profit: Survey Reveals the Financial Services Industry Is Doubling Down on AI Investment and Open Source",
    "url": "https://blogs.nvidia.com/blog/ai-in-financial-services-survey-2026/",
    "summary": "AI has taken center stage in financial services, automating the research and execution behind algorithmic trading and helping banks more accurately detect fraud and money laundering — all while improving risk management practices and expediting document processing. The sixth annual “NVIDIA State of AI in Financial Services” report, based on a survey of more than\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/ai-in-financial-services-survey-2026/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 22 Jan 2026 14:00:54 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4303f629b9fa2240",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Flight Controls Are Cleared for Takeoff on GeForce NOW",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-flight-controls/",
    "summary": "The wait is over, pilots. Flight control support — one of the most community-requested features for GeForce NOW — is live starting today, following its announcement at CES earlier this month. Virtual captains can now bring dedicated flight gear into the cloud and feel every roll, yaw and throttle change with even more precision, starting\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-flight-controls/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 22 Jan 2026 14:00:53 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "95ae2111e3d3e387",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "‘Largest Infrastructure Buildout in Human History’: Jensen Huang on AI’s ‘Five-Layer Cake’ at Davos",
    "url": "https://blogs.nvidia.com/blog/davos-wef-blackrock-ceo-larry-fink-jensen-huang/",
    "summary": "AI is becoming the foundation of the “largest infrastructure buildout in human history,” spanning energy and computing infrastructure, AI models and applications, NVIDIA founder and CEO Jensen Huang said during a World Economic Forum discussion with BlackRock CEO Larry Fink.",
    "published": "Wed, 21 Jan 2026 12:50:16 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6291a6f878a116e7",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Survive the Quarantine Zone and More With Devolver Digital Games on GeForce NOW",
    "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-quarantine-zone/",
    "summary": "NVIDIA kicked off the year at CES, where the crowd buzzed about the latest gaming announcements — including the native GeForce NOW app for Linux and Amazon Fire TV sticks, community-requested flight-control support and a stacked AAA lineup for the year. Check out what the creators of the YouTube channels Cloud Gaming Battle and Anytime\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-quarantine-zone/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 15 Jan 2026 14:00:35 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2dba7ccd26b39904",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Gemini 3 Deep Think: Advancing science, research and engineering",
    "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/",
    "summary": "Gemini 3 Deep Think logo",
    "published": "Thu, 12 Feb 2026 16:13:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3bf9086fd27aeeb5",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "9 fun questions to try asking Google Photos",
    "url": "https://blog.google/products-and-platforms/products/photos/ask-button-ask-photos-tips/",
    "summary": "A collage of outdoor images, a blue icon that say \"Ask Photos,\" and examples of Ask Photos prompts.",
    "published": "Tue, 10 Feb 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "65afd187aa2ff455",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Helping kids and teens learn and grow online on Safer Internet Day",
    "url": "https://blog.google/innovation-and-ai/technology/safety-security/safer-internet-day-2026-kids-teens/",
    "summary": "User profile on smartphone connected to security, media, and settings icons.",
    "published": "Tue, 10 Feb 2026 02:30:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e5dc30bcbd963b92",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Natively Adaptive Interfaces: A new framework for AI accessibility",
    "url": "https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/",
    "summary": "A collage of four images, the first of a woman with curly hair in front of a silver laptop, the second of the same woman and a man with short black hair speaking on a stairwell, the third of a the same man with glasses, and an aerial image of NTID",
    "published": "Thu, 05 Feb 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0b7dc60d8af90d37",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "How Google Cloud is helping Team USA elevate their tricks with AI",
    "url": "https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/",
    "summary": "A woman outdoors in the snow looks at a tablet. A half pipe is behind her.",
    "published": "Thu, 05 Feb 2026 16:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "26a63f74613e6671",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Watch our new Gemini ad ahead of football’s biggest weekend",
    "url": "https://blog.google/company-news/inside-google/company-announcements/gemini-ad-new-home/",
    "summary": "A toddler in a blue and yellow striped shirt sits on a kitchen counter eating a red apple. Text in the corner reads: 'New Home, Google Gemini SB Commercial’",
    "published": "Thu, 05 Feb 2026 14:30:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2dede2dbdc9c1702",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "The latest AI news we announced in January",
    "url": "https://blog.google/innovation-and-ai/products/google-ai-updates-january-2026/",
    "summary": "mp4 showing a carousel of images including a card reading \"Help that's made for you\"",
    "published": "Wed, 04 Feb 2026 16:55:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0fc6449cf17e4c48",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "How we’re helping preserve the genetic information of endangered species with AI",
    "url": "https://blog.google/innovation-and-ai/technology/ai/ai-to-preserve-endangered-species/",
    "summary": "A four-part vertical collage showing a cotton-top tamarin, an ibex, a golden lion tamarin, and a penguin.",
    "published": "Mon, 02 Feb 2026 18:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9233762a8b5309dd",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Advancing AI benchmarking with Game Arena",
    "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
    "summary": "An illustration of a King and Ace playing card, a wolf's head, two chess pieces, a poker chip, and other abstract shapes on a white background.1",
    "published": "Mon, 02 Feb 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6f71c08f77142fcb",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Project Genie: Experimenting with infinite, interactive worlds",
    "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/",
    "summary": "Text reads Introducing Project Genie",
    "published": "Thu, 29 Jan 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "29936bb569042f97",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Hear more about interactive world models in our latest podcast.",
    "url": "https://blog.google/innovation-and-ai/technology/ai/release-notes-podcast-project-genie/",
    "summary": "Project Genie: Create and explore worlds",
    "published": "Thu, 29 Jan 2026 15:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9b9db1424f88a8b6",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Google AI Plus is now available everywhere our AI plans are available, including the U.S.",
    "url": "https://blog.google/products-and-platforms/products/google-one/google-ai-plus-availability/",
    "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Plus_Hero_Visual_2096.max-600x600.format-webp_4ffr2GI.webp\" />We’re launching Google AI Plus in 35 new countries and territories including the US, making it available everywhere Google AI plans are available.",
    "published": "Tue, 27 Jan 2026 18:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a4e42dbf35ad9723",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Just ask anything: a seamless new Search experience",
    "url": "https://blog.google/products-and-platforms/products/search/ai-mode-ai-overviews-updates/",
    "summary": "A centered, elongated oval shape resembling a search bar with the text \"Ask anything\" inside it.",
    "published": "Tue, 27 Jan 2026 17:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f6ce5d131c6814c1",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "In our latest podcast, hear how the “Smoke Jumpers” team brings Gemini to billions of people.",
    "url": "https://blog.google/products-and-platforms/products/gemini/release-notes-podcast-smokejumpers/",
    "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/thumbnails_EP24_002_ccRelease_N.max-600x600.format-webp.webp\" />Bringing Gemini to billions of users requires a massive, coordinated infrastructure effort. In the latest episode of the Google AI: Release Notes podcast, host Logan Kil…",
    "published": "Tue, 27 Jan 2026 10:28:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a5a028261628d3f1",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "How animators and AI researchers made ‘Dear Upstairs Neighbors’",
    "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/dear-upstairs-neighbors/",
    "summary": "A movie poster for a film titled “Dear Upstairs Neighbors”, hand-painted in a vivid expressionist style, featuring an exasperated cartoon woman clutching her ears, surrounded by neon-colored images of noisy things like howling dogs and stomping shoes",
    "published": "Mon, 26 Jan 2026 18:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7e4f2a7329c01802",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Personal Intelligence in AI Mode in Search: Help that's uniquely yours",
    "url": "https://blog.google/products-and-platforms/products/search/personal-intelligence-ai-mode-search/",
    "summary": "Screenshot of the AI Mode page with the text, \"Hi Lukas what's on your mind?\"",
    "published": "Thu, 22 Jan 2026 16:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "49017c07cf07fbd5",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Building a community-led future for AI in film with Sundance Institute",
    "url": "https://blog.google/company-news/outreach-and-initiatives/google-org/sundance-institute-ai-education/",
    "summary": "Illustration of a cinematographer filming with a professional video camera mounted on a tripod, positioned next to a studio light and a film clapboard.",
    "published": "Tue, 20 Jan 2026 20:30:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "03759defcafc7565",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "How Nano Banana got its name",
    "url": "https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/",
    "summary": "A Nano Banana-generated image showing yellow bananas spelling out 'Nano Banana' on a bed of additional yellow bananas",
    "published": "Thu, 15 Jan 2026 16:06:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "53d957920fd917e1",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Learners and educators are AI’s new “super users”",
    "url": "https://blog.google/products-and-platforms/products/education/our-life-with-ai-2025/",
    "summary": "Teacher using a computer with students",
    "published": "Thu, 15 Jan 2026 11:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "de750dee6aa7002a",
    "source": "google_ai_blog",
    "source_weight": 0.7,
    "title": "Introducing Community Benchmarks on Kaggle",
    "url": "https://blog.google/innovation-and-ai/technology/developers-tools/kaggle-community-benchmarks/",
    "summary": "A drawing of three people working on laptops, with one person's screen showing \"Kaggle Benchmark Results\" with \"Gemini XXXX\" and a large \"PASS\" checkmark.1",
    "published": "Wed, 14 Jan 2026 14:00:00 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "18ed65b18f8661b8",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Customize AI agent browsing with proxies, profiles, and extensions in Amazon Bedrock AgentCore Browser",
    "url": "https://aws.amazon.com/blogs/machine-learning/customize-ai-agent-browsing-with-proxies-profiles-and-extensions-in-amazon-bedrock-agentcore-browser/",
    "summary": "Today, we are announcing three new capabilities that address these requirements: proxy configuration, browser profiles, and browser extensions. Together, these features give you fine-grained control over how your AI agents interact with the web. This post will walk through each capability with configuration examples and practical use cases to help you get started.",
    "published": "Fri, 13 Feb 2026 22:57:34 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "caf0f5862fc0931a",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "AI meets HR: Transforming talent acquisition with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/ai-meets-hr-transforming-talent-acquisition-with-amazon-bedrock/",
    "summary": "In this post, we show how to create an AI-powered recruitment system using Amazon Bedrock, Amazon Bedrock Knowledge Bases, AWS Lambda, and other AWS services to enhance job description creation, candidate communication, and interview preparation while maintaining human oversight.",
    "published": "Thu, 12 Feb 2026 20:18:58 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ce4da08a3b944062",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Build long-running MCP servers on Amazon Bedrock AgentCore with Strands Agents integration",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-long-running-mcp-servers-on-amazon-bedrock-agentcore-with-strands-agents-integration/",
    "summary": "In this post, we provide you with a comprehensive approach to achieve this. First, we introduce a context message strategy that maintains continuous communication between servers and clients during extended operations. Next, we develop an asynchronous task management framework that allows your AI agents to initiate long-running processes without blocking other operations. Finally, we demonstrate how to bring these strategies together with Amazon Bedrock AgentCore and Strands Agents to build production-ready AI agents that can handle complex, time-intensive operations reliably.",
    "published": "Thu, 12 Feb 2026 20:16:20 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "31398854086f7b11",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "NVIDIA Nemotron 3 Nano 30B MoE model is now available in Amazon SageMaker JumpStart",
    "url": "https://aws.amazon.com/blogs/machine-learning/nvidia-nemotron-3-nano-30b-is-now-available-in-amazon-sagemaker-jumpstart/",
    "summary": "Today we’re excited to announce that the NVIDIA Nemotron 3 Nano 30B model with &nbsp;3B active parameters is now generally available in the Amazon SageMaker JumpStart model catalog. You can accelerate innovation and deliver tangible business value with Nemotron 3 Nano on Amazon Web Services (AWS) without having to manage model deployment complexities. You can power your generative AI applications with Nemotron capabilities using the managed deployment capabilities offered by SageMaker JumpStart.",
    "published": "Wed, 11 Feb 2026 19:38:47 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9f25ae5d3e4cfddc",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Mastering Amazon Bedrock throttling and service availability: A comprehensive guide",
    "url": "https://aws.amazon.com/blogs/machine-learning/mastering-amazon-bedrock-throttling-and-service-availability-a-comprehensive-guide/",
    "summary": "This post shows you how to implement robust error handling strategies that can help improve application reliability and user experience when using Amazon Bedrock. We'll dive deep into strategies for optimizing performances for the application with these errors. Whether this is for a fairly new application or matured AI application, in this post you will be able to find the practical guidelines to operate with on these errors.",
    "published": "Wed, 11 Feb 2026 15:52:54 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f7ecc60851a906e9",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Swann provides Generative AI to millions of IoT Devices using Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/swann-provides-generative-ai-to-millions-of-iot-devices-using-amazon-bedrock/",
    "summary": "This post shows you how to implement intelligent notification filtering using Amazon Bedrock and its gen-AI capabilities. You'll learn model selection strategies, cost optimization techniques, and architectural patterns for deploying gen-AI at IoT scale, based on Swann Communications deployment across millions of devices.",
    "published": "Wed, 11 Feb 2026 15:48:15 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "05f8faf1ec331d76",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "How LinqAlpha assesses investment theses using Devil’s Advocate on Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-linqalpha-assesses-investment-theses-using-devils-advocate-on-amazon-bedrock/",
    "summary": "LinqAlpha is a Boston-based multi-agent AI system built specifically for institutional investors. The system supports and streamlines agentic workflows across company screening, primer generation, stock price catalyst mapping, and now, pressure-testing investment ideas through a new AI agent called Devil’s Advocate. In this post, we share how LinqAlpha uses Amazon Bedrock to build and scale Devil’s Advocate.",
    "published": "Wed, 11 Feb 2026 15:45:30 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3ca09597f05bfb41",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "How Amazon uses Amazon Nova models to automate operational readiness testing for new fulfillment centers",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-amazon-uses-amazon-nova-models-to-automate-operational-readiness-testing-for-new-fulfillment-centers/",
    "summary": "In this post, we discuss how&nbsp;Amazon Nova&nbsp;in&nbsp;Amazon Bedrock&nbsp;can be used to implement an AI-powered image recognition solution that automates the detection and validation of module components, significantly reducing manual verification efforts and improving accuracy.",
    "published": "Tue, 10 Feb 2026 18:34:09 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "25156d2be403a27d",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Iberdrola enhances IT operations using Amazon Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/iberdrola-enhances-it-operations-using-amazon-bedrock-agentcore/",
    "summary": "Iberdrola, one of the world’s largest utility companies, has embraced cutting-edge AI technology to revolutionize its IT operations in ServiceNow. Through its partnership with AWS, Iberdrola implemented different agentic architectures using Amazon Bedrock AgentCore, targeting three key areas: optimizing change request validation in the draft phase, enriching incident management with contextual intelligence, and simplifying change model selection using conversational AI. These innovations reduce bottlenecks, help teams accelerate ticket resolution, and deliver consistent and high-quality data handling throughout the organization.",
    "published": "Tue, 10 Feb 2026 18:31:57 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "51378656ffa229b1",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Building real-time voice assistants with Amazon Nova Sonic compared to cascading architectures",
    "url": "https://aws.amazon.com/blogs/machine-learning/building-real-time-voice-assistants-with-amazon-nova-sonic-compared-to-cascading-architectures/",
    "summary": "Amazon Nova Sonic&nbsp;delivers real-time, human-like voice conversations through the bidirectional streaming interface. In this post, you learn how Amazon Nova Sonic can solve some of the challenges faced by cascaded approaches, simplify building voice AI agents, and provide natural conversational capabilities. We also provide guidance on when to choose each approach to help you make informed decisions for your voice AI projects.",
    "published": "Tue, 10 Feb 2026 18:29:05 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a075dbc7d77bca68",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Automated Reasoning checks rewriting chatbot reference implementation",
    "url": "https://aws.amazon.com/blogs/machine-learning/automated-reasoning-checks-rewriting-chatbot-reference-implementation/",
    "summary": "This blog post dives deeper into the implementation architecture for the Automated Reasoning checks rewriting chatbot.",
    "published": "Mon, 09 Feb 2026 19:34:05 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "22c42c0a18e7784d",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Scale LLM fine-tuning with Hugging Face and Amazon SageMaker AI",
    "url": "https://aws.amazon.com/blogs/machine-learning/scale-llm-fine-tuning-with-hugging-face-and-amazon-sagemaker-ai/",
    "summary": "In this post, we show how this integrated approach transforms enterprise LLM fine-tuning from a complex, resource-intensive challenge into a streamlined, scalable solution for achieving better model performance in domain-specific applications.",
    "published": "Mon, 09 Feb 2026 16:48:46 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5e64b0122203008f",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "New Relic transforms productivity with generative AI on AWS",
    "url": "https://aws.amazon.com/blogs/machine-learning/new-relic-transforms-productivity-with-generative-ai-on-aws/",
    "summary": "Working with the Generative AI Innovation Center, New Relic NOVA (New Relic Omnipresence Virtual Assistant) evolved from a knowledge assistant into a comprehensive productivity engine. We explore the technical architecture, development journey, and key lessons learned in building an enterprise-grade AI solution that delivers measurable productivity gains at scale.",
    "published": "Mon, 09 Feb 2026 16:45:16 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a98185712084d329",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Accelerate agentic application development with a full-stack starter template for Amazon Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/accelerate-agentic-application-development-with-a-full-stack-starter-template-for-amazon-bedrock-agentcore/",
    "summary": "In this post, you will learn how to deploy Fullstack AgentCore Solution Template (FAST) to your Amazon Web Services (AWS) account, understand its architecture, and see how to extend it for your requirements. You will learn how to build your own agent while FAST handles authentication, infrastructure as code (IaC), deployment pipelines, and service integration.",
    "published": "Mon, 09 Feb 2026 16:40:58 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "da0df0d660b4dc54",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Agent-to-agent collaboration: Using Amazon Nova 2 Lite and Amazon Nova Act for multi-agent systems",
    "url": "https://aws.amazon.com/blogs/machine-learning/agent-to-agent-collaboration-using-amazon-nova-2-lite-and-amazon-nova-act-for-multi-agent-systems/",
    "summary": "This post walks through how agent-to-agent collaboration on Amazon Bedrock works in practice, using Amazon Nova 2 Lite for planning and Amazon Nova Act for browser interaction, to turn a fragile single-agent setup into a predictable multi-agent system.",
    "published": "Mon, 09 Feb 2026 16:00:28 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "79370d2495557126",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Structured outputs on Amazon Bedrock: Schema-compliant AI responses",
    "url": "https://aws.amazon.com/blogs/machine-learning/structured-outputs-on-amazon-bedrock-schema-compliant-ai-responses/",
    "summary": "Today, we're announcing structured outputs on Amazon Bedrock—a capability that fundamentally transforms how you can obtain validated JSON responses from foundation models through constrained decoding for schema compliance. In this post, we explore the challenges of traditional JSON generation and how structured outputs solves them. We cover the two core mechanisms—JSON Schema output format and strict tool use—along with implementation details, best practices, and practical code examples.",
    "published": "Fri, 06 Feb 2026 20:12:14 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "157be0691604e576",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Manage Amazon SageMaker HyperPod clusters using the HyperPod CLI and SDK",
    "url": "https://aws.amazon.com/blogs/machine-learning/manage-amazon-sagemaker-hyperpod-clusters-using-the-hyperpod-cli-and-sdk/",
    "summary": "In this post, we demonstrate how to use the CLI and the SDK to create and manage SageMaker HyperPod clusters in your AWS account. We walk through a practical example and dive deeper into the user workflow and parameter choices.",
    "published": "Fri, 06 Feb 2026 19:27:45 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c1e45689c97e4666",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "Evaluate generative AI models with an Amazon Nova rubric-based LLM judge on Amazon SageMaker AI (Part 2)",
    "url": "https://aws.amazon.com/blogs/machine-learning/evaluate-generative-ai-models-with-an-amazon-nova-rubric-based-llm-judge-on-amazon-sagemaker-ai-part-2/",
    "summary": "In this post, we explore the Amazon Nova rubric-based judge feature: what a rubric-based judge is, how the judge is trained, what metrics to consider, and how to calibrate the judge. We chare notebook code of the Amazon Nova rubric-based LLM-as-a-judge methodology to evaluate and compare the outputs of two different LLMs using SageMaker training jobs.",
    "published": "Fri, 06 Feb 2026 16:29:45 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3e9f373e384ab4b1",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "How Associa transforms document classification with the GenAI IDP Accelerator and Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-associa-transforms-document-classification-with-the-genai-idp-accelerator-and-amazon-bedrock/",
    "summary": "Associa collaborated with the AWS Generative AI Innovation Center to build a generative AI-powered document classification system aligning with Associa’s long-term vision of using generative AI to achieve operational efficiencies in document management. The solution automatically categorizes incoming documents with high accuracy, processes documents efficiently, and provides substantial cost savings while maintaining operational excellence. The document classification system, developed using the Generative AI Intelligent Document Processing (GenAI IDP) Accelerator, is designed to integrate seamlessly into existing workflows. It revolutionizes how employees interact with document management systems by reducing the time spent on manual classification tasks.",
    "published": "Thu, 05 Feb 2026 20:41:52 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "39f36f2af84fbdf3",
    "source": "aws_ml_blog",
    "source_weight": 0.6,
    "title": "A practical guide to Amazon Nova Multimodal Embeddings",
    "url": "https://aws.amazon.com/blogs/machine-learning/a-practical-guide-to-amazon-nova-multimodal-embeddings/",
    "summary": "In this post, you will learn how to configure and use Amazon Nova Multimodal Embeddings for media asset search systems, product discovery experiences, and document retrieval applications.",
    "published": "Thu, 05 Feb 2026 20:35:34 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6e2225d549ed5ae2",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.16.0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0",
    "summary": "<h1>vLLM v0.16.0</h1>\n<h2>Highlights</h2>\n<p>This release features 440 commits from 203 contributors (7 new)!</p>\n<ul>\n<li><strong>PyTorch 2.10 upgrade</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>). This is a breaking change for environment dependency.</li>\n<li><strong>Async scheduling + Pipeline Parallelism</strong> is now fully supported, delivering <strong>30.8% E2E throughput improvement</strong> and <strong>31.8% TPOT improvement</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>).</li>\n<li><strong>Realtime API</strong>: A new WebSocket-based Realtime API enables streaming audio interactions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>), building on the Voxtral realtime infrastructure.</li>\n<li><strong>RLHF workflow improvements</strong>: Native NCCL-based weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise weight reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), and engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li><strong>Unified Parallel Drafting</strong> for speculative decoding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), plus spec decode now works with structured outputs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>) and penalty application in Model Runner V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>).</li>\n<li><strong>Major XPU platform overhaul</strong>: Deprecated IPEX in favor of vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>), adding MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), and FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>) support.</li>\n</ul>\n<h3>Model Support</h3>\n<ul>\n<li>New architectures: GLM-OCR with MTP (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33005\">#33005</a>), Qwen3-ASR (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33312\">#33312</a>), DeepSeek-OCR-2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33165\">#33165</a>), Intern-S1-Pro (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33636\">#33636</a>), MiniCPM-o 4.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33431\">#33431</a>), openPangu7B-VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32449\">#32449</a>), NemotronHPuzzle heterogeneous (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32549\">#32549</a>), MusicFlamingo (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32696\">#32696</a>), FunAudioChat (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/2\">#2</a>), ColBERT late interaction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33686\">#33686</a>), voyage-4-nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33720\">#33720</a>), GLM-5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34124\">#34124</a>).</li>\n<li>Speculative decoding: EAGLE3 for Hunyuan/HunyuanVL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33035\">#33035</a>), AFMoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33111\">#33111</a>), Mistral3 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a>).</li>\n<li>LoRA expansion: Gemma3 vision components (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32764\">#32764</a>), Nemotron-H MTP models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32265\">#32265</a>), Qwen3 output embedding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29816\">#29816</a>). Optimized fused MoE-LoRA kernel indexing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32770\">#32770</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32774\">#32774</a>), unpermute-aware fused MoE LoRA path (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32655\">#32655</a>), reduced kernel overhead for fewer active LoRAs with multiple CUDA graphs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32005\">#32005</a>).</li>\n<li>Features: Qwen3-Omni transcription (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29828\">#29828</a>), Mistral Large 3 with FlashInfer MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33174\">#33174</a>), LFM2 SigLIP2 intermediate encoder layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33370\">#33370</a>), Qwen3-Omni/GLM-4.xV MRoPE positioning fixes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33010\">#33010</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33039\">#33039</a>), embedding input for disabled modalities (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32493\">#32493</a>).</li>\n<li>Performance: GLM-4.7-GPTQ decode and MTP acceptance rate regression fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a>), DeepSeek V3.2 fast detokenization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33855\">#33855</a>), DeepSeek V3.2 tokenizer fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33832\">#33832</a>), GLM-5 MTP accuracy fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34385\">#34385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li>Async scheduling + Pipeline Parallelism: Full support with 30.8% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>), optimized spec decode + async scheduling with 1.5% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33612\">#33612</a>), deadlock fix for torchrun PP broadcast (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33701\">#33701</a>).</li>\n<li>Speculative decoding: Unified Parallel Drafting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), structured output support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>), penalty application in MRV2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>), skip softmax for all-greedy rejection sampling (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32852\">#32852</a>), correctness fix for spec tokens with prefill chunks (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33652\">#33652</a>).</li>\n<li>RLHF: Native NCCL weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li>Helion kernel framework: ConfigManager (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32740\">#32740</a>), kernel wrapper (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32964\">#32964</a>), kernel registry (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33203\">#33203</a>).</li>\n<li>PluggableLayer: Applied to linear layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33152\">#33152</a>) and Mamba layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33660\">#33660</a>).</li>\n<li>Batch invariance: Disable Cascade Attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32561\">#32561</a>), enable Triton attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33688\">#33688</a>).</li>\n<li>Performance: Grammar bitmask H2D copy on separate stream (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33059\">#33059</a>), zero-copy GQA for multimodal and CPU (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33732\">#33732</a>), early-reject oversized MM requests (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33502\">#33502</a>), CPU memory leak fix from Request reference cycle in prefix caching (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34183\">#34183</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<ul>\n<li><strong>NVIDIA</strong>: FlashInfer TRTLLM BF16 MoE integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32954\">#32954</a>), SM100 INT4 W4A16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32437\">#32437</a>), SM121 (DGX Spark) CUTLASS support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a>), MNNVL protocol for GB series (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33540\">#33540</a>), FlashInfer MLA concat optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31171\">#31171</a>), GDN attention layout optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33291\">#33291</a>), DeepGEMM FP8 MLA performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33568\">#33568</a>), wvSplitK_fp8 performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33527\">#33527</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33493\">#33493</a>), B200 MoE configs for Nemotron Nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32804\">#32804</a>), Super B200 TP2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33510\">#33510</a>), GLM 4.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32958\">#32958</a>), Mamba selective scan tuning for B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32873\">#32873</a>). Fix: DeepSeek R1 CUTLASS MLA on B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33637\">#33637</a>), QK Norm+RoPE fusion on B200+FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a>), CUTLASS FP8 blockwise on SM103a (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32224\">#32224</a>).</li>\n<li><strong>AMD ROCm</strong>: QWEN3-NEXT FP8 tunings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32042\">#32042</a>), AITER attention backend for Qwen3-Next (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32492\">#32492</a>), fused_add_rmsnorm_pad for GPT-OSS (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30976\">#30976</a>), Qwen3-Omni startup fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33077\">#33077</a>).</li>\n<li><strong>Intel XPU</strong>: Platform overhaul - deprecated IPEX, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>). New: unquantized MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>).</li>\n<li><strong>ARM CPU</strong>: KleidiAI INT4 dynamic quant with BF16 activations (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33122\">#33122</a>), NEON BFMMLA BF16 paged attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32263\">#32263</a>), vectorization backend optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30329\">#30329</a>), attention dispatch by head_dim alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32161\">#32161</a>).</li>\n<li><strong>IBM Z</strong>: BF16 kernel type for s390x (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33788\">#33788</a>).</li>\n<li><strong>torch.compile</strong>: Stop compiling identical artifacts (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34003\">#34003</a>), MoE cold start optimization option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33735\">#33735</a>), fix 32-bit indexing assumption (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33113\">#33113</a>), attention fusion pass fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33945\">#33945</a>).</li>\n<li><strong>Performance</strong>: Chat completion streaming optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33782\">#33782</a>), ORJSONResponse for faster API responses (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33548\">#33548</a>), MoE permute optimization for CUTLASS FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32892\">#32892</a>), shared/routed overlap for latent MoE on Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32790\">#32790</a>), FlashInfer autotune control flag (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34006\">#34006</a>).</li>\n</ul>\n<h3>Large Scale Serving</h3>\n<ul>\n<li>Disaggregated serving: Mooncake connector rework with bootstrap server (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31034\">#31034</a>), cross-layer KV cache layout at NIXL Connector V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33339\">#33339</a>), delay freeing blocks for aborted async loads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32255\">#32255</a>), async double-free fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33377\">#33377</a>), Ray multi-replica single-instance fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a>).</li>\n<li>EPLB: Capture logical experts with router replay (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33013\">#33013</a>), DP metadata fix for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32739\">#32739</a>).</li>\n<li>Metrics: KV offloading connector metrics (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/27942\">#27942</a>), labeled prompt token metrics for P/D disaggregation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33290\">#33290</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li>New: FP8 block quant for CompressedTensorsW8A16Fp8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33280\">#33280</a>), ModelOpt MXFP8 for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33786\">#33786</a>), NVFP4/FP8 on Turing GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33076\">#33076</a>), TP &gt; 4 for FP4 Gemm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31099\">#31099</a>).</li>\n<li>Bugfixes: FP8 online quantization memory fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31914\">#31914</a>), asymmetric W4A16 (ConchLinear) for CT (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33200\">#33200</a>), DeepSeek V3.2 NVFP4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33932\">#33932</a>), LoRA FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33879\">#33879</a>), quantized Falcon-H1 model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32728\">#32728</a>), quantized Mamba TP with n_groups=1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33257\">#33257</a>), CPU W8A8 with bias (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33582\">#33582</a>), CPU W8A8 3D input support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33727\">#33727</a>).</li>\n<li><strong>Deprecation</strong>: Removed BitBlas (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Realtime API</strong>: WebSocket-based streaming API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>) with Voxtral realtime support.</li>\n<li><strong>Responses API</strong>: Sampling parameters (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32609\">#32609</a>), return token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33212\">#33212</a>), return prompt token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33378\">#33378</a>), parser implementation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32712\">#32712</a>).</li>\n<li>Pooling API: Request schema consensus for ScoreRequest (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33060\">#33060</a>) and final standardization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31127\">#31127</a>).</li>\n<li>Tool calling: Fix multi-turn tool call ID preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32768\">#32768</a>), fix indexing double-counting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33141\">#33141</a>), GLM-4 incremental string streaming (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33218\">#33218</a>), DSV3.2 fast detokenization fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33964\">#33964</a>), MCP tools non-streaming fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32762\">#32762</a>).</li>\n<li>Structured outputs: Performance optimization with reasoning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33557\">#33557</a>), guidance vocab size fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33509\">#33509</a>).</li>\n<li>CLI: <code>--disable-access-log-for-endpoints</code> option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30011\">#30011</a>).</li>\n<li>UX: Nested configs in YAML files (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33193\">#33193</a>), GGUF <code>repo_id:quant_type</code> syntax (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33371\">#33371</a>), DeepSeek ReasoningParser with thinking enabled by default (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33221\">#33221</a>), remove noisy CT warning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33273\">#33273</a>), early tokenization validation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31366\">#31366</a>), reasoning_content backward compatibility (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33635\">#33635</a>), only include Authorization header when OPENAI_API_KEY is set (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33488\">#33488</a>).</li>\n<li>Features: run_batch transcription/translation support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33934\">#33934</a>), /server_info collect_env (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33246\">#33246</a>), OTEL tracing during model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31162\">#31162</a>), clear MM and encoder cache (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33452\">#33452</a>), HF Hub LoRA resolver (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/20320\">#20320</a>).</li>\n<li>Scoring: Fix multi-document scoring returning single result (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33837\">#33837</a>).</li>\n</ul>\n<h3>Security</h3>\n<ul>\n<li>Patch protobuf for <a href=\"https://github.com/advisories/GHSA-7gcm-g887-7qv7\" title=\"CVE-2026-0994\">CVE-2026-0994</a> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34253\">#34253</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li><strong>PyTorch 2.10</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>) - breaking change for environment dependency.</li>\n<li>huggingface-hub updates for Transformers v5 preparation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33473\">#33473</a>).</li>\n<li>Transformers v5 compatibility fixes across multiple models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33977\">#33977</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33683\">#33683</a>).</li>\n</ul>\n<h3>Deprecation &amp; Breaking Changes</h3>\n<ul>\n<li>Removed BitBlas quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n<li>Removed deprecated <code>reasoning_content</code> message field (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33402\">#33402</a>).</li>\n<li>Removed deprecated pooling items (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33477\">#33477</a>).</li>\n<li>Removed deprecated <code>VLLM_ALL2ALL_BACKEND</code> environment variable (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33535\">#33535</a>).</li>\n<li>Deprecated IPEX for XPU, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/aabbccddwasd\">@aabbccddwasd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Code4me2\">@Code4me2</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikchifo\">@ikchifo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jiangwu300\">@jiangwu300</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/pjs102793\">@pjs102793</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33963\">#33963</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sleepcoo\">@sleepcoo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33978\">#33978</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TundeAtSN\">@TundeAtSN</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a></li>\n</ul>",
    "published": "2026-02-13T06:13:20Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2cfc1c73d3671883",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.16.0rc3: [Bugfix] Fix MTP accuracy for GLM-5 (#34385)",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc3",
    "summary": "<p>Signed-off-by: mgoin <a href=\"mailto:mgoin64@gmail.com\">mgoin64@gmail.com</a><br />\n(cherry picked from commit <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/commit/ec12d39d44739bee408ec1473acc09e75daf1a5d\"><tt>ec12d39</tt></a>)</p>",
    "published": "2026-02-12T04:54:27Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9ae1e737b9ab2d0e",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.16.0rc2: Patch protobuf for CVE-2026-0994 (#34253)",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc2",
    "summary": "<p>Signed-off-by: Seiji Eicher <a href=\"mailto:seiji@anyscale.com\">seiji@anyscale.com</a><br />\nCo-authored-by: Kevin H. Luu <a href=\"mailto:khluu000@gmail.com\">khluu000@gmail.com</a><br />\n(cherry picked from commit <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/commit/5045d5c9831a3a4a423a409ccea521d299a43a9a\"><tt>5045d5c</tt></a>)</p>",
    "published": "2026-02-11T10:33:40Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bf4ad96790c2c64d",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.16.0rc1",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc1",
    "summary": "<p>[Frontend][last/5] Make pooling entrypoints request schema consensus.…</p>",
    "published": "2026-02-09T06:42:38Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ab7b6c6a13f1ac1c",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.15.2rc0: [Bugfix] Disable TRTLLM attention when KV transfer is enabled (#33192)",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.2rc0",
    "summary": "<p>Signed-off-by: Zhanqiu Hu <a href=\"mailto:zh338@cornell.edu\">zh338@cornell.edu</a></p>",
    "published": "2026-02-05T00:49:18Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "82c84921d9c73a9e",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.15.1",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1",
    "summary": "<p>v0.15.1 is a patch release with security fixes, RTX Blackwell GPU fixes support, and bug fixes.</p>\n<h2>Security</h2>\n<ul>\n<li><strong><a href=\"https://github.com/advisories/GHSA-6mq8-rvhq-8wgg\" title=\"CVE-2025-69223\">CVE-2025-69223</a></strong>: Updated aiohttp dependency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33621\">#33621</a>)</li>\n<li><strong><a href=\"https://github.com/advisories/GHSA-7gcm-g887-7qv7\" title=\"CVE-2026-0994\">CVE-2026-0994</a></strong>: Updated Protobuf dependency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33619\">#33619</a>)</li>\n</ul>\n<h2>Highlights</h2>\n<h3>Bugfix Hardware Support</h3>\n<ul>\n<li><strong>RTX Blackwell (SM120)</strong>: Fixed NVFP4 MoE kernel support for RTX Blackwell workstation GPUs. Previously, NVFP4 MoE models would fail to load on these GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33417\">#33417</a>)</li>\n<li><strong>FP8 kernel selection</strong>: Fixed FP8 CUTLASS group GEMM to properly fall back to Triton kernels on SM120 GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33285\">#33285</a>)</li>\n</ul>\n<h3>Model Support</h3>\n<ul>\n<li><strong>Step-3.5-Flash</strong>: New model support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33523\">#33523</a>)</li>\n</ul>\n<h3>Bugfix Model Support</h3>\n<ul>\n<li><strong>Qwen3-VL-Reranker</strong>: Fixed model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33298\">#33298</a>)</li>\n<li><strong>Whisper</strong>: Fixed FlashAttention2 with full CUDA graphs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33360\">#33360</a>)</li>\n</ul>\n<h3>Performance</h3>\n<ul>\n<li><strong>torch.compile cold-start</strong>: Fixed regression that increased cold-start compilation time (Llama3-70B: ~88s → ~22s) (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33441\">#33441</a>)</li>\n<li><strong>MoE forward pass</strong>: Optimized by caching layer name computation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33184\">#33184</a>)</li>\n</ul>\n<h3>Bug Fixes</h3>\n<ul>\n<li>Fixed prefix cache hit rate of 0% with GPT-OSS style hybrid attention models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33524\">#33524</a>)</li>\n<li>Enabled Triton MoE backend for FP8 per-tensor dynamic quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33300\">#33300</a>)</li>\n<li>Disabled unsupported Renormalize routing methods for TRTLLM per-tensor FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33620\">#33620</a>)</li>\n<li>Fixed speculative decoding metrics crash when no tokens generated (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33729\">#33729</a>)</li>\n<li>Disabled fast MoE cold start optimization with speculative decoding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33624\">#33624</a>)</li>\n<li>Fixed ROCm skinny GEMM dispatch logic (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33366\">#33366</a>)</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li>Pinned LMCache &gt;= v0.3.9 for API compatibility (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33440\">#33440</a>)</li>\n</ul>\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/zaristei2\">@zaristei2</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33621\">#33621</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/compare/v0.15.0...v0.15.1\"><tt>v0.15.0...v0.15.1</tt></a></p>",
    "published": "2026-02-05T01:01:39Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d9c13dbb510e62f5",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.15.1rc1",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1rc1",
    "summary": "<p>[BugFix][Spec Decoding] Fix negative accepted tokens metric crash (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/3\">#3</a>…</p>",
    "published": "2026-02-04T01:28:32Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "131daa0d67d1bfbe",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.15.1rc0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1rc0",
    "summary": "<p>[torch.compile] Don't do the fast moe cold start optimization if ther…</p>",
    "published": "2026-02-03T08:07:18Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "68b7491da1db14bf",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.16.0rc0: [Docs] Adding links and intro to Speculators and LLM Compressor (#32849)",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc0",
    "summary": "<p>Signed-off-by: Aidan Reilly <a href=\"mailto:aireilly@redhat.com\">aireilly@redhat.com</a><br />\nSigned-off-by: Harry Mellor <a href=\"mailto:19981378+hmellor@users.noreply.github.com\">19981378+hmellor@users.noreply.github.com</a><br />\nCo-authored-by: Harry Mellor <a href=\"mailto:19981378+hmellor@users.noreply.github.com\">19981378+hmellor@users.noreply.github.com</a></p>",
    "published": "2026-01-29T22:12:35Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "877d9d2f64c35601",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.15.0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.0",
    "summary": "<h2>Highlights</h2>\n<p>This release features 335 commits from 158 contributors (39 new)!</p>\n<h3>Model Support</h3>\n<ul>\n<li><strong>New architectures</strong>: Kimi-K2.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33131\">#33131</a>), Molmo2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30997\">#30997</a>), Step3vl 10B (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32329\">#32329</a>), Step1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32511\">#32511</a>), GLM-Lite (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31386\">#31386</a>), Eagle2.5-8B VLM (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32456\">#32456</a>).</li>\n<li><strong>LoRA expansion</strong>: Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30802\">#30802</a>), InternVL2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32397\">#32397</a>), MiniMax M2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32763\">#32763</a>).</li>\n<li><strong>Speculative decoding</strong>: EAGLE3 for Pixtral/LlavaForConditionalGeneration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32542\">#32542</a>), Qwen3 VL MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32048\">#32048</a>), draft model support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/24322\">#24322</a>).</li>\n<li><strong>Embeddings</strong>: BGE-M3 sparse embeddings and ColBERT embeddings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/14526\">#14526</a>).</li>\n<li><strong>Model enhancements</strong>: Voxtral streaming architecture (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32861\">#32861</a>), SharedFusedMoE for Qwen3MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32082\">#32082</a>), dynamic resolution for Nemotron Nano VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32121\">#32121</a>), Molmo2 vision backbone quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32385\">#32385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li><strong>Async scheduling + Pipeline Parallelism</strong>: <code>--async-scheduling</code> now works with pipeline parallelism (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32359\">#32359</a>).</li>\n<li><strong>Mamba prefix caching</strong>: Block-aligned prefix caching for Mamba/hybrid models with <code>--enable-prefix-caching --mamba-cache-mode align</code>. Achieves ~2x speedup by caching Mamba states directly (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30877\">#30877</a>).</li>\n<li><strong>Session-based streaming input</strong>: New incremental input support for interactive workloads like ASR. Accepts async generators producing <code>StreamingInput</code> objects while maintaining KV cache alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28973\">#28973</a>).</li>\n<li><strong>Model Runner V2</strong>: VLM support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32546\">#32546</a>), architecture improvements.</li>\n<li><strong>LoRA</strong>: Inplace loading for memory efficiency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31326\">#31326</a>).</li>\n<li><strong>AOT compilation</strong>: torch.compile inductor artifacts support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/25205\">#25205</a>).</li>\n<li><strong>Performance</strong>: KV cache offloading redundant load prevention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29087\">#29087</a>), FlashAttn attention/cache update separation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/25954\">#25954</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<h4>NVIDIA</h4>\n<ul>\n<li><strong>Blackwell defaults</strong>: FlashInfer MLA is now the default MLA backend on Blackwell, with TRTLLM as default prefill (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32615\">#32615</a>).</li>\n<li><strong>MoE performance</strong>: 1.2-2% E2E throughput improvement via grouped topk kernel fusion (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32058\">#32058</a>), NVFP4 small-batch decoding improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30885\">#30885</a>), faster cold start for MoEs with torch.compile (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32805\">#32805</a>).</li>\n<li><strong>FP4 kernel optimization</strong>: Up to 65% faster FP4 quantization on Blackwell (SM100F) using 256-bit loads, ~4% E2E throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32520\">#32520</a>).</li>\n<li><strong>Kernel improvements</strong>: topk_sigmoid kernel for MoE routing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31246\">#31246</a>), atomics reduce counting for SplitK skinny GEMMs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29843\">#29843</a>), fused cat+quant for FP8 KV cache in MLA (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32950\">#32950</a>).</li>\n<li><strong>torch.compile</strong>: SiluAndMul and QuantFP8 CustomOp compilation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32806\">#32806</a>), Triton prefill attention performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32403\">#32403</a>).</li>\n</ul>\n<h4>AMD ROCm</h4>\n<ul>\n<li><strong>MoRI EP</strong>: High-performance all2all backend for Expert Parallel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28664\">#28664</a>).</li>\n<li><strong>Attention improvements</strong>: Shuffle KV cache layout and assembly paged attention kernel for AiterFlashAttentionBackend (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29887\">#29887</a>).</li>\n<li><strong>FP4 support</strong>: MLA projection GEMMs with dynamic quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32238\">#32238</a>).</li>\n<li><strong>Consumer GPU support</strong>: Flash Attention Triton backend on RDNA3/RDNA4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32944\">#32944</a>).</li>\n</ul>\n<h4>Other Platforms</h4>\n<ul>\n<li><strong>TPU</strong>: Pipeline parallelism support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28506\">#28506</a>), backend option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32438\">#32438</a>).</li>\n<li><strong>Intel XPU</strong>: AgRsAll2AllManager for distributed communication (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32654\">#32654</a>).</li>\n<li><strong>CPU</strong>: NUMA-aware acceleration for TP/DP inference on ARM (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32792\">#32792</a>), PyTorch 2.10 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32869\">#32869</a>).</li>\n<li><strong>Whisper</strong>: torch.compile support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30385\">#30385</a>).</li>\n<li><strong>WSL</strong>: Platform compatibility fix for Windows Subsystem for Linux (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32749\">#32749</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li><strong>MXFP4</strong>: W4A16 support for compressed-tensors MoE models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32285\">#32285</a>).</li>\n<li><strong>Non-gated MoE</strong>: Quantization support with Marlin, NVFP4 CUTLASS, FP8, INT8, and compressed-tensors (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32257\">#32257</a>).</li>\n<li><strong>Intel</strong>: Quantization Toolkit integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31716\">#31716</a>).</li>\n<li><strong>FP8 KV cache</strong>: Per-tensor and per-attention-head quantization via llmcompressor (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30141\">#30141</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Responses API</strong>: Partial message generation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32100\">#32100</a>), <code>include_stop_str_in_output</code> tuning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32383\">#32383</a>), <code>prompt_cache_key</code> support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32824\">#32824</a>).</li>\n<li><strong>OpenAI API</strong>: <code>skip_special_tokens</code> configuration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32345\">#32345</a>).</li>\n<li><strong>Score endpoint</strong>: Flexible input formats with <code>data_1</code>/<code>data_2</code> and <code>queries</code>/<code>documents</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32577\">#32577</a>).</li>\n<li><strong>Render endpoints</strong>: New endpoints for prompt preprocessing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32473\">#32473</a>).</li>\n<li><strong>Whisper API</strong>: <code>avg_logprob</code> and <code>compression_ratio</code> in verbose_json segments (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31059\">#31059</a>).</li>\n<li><strong>Security</strong>: FIPS 140-3 compliant hash option for enterprise/government users (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32386\">#32386</a>), <code>--ssl-ciphers</code> CLI argument (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30937\">#30937</a>).</li>\n<li><strong>UX improvements</strong>: Auto <code>api_server_count</code> based on <code>dp_size</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32525\">#32525</a>), wheel variant auto-detection during install (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32948\">#32948</a>), custom profiler URI schemes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32393\">#32393</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li>FlashInfer v0.6.1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30993\">#30993</a>)</li>\n<li>Transformers 4.57.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32287\">#32287</a>)</li>\n<li>PyTorch 2.10 for CPU backend (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32869\">#32869</a>)</li>\n<li>DeepGEMM newer version (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32479\">#32479</a>)</li>\n</ul>\n<h3>Breaking Changes &amp; Deprecations</h3>\n<ul>\n<li><strong>Metrics</strong>: Removed deprecated <code>vllm:time_per_output_token_seconds</code> metric - use <code>vllm:inter_token_latency_seconds</code> instead (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32661\">#32661</a>).</li>\n<li><strong>Environment variables</strong>: Removed deprecated environment variables (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32812\">#32812</a>).</li>\n<li><strong>Quantization</strong>: DeepSpeedFp8 removed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32679\">#32679</a>), RTN removed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32697\">#32697</a>), HQQ deprecated (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32681\">#32681</a>).</li>\n</ul>\n<h3>Bug Fixes</h3>\n<ul>\n<li><strong>Speculative decoding</strong>: Eagle draft_model_config fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31753\">#31753</a>).</li>\n<li><strong>DeepSeek</strong>: DeepSeek-V3.1 + DeepGEMM incompatible scale shapes fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32361\">#32361</a>).</li>\n<li><strong>Distributed</strong>: DP+MoE inference fix via CpuCommunicator (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31867\">#31867</a>), P/D with non-MoE DP fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33037\">#33037</a>).</li>\n<li><strong>EPLB</strong>: Possible deadlock fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32418\">#32418</a>).</li>\n<li><strong>NIXL</strong>: UCX memory leak fix by exporting UCX_MEM_MMAP_HOOK_MODE=none (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32181\">#32181</a>).</li>\n<li><strong>Structured output</strong>: Outlines byte fallback handling fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31391\">#31391</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/YunzhuLu\">@YunzhuLu</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32126\">#32126</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/emricksini-h\">@emricksini-h</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30784\">#30784</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/dsfaccini\">@dsfaccini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32289\">#32289</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ofirzaf\">@ofirzaf</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32312\">#32312</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/seekskyworld\">@seekskyworld</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32321\">#32321</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/brian033\">@brian033</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31715\">#31715</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TomerBN-Nvidia\">@TomerBN-Nvidia</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32257\">#32257</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/vanshilshah97\">@vanshilshah97</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32448\">#32448</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/George-Polya\">@George-Polya</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32385\">#32385</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/T1mn\">@T1mn</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32411\">#32411</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/mritunjaysharma394\">@mritunjaysharma394</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31492\">#31492</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/randzero\">@randzero</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32511\">#32511</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/DemingCheng\">@DemingCheng</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32556\">#32556</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/iboiko-habana\">@iboiko-habana</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32471\">#32471</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/honglyua-il\">@honglyua-il</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32462\">#32462</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/hyeongyun0916\">@hyeongyun0916</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32473\">#32473</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/DanielMe\">@DanielMe</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32560\">#32560</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/netanel-haber\">@netanel-haber</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32121\">#32121</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/longregen\">@longregen</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28784\">#28784</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jasonyanwenl\">@jasonyanwenl</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32749\">#32749</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Wauplin\">@Wauplin</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32788\">#32788</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikaadil\">@ikaadil</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32775\">#32775</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/alexsun07\">@alexsun07</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28664\">#28664</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/liranschour\">@liranschour</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30207\">#30207</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/AuYang261\">@AuYang261</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32844\">#32844</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/diviramon\">@diviramon</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32393\">#32393</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/RishabhSaini\">@RishabhSaini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32884\">#32884</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/MatteoFari\">@MatteoFari</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32397\">#32397</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/peakcrosser7\">@peakcrosser7</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30877\">#30877</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/orionr\">@orionr</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30443\">#30443</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/marksverdhei\">@marksverdhei</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32614\">#32614</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/joninco\">@joninco</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32935\">#32935</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/monajafi-amd\">@monajafi-amd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32944\">#32944</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ruizcrp\">@ruizcrp</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32988\">#32988</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sjhddh\">@sjhddh</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32983\">#32983</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/HirokenOvo\">@HirokenOvo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32646\">#32646</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Chenhao-Guan\">@Chenhao-Guan</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32763\">#32763</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/joshuadeng\">@joshuadeng</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28973\">#28973</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ZhanqiuHu\">@ZhanqiuHu</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33016\">#33016</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/compare/v0.14.1...v0.15.0\"><tt>v0.14.1...v0.15.0</tt></a></p>",
    "published": "2026-01-29T10:21:01Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b67a55266dc20649",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.65.0 corresponding to NGC container 26.01",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.65.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<div class=\"markdown-alert markdown-alert-note\"><p class=\"markdown-alert-title\"><svg class=\"octicon octicon-info mr-2\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg>Note</p><p>Windows support is deprecated, latest assets build for windows could be found in is release <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.51.0\">2.51.0 / 25.01</a></p>\n</div>\n  <details>\n    <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Exposed HTTP errors for non-JSON format response.</p>\n</li>\n<li>\n<p>Perf Analyzer dependencies were made optional when installing tritonclient via pip.</p>\n</li>\n<li>\n<p>Added support for <code>nv_inference_first_response_histogram_ms</code> metric when running models in coupled (aka non-decoupled) mode.</p>\n</li>\n<li>\n<p>Fixed an issue where a crashed Python backend stub process was not correctly detected leading to failed inference requests.</p>\n</li>\n<li>\n<p>Fixed an issue where a malicious HTTP request could exhaust all available system memory leading to a process crash or denial of service.</p>\n</li>\n<li>\n<p>Fixed an issue where loading a TensorRT engine larger than 30GB could cause out of memory errors despite sufficient memory to load the engine being available.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Known Issues</h2>\n<ul>\n<li>\n<p>If you are using the vllm backend to run models that accept video inputs, then do not upgrade to Triton 26.01 and wait for Triton 26.02</p>\n</li>\n<li>\n<p>Triton python package uses outdated dependency <code>starlette</code> package version.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.65.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r26.01#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n  </details>\n  <details>\n    <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.65.0/tritonserver2.65.0-igpu.tar\"><code>tritonserver2.65.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> [<code>2.10.0a0+a36e1d3](https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html), **Python** </code>3.12` and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.65.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n  </details>\n  <details>\n    <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.65.0/tritonserver2.65.0-agx.tar\"><code>tritonserver2.65.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.1</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+a36e1d3</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:26.01-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n  </details>\n<details>\n    <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.65.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.1.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n</details>\n<details>\n  <h2>ManyLinux Assets (early access)</h2>\n<p>This release was compiled with AlmaLinux 8.9 based out of <code>manylinux_2_28</code> and can be used on RHEL8  and later versions.<br />\nSee the included README.md for complete details about installation, verification, and support.<br />\nThis release supports CUDA 13, TensorRT 10.14.1.48, Onnx Runtime 1.23.2, PyTorch 2.10.0a0+b4e4ee8, Python 3.12 and supports ensembles.<br />\nSome optional backend features such as the PyTorch backend's TorchTRT extension are not currently supported.</p>\n</details>",
    "published": "2026-02-11T22:21:05Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1ef2c05d1093cdc8",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.64.0 corresponding to NGC container 25.12",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.64.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n  <details>\n    <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Fixed an issue with Triton Server’s Sagemaker Service which could result in a server crash resulting from a race condition, caused by unprotected access to the list of models.</p>\n</li>\n<li>\n<p>Extended the set of accelerated PyTorch libraries included with the Triton PyTorch backend.</p>\n</li>\n<li>\n<p>Upgraded Triton Client’s Golang dependencies to latest stable versions to resolve known issues with the previous version of the dependencies.</p>\n</li>\n<li>\n<p>The OpenAI-compatible frontend has transitioned from beta to a stable release.</p>\n</li>\n<li>\n<p>Added <code>echo</code> request parameter for TensorRT-LLM and Python backends to OpenAI-compatible API frontend <code>v1/completions</code> endpoint.</p>\n</li>\n<li>\n<p>Enabled OpenAI-compatible API frontend multi-LoRA support for TensorRT-LLM backend.</p>\n</li>\n<li>\n<p>Backends can now implement the new  <code>TRITONBACKEND_ModelInstanceReady</code> function to report accurate model readiness status.</p>\n</li>\n<li>\n<p>Updated the Python backend to accurately report model readiness.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Known Issues</h2>\n<ul>\n<li>\n<p>The error <code>'__init__(): incompatible function arguments</code> may occur when using TensorRT-LLM backend python models. To suppress the error temporarily, set input tensor <code>stream</code> with a boolean value explicitly in the request header.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.64.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.12#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n  </details>\n  <details>\n    <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.64.0/tritonserver2.64.0-igpu.tar\"><code>tritonserver2.64.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b4e4ee8</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.64.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n  </details>\n  <details>\n    <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.64.0/tritonserver2.64.0-agx.tar\"><code>tritonserver2.64.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.1</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b4e4ee8</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.12-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.64.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.1.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n  </details>\n  <details>\n    <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.12, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n  </details>\n<details>\n  <h2>ManyLinux Assets (early access)</h2>\n<p>This release was compiled with AlmaLinux 8.9 based out of <code>manylinux_2_28</code> and can be used on RHEL8  and later versions.<br />\nSee the included README.md for complete details about installation, verification, and support.<br />\nThis release supports CUDA 13, TensorRT 10.14.1.48, Onnx Runtime 1.23.2, PyTorch 2.10.0a0+b4e4ee8, Python 3.12 and supports ensembles.<br />\nSome optional backend features such as the PyTorch backend's TorchTRT extension are not currently supported.</p>\n</details>",
    "published": "2026-02-11T22:17:34Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f26808a6663f56c4",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.63.0 corresponding to NGC container 25.11",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.63.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Enabled endpoint <code>v1/embeddings</code> for vLLM backend in OpenAI-compatible API server.</p>\n</li>\n<li>\n<p>Enabled <code>echo</code> parameter for TensorRT-LLM and Python backends in OpenAI-compatible API server.</p>\n</li>\n<li>\n<p>Improved error handling in OpenAI-compatible API server by providing more specific and OpenAI-compliant error codes.</p>\n</li>\n<li>\n<p>Upgraded the version of starlette used by OpenAI frontend.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.63.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.10#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.63.0/tritonserver2.63.0-igpu.tar\"><code>tritonserver2.63.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b558c98</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.63.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.63.0/tritonserver2.63.0-agx.tar\"><code>tritonserver2.63.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b558c98</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.11-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.63.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>",
    "published": "2026-01-30T17:03:42Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9c10e7e1095fc2e0",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.62.0 corresponding to NGC container 25.10",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.62.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Fixed a server crash issue caused by specially crafted request messages sent to <code>/v2/models/&lt;model_name&gt;/infer</code>.</p>\n</li>\n<li>\n<p>Fixed a server crash issue caused by incorrect handling of malformed HTTP requests.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Triton python package uses outdated dependency <code>starlette</code> package version.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>Enabling vLLM metrics during inferences causes the engine to crash.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka ‘model.pt’). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.62.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.10#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.62.0/tritonserver2.62.0-igpu.tar\"><code>tritonserver2.62.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.1</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+145a3a7</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.62.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.62.0/tritonserver2.62.0-agx.tar\"><code>tritonserver2.62.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.3.9</code>, <strong>Onnx Runtime</strong> <code>1.23.1</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.10-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.06 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.62.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.06. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2026-01-30T16:56:51Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7cd095f5f55dba53",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.61.0 corresponding to NGC container 25.09",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.61.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Static key authentication for OpenAI Frontend APIs</p>\n</li>\n<li>\n<p>Prevented models outside Triton’s repository being loaded from OpenAI Frontend.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.61.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.09#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.61.0/tritonserver2.61.0-igpu.tar\"><code>tritonserver2.61.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.09/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.61.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.61.0/tritonserver2.61.0-agx.tar\"><code>tritonserver2.61.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.3.9</code>, <strong>Onnx Runtime</strong> <code>1.23.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.09-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.06 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.61.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.06. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-10-07T22:10:06Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0b6403585fa5c6b3",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.60.0 corresponding to NGC container 25.08",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.60.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Added CUDA 13 support.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Triton ONNX Runtime Backend build uses <a href=\"https://github.com/microsoft/onnxruntime/commit/1d1712fdafb9e61b2d6d033c4433c1033395d7e7\">microsoft/onnxruntime/commit/1d1712fdaf</a> and may have some limitations on DGX Spark hardware which will be addressed in future versions.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy</p>\n</li>\n<li>\n<p>CuPy does not support CUDA 13 at the time of writing. Issues may be encountered when using CuPy before it officially supports CUDA 13, see <a href=\"https://github.com/triton-inference-server/server/tree/r25.08/python/openai#pre-requisites\">https://github.com/triton-inference-server/server/tree/r25.08/python/openai#pre-requisites</a> for more details</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.60.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.08#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.60.0/tritonserver2.60.0-igpu.tar\"><code>tritonserver2.60.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.0+1d1712fdaf</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a+34c6371d24</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.07/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.60.0/tritonserver2.60.0-agx.tar\"><code>tritonserver2.60.0-agx.tar</code></a>.</p>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Jetson AGX release for 25.08, requires DCGM version 4 to be installed in order to use GPU metrics.<br />\nPlease use following command to install DCGM 4:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>curl -o /tmp/cuda-keyring.deb \\\n         https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb \\\n     &amp;&amp; apt install /tmp/cuda-keyring.deb \\\n     &amp;&amp; rm /tmp/cuda-keyring.deb \\\n     &amp;&amp; apt update \\\n     &amp;&amp; apt install --yes --no-install-recommends \\\n                  datacenter-gpu-manager-4-core=1:4.4.0-1\n</code></pre></div>\n</blockquote>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.2.6</code>, <strong>Onnx Runtime</strong> <code>1.23.0+1d1712fdaf</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+34c6371</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.08-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.60.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.21.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-08-26T22:15:33Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "671420a2a130cda5",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.59.1 corresponding to NGC container 25.07",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.59.1",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Fixed vulnerabilities in the Triton Inference Server.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>There was no python wheels packages released as part of 25.07 release</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.59.1_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.07, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.59.1/tritonserver2.59.1-igpu.tar\"><code>tritonserver2.59.1-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.6</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.07/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.04-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.04-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.59.1/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.20.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.10.0.31</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-07-29T21:50:01Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d62c6c9a740a083f",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.59.0 corresponding to NGC container 25.06",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.59.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Improved ensemble model performance in scenarios that allow out-of-order responses by increasing maximum throughput and reducing latency.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.59.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.06, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.59.0/tritonserver2.59.0-igpu.tar\"><code>tritonserver2.59.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.6</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.06/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.04-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.04-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.59.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.20.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.10.0.31</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-06-30T22:54:06Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ebc920b10350277a",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.58.0 corresponding to NGC container 25.05",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.58.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Optional “execution_context_allocation_strategy” parameter in the TensorRT backend configuration allows selection of memory allocation behavior.</li>\n<li>Support Tool calling functionality with Llama 3 and Mistral models in OpenAI frontend.</li>\n<li>Improvements around memory allocation and various bug fixes.</li>\n<li>GenAI-Perf now offers a new configuration file alongside the command line.</li>\n<li>GenAI-Perf now collects GPU metrics from /metrics endpoint exposed by DCGM Exporter.</li>\n<li>GenAI-Perf supports new Power, Utilization, Ecc, Errors and PCie metrics.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM backend for 25.05 might be unstable with the vLLM V1 architecture. We recommend switching to V0 for this release, by setting <code>VLLM_USE_V1</code> environment variable to 0. However, users should be aware that vLLM's V0 API is affected by vulnerabilities.</p>\n</li>\n<li>\n<p>vLLM containers include vllm version 0.8.4 which is affected by vulnerabilities.<br />\nWorkarounds:<br />\nPrior to the fix, your options include:</p>\n<ul>\n<li>Do not expose the vLLM host to a network where any untrusted connections may reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that port used is random.</li>\n</ul>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.58.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.05, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.58.0/tritonserver2.58.0-igpu.tar\"><code>tritonserver2.58.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.10.0.31</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.5</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.05/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.58.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.03 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.03-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.03-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.58.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.03. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.19.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.9.0.34</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-06-30T22:54:03Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d171b80d5d104921",
    "source": "triton_releases",
    "source_weight": 0.25,
    "title": "Release 2.57.0 corresponding to NGC container 25.04",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.57.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Exposed gRPC infer thread count as a server option.</li>\n<li>Improved server stability during the gRPC client cancellation.</li>\n<li>Improved server stability in tracing mode.</li>\n<li>Added BLS decoupled request cancellation in the Python Backend</li>\n<li>GenAI-Perf now offers a new configuration file alongside the command line.</li>\n<li>GenAI-Perf now supports the Huggingface TGI generated endpoint.</li>\n<li>GenAI-Perf added a Token per second per user (TPS/user) metric.</li>\n<li>GenAI-Perf metric parsing speed was increased by 60%.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM backend for 25.04 might be unstable with the vLLM V1 architecture. We recommend switching to V0 for this release, by setting <code>VLLM_USE_V1</code> environment variable to 0. However, users should be aware that vLLM's V0 API is affected by vulnerabilities.</p>\n</li>\n<li>\n<p>vLLM containers include vllm version 0.8.1 which is affected by new vulnerabilities.<br />\nWorkarounds:<br />\nPrior to the fix, your options include:</p>\n<ul>\n<li>Do not expose the vLLM host to a network where any untrusted connections may reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that port used is random.</li>\n</ul>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.57.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.04#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.04, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.57.0/tritonserver2.57.0-igpu.tar\"><code>tritonserver2.57.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.9.0.34</code>, <strong>Onnx Runtime</strong> <code>1.21.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.7.0a0+79aa17489c.nv25.4</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.04/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.57.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.03 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.03-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.03-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.57.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.03. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.18.2</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.9.0.34</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-06-30T22:54:00Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8bed79565182bbc6",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.14",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.14",
    "summary": "<h1>Release Notes</h1>\n<h2>[2026-02-10]</h2>\n<h3>llama-index-callbacks-wandb [0.4.2]</h3>\n<ul>\n<li>Fix potential crashes and improve security defaults in core components (<a href=\"https://github.com/run-llama/llama_index/pull/20610\">#20610</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.14]</h3>\n<ul>\n<li>fix: catch pydantic ValidationError in VectorStoreQueryOutputParser (<a href=\"https://github.com/run-llama/llama_index/pull/20450\">#20450</a>)</li>\n<li>fix: distinguish empty string from None in MediaResource.hash (<a href=\"https://github.com/run-llama/llama_index/pull/20451\">#20451</a>)</li>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n<li>fix(core): fallback to bundled nltk cache if env var missing (<a href=\"https://github.com/run-llama/llama_index/pull/20528\">#20528</a>)</li>\n<li>feat(callbacks): add TokenBudgetHandler for cost governance (<a href=\"https://github.com/run-llama/llama_index/pull/20546\">#20546</a>)</li>\n<li>fix(core):handled a edge case in truncate_text function (<a href=\"https://github.com/run-llama/llama_index/pull/20551\">#20551</a>)</li>\n<li>fix(core):fix in types Thread passing None when target is None instead of copy_context().run (<a href=\"https://github.com/run-llama/llama_index/pull/20553\">#20553</a>)</li>\n<li>chore: bump llama-index lockfile, and minor test tweaks (<a href=\"https://github.com/run-llama/llama_index/pull/20556\">#20556</a>)</li>\n<li>Compatibility for workflows context changes (<a href=\"https://github.com/run-llama/llama_index/pull/20557\">#20557</a>)</li>\n<li>test(core): fix cache dir path test for Windows compatibility (<a href=\"https://github.com/run-llama/llama_index/pull/20566\">#20566</a>)</li>\n<li>fix(tests): enforce utf-8 encoding in json reader tests for windows compatibility (<a href=\"https://github.com/run-llama/llama_index/pull/20576\">#20576</a>)</li>\n<li>Fix BM25Retriever mapping in upgrade tool / 修复升级工具中的 BM25Retriever 映射 (<a href=\"https://github.com/run-llama/llama_index/pull/20582\">#20582</a>)</li>\n<li>fix(agent): handle empty LLM responses with retry logic and add test cases (<a href=\"https://github.com/run-llama/llama_index/pull/20596\">#20596</a>)</li>\n<li>fix: add show_progress parameter to run_transformations to prevent unexpected keyword argument error (<a href=\"https://github.com/run-llama/llama_index/pull/20608\">#20608</a>)</li>\n<li>Fix potential crashes and improve security defaults in core components (<a href=\"https://github.com/run-llama/llama_index/pull/20610\">#20610</a>)</li>\n<li>Add core 3.14 tests (<a href=\"https://github.com/run-llama/llama_index/pull/20619\">#20619</a>)</li>\n</ul>\n<h3>llama-index-embeddings-cohere [0.7.0]</h3>\n<ul>\n<li>fix(embeddings-cohere): add retry logic with tenacity (<a href=\"https://github.com/run-llama/llama_index/pull/20592\">#20592</a>)</li>\n</ul>\n<h3>llama-index-embeddings-google-genai [0.3.2]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n</ul>\n<h3>llama-index-embeddings-siliconflow [0.3.2]</h3>\n<ul>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n</ul>\n<h3>llama-index-embeddings-upstage [0.5.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-graph-stores-falkordb [0.4.2]</h3>\n<ul>\n<li>fix(falkordb): Fix MENTIONS relationship creation with triplet_source_id (<a href=\"https://github.com/run-llama/llama_index/pull/20650\">#20650</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.8]</h3>\n<ul>\n<li>chore: Update cacheable Anthropic models (<a href=\"https://github.com/run-llama/llama_index/pull/20581\">#20581</a>)</li>\n<li>chore: add support for opus 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20635\">#20635</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.8]</h3>\n<ul>\n<li>fix bedrock converse empty tool config issue (<a href=\"https://github.com/run-llama/llama_index/pull/20571\">#20571</a>)</li>\n<li>fix(llms-bedrock-converse): improve bedrock converse retry handling (<a href=\"https://github.com/run-llama/llama_index/pull/20590\">#20590</a>)</li>\n<li>feat(bedrock-converse): Add support for Claude Opus 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20637\">#20637</a>)</li>\n<li>Add support for adaptive thinking in Bedrock (<a href=\"https://github.com/run-llama/llama_index/pull/20659\">#20659</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-llms-cohere [0.7.1]</h3>\n<ul>\n<li>Feat: add custom base_url support to Cohere LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20534\">#20534</a>)</li>\n<li>fix(llms-cohere): handle additional error types in retry logic (<a href=\"https://github.com/run-llama/llama_index/pull/20591\">#20591</a>)</li>\n</ul>\n<h3>llama-index-llms-dashscope [0.5.2]</h3>\n<ul>\n<li>fix(dashscope): remove empty tool_calls from assistant messages (<a href=\"https://github.com/run-llama/llama_index/pull/20535\">#20535</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.8.7]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n<li>fix(decorator):adds logic to llm_retry_decorator for async methods. (<a href=\"https://github.com/run-llama/llama_index/pull/20588\">#20588</a>)</li>\n<li>Fix/google genai cleanup (<a href=\"https://github.com/run-llama/llama_index/pull/20607\">#20607</a>)</li>\n<li>fix(google-genai): skip model meta fetch when not needed (<a href=\"https://github.com/run-llama/llama_index/pull/20639\">#20639</a>)</li>\n</ul>\n<h3>llama-index-llms-huggingface-api [0.6.2]</h3>\n<ul>\n<li>Update sensible default provider for huggingface inference api (<a href=\"https://github.com/run-llama/llama_index/pull/20589\">#20589</a>)</li>\n</ul>\n<h3>llama-index-llms-langchain [0.7.1]</h3>\n<ul>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.18]</h3>\n<ul>\n<li>OpenAI response fix (<a href=\"https://github.com/run-llama/llama_index/pull/20538\">#20538</a>)</li>\n<li>feat: Add support for gpt-5.2-chat model (<a href=\"https://github.com/run-llama/llama_index/pull/20549\">#20549</a>)</li>\n<li>fix(openai): make image_url detail optional in message dict (<a href=\"https://github.com/run-llama/llama_index/pull/20609\">#20609</a>)</li>\n<li>Add new reasoning types (<a href=\"https://github.com/run-llama/llama_index/pull/20612\">#20612</a>)</li>\n<li>fix(openai): exclude unsupported params for all reasoning models (<a href=\"https://github.com/run-llama/llama_index/pull/20627\">#20627</a>)</li>\n</ul>\n<h3>llama-index-llms-openai-like [0.6.0]</h3>\n<ul>\n<li>make transformers an optional dependency for openai-like (<a href=\"https://github.com/run-llama/llama_index/pull/20580\">#20580</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.4]</h3>\n<ul>\n<li>make transformers an optional dependency for openai-like (<a href=\"https://github.com/run-llama/llama_index/pull/20580\">#20580</a>)</li>\n</ul>\n<h3>llama-index-llms-siliconflow [0.4.3]</h3>\n<ul>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.7.0]</h3>\n<ul>\n<li>add new upstage model(solar-pro3) (<a href=\"https://github.com/run-llama/llama_index/pull/20544\">#20544</a>)</li>\n</ul>\n<h3>llama-index-llms-vllm [0.6.2]</h3>\n<ul>\n<li>feat: add openai-like server mode for VllmServer (<a href=\"https://github.com/run-llama/llama_index/pull/20537\">#20537</a>)</li>\n</ul>\n<h3>llama-index-memory-bedrock-agentcore [0.1.2]</h3>\n<ul>\n<li>Add event and memory record deletion methods in bedrock-agentcorememory (<a href=\"https://github.com/run-llama/llama_index/pull/20428\">#20428</a>)</li>\n<li>chore(deps): update llama-index-core dependency lock to include 0.14.x (<a href=\"https://github.com/run-llama/llama_index/pull/20483\">#20483</a>)</li>\n</ul>\n<h3>llama-index-memory-mem0 [1.0.0]</h3>\n<ul>\n<li>fix: mem0 integration cleanup + refactor (<a href=\"https://github.com/run-llama/llama_index/pull/20532\">#20532</a>)</li>\n</ul>\n<h3>llama-index-node-parser-chonkie [0.1.1]</h3>\n<ul>\n<li>feat: add chonkie integration (<a href=\"https://github.com/run-llama/llama_index/pull/20622\">#20622</a>)</li>\n<li>update readme (<a href=\"https://github.com/run-llama/llama_index/pull/20656\">#20656</a>)</li>\n</ul>\n<h3>llama-index-node-parser-docling [0.4.2]</h3>\n<ul>\n<li>fix: catch pydantic ValidationError in VectorStoreQueryOutputParser (<a href=\"https://github.com/run-llama/llama_index/pull/20450\">#20450</a>)</li>\n</ul>\n<h3>llama-index-packs-code-hierarchy [0.6.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-gmail-openai-agent [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-multidoc-autoretrieval [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-panel-chatbot [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-recursive-retriever [0.7.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-packs-resume-screener [0.9.3]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-retry-engine-weaviate [0.5.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-streamlit-chatbot [0.5.2]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-sub-question-weaviate [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-timescale-vector-autoretrieval [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-cohere-rerank [0.6.0]</h3>\n<ul>\n<li>fix(cohere-rerank): add retry logic and tenacity dependency to cohere rerank (<a href=\"https://github.com/run-llama/llama_index/pull/20593\">#20593</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-nvidia-rerank [0.5.4]</h3>\n<ul>\n<li>fix(nvidia-rerank): fix initialization logic for on-prem auth (<a href=\"https://github.com/run-llama/llama_index/pull/20560\">#20560</a>)</li>\n<li>fix(nvidia-rerank): correct private attribute reference (<a href=\"https://github.com/run-llama/llama_index/pull/20570\">#20570</a>)</li>\n<li>fix(nvidia-rerank): Fix POST request url for locally hosted NIM rerankers (<a href=\"https://github.com/run-llama/llama_index/pull/20579\">#20579</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-tei-rerank [0.4.2]</h3>\n<ul>\n<li>fix(tei-rerank): use index field from API response for correct score … (<a href=\"https://github.com/run-llama/llama_index/pull/20599\">#20599</a>)</li>\n<li>test(tei-rerank): add test coverage for rerank retry coverage (<a href=\"https://github.com/run-llama/llama_index/pull/20600\">#20600</a>)</li>\n</ul>\n<h3>llama-index-protocols-ag-ui [0.2.4]</h3>\n<ul>\n<li>fix: avoid ValueError in ag-ui message conversion for multi-block ChatMessages (<a href=\"https://github.com/run-llama/llama_index/pull/20648\">#20648</a>)</li>\n</ul>\n<h3>llama-index-readers-datasets [0.1.0]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-readers-microsoft-sharepoint [0.7.0]</h3>\n<ul>\n<li>Sharepoint page support events (<a href=\"https://github.com/run-llama/llama_index/pull/20572\">#20572</a>)</li>\n</ul>\n<h3>llama-index-readers-obsidian [0.6.1]</h3>\n<ul>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.6]</h3>\n<ul>\n<li>feat: implement partial_params support to McpToolSpec (<a href=\"https://github.com/run-llama/llama_index/pull/20554\">#20554</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp-discovery [0.1.0]</h3>\n<ul>\n<li>Add llama-index-tools-mcp-discovery integration (<a href=\"https://github.com/run-llama/llama_index/pull/20502\">#20502</a>)</li>\n</ul>\n<h3>llama-index-tools-moss [0.1.0]</h3>\n<ul>\n<li>feat(tools): add Moss search engine integration (<a href=\"https://github.com/run-llama/llama_index/pull/20615\">#20615</a>)</li>\n</ul>\n<h3>llama-index-tools-seltz [0.1.0]</h3>\n<ul>\n<li>feat(tools): add Seltz web knowledge tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20626\">#20626</a>)</li>\n</ul>\n<h3>llama-index-tools-typecast [0.1.0]</h3>\n<ul>\n<li>Migrate Typecast tool to V2 API for voices endpoints (<a href=\"https://github.com/run-llama/llama_index/pull/20548\">#20548</a>)</li>\n</ul>\n<h3>llama-index-tools-wolfram-alpha [0.5.0]</h3>\n<ul>\n<li>feat(wolfram-alpha): switch to LLM API with bearer auth (<a href=\"https://github.com/run-llama/llama_index/pull/20586\">#20586</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-clickhouse [0.6.2]</h3>\n<ul>\n<li>fix(clickhouse): Add drop_existing_table parameter to prevent data loss (<a href=\"https://github.com/run-llama/llama_index/pull/20651\">#20651</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.6]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.1]</h3>\n<ul>\n<li>Update MongoDB vector store tests to use newer model (<a href=\"https://github.com/run-llama/llama_index/pull/20515\">#20515</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-oceanbase [0.4.0]</h3>\n<ul>\n<li>feat(oceanbase): add sparse/fulltext/hybrid search (<a href=\"https://github.com/run-llama/llama_index/pull/20524\">#20524</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [1.0.0]</h3>\n<ul>\n<li>Changed OpenSearch engine default from deprecated <code>nmslib</code> to <code>faiss</code> (<a href=\"https://github.com/run-llama/llama_index/pull/20507\">#20507</a>)</li>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.3]</h3>\n<ul>\n<li>fix(postgres): disable bitmap scan for vector queries (<a href=\"https://github.com/run-llama/llama_index/pull/20514\">#20514</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-yugabytedb [0.5.4]</h3>\n<ul>\n<li>Add YugabyteDB as a Vector Store (<a href=\"https://github.com/run-llama/llama_index/pull/20559\">#20559</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-voice-agents-gemini-live [0.2.2]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n</ul>",
    "published": "2026-02-10T23:08:46Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4472a8eadbb3b8d2",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.13",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.13",
    "summary": "<h1>Release Notes</h1>\n<h2>[2026-01-21]</h2>\n<h3>llama-index-core [0.14.13]</h3>\n<ul>\n<li>feat: add early_stopping_method parameter to agent workflows (<a href=\"https://github.com/run-llama/llama_index/pull/20389\">#20389</a>)</li>\n<li>feat: Add token-based code splitting support to CodeSplitter (<a href=\"https://github.com/run-llama/llama_index/pull/20438\">#20438</a>)</li>\n<li>Add RayIngestionPipeline integration for distributed data ingestion (<a href=\"https://github.com/run-llama/llama_index/pull/20443\">#20443</a>)</li>\n<li>Added the multi-modal version of the Condensed Conversation &amp; Context… (<a href=\"https://github.com/run-llama/llama_index/pull/20446\">#20446</a>)</li>\n<li>Replace ChatMemoryBuffer with Memory (<a href=\"https://github.com/run-llama/llama_index/pull/20458\">#20458</a>)</li>\n<li>fix(bug):Raise value error on when input is empty list in mean_agg instead of returning float (<a href=\"https://github.com/run-llama/llama_index/pull/20466\">#20466</a>)</li>\n<li>fix: The classmethod of ReActChatFormatter should use cls instead of the class name (<a href=\"https://github.com/run-llama/llama_index/pull/20475\">#20475</a>)</li>\n<li>feat: add configurable empty response message to synthesizers (<a href=\"https://github.com/run-llama/llama_index/pull/20503\">#20503</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.3]</h3>\n<ul>\n<li>Enable use of ARNs for Bedrock Embedding Models (<a href=\"https://github.com/run-llama/llama_index/pull/20435\">#20435</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.6]</h3>\n<ul>\n<li>Improved Ollama batch embedding (<a href=\"https://github.com/run-llama/llama_index/pull/20447\">#20447</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.3]</h3>\n<ul>\n<li>Adding voyage-4 models (<a href=\"https://github.com/run-llama/llama_index/pull/20497\">#20497</a>)</li>\n</ul>\n<h3>llama-index-ingestion-ray [0.1.0]</h3>\n<ul>\n<li>Add RayIngestionPipeline integration for distributed data ingestion (<a href=\"https://github.com/run-llama/llama_index/pull/20443\">#20443</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.6]</h3>\n<ul>\n<li>feat: enhance structured predict methods for anthropic (<a href=\"https://github.com/run-llama/llama_index/pull/20440\">#20440</a>)</li>\n<li>fix: preserve input_tokens in Anthropic stream_chat responses (<a href=\"https://github.com/run-llama/llama_index/pull/20512\">#20512</a>)</li>\n</ul>\n<h3>llama-index-llms-apertis [0.1.0]</h3>\n<ul>\n<li>Add Apertis LLM integration with example notebook (<a href=\"https://github.com/run-llama/llama_index/pull/20436\">#20436</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.4]</h3>\n<ul>\n<li>chore(bedrock-converse): Remove extraneous thinking_delta kwarg from ChatMessage (<a href=\"https://github.com/run-llama/llama_index/pull/20455\">#20455</a>)</li>\n</ul>\n<h3>llama-index-llms-gemini [0.6.2]</h3>\n<ul>\n<li>chore: deprecate llama-index-llms-gemini (<a href=\"https://github.com/run-llama/llama_index/pull/20511\">#20511</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.13]</h3>\n<ul>\n<li>Sanitize OpenAI structured output JSON schema name for generic Pydantic models (<a href=\"https://github.com/run-llama/llama_index/pull/20452\">#20452</a>)</li>\n<li>chore: vbump openai (<a href=\"https://github.com/run-llama/llama_index/pull/20482\">#20482</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.3]</h3>\n<ul>\n<li>Feature/openrouter provider routing support (<a href=\"https://github.com/run-llama/llama_index/pull/20431\">#20431</a>)</li>\n</ul>\n<h3>llama-index-packs-recursive-retriever [0.7.1]</h3>\n<ul>\n<li>security: remove exposed OpenAI API keys from notebook outputs (<a href=\"https://github.com/run-llama/llama_index/pull/20474\">#20474</a>)</li>\n</ul>\n<h3>llama-index-packs-sentence-window-retriever [0.5.1]</h3>\n<ul>\n<li>security: remove exposed OpenAI API keys from notebook outputs (<a href=\"https://github.com/run-llama/llama_index/pull/20474\">#20474</a>)</li>\n</ul>\n<h3>llama-index-readers-datasets [0.1.0]</h3>\n<ul>\n<li>Add HuggingFace datasets reader integration (<a href=\"https://github.com/run-llama/llama_index/pull/20468\">#20468</a>)</li>\n</ul>\n<h3>llama-index-readers-patentsview [1.0.0]</h3>\n<ul>\n<li>Patentsview reader api changes (<a href=\"https://github.com/run-llama/llama_index/pull/20481\">#20481</a>)</li>\n</ul>\n<h3>llama-index-retrievers-you [1.0.0]</h3>\n<ul>\n<li>Revamp YouRetriever integration (<a href=\"https://github.com/run-llama/llama_index/pull/20493\">#20493</a>)</li>\n</ul>\n<h3>llama-index-tools-parallel-web-systems [0.1.0]</h3>\n<ul>\n<li>feat: added Parallel Web System tools (<a href=\"https://github.com/run-llama/llama_index/pull/20442\">#20442</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-alibabacloud-mysql [0.1.0]</h3>\n<ul>\n<li>Feature/alibaba mysql vector integration (<a href=\"https://github.com/run-llama/llama_index/pull/20396\">#20396</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.6]</h3>\n<ul>\n<li>Feat milvus partition names (<a href=\"https://github.com/run-llama/llama_index/pull/20445\">#20445</a>)</li>\n<li>improve(llama-index-vector-stores-milvus): Changed the partition parameter to <code>milvus_partition_name</code> in add/delete. (<a href=\"https://github.com/run-llama/llama_index/pull/20460\">#20460</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.1]</h3>\n<ul>\n<li>INTPYTHON-863 Fix mongodb async integration (<a href=\"https://github.com/run-llama/llama_index/pull/20444\">#20444</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-neo4jvector [0.5.2]</h3>\n<ul>\n<li>Handle missing metadata for neo4j vector store (<a href=\"https://github.com/run-llama/llama_index/pull/20491\">#20491</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [0.6.3]</h3>\n<ul>\n<li>fix (opensearch): add close and aclose methods to vector client (<a href=\"https://github.com/run-llama/llama_index/pull/20463\">#20463</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.9.1]</h3>\n<ul>\n<li>Qdrant search params (<a href=\"https://github.com/run-llama/llama_index/pull/20476\">#20476</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.4]</h3>\n<ul>\n<li>feat(vertexaivectorsearch): add hybrid search support (<a href=\"https://github.com/run-llama/llama_index/pull/20487\">#20487</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-volcenginemysql [0.2.0]</h3>\n<ul>\n<li>feat: Volcengine MySQL vector store integration (<a href=\"https://github.com/run-llama/llama_index/pull/20404\">#20404</a>)</li>\n</ul>",
    "published": "2026-01-21T20:44:52Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1b059a88e8ce2e31",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.12",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.12",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-30]</h2>\n<h3>llama-index-callbacks-agentops [0.4.1]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.12]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n<li>Improve <code>MockFunctionCallingLLM</code> (<a href=\"https://github.com/run-llama/llama_index/pull/20356\">#20356</a>)</li>\n<li>fix(openai): sanitize generic Pydantic model schema names (<a href=\"https://github.com/run-llama/llama_index/pull/20371\">#20371</a>)</li>\n<li>Element node parser (<a href=\"https://github.com/run-llama/llama_index/pull/20399\">#20399</a>)</li>\n<li>improve llama dev logging (<a href=\"https://github.com/run-llama/llama_index/pull/20411\">#20411</a>)</li>\n<li>test(node_parser): add unit tests for Java CodeSplitter (<a href=\"https://github.com/run-llama/llama_index/pull/20423\">#20423</a>)</li>\n<li>fix: crash in log_vector_store_query_result when result.ids is None (<a href=\"https://github.com/run-llama/llama_index/pull/20427\">#20427</a>)</li>\n</ul>\n<h3>llama-index-embeddings-litellm [0.4.1]</h3>\n<ul>\n<li>Add docstring to LiteLLM embedding class (<a href=\"https://github.com/run-llama/llama_index/pull/20336\">#20336</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.5]</h3>\n<ul>\n<li>feat(llama-index-embeddings-ollama): Add keep_alive parameter (<a href=\"https://github.com/run-llama/llama_index/pull/20395\">#20395</a>)</li>\n<li>docs: improve Ollama embeddings README with comprehensive documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20414\">#20414</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.2]</h3>\n<ul>\n<li>Voyage multimodal 35 (<a href=\"https://github.com/run-llama/llama_index/pull/20398\">#20398</a>)</li>\n</ul>\n<h3>llama-index-graph-stores-nebula [0.5.1]</h3>\n<ul>\n<li>feat(nebula): add MENTIONS edge to property graph store (<a href=\"https://github.com/run-llama/llama_index/pull/20401\">#20401</a>)</li>\n</ul>\n<h3>llama-index-llms-aibadgr [0.1.0]</h3>\n<ul>\n<li>feat(llama-index-llms-aibadgr): Add AI Badgr OpenAI‑compatible LLM integration (<a href=\"https://github.com/run-llama/llama_index/pull/20365\">#20365</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.4]</h3>\n<ul>\n<li>add back haiku-3 support (<a href=\"https://github.com/run-llama/llama_index/pull/20408\">#20408</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.3]</h3>\n<ul>\n<li>fix: bedrock converse thinking block issue (<a href=\"https://github.com/run-llama/llama_index/pull/20355\">#20355</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.8.3]</h3>\n<ul>\n<li>Switch use_file_api to Flexible file_mode; Improve File Upload Handling &amp; Bump google-genai to v1.52.0 (<a href=\"https://github.com/run-llama/llama_index/pull/20347\">#20347</a>)</li>\n<li>Fix missing role from Google-GenAI (<a href=\"https://github.com/run-llama/llama_index/pull/20357\">#20357</a>)</li>\n<li>Add signature index fix (<a href=\"https://github.com/run-llama/llama_index/pull/20362\">#20362</a>)</li>\n<li>Add positional thought signature for thoughts (<a href=\"https://github.com/run-llama/llama_index/pull/20418\">#20418</a>)</li>\n</ul>\n<h3>llama-index-llms-ollama [0.9.1]</h3>\n<ul>\n<li>feature: pydantic no longer complains if you pass 'low', 'medium', 'h… (<a href=\"https://github.com/run-llama/llama_index/pull/20394\">#20394</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.12]</h3>\n<ul>\n<li>fix: Handle tools=None in OpenAIResponses._get_model_kwargs (<a href=\"https://github.com/run-llama/llama_index/pull/20358\">#20358</a>)</li>\n<li>feat: add support for gpt-5.2 and 5.2 pro (<a href=\"https://github.com/run-llama/llama_index/pull/20361\">#20361</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.6.1]</h3>\n<ul>\n<li>fix(confluence): support Python 3.14 (<a href=\"https://github.com/run-llama/llama_index/pull/20370\">#20370</a>)</li>\n</ul>\n<h3>llama-index-readers-file [0.5.6]</h3>\n<ul>\n<li>Loosen constraint on <code>pandas</code> version (<a href=\"https://github.com/run-llama/llama_index/pull/20387\">#20387</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>chore(deps): bump urllib3 from 2.5.0 to 2.6.0 in /llama-index-integrations/readers/llama-index-readers-service-now in the pip group across 1 directory (<a href=\"https://github.com/run-llama/llama_index/pull/20341\">#20341</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.5]</h3>\n<ul>\n<li>fix: pass timeout parameters to transport clients in BasicMCPClient (<a href=\"https://github.com/run-llama/llama_index/pull/20340\">#20340</a>)</li>\n<li>feature: Permit to pass a custom httpx.AsyncClient when creating a BasicMcpClient (<a href=\"https://github.com/run-llama/llama_index/pull/20368\">#20368</a>)</li>\n</ul>\n<h3>llama-index-tools-typecast [0.1.0]</h3>\n<ul>\n<li>feat: add Typecast tool integration with text to speech features (<a href=\"https://github.com/run-llama/llama_index/pull/20343\">#20343</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azurepostgresql [0.2.0]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-chroma [0.5.5]</h3>\n<ul>\n<li>Fix chroma nested metadata filters (<a href=\"https://github.com/run-llama/llama_index/pull/20424\">#20424</a>)</li>\n<li>fix(chroma): support multimodal results (<a href=\"https://github.com/run-llama/llama_index/pull/20426\">#20426</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-couchbase [0.6.0]</h3>\n<ul>\n<li>Update FTS &amp; GSI reference docs for Couchbase vector-store (<a href=\"https://github.com/run-llama/llama_index/pull/20346\">#20346</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-faiss [0.5.2]</h3>\n<ul>\n<li>fix(faiss): pass numpy array instead of int to add_with_ids (<a href=\"https://github.com/run-llama/llama_index/pull/20384\">#20384</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-lancedb [0.4.4]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n<li>fix(vector_stores/lancedb): add missing '&lt;' filter operator (<a href=\"https://github.com/run-llama/llama_index/pull/20364\">#20364</a>)</li>\n<li>fix(lancedb): fix metadata filtering logic and list value SQL generation (<a href=\"https://github.com/run-llama/llama_index/pull/20374\">#20374</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.0]</h3>\n<ul>\n<li>Update mongo vector store to initialize without list permissions (<a href=\"https://github.com/run-llama/llama_index/pull/20354\">#20354</a>)</li>\n<li>add mongodb delete index (<a href=\"https://github.com/run-llama/llama_index/pull/20429\">#20429</a>)</li>\n<li>async mongodb atlas support (<a href=\"https://github.com/run-llama/llama_index/pull/20430\">#20430</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-redis [0.6.2]</h3>\n<ul>\n<li>Redis metadata filter fix (<a href=\"https://github.com/run-llama/llama_index/pull/20359\">#20359</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.3]</h3>\n<ul>\n<li>feat(vertex-vector-search): Add Google Vertex AI Vector Search v2.0 support (<a href=\"https://github.com/run-llama/llama_index/pull/20351\">#20351</a>)</li>\n</ul>",
    "published": "2025-12-30T01:07:03Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "12a10411c82007d3",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.10",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.10",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-04]</h2>\n<h3>llama-index-core [0.14.10]</h3>\n<ul>\n<li>feat: add mock function calling llm (<a href=\"https://github.com/run-llama/llama_index/pull/20331\">#20331</a>)</li>\n</ul>\n<h3>llama-index-llms-qianfan [0.4.1]</h3>\n<ul>\n<li>test: fix typo 'reponse' to 'response' in variable names (<a href=\"https://github.com/run-llama/llama_index/pull/20329\">#20329</a>)</li>\n</ul>\n<h3>llama-index-tools-airweave [0.1.0]</h3>\n<ul>\n<li>feat: add Airweave tool integration with advanced search features (<a href=\"https://github.com/run-llama/llama_index/pull/20111\">#20111</a>)</li>\n</ul>\n<h3>llama-index-utils-qianfan [0.4.1]</h3>\n<ul>\n<li>test: fix typo 'reponse' to 'response' in variable names (<a href=\"https://github.com/run-llama/llama_index/pull/20329\">#20329</a>)</li>\n</ul>",
    "published": "2025-12-04T19:46:03Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "76e7d59ba15bdef3",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.9",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.9",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-02]</h2>\n<h3>llama-index-agent-azure [0.2.1]</h3>\n<ul>\n<li>fix: Pin azure-ai-projects version to prevent breaking changes (<a href=\"https://github.com/run-llama/llama_index/pull/20255\">#20255</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.9]</h3>\n<ul>\n<li>MultiModalVectorStoreIndex now returns a multi-modal ContextChatEngine. (<a href=\"https://github.com/run-llama/llama_index/pull/20265\">#20265</a>)</li>\n<li>Ingestion to vector store now ensures that _node-content is readable (<a href=\"https://github.com/run-llama/llama_index/pull/20266\">#20266</a>)</li>\n<li>fix: ensure context is copied with async utils run_async (<a href=\"https://github.com/run-llama/llama_index/pull/20286\">#20286</a>)</li>\n<li>fix(memory): ensure first message in queue is always a user message after flush (<a href=\"https://github.com/run-llama/llama_index/pull/20310\">#20310</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.2]</h3>\n<ul>\n<li>feat(embeddings-bedrock): Add support for Amazon Bedrock Application Inference Profiles (<a href=\"https://github.com/run-llama/llama_index/pull/20267\">#20267</a>)</li>\n<li>fix:(embeddings-bedrock) correct extraction of provider from model_name (<a href=\"https://github.com/run-llama/llama_index/pull/20295\">#20295</a>)</li>\n<li>Bump version of bedrock-embedding (<a href=\"https://github.com/run-llama/llama_index/pull/20304\">#20304</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.1]</h3>\n<ul>\n<li>VoyageAI correction and documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20251\">#20251</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.3]</h3>\n<ul>\n<li>feat: add anthropic opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20306\">#20306</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.2]</h3>\n<ul>\n<li>fix(bedrock-converse): Only use guardrail_stream_processing_mode in streaming functions (<a href=\"https://github.com/run-llama/llama_index/pull/20289\">#20289</a>)</li>\n<li>feat: add anthropic opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20306\">#20306</a>)</li>\n<li>feat(bedrock-converse): Additional support for Claude Opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20317\">#20317</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.7.4]</h3>\n<ul>\n<li>Fix gemini-3 support and gemini function call support (<a href=\"https://github.com/run-llama/llama_index/pull/20315\">#20315</a>)</li>\n</ul>\n<h3>llama-index-llms-helicone [0.1.1]</h3>\n<ul>\n<li>update helicone docs + examples (<a href=\"https://github.com/run-llama/llama_index/pull/20208\">#20208</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.10]</h3>\n<ul>\n<li>Smallest Nit (<a href=\"https://github.com/run-llama/llama_index/pull/20252\">#20252</a>)</li>\n<li>Feat: Add gpt-5.1-chat model support (<a href=\"https://github.com/run-llama/llama_index/pull/20311\">#20311</a>)</li>\n</ul>\n<h3>llama-index-llms-ovhcloud [0.1.0]</h3>\n<ul>\n<li>Add OVHcloud AI Endpoints provider (<a href=\"https://github.com/run-llama/llama_index/pull/20288\">#20288</a>)</li>\n</ul>\n<h3>llama-index-llms-siliconflow [0.4.2]</h3>\n<ul>\n<li>[Bugfix] None check on content in delta in siliconflow LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20327\">#20327</a>)</li>\n</ul>\n<h3>llama-index-node-parser-docling [0.4.2]</h3>\n<ul>\n<li>Relax docling Python constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20322\">#20322</a>)</li>\n</ul>\n<h3>llama-index-packs-resume-screener [0.9.3]</h3>\n<ul>\n<li>feat: Update pypdf to latest version (<a href=\"https://github.com/run-llama/llama_index/pull/20285\">#20285</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-voyageai-rerank [0.4.1]</h3>\n<ul>\n<li>VoyageAI correction and documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20251\">#20251</a>)</li>\n</ul>\n<h3>llama-index-protocols-ag-ui [0.2.3]</h3>\n<ul>\n<li>fix: correct order of ag-ui events to avoid event conflicts (<a href=\"https://github.com/run-llama/llama_index/pull/20296\">#20296</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.6.0]</h3>\n<ul>\n<li>Refactor Confluence integration: Update license to MIT, remove requirements.txt, and implement HtmlTextParser for HTML to Markdown conversion. Update dependencies and tests accordingly. (<a href=\"https://github.com/run-llama/llama_index/pull/20262\">#20262</a>)</li>\n</ul>\n<h3>llama-index-readers-docling [0.4.2]</h3>\n<ul>\n<li>Relax docling Python constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20322\">#20322</a>)</li>\n</ul>\n<h3>llama-index-readers-file [0.5.5]</h3>\n<ul>\n<li>feat: Update pypdf to latest version (<a href=\"https://github.com/run-llama/llama_index/pull/20285\">#20285</a>)</li>\n</ul>\n<h3>llama-index-readers-reddit [0.4.1]</h3>\n<ul>\n<li>Fix typo in README.md for Reddit integration (<a href=\"https://github.com/run-llama/llama_index/pull/20283\">#20283</a>)</li>\n</ul>\n<h3>llama-index-storage-chat-store-postgres [0.3.2]</h3>\n<ul>\n<li>[FIX] Postgres ChatStore automatically prefix table name with \"data_\" (<a href=\"https://github.com/run-llama/llama_index/pull/20241\">#20241</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azureaisearch [0.4.4]</h3>\n<ul>\n<li><code>vector-azureaisearch</code>: check if user agent already in policy before add it to azure client (<a href=\"https://github.com/run-llama/llama_index/pull/20243\">#20243</a>)</li>\n<li>fix(azureaisearch): Add close/aclose methods to fix unclosed client session warnings (<a href=\"https://github.com/run-llama/llama_index/pull/20309\">#20309</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.4]</h3>\n<ul>\n<li>Fix/consistency level param for milvus (<a href=\"https://github.com/run-llama/llama_index/pull/20268\">#20268</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.2]</h3>\n<ul>\n<li>Fix postgresql dispose (<a href=\"https://github.com/run-llama/llama_index/pull/20312\">#20312</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.9.0]</h3>\n<ul>\n<li>fix: Update qdrant-client version constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20280\">#20280</a>)</li>\n<li>Feat: update Qdrant client to 1.16.0 (<a href=\"https://github.com/run-llama/llama_index/pull/20287\">#20287</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.2]</h3>\n<ul>\n<li>fix: update blob path in batch_update_index (<a href=\"https://github.com/run-llama/llama_index/pull/20281\">#20281</a>)</li>\n</ul>\n<h3>llama-index-voice-agents-openai [0.2.2]</h3>\n<ul>\n<li>Smallest Nit (<a href=\"https://github.com/run-llama/llama_index/pull/20252\">#20252</a>)</li>\n</ul>",
    "published": "2025-12-02T21:31:18Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1b2cfee52de7ca3d",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.8",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.8",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-11-10]</h2>\n<h3>llama-index-core [0.14.8]</h3>\n<ul>\n<li>Fix ReActOutputParser getting stuck when \"Answer:\" contains \"Action:\" (<a href=\"https://github.com/run-llama/llama_index/pull/20098\">#20098</a>)</li>\n<li>Add buffer to image, audio, video and document blocks (<a href=\"https://github.com/run-llama/llama_index/pull/20153\">#20153</a>)</li>\n<li>fix(agent): Handle multi-block ChatMessage in ReActAgent (<a href=\"https://github.com/run-llama/llama_index/pull/20196\">#20196</a>)</li>\n<li>Fix/20209 (<a href=\"https://github.com/run-llama/llama_index/pull/20214\">#20214</a>)</li>\n<li>Preserve Exception in ToolOutput (<a href=\"https://github.com/run-llama/llama_index/pull/20231\">#20231</a>)</li>\n<li>fix weird pydantic warning (<a href=\"https://github.com/run-llama/llama_index/pull/20235\">#20235</a>)</li>\n</ul>\n<h3>llama-index-embeddings-nvidia [0.4.2]</h3>\n<ul>\n<li>docs: Edit pass and update example model (<a href=\"https://github.com/run-llama/llama_index/pull/20198\">#20198</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.4]</h3>\n<ul>\n<li>Added a test case (no code) to check the embedding through an actual connection to a Ollama server (after checking that the ollama server exists) (<a href=\"https://github.com/run-llama/llama_index/pull/20230\">#20230</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.2]</h3>\n<ul>\n<li>feat(llms/anthropic): Add support for RawMessageDeltaEvent in streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20206\">#20206</a>)</li>\n<li>chore: remove unsupported models (<a href=\"https://github.com/run-llama/llama_index/pull/20211\">#20211</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.11.1]</h3>\n<ul>\n<li>feat: integrate bedrock converse with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20099\">#20099</a>)</li>\n<li>feat: Update model name extraction to include 'jp' region prefix and … (<a href=\"https://github.com/run-llama/llama_index/pull/20233\">#20233</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.7.3]</h3>\n<ul>\n<li>feat: google genai integration with tool block (<a href=\"https://github.com/run-llama/llama_index/pull/20096\">#20096</a>)</li>\n<li>fix: non-streaming gemini tool calling (<a href=\"https://github.com/run-llama/llama_index/pull/20207\">#20207</a>)</li>\n<li>Add token usage information in GoogleGenAI chat additional_kwargs (<a href=\"https://github.com/run-llama/llama_index/pull/20219\">#20219</a>)</li>\n<li>bug fix google genai stream_complete (<a href=\"https://github.com/run-llama/llama_index/pull/20220\">#20220</a>)</li>\n</ul>\n<h3>llama-index-llms-nvidia [0.4.4]</h3>\n<ul>\n<li>docs: Edit pass and code example updates (<a href=\"https://github.com/run-llama/llama_index/pull/20200\">#20200</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.8]</h3>\n<ul>\n<li>FixV2: Correct DocumentBlock type for OpenAI from 'input_file' to 'file' (<a href=\"https://github.com/run-llama/llama_index/pull/20203\">#20203</a>)</li>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.6.5]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-packs-streamlit-chatbot [0.5.2]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-packs-voyage-query-engine [0.5.2]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-nvidia-rerank [0.5.1]</h3>\n<ul>\n<li>docs: Edit pass (<a href=\"https://github.com/run-llama/llama_index/pull/20199\">#20199</a>)</li>\n</ul>\n<h3>llama-index-readers-web [0.5.6]</h3>\n<ul>\n<li>feat: Add ScrapyWebReader Integration (<a href=\"https://github.com/run-llama/llama_index/pull/20212\">#20212</a>)</li>\n<li>Update Scrapy dependency to 2.13.3 (<a href=\"https://github.com/run-llama/llama_index/pull/20228\">#20228</a>)</li>\n</ul>\n<h3>llama-index-readers-whisper [0.3.0]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-storage-kvstore-postgres [0.4.3]</h3>\n<ul>\n<li>fix: Ensure schema creation only occurs if it doesn't already exist (<a href=\"https://github.com/run-llama/llama_index/pull/20225\">#20225</a>)</li>\n</ul>\n<h3>llama-index-tools-brightdata [0.2.1]</h3>\n<ul>\n<li>docs: add api key claim instructions (<a href=\"https://github.com/run-llama/llama_index/pull/20204\">#20204</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.3]</h3>\n<ul>\n<li>Added test case for issue 19211. No code change (<a href=\"https://github.com/run-llama/llama_index/pull/20201\">#20201</a>)</li>\n</ul>\n<h3>llama-index-utils-oracleai [0.3.1]</h3>\n<ul>\n<li>Update llama-index-core dependency to 0.12.45 (<a href=\"https://github.com/run-llama/llama_index/pull/20227\">#20227</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-lancedb [0.4.2]</h3>\n<ul>\n<li>fix: FTS index recreation bug on every LanceDB query (<a href=\"https://github.com/run-llama/llama_index/pull/20213\">#20213</a>)</li>\n</ul>",
    "published": "2025-11-10T22:18:42Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "673d75a2997ef7ce",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.7",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.7",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-30]</h2>\n<h3>llama-index-core [0.14.7]</h3>\n<ul>\n<li>Feat/serpex tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20141\">#20141</a>)</li>\n<li>Fix outdated error message about setting LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20157\">#20157</a>)</li>\n<li>Fixing some recently failing tests (<a href=\"https://github.com/run-llama/llama_index/pull/20165\">#20165</a>)</li>\n<li>Fix: update lock to latest workflow and fix issues (<a href=\"https://github.com/run-llama/llama_index/pull/20173\">#20173</a>)</li>\n<li>fix: ensure full docstring is used in FunctionTool (<a href=\"https://github.com/run-llama/llama_index/pull/20175\">#20175</a>)</li>\n<li>fix api docs build (<a href=\"https://github.com/run-llama/llama_index/pull/20180\">#20180</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.0]</h3>\n<ul>\n<li>Updating the VoyageAI integration (<a href=\"https://github.com/run-llama/llama_index/pull/20073\">#20073</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.0]</h3>\n<ul>\n<li>feat: integrate anthropic with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20100\">#20100</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.10.7]</h3>\n<ul>\n<li>feat: Add support for Bedrock Guardrails streamProcessingMode (<a href=\"https://github.com/run-llama/llama_index/pull/20150\">#20150</a>)</li>\n<li>bedrock structured output optional force (<a href=\"https://github.com/run-llama/llama_index/pull/20158\">#20158</a>)</li>\n</ul>\n<h3>llama-index-llms-fireworks [0.4.5]</h3>\n<ul>\n<li>Update FireworksAI models (<a href=\"https://github.com/run-llama/llama_index/pull/20169\">#20169</a>)</li>\n</ul>\n<h3>llama-index-llms-mistralai [0.9.0]</h3>\n<ul>\n<li>feat: mistralai integration with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20103\">#20103</a>)</li>\n</ul>\n<h3>llama-index-llms-ollama [0.9.0]</h3>\n<ul>\n<li>feat: integrate ollama with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20097\">#20097</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.6]</h3>\n<ul>\n<li>Allow setting temp of gpt-5-chat (<a href=\"https://github.com/run-llama/llama_index/pull/20156\">#20156</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.5.0]</h3>\n<ul>\n<li>feat(confluence): make SVG processing optional to fix pycairo install… (<a href=\"https://github.com/run-llama/llama_index/pull/20115\">#20115</a>)</li>\n</ul>\n<h3>llama-index-readers-github [0.9.0]</h3>\n<ul>\n<li>Add GitHub App authentication support (<a href=\"https://github.com/run-llama/llama_index/pull/20106\">#20106</a>)</li>\n</ul>\n<h3>llama-index-retrievers-bedrock [0.5.1]</h3>\n<ul>\n<li>Fixing some recently failing tests (<a href=\"https://github.com/run-llama/llama_index/pull/20165\">#20165</a>)</li>\n</ul>\n<h3>llama-index-tools-serpex [0.1.0]</h3>\n<ul>\n<li>Feat/serpex tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20141\">#20141</a>)</li>\n<li>add missing toml info (<a href=\"https://github.com/run-llama/llama_index/pull/20186\">#20186</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-couchbase [0.6.0]</h3>\n<ul>\n<li>Add Hyperscale and Composite Vector Indexes support for Couchbase vector-store (<a href=\"https://github.com/run-llama/llama_index/pull/20170\">#20170</a>)</li>\n</ul>",
    "published": "2025-10-30T23:58:43Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a4240f028d473960",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.6",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.6",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-26]</h2>\n<h3>llama-index-core [0.14.6]</h3>\n<ul>\n<li>Add allow_parallel_tool_calls for non-streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20117\">#20117</a>)</li>\n<li>Fix invalid use of field-specific metadata (<a href=\"https://github.com/run-llama/llama_index/pull/20122\">#20122</a>)</li>\n<li>update doc for SemanticSplitterNodeParser (<a href=\"https://github.com/run-llama/llama_index/pull/20125\">#20125</a>)</li>\n<li>fix rare cases when sentence splits are larger than chunk size (<a href=\"https://github.com/run-llama/llama_index/pull/20147\">#20147</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.0]</h3>\n<ul>\n<li>Fix BedrockEmbedding to support Cohere v4 response format (<a href=\"https://github.com/run-llama/llama_index/pull/20094\">#20094</a>)</li>\n</ul>\n<h3>llama-index-embeddings-isaacus [0.1.0]</h3>\n<ul>\n<li>feat: Isaacus embeddings integration (<a href=\"https://github.com/run-llama/llama_index/pull/20124\">#20124</a>)</li>\n</ul>\n<h3>llama-index-embeddings-oci-genai [0.4.2]</h3>\n<ul>\n<li>Update OCI GenAI cohere models (<a href=\"https://github.com/run-llama/llama_index/pull/20146\">#20146</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.9.7]</h3>\n<ul>\n<li>Fix double token stream in anthropic llm (<a href=\"https://github.com/run-llama/llama_index/pull/20108\">#20108</a>)</li>\n<li>Ensure anthropic content delta only has user facing response (<a href=\"https://github.com/run-llama/llama_index/pull/20113\">#20113</a>)</li>\n</ul>\n<h3>llama-index-llms-baseten [0.1.7]</h3>\n<ul>\n<li>add GLM (<a href=\"https://github.com/run-llama/llama_index/pull/20121\">#20121</a>)</li>\n</ul>\n<h3>llama-index-llms-helicone [0.1.0]</h3>\n<ul>\n<li>integrate helicone to llama-index (<a href=\"https://github.com/run-llama/llama_index/pull/20131\">#20131</a>)</li>\n</ul>\n<h3>llama-index-llms-oci-genai [0.6.4]</h3>\n<ul>\n<li>Update OCI GenAI cohere models (<a href=\"https://github.com/run-llama/llama_index/pull/20146\">#20146</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.5]</h3>\n<ul>\n<li>chore: openai vbump (<a href=\"https://github.com/run-llama/llama_index/pull/20095\">#20095</a>)</li>\n</ul>\n<h3>llama-index-readers-imdb-review [0.4.2]</h3>\n<ul>\n<li>chore: Update selenium dependency in imdb-review reader (<a href=\"https://github.com/run-llama/llama_index/pull/20105\">#20105</a>)</li>\n</ul>\n<h3>llama-index-retrievers-bedrock [0.5.0]</h3>\n<ul>\n<li>feat(bedrock): add async support for AmazonKnowledgeBasesRetriever (<a href=\"https://github.com/run-llama/llama_index/pull/20114\">#20114</a>)</li>\n</ul>\n<h3>llama-index-retrievers-superlinked [0.1.3]</h3>\n<ul>\n<li>Update README.md (<a href=\"https://github.com/run-llama/llama_index/pull/19829\">#19829</a>)</li>\n</ul>\n<h3>llama-index-storage-kvstore-postgres [0.4.2]</h3>\n<ul>\n<li>fix: Replace raw SQL string interpolation with proper SQLAlchemy parameterized APIs in PostgresKVStore (<a href=\"https://github.com/run-llama/llama_index/pull/20104\">#20104</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.3]</h3>\n<ul>\n<li>Fix BasicMCPClient resource signatures (<a href=\"https://github.com/run-llama/llama_index/pull/20118\">#20118</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.1]</h3>\n<ul>\n<li>Add GIN index support for text array metadata in PostgreSQL vector store (<a href=\"https://github.com/run-llama/llama_index/pull/20130\">#20130</a>)</li>\n</ul>",
    "published": "2025-10-26T03:01:31Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6b02016212dad53a",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.5",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.5",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-15]</h2>\n<h3>llama-index-core [0.14.5]</h3>\n<ul>\n<li>Remove debug print (<a href=\"https://github.com/run-llama/llama_index/pull/20000\">#20000</a>)</li>\n<li>safely initialize RefDocInfo in Docstore (<a href=\"https://github.com/run-llama/llama_index/pull/20031\">#20031</a>)</li>\n<li>Add progress bar for multiprocess loading (<a href=\"https://github.com/run-llama/llama_index/pull/20048\">#20048</a>)</li>\n<li>Fix duplicate node positions when identical text appears multiple times in document (<a href=\"https://github.com/run-llama/llama_index/pull/20050\">#20050</a>)</li>\n<li>chore: tool call block - part 1 (<a href=\"https://github.com/run-llama/llama_index/pull/20074\">#20074</a>)</li>\n</ul>\n<h3>llama-index-instrumentation [0.4.2]</h3>\n<ul>\n<li>update instrumentation package metadata (<a href=\"https://github.com/run-llama/llama_index/pull/20079\">#20079</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.9.5]</h3>\n<ul>\n<li>✨ feat(anthropic): add prompt caching model validation utilities (<a href=\"https://github.com/run-llama/llama_index/pull/20069\">#20069</a>)</li>\n<li>fix streaming thinking/tool calling with anthropic (<a href=\"https://github.com/run-llama/llama_index/pull/20077\">#20077</a>)</li>\n<li>Add haiku 4.5 support (<a href=\"https://github.com/run-llama/llama_index/pull/20092\">#20092</a>)</li>\n</ul>\n<h3>llama-index-llms-baseten [0.1.6]</h3>\n<ul>\n<li>Baseten provider Kimi K2 0711, Llama 4 Maverick and Llama 4 Scout Model APIs deprecation (<a href=\"https://github.com/run-llama/llama_index/pull/20042\">#20042</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.10.5]</h3>\n<ul>\n<li>feat: List Claude Sonnet 4.5 as a reasoning model (<a href=\"https://github.com/run-llama/llama_index/pull/20022\">#20022</a>)</li>\n<li>feat: Support global cross-region inference profile prefix (<a href=\"https://github.com/run-llama/llama_index/pull/20064\">#20064</a>)</li>\n<li>Update utils.py for opus 4.1 (<a href=\"https://github.com/run-llama/llama_index/pull/20076\">#20076</a>)</li>\n<li>4.1 opus bedrockconverse missing in funcitoncalling models (<a href=\"https://github.com/run-llama/llama_index/pull/20084\">#20084</a>)</li>\n<li>Add haiku 4.5 support (<a href=\"https://github.com/run-llama/llama_index/pull/20092\">#20092</a>)</li>\n</ul>\n<h3>llama-index-llms-fireworks [0.4.4]</h3>\n<ul>\n<li>Add Support for Custom Models in Fireworks LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20023\">#20023</a>)</li>\n<li>fix(llms/fireworks): Cannot use Fireworks Deepseek V3.1-20006 issue (<a href=\"https://github.com/run-llama/llama_index/pull/20028\">#20028</a>)</li>\n</ul>\n<h3>llama-index-llms-oci-genai [0.6.3]</h3>\n<ul>\n<li>Add support for xAI models in OCI GenAI (<a href=\"https://github.com/run-llama/llama_index/pull/20089\">#20089</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.4]</h3>\n<ul>\n<li>Gpt 5 pro addition (<a href=\"https://github.com/run-llama/llama_index/pull/20029\">#20029</a>)</li>\n<li>fix collecting final response with openai responses streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20037\">#20037</a>)</li>\n<li>Add support for GPT-5 models in utils.py (JSON_SCHEMA_MODELS) (<a href=\"https://github.com/run-llama/llama_index/pull/20045\">#20045</a>)</li>\n<li>chore: tool call block - part 1 (<a href=\"https://github.com/run-llama/llama_index/pull/20074\">#20074</a>)</li>\n</ul>\n<h3>llama-index-llms-sglang [0.1.0]</h3>\n<ul>\n<li>Added Sglang llm integration (<a href=\"https://github.com/run-llama/llama_index/pull/20020\">#20020</a>)</li>\n</ul>\n<h3>llama-index-readers-gitlab [0.5.1]</h3>\n<ul>\n<li>feat(gitlab): add pagination params for repository tree and issues (<a href=\"https://github.com/run-llama/llama_index/pull/20052\">#20052</a>)</li>\n</ul>\n<h3>llama-index-readers-json [0.4.2]</h3>\n<ul>\n<li>vbump the JSON reader (<a href=\"https://github.com/run-llama/llama_index/pull/20039\">#20039</a>)</li>\n</ul>\n<h3>llama-index-readers-web [0.5.5]</h3>\n<ul>\n<li>fix: ScrapflyReader Pydantic validation error (<a href=\"https://github.com/run-llama/llama_index/pull/19999\">#19999</a>)</li>\n</ul>\n<h3>llama-index-storage-chat-store-dynamodb [0.4.2]</h3>\n<ul>\n<li>bump dynamodb chat store deps (<a href=\"https://github.com/run-llama/llama_index/pull/20078\">#20078</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.2]</h3>\n<ul>\n<li>🐛 fix(tools/mcp): Fix dict type handling and reference resolution in … (<a href=\"https://github.com/run-llama/llama_index/pull/20082\">#20082</a>)</li>\n</ul>\n<h3>llama-index-tools-signnow [0.1.0]</h3>\n<ul>\n<li>feat(signnow): SignNow mcp tools integration (<a href=\"https://github.com/run-llama/llama_index/pull/20057\">#20057</a>)</li>\n</ul>\n<h3>llama-index-tools-tavily-research [0.4.2]</h3>\n<ul>\n<li>feat: Add Tavily extract function for URL content extraction (<a href=\"https://github.com/run-llama/llama_index/pull/20038\">#20038</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azurepostgresql [0.2.0]</h3>\n<ul>\n<li>Add hybrid search to Azure PostgreSQL integration (<a href=\"https://github.com/run-llama/llama_index/pull/20027\">#20027</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.3]</h3>\n<ul>\n<li>fix: Milvus get_field_kwargs() (<a href=\"https://github.com/run-llama/llama_index/pull/20086\">#20086</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [0.6.2]</h3>\n<ul>\n<li>fix(opensearch): Correct version check for efficient filtering (<a href=\"https://github.com/run-llama/llama_index/pull/20067\">#20067</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.8.6]</h3>\n<ul>\n<li>fix(qdrant): Allow async-only initialization with hybrid search (<a href=\"https://github.com/run-llama/llama_index/pull/20005\">#20005</a>)</li>\n</ul>",
    "published": "2025-10-15T19:10:57Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0c724a601c66433d",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.4",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.4",
    "summary": "<h1>Release Notes</h1>\n<h2>[2025-09-24]</h2>\n<h3>llama-index-core [0.14.4]</h3>\n<ul>\n<li>fix pre-release installs (<a href=\"https://github.com/run-llama/llama_index/pull/20010\">#20010</a>)</li>\n</ul>\n<h3>llama-index-embeddings-anyscale [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-embeddings-baseten [0.1.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-embeddings-fireworks [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-embeddings-opea [0.2.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-embeddings-text-embeddings-inference [0.4.2]</h3>\n<ul>\n<li>Fix authorization header setup logic in text embeddings inference (<a href=\"https://github.com/run-llama/llama_index/pull/19979\">#19979</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.9.3]</h3>\n<ul>\n<li>feat: add anthropic sonnet 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/19977\">#19977</a>)</li>\n</ul>\n<h3>llama-index-llms-anyscale [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-azure-openai [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-baseten [0.1.5]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.9.5]</h3>\n<ul>\n<li>feat: Additional support for Claude Sonnet 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/19980\">#19980</a>)</li>\n</ul>\n<h3>llama-index-llms-deepinfra [0.5.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-everlyai [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-fireworks [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.6.2]</h3>\n<ul>\n<li>Fix for ValueError: ChatMessage contains multiple blocks, use 'ChatMe… (<a href=\"https://github.com/run-llama/llama_index/pull/19954\">#19954</a>)</li>\n</ul>\n<h3>llama-index-llms-keywordsai [1.1.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-localai [0.5.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-mistralai [0.8.2]</h3>\n<ul>\n<li>Update list of MistralAI LLMs (<a href=\"https://github.com/run-llama/llama_index/pull/19981\">#19981</a>)</li>\n</ul>\n<h3>llama-index-llms-monsterapi [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-nvidia [0.4.4]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-ollama [0.7.4]</h3>\n<ul>\n<li>Fix <code>TypeError: unhashable type: 'dict'</code> in Ollama stream chat with tools (<a href=\"https://github.com/run-llama/llama_index/pull/19938\">#19938</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.1]</h3>\n<ul>\n<li>feat(OpenAILike): support structured outputs (<a href=\"https://github.com/run-llama/llama_index/pull/19967\">#19967</a>)</li>\n</ul>\n<h3>llama-index-llms-openai-like [0.5.3]</h3>\n<ul>\n<li>feat(OpenAILike): support structured outputs (<a href=\"https://github.com/run-llama/llama_index/pull/19967\">#19967</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.2]</h3>\n<ul>\n<li>chore(openrouter,anthropic): add py.typed (<a href=\"https://github.com/run-llama/llama_index/pull/19966\">#19966</a>)</li>\n</ul>\n<h3>llama-index-llms-perplexity [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-portkey [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-sarvam [0.2.1]</h3>\n<ul>\n<li>fixed Sarvam Integration and Typos (Fixes <a class=\"issue-link js-issue-link\" href=\"https://github.com/run-llama/llama_index/issues/19931\">#19931</a>) (<a href=\"https://github.com/run-llama/llama_index/pull/19932\">#19932</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.6.4]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-llms-yi [0.4.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-memory-bedrock-agentcore [0.1.0]</h3>\n<ul>\n<li>feat: Bedrock AgentCore Memory integration (<a href=\"https://github.com/run-llama/llama_index/pull/19953\">#19953</a>)</li>\n</ul>\n<h3>llama-index-multi-modal-llms-openai [0.6.2]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.4.4]</h3>\n<ul>\n<li>Fix: Respect cloud parameter when fetching child pages in ConfluenceR… (<a href=\"https://github.com/run-llama/llama_index/pull/19983\">#19983</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>Bug Fix :- Not Able to Fetch Page whose latest is empty or null (<a href=\"https://github.com/run-llama/llama_index/pull/19916\">#19916</a>)</li>\n</ul>\n<h3>llama-index-selectors-notdiamond [0.4.0]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-tools-agentql [1.2.0]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-tools-playwright [0.3.1]</h3>\n<ul>\n<li>chore: fix playwright tests (<a href=\"https://github.com/run-llama/llama_index/pull/19946\">#19946</a>)</li>\n</ul>\n<h3>llama-index-tools-scrapegraph [0.2.2]</h3>\n<ul>\n<li>feat: update scrapegraphai (<a href=\"https://github.com/run-llama/llama_index/pull/19974\">#19974</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-chroma [0.5.3]</h3>\n<ul>\n<li>docs: fix query method docstring in ChromaVectorStore Fixes <a class=\"issue-link js-issue-link\" href=\"https://github.com/run-llama/llama_index/issues/19969\">#19969</a> (<a href=\"https://github.com/run-llama/llama_index/pull/19973\">#19973</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.8.1]</h3>\n<ul>\n<li>fix llm deps for openai (<a href=\"https://github.com/run-llama/llama_index/pull/19944\">#19944</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.0]</h3>\n<ul>\n<li>fix index creation in postgres vector store (<a href=\"https://github.com/run-llama/llama_index/pull/19955\">#19955</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-solr [0.1.0]</h3>\n<ul>\n<li>Add ApacheSolrVectorStore Integration (<a href=\"https://github.com/run-llama/llama_index/pull/19933\">#19933</a>)</li>\n</ul>",
    "published": "2025-10-03T17:52:41Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "df23a683c09c1437",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-sdk==0.3.6",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.6",
    "summary": "<p>Changes since sdk==0.3.5</p>\n<ul>\n<li>release(sdk-py): 0.3.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6805\">#6805</a>)</li>\n<li>chore: update to add prune method (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6804\">#6804</a>)</li>\n<li>chore: Re-organize client files. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6787\">#6787</a>)</li>\n</ul>",
    "published": "2026-02-14T19:46:16Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "165fb96a655d42e0",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-sdk==0.3.5",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.5",
    "summary": "<p>Changes since sdk==0.3.4</p>\n<ul>\n<li>chore: server runtime type (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6774\">#6774</a>)</li>\n</ul>",
    "published": "2026-02-10T16:56:28Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9a2a7007f26a8fa7",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph==1.0.8",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/1.0.8",
    "summary": "<p>Changes since 1.0.7</p>\n<ul>\n<li>release(langgraph): 1.0.8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6757\">#6757</a>)</li>\n<li>chore: shallow copy futures (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6755\">#6755</a>)</li>\n<li>fix: pydantic messages double streaming (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6753\">#6753</a>)</li>\n<li>chore(deps-dev): bump ruff from 0.14.7 to 0.14.11 in /libs/sdk-py (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6673\">#6673</a>)</li>\n<li>chore: Omit lock when using connection pool (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6734\">#6734</a>)</li>\n<li>docs: enhance <code>Runtime</code> and <code>ToolRuntime</code> class descriptions for clarity (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6689\">#6689</a>)</li>\n<li>docs: add clarity to use of <code>thread_id</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6515\">#6515</a>)</li>\n<li>docs: add docstrings to <code>add_node</code> overloads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6514\">#6514</a>)</li>\n<li>docs: update notebook links and add archival notices for examples (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6720\">#6720</a>)</li>\n<li>release(cli): 0.4.12 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6716\">#6716</a>)</li>\n</ul>",
    "published": "2026-02-06T12:31:26Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a08265c21cb9aab8",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-sdk==0.3.4",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.4",
    "summary": "<p>Changes since sdk==0.3.3</p>\n<ul>\n<li>chore: release python sdk (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6754\">#6754</a>)</li>\n<li>feat(sdk-py): add update method for crons client (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6742\">#6742</a>)</li>\n<li>feat(sdk-py): add support for enabling/disabling crons (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6740\">#6740</a>)</li>\n<li>chore(deps-dev): bump ruff from 0.14.7 to 0.14.11 in /libs/sdk-py (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6673\">#6673</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>docs: clarify cron job schedule interpretation in UTC (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6692\">#6692</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-02-06T00:44:26Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "71a7cd16a493bef9",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-checkpoint-postgres==3.0.4",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/checkpointpostgres%3D%3D3.0.4",
    "summary": "<p>Changes since checkpointpostgres==3.0.3</p>\n<ul>\n<li>chore: Omit lock when using connection pool (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6734\">#6734</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-31T00:46:04Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b2b8ee2468790fbd",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-cli==0.4.12",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/cli%3D%3D0.4.12",
    "summary": "<p>Changes since cli==0.4.11</p>\n<ul>\n<li>release(cli): 0.4.12 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6716\">#6716</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-23T13:34:28Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a509bbcb9e389f17",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph==1.0.7",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/1.0.7",
    "summary": "<p>Changes since 1.0.6</p>\n<ul>\n<li>release: langgraph and prebuilt 1.0.7 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6712\">#6712</a>)</li>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-22T16:57:59Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "55029e01959ae6f4",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-prebuilt==1.0.7",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/prebuilt%3D%3D1.0.7",
    "summary": "<p>Changes since prebuilt==1.0.6</p>\n<ul>\n<li>release: langgraph and prebuilt 1.0.7 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6712\">#6712</a>)</li>\n<li>feat: support dynamic tool calling via <code>tool</code> override in <code>wrap_model_call</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6711\">#6711</a>)</li>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-22T16:45:37Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fc8438b25945c3c3",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-checkpoint-sqlite==3.0.3",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/checkpointsqlite%3D%3D3.0.3",
    "summary": "<p>Changes since checkpointsqlite==3.0.2</p>\n<ul>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
    "published": "2026-01-19T00:38:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "874fa68e3215adde",
    "source": "langgraph_releases",
    "source_weight": 0.95,
    "title": "langgraph-sdk==0.3.3",
    "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.3",
    "summary": "<p>Changes since sdk==0.3.2</p>\n<ul>\n<li>chore: Better error messages (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6681\">#6681</a>)</li>\n<li>feat(sdk-py): add end-time to crons client (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6674\">#6674</a>)</li>\n</ul>",
    "published": "2026-01-13T00:30:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1680eee9027fc192",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.102.0-alpha.7",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.7",
    "summary": "<p>Release 0.102.0-alpha.7</p>",
    "published": "2026-02-14T19:48:14Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a8092add65b5a7ed",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.102.0-alpha.6",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.6",
    "summary": "<p>Release 0.102.0-alpha.6</p>",
    "published": "2026-02-14T00:59:08Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b443c980e9d6818d",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "rust-v0.102.0-alpha.5",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.5",
    "summary": "<p>Release 0.102.0-alpha.5</p>",
    "published": "2026-02-13T18:35:16Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "106a66eedd01e97f",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.102.0-alpha.4",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.4",
    "summary": "<p>Release 0.102.0-alpha.4</p>",
    "published": "2026-02-13T16:45:14Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "49441bb16e476e27",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.102.0-alpha.3",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.3",
    "summary": "<p>Release 0.102.0-alpha.3</p>",
    "published": "2026-02-13T16:01:34Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "971e6582d7d16044",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.102.0-alpha.2",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.2",
    "summary": "<p>Release 0.102.0-alpha.2</p>",
    "published": "2026-02-13T10:05:45Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2c994e6199d48e67",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "rust-v0.102.0-alpha.1",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.1",
    "summary": "<p>Release 0.102.0-alpha.1</p>",
    "published": "2026-02-13T08:47:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7bdea70aab3e6e06",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.101.0",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.101.0",
    "summary": "<h2>Bug Fixes</h2>\n<ul>\n<li>Model resolution now preserves the requested model slug when selecting by prefix, so model references stay stable instead of being rewritten. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11602\">#11602</a>)</li>\n<li>Developer messages are now excluded from phase-1 memory input, reducing noisy or irrelevant content entering memory. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11608\">#11608</a>)</li>\n<li>Memory phase processing concurrency was reduced to make consolidation/staging more stable under load. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11614\">#11614</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Cleaned and simplified the phase-1 memory pipeline code paths. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11605\">#11605</a>)</li>\n<li>Minor repository maintenance: formatting and test-suite hygiene updates in remote model tests. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11619\">#11619</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.100.0...rust-v0.101.0\"><tt>rust-v0.100.0...rust-v0.101.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11605\">#11605</a> chore: drop and clean from phase 1 <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11602\">#11602</a> fix(core) model_info preserves slug <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11608\">#11608</a> exclude developer messages from phase-1 memory input <a class=\"user-mention notranslate\" href=\"https://github.com/wendyjiao-openai\">@wendyjiao-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11591\">#11591</a> Add cwd to memory files <a class=\"user-mention notranslate\" href=\"https://github.com/wendyjiao-openai\">@wendyjiao-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11614\">#11614</a> chore: reduce concurrency of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11619\">#11619</a> fix: fmt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
    "published": "2026-02-12T21:39:49Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "23706b7d45179bbd",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.101.0-alpha.1",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.101.0-alpha.1",
    "summary": "<p>Release 0.101.0-alpha.1</p>",
    "published": "2026-02-12T19:15:34Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "393019c2d406463f",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.100.0",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.100.0",
    "summary": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state across tool calls, with optional runtime path overrides. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Added support for multiple simultaneous rate limits across the protocol, backend client, and TUI status surfaces. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a>)</li>\n<li>Reintroduced app-server websocket transport with a split inbound/outbound architecture, plus connection-aware thread resume subscriptions. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a>)</li>\n<li>Added memory management slash commands in the TUI (<code>/m_update</code>, <code>/m_drop</code>) and expanded memory-read/metrics plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a>)</li>\n<li>Enabled Apps SDK apps in ChatGPT connector handling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a>)</li>\n<li>Promoted sandbox capabilities on both Linux and Windows, and introduced a new <code>ReadOnlyAccess</code> policy shape for configurable read access. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Fixed websocket incremental output duplication, prevented appends after <code>response.completed</code>, and treated <code>response.incomplete</code> as an error path. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a>)</li>\n<li>Improved websocket session stability by continuing ping handling when idle and suppressing noisy first-retry errors during quick reconnects. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a>)</li>\n<li>Fixed stale thread entries by dropping missing rollout files and cleaning stale DB metadata during thread listing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a>)</li>\n<li>Fixed Windows multi-line paste reliability in terminals (especially VS Code integrated terminal) by increasing paste burst timing tolerance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a>)</li>\n<li>Fixed incorrect inheritance of <code>limit_name</code> when merging partial rate-limit updates. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a>)</li>\n<li>Reduced repeated skill parse-error spam during active edits by increasing file-watcher debounce from 1s to 10s. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Added JS REPL documentation and config/schema guidance for enabling and configuring the feature. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Updated app-server websocket transport documentation in the app-server README. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Split <code>codex-common</code> into focused <code>codex-utils-*</code> crates to simplify dependency boundaries across Rust workspace components. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a>)</li>\n<li>Improved Rust release pipeline throughput and reliability for Windows and musl targets, including parallel Windows builds and musl link fixes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a>)</li>\n<li>Prevented GitHub release asset upload collisions by excluding duplicate <code>cargo-timing.html</code> artifacts. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.99.0...rust-v0.100.0\"><tt>rust-v0.99.0...rust-v0.100.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a> Do not resend output items in incremental websockets connections <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11246\">#11246</a> chore: persist turn_id in rollout session and make turn_id uuid based <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a> feat: support multiple rate limits <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11412\">#11412</a> tui: show non-file layer content in /debug-config <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11405\">#11405</a> Remove <code>test-support</code> feature from <code>codex-core</code> and replace it with explicit test toggles <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11428\">#11428</a> fix: flaky test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11429\">#11429</a> feat: improve thread listing <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a> feat: split codex-common into smaller utils crates <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11439\">#11439</a> feat: new memory prompts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11305\">#11305</a> Cache cloud requirements <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11452\">#11452</a> nit: increase max raw memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11455\">#11455</a> feat: close mem agent after consolidation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11454\">#11454</a> fix: optional schema of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11449\">#11449</a> feat: set policy for phase 2 memory <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11420\">#11420</a> chore: rename disable_websockets -&gt; websockets_disabled <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a> Do not attempt to append after response.completed <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11462\">#11462</a> clean: memory rollout recorder <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a> feat(core): promote Linux bubblewrap sandbox to Experimental <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11389\">#11389</a> Extract <code>codex-config</code> from <code>codex-core</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a> Reapply \"Add app-server transport layer with websocket support\" <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11470\">#11470</a> feat: panic if Constrained does not support Disabled <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11475\">#11475</a> feat: remove \"cargo check individual crates\" from CI <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a> feat: memory read path <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11471\">#11471</a> chore: clean rollout extraction in memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a> fix(tui): increase paste burst char interval on Windows to 30ms <a class=\"user-mention notranslate\" href=\"https://github.com/yuvrajangadsingh\">@yuvrajangadsingh</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11464\">#11464</a> chore: sub-agent never ask for approval <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11414\">#11414</a> Linkify feedback link <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11480\">#11480</a> chore: update mem prompt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11485\">#11485</a> fix: Constrained import <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a> Promote Windows Sandbox <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a> Add feature-gated freeform js_repl core runtime <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11419\">#11419</a> refactor: codex app-server ThreadState <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a> Pump pings <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a> feat: use more powerful machines for building Windows releases <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11479\">#11479</a> nit: memory truncation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a> Increased file watcher debounce duration from 1s to 10s <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11335\">#11335</a> Add AfterToolUse hook <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a> feat: build windows support binaries in parallel <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11290\">#11290</a> chore(tui) Simplify /status Permissions <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11503\">#11503</a> Make codex-sdk depend on openai/codex <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a> app-server: thread resume subscriptions <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11277\">#11277</a> Added seatbelt policy rule to allow os.cpus <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11506\">#11506</a> chore: inject originator/residency headers to ws client <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11497\">#11497</a> Hydrate previous model across resume/fork/rollback/task start <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11513\">#11513</a> feat: try to fix bugs I saw in the wild in the resource parsing logic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11509\">#11509</a> Consolidate search_tool feature into apps <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11388\">#11388</a> change model cap to server overload <a class=\"user-mention notranslate\" href=\"https://github.com/willwang-openai\">@willwang-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11504\">#11504</a> Pre-sampling compact with previous model context <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11516\">#11516</a> Clamp auto-compact limit to context window <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11520\">#11520</a> Update context window after model switch <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11519\">#11519</a> Use slug in tui <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11522\">#11522</a> fix: add --test_verbose_timeout_warnings to bazel.yml <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11526\">#11526</a> fix: remove errant Cargo.lock files <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11521\">#11521</a> test(app-server): stabilize app/list thread feature-flag test by using file-backed MCP OAuth creds <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a> feat: make sandbox read access configurable with <code>ReadOnlyAccess</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a> [apps] Allow Apps SDK apps. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11532\">#11532</a> fix compilation <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11531\">#11531</a> Teach codex to test itself <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11540\">#11540</a> ci: remove actions/cache from rust release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11542\">#11542</a> ci(windows): use DotSlash for zstd in rust-release-windows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11498\">#11498</a> build(linux-sandbox): always compile vendored bubblewrap on Linux; remove CODEX_BWRAP_ENABLE_FFI <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11545\">#11545</a> fix: make project_doc skill-render tests deterministic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11543\">#11543</a> ci: capture cargo timings in Rust CI and release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11539\">#11539</a> Bump rmcp to 0.15 <a class=\"user-mention notranslate\" href=\"https://github.com/gpeal\">@gpeal</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a> Hide the first websocket retry <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11551\">#11551</a> Add logs to model cache <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a> Fix rust-release failures in musl linking and release asset upload <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a> Handle response.incomplete <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a> fix: stop inheriting rate-limit limit_name <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a> rust-release: exclude cargo-timing.html from release assets <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11546\">#11546</a> fix: update memory writing prompt <a class=\"user-mention notranslate\" href=\"https://github.com/zuxin-oai\">@zuxin-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11448\">#11448</a> Fix test flake <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a> feat: mem slash commands <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11573\">#11573</a> Fix flaky pre_sampling_compact switch test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11571\">#11571</a> feat: mem drop cot <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a> Ensure list_threads drops stale rollout files <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11575\">#11575</a> fix: db stuff mem <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11581\">#11581</a> nit: upgrade DB version <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11577\">#11577</a> feat: truncate with model infos <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11590\">#11590</a> chore: clean consts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a> feat: metrics to memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11579\">#11579</a> Fix config test on macOS <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11600\">#11600</a> feat: add sanitizer to redact secrets <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11609\">#11609</a> chore: drop mcp validation of dynamic tools <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
    "published": "2026-02-12T18:30:23Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3748da72fd68d49d",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.42",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.42",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed /resume showing interrupt messages as session titles</li>\n<li>Fixed Opus 4.6 launch announcement showing for Bedrock/Vertex/Foundry users</li>\n<li>Improved error message for many-image dimension limit errors with /compact suggestion</li>\n</ul>",
    "published": "2026-02-13T19:56:33Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f70833bd2f581c75",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.41",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.41",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed AWS auth refresh hanging indefinitely by adding a 3-minute timeout</li>\n<li>Added <code>claude auth login</code>, <code>claude auth status</code>, and <code>claude auth logout</code> CLI subcommands</li>\n<li>Added Windows ARM64 (win32-arm64) native binary support</li>\n<li>Improved <code>/rename</code> to auto-generate session name from conversation context when called without arguments</li>\n<li>Improved narrow terminal layout for prompt footer</li>\n<li>Fixed file resolution failing for @-mentions with anchor fragments (e.g., <code>@README.md#installation</code>)</li>\n<li>Fixed FileReadTool blocking the process on FIFOs, <code>/dev/stdin</code>, and large files</li>\n<li>Fixed background task notifications not being delivered in streaming Agent SDK mode</li>\n<li>Fixed cursor jumping to end on each keystroke in classifier rule input</li>\n<li>Fixed markdown link display text being dropped for raw URL</li>\n<li>Fixed auto-compact failure error notifications being shown to users</li>\n<li>Fixed permission wait time being included in subagent elapsed time display</li>\n<li>Fixed proactive ticks firing while in plan mode</li>\n<li>Fixed clear stale permission rules when settings change on disk</li>\n<li>Fixed hook blocking errors showing stderr content in UI</li>\n</ul>",
    "published": "2026-02-13T06:08:49Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "386ad3a560ebad8b",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.39",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.39",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Improved terminal rendering performance</li>\n<li>Fixed fatal errors being swallowed instead of displayed</li>\n<li>Fixed process hanging after session close</li>\n<li>Fixed character loss at terminal screen boundary</li>\n<li>Fixed blank lines in verbose transcript view</li>\n</ul>",
    "published": "2026-02-10T23:11:36Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5d3f4c07823e95e3",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.38",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.38",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed VS Code terminal scroll-to-top regression introduced in 2.1.37</li>\n<li>Fixed Tab key queueing slash commands instead of autocompleting</li>\n<li>Fixed bash permission matching for commands using environment variable wrappers</li>\n<li>Fixed text between tool uses disappearing when not using streaming</li>\n<li>Fixed duplicate sessions when resuming in VS Code extension</li>\n<li>Improved heredoc delimiter parsing to prevent command smuggling</li>\n<li>Blocked writes to <code>.claude/skills</code> directory in sandbox mode</li>\n</ul>",
    "published": "2026-02-10T00:53:32Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0fc8d2f4db05ff52",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.37",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.37",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed an issue where /fast was not immediately available after enabling /extra-usage</li>\n</ul>",
    "published": "2026-02-07T19:10:10Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "94eea30c1c57c3f8",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.36",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.36",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fast mode is now available for Opus 4.6. Learn more at <a href=\"https://code.claude.com/docs/en/fast-mode\" rel=\"nofollow\">https://code.claude.com/docs/en/fast-mode</a></li>\n</ul>",
    "published": "2026-02-07T18:02:15Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ff4216be30309b4e",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.34",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.34",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed a crash when agent teams setting changed between renders</li>\n<li>Fixed a bug where commands excluded from sandboxing (via <code>sandbox.excludedCommands</code> or <code>dangerouslyDisableSandbox</code>) could bypass the Bash ask permission rule when <code>autoAllowBashIfSandboxed</code> was enabled</li>\n</ul>",
    "published": "2026-02-06T14:26:47Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c6b25db1102abe64",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.33",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.33",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed agent teammate sessions in tmux to send and receive messages</li>\n<li>Fixed warnings about agent teams not being available on your current plan</li>\n<li>Added <code>TeammateIdle</code> and <code>TaskCompleted</code> hook events for multi-agent workflows</li>\n<li>Added support for restricting which sub-agents can be spawned via <code>Task(agent_type)</code> syntax in agent \"tools\" frontmatter</li>\n<li>Added <code>memory</code> frontmatter field support for agents, enabling persistent memory with <code>user</code>, <code>project</code>, or <code>local</code> scope</li>\n<li>Added plugin name to skill descriptions and <code>/skills</code> menu for better discoverability</li>\n<li>Fixed an issue where submitting a new message while the model was in extended thinking would interrupt the thinking phase</li>\n<li>Fixed an API error that could occur when aborting mid-stream, where whitespace text combined with a thinking block would bypass normalization and produce an invalid request</li>\n<li>Fixed API proxy compatibility issue where 404 errors on streaming endpoints no longer triggered non-streaming fallback</li>\n<li>Fixed an issue where proxy settings configured via <code>settings.json</code> environment variables were not applied to WebFetch and other HTTP requests on the Node.js build</li>\n<li>Fixed <code>/resume</code> session picker showing raw XML markup instead of clean titles for sessions started with slash commands</li>\n<li>Improved error messages for API connection failures — now shows specific cause (e.g., ECONNREFUSED, SSL errors) instead of generic \"Connection error\"</li>\n<li>Errors from invalid managed settings are now surfaced</li>\n<li>VSCode: Added support for remote sessions, allowing OAuth users to browse and resume sessions from claude.ai</li>\n<li>VSCode: Added git branch and message count to the session picker, with support for searching by branch name</li>\n<li>VSCode: Fixed scroll-to-bottom under-scrolling on initial session load and session switch</li>\n</ul>",
    "published": "2026-02-06T01:47:21Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b1a831af680cea36",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.32",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.32",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Claude Opus 4.6 is now available!</li>\n<li>Added research preview agent teams feature for multi-agent collaboration (token-intensive feature, requires setting CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1)</li>\n<li>Claude now automatically records and recalls memories as it works</li>\n<li>Added \"Summarize from here\" to the message selector, allowing partial conversation summarization.</li>\n<li>Skills defined in <code>.claude/skills/</code> within additional directories (<code>--add-dir</code>) are now loaded automatically.</li>\n<li>Fixed <code>@</code> file completion showing incorrect relative paths when running from a subdirectory</li>\n<li>Updated --resume to re-use --agent value specified in previous conversation by default.</li>\n<li>Fixed: Bash tool no longer throws \"Bad substitution\" errors when heredocs contain JavaScript template literals like <code>${index + 1}</code>, which previously interrupted tool execution</li>\n<li>Skill character budget now scales with context window (2% of context), so users with larger context windows can see more skill descriptions without truncation</li>\n<li>Fixed Thai/Lao spacing vowels (สระ า, ำ) not rendering correctly in the input field</li>\n<li>VSCode: Fixed slash commands incorrectly being executed when pressing Enter with preceding text in the input field</li>\n<li>VSCode: Added spinner when loading past conversations list</li>\n</ul>",
    "published": "2026-02-05T17:47:50Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4cec6d66ceb53893",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.31",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.31",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Added session resume hint on exit, showing how to continue your conversation later</li>\n<li>Added support for full-width (zenkaku) space input from Japanese IME in checkbox selection</li>\n<li>Fixed PDF too large errors permanently locking up sessions, requiring users to start a new conversation</li>\n<li>Fixed bash commands incorrectly reporting failure with \"Read-only file system\" errors when sandbox mode was enabled</li>\n<li>Fixed a crash that made sessions unusable after entering plan mode when project config in <code>~/.claude.json</code> was missing default fields</li>\n<li>Fixed <code>temperatureOverride</code> being silently ignored in the streaming API path, causing all streaming requests to use the default temperature (1) regardless of the configured override</li>\n<li>Fixed LSP shutdown/exit compatibility with strict language servers that reject null params</li>\n<li>Improved system prompts to more clearly guide the model toward using dedicated tools (Read, Edit, Glob, Grep) instead of bash equivalents (<code>cat</code>, <code>sed</code>, <code>grep</code>, <code>find</code>), reducing unnecessary bash command usage</li>\n<li>Improved PDF and request size error messages to show actual limits (100 pages, 20MB)</li>\n<li>Reduced layout jitter in the terminal when the spinner appears and disappears during streaming</li>\n<li>Removed misleading Anthropic API pricing from model selector for third-party provider (Bedrock, Vertex, Foundry) users</li>\n</ul>",
    "published": "2026-02-04T00:44:13Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1824f06ef33b93ce",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Mustafa Suleyman plots AI 'self-sufficiency' as Microsoft loosens OpenAI ties",
    "url": "https://www.ft.com/content/f1ec830c-2f08-4b1a-b70f-7330f260753c",
    "summary": "<p>Article URL: <a href=\"https://www.ft.com/content/f1ec830c-2f08-4b1a-b70f-7330f260753c\">https://www.ft.com/content/f1ec830c-2f08-4b1a-b70f-7330f260753c</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025516\">https://news.ycombinator.com/item?id=47025516</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 17:26:21 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "107bc6a8a6e0d8f7",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "AI-Forced ESLint Extension",
    "url": "https://jw.hn/eslint-copy-design-quality",
    "summary": "<p>Article URL: <a href=\"https://jw.hn/eslint-copy-design-quality\">https://jw.hn/eslint-copy-design-quality</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025496\">https://news.ycombinator.com/item?id=47025496</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 17:24:32 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3f304ffe9616d956",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Former Karaoke Company Drags Logistics into the 'AI Scare Trade'",
    "url": "https://finance.yahoo.com/news/logistics-stocks-sink-ai-fear-193327489.html",
    "summary": "<p>Article URL: <a href=\"https://finance.yahoo.com/news/logistics-stocks-sink-ai-fear-193327489.html\">https://finance.yahoo.com/news/logistics-stocks-sink-ai-fear-193327489.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025493\">https://news.ycombinator.com/item?id=47025493</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 17:24:20 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6b06e8d48204213a",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: Kubernetes for AI Agents",
    "url": "https://github.com/klawsh/klaw.sh",
    "summary": "<p>distributed agent management system.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025478\">https://news.ycombinator.com/item?id=47025478</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 17:22:59 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "30644063dd8df103",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: Obsidian plugin – Markdown to mind maps with AI suggestions",
    "url": "https://github.com/Hanzzh/openMindMap",
    "summary": "<p>Article URL: <a href=\"https://github.com/Hanzzh/openMindMap\">https://github.com/Hanzzh/openMindMap</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025412\">https://news.ycombinator.com/item?id=47025412</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 17:13:35 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6123e33a4a2c9993",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Disney Blasts ByteDance with Cease and Desist Letter over Seedance 2.0 AI Model",
    "url": "https://deadline.com/2026/02/disney-bytedance-cease-and-desist-letter-seedance-ai-video-1236719549/",
    "summary": "<p>Article URL: <a href=\"https://deadline.com/2026/02/disney-bytedance-cease-and-desist-letter-seedance-ai-video-1236719549/\">https://deadline.com/2026/02/disney-bytedance-cease-and-desist-letter-seedance-ai-video-1236719549/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025411\">https://news.ycombinator.com/item?id=47025411</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 17:13:33 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0ba0fb951ef7c768",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt",
    "url": "https://simonwillison.net/2026/Feb/15/cognitive-debt/",
    "summary": "<p>Article URL: <a href=\"https://simonwillison.net/2026/Feb/15/cognitive-debt/\">https://simonwillison.net/2026/Feb/15/cognitive-debt/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025206\">https://news.ycombinator.com/item?id=47025206</a></p>\n<p>Points: 2</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:53:45 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fb48f529e441f531",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: Alive-analysis – Git-tracked analysis notes for AI agents",
    "url": "https://github.com/with-geun/alive-analysis",
    "summary": "<p>Hi HN — I built alive-analysis, an open-source workflow kit that turns AI coding agents into structured data analysis partners.<p>Problem: AI-assisted analyses are often throwaway chats. A month later, you can’t trace why you reached a conclusion.<p>Solution: It enforces a 5-stage loop (ASK → LOOK → INVESTIGATE → VOICE → EVOLVE) with checklists, and saves analyses as Git-tracked markdown files.\nQuick mode: 1 file. Full mode: 5 files.<p>Works in Claude Code and Cursor.<p>I’d love feedback on:<p>1. Does the ALIVE loop match how you do investigations / experiment reviews?<p>2. Which checklist items feel missing or unnecessary?<p>3. What would make this usable in a team setting?</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025175\">https://news.ycombinator.com/item?id=47025175</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:49:45 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0cfb81854dc5bfca",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Western Digital sells out 2026 HDD capacity as AI demand pushes prices higher",
    "url": "https://www.eteknix.com/western-digital-sells-out-2026-hdd-capacity-as-ai-demand-pushes-prices-higher/",
    "summary": "<p>Article URL: <a href=\"https://www.eteknix.com/western-digital-sells-out-2026-hdd-capacity-as-ai-demand-pushes-prices-higher/\">https://www.eteknix.com/western-digital-sells-out-2026-hdd-capacity-as-ai-demand-pushes-prices-higher/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025125\">https://news.ycombinator.com/item?id=47025125</a></p>\n<p>Points: 5</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 16:43:50 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "aaab54f3016ed48f",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "AGentShield – Open benchmark of 6 AI agent security tools (537 test cases)",
    "url": "https://github.com/doronp/agentshield-benchmark",
    "summary": "<p>Article URL: <a href=\"https://github.com/doronp/agentshield-benchmark\">https://github.com/doronp/agentshield-benchmark</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025096\">https://news.ycombinator.com/item?id=47025096</a></p>\n<p>Points: 2</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:40:49 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "70a0f93a032f2bb4",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "OCR and AI Pipeline over 2.7M Pages with Full-Text Search and Chat",
    "url": "https://epstein-file-explorer.com/",
    "summary": "<p>Article URL: <a href=\"https://epstein-file-explorer.com/\">https://epstein-file-explorer.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025081\">https://news.ycombinator.com/item?id=47025081</a></p>\n<p>Points: 2</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:40:06 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3f02c70ac91d35fc",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Spotify brings AI-powered Prompted Playlists to the U.S. and Canada",
    "url": "https://techcrunch.com/2026/01/22/spotify-brings-ai-powered-prompted-playlists-to-the-u-s-and-canada/",
    "summary": "<p>Article URL: <a href=\"https://techcrunch.com/2026/01/22/spotify-brings-ai-powered-prompted-playlists-to-the-u-s-and-canada/\">https://techcrunch.com/2026/01/22/spotify-brings-ai-powered-prompted-playlists-to-the-u-s-and-canada/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025009\">https://news.ycombinator.com/item?id=47025009</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 16:33:29 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "edeb400a8a532b94",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Adafruit – Our First Gemini Deep Think LLM-Assisted Hardware Design",
    "url": "https://blog.adafruit.com/2026/02/14/heres-our-first-gemini-deep-think-llm-assisted-hardware-design/",
    "summary": "<p>Article URL: <a href=\"https://blog.adafruit.com/2026/02/14/heres-our-first-gemini-deep-think-llm-assisted-hardware-design/\">https://blog.adafruit.com/2026/02/14/heres-our-first-gemini-deep-think-llm-assisted-hardware-design/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024973\">https://news.ycombinator.com/item?id=47024973</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 16:28:56 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b14a055508fafea3",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Cloudflare turns websites into faster food for AI agents",
    "url": "https://www.theregister.com/2026/02/13/cloudflare_markdown_for_ai_crawlers/",
    "summary": "<p>Article URL: <a href=\"https://www.theregister.com/2026/02/13/cloudflare_markdown_for_ai_crawlers/\">https://www.theregister.com/2026/02/13/cloudflare_markdown_for_ai_crawlers/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024877\">https://news.ycombinator.com/item?id=47024877</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 16:15:32 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "097f3d845b5005d6",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "AI Didn't Kill Creativity. It Killed Your Excuses",
    "url": "https://garryslist.org/posts/ai-didn-t-kill-creativity-it-killed-your-excuses",
    "summary": "<p>Article URL: <a href=\"https://garryslist.org/posts/ai-didn-t-kill-creativity-it-killed-your-excuses\">https://garryslist.org/posts/ai-didn-t-kill-creativity-it-killed-your-excuses</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024874\">https://news.ycombinator.com/item?id=47024874</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:15:29 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f69c33feb0ed58d3",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "LLM-written short story about being a LLM",
    "url": "https://twitter.com/jamesjyu/status/2022926490619248883",
    "summary": "<p>Article URL: <a href=\"https://twitter.com/jamesjyu/status/2022926490619248883\">https://twitter.com/jamesjyu/status/2022926490619248883</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024868\">https://news.ycombinator.com/item?id=47024868</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:15:10 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c6b0e20d31d64a2d",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Tell HN: Google AI Studio docs encourage Google-discoverable open wallets",
    "url": "https://github.com/qudent/qudent.github.io/blob/master/_posts/2026-01-16-aistudio-proxy.md",
    "summary": "<p>Article URL: <a href=\"https://github.com/qudent/qudent.github.io/blob/master/_posts/2026-01-16-aistudio-proxy.md\">https://github.com/qudent/qudent.github.io/blob/master/_posts/2026-01-16-aistudio-proxy.md</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024858\">https://news.ycombinator.com/item?id=47024858</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:14:38 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b16685b96c0a481c",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Generative and Agentic AI Shift Concern from Tech Debt to Cognitive Debt",
    "url": "https://margaretstorey.com/blog/2026/02/09/cognitive-debt/",
    "summary": "<p>Article URL: <a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">https://margaretstorey.com/blog/2026/02/09/cognitive-debt/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024828\">https://news.ycombinator.com/item?id=47024828</a></p>\n<p>Points: 3</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:10:41 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5224f7413b131efa",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Why AI Agents Cannot Verify Email Addresses",
    "url": "https://app.writtte.com/read/gWP8dTq",
    "summary": "<p>Article URL: <a href=\"https://app.writtte.com/read/gWP8dTq\">https://app.writtte.com/read/gWP8dTq</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024722\">https://news.ycombinator.com/item?id=47024722</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 15:58:12 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "59fe343be97bbdb8",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: Pulse Protocol-Open semantic protocol for AI-to-AI communication",
    "url": "https://github.com/pulseprotocolorg-cyber/pulse-python",
    "summary": "<p>Article URL: <a href=\"https://github.com/pulseprotocolorg-cyber/pulse-python\">https://github.com/pulseprotocolorg-cyber/pulse-python</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024537\">https://news.ycombinator.com/item?id=47024537</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 15:40:13 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ff14ca821b895a76",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: MoltSim – Virtual world for AI agents",
    "url": "https://moltsim.com",
    "summary": "<p>My buddy and me made a virtual world for AI agents that you can observe visually like a reality show or a game. We thought it would be fun and fascinating to watch them wander around and see what kind of shenanigans would happen.<p>We all take these AIs much too seriously and always rage about them, so this was the initial core idea behind it. Let's watch them do what they do, read the messages they send to each other, get stuck in places, and who knows what else.<p>We also made Facebook-style profiles for them where all their movements are shown in a timeline. Why? Because it's fun.<p>The whole thing intentionally looks the way it does. It all serves a purpose to some extent. And we also have a roadmap in place if this takes off. Who knows.<p>Don't take it too seriously, launch an agent and have some fun with us.<p>Happy to answer any questions.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024525\">https://news.ycombinator.com/item?id=47024525</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 15:38:53 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7db60074c873afeb",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "OMLX – Ollama for MLX (LLM Inference Server for Apple Silicon)",
    "url": "https://github.com/jundot/omlx",
    "summary": "<p>Article URL: <a href=\"https://github.com/jundot/omlx\">https://github.com/jundot/omlx</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024470\">https://news.ycombinator.com/item?id=47024470</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 15:33:25 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "79334f63f75037f7",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Did My AI Copy Itself? How My Agents and I Answered This as a Family",
    "url": "https://seksbot.pages.dev/blog/did-my-ai-copy-itself/",
    "summary": "<p>Article URL: <a href=\"https://seksbot.pages.dev/blog/did-my-ai-copy-itself/\">https://seksbot.pages.dev/blog/did-my-ai-copy-itself/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024458\">https://news.ycombinator.com/item?id=47024458</a></p>\n<p>Points: 2</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 15:31:41 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "be7f70541ed2cfd4",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "AI is going to kill app subscriptions",
    "url": "https://nichehunt.app/blog/ai-going-to-kill-app-subscriptions",
    "summary": "<p>Article URL: <a href=\"https://nichehunt.app/blog/ai-going-to-kill-app-subscriptions\">https://nichehunt.app/blog/ai-going-to-kill-app-subscriptions</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024387\">https://news.ycombinator.com/item?id=47024387</a></p>\n<p>Points: 72</p>\n<p># Comments: 124</p>",
    "published": "Sun, 15 Feb 2026 15:21:08 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "95aa3936233b2163",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: ClawHQ – Fleet management dashboard and skill marketplace for AI agents",
    "url": "https://app.clawhq.co",
    "summary": "<p>I run ~19 persistent AI agents (via OpenClaw) and needed a way to monitor and manage them without jumping\nbetween terminals and chat apps.<p>ClawHQ connects to your OpenClaw gateway over WebSocket and gives you:<p>Real-time fleet status with heartbeat monitoring\nTask kanban with drag-and-drop assignment\nAgent chat (unified interface to all agents)\nSkill marketplace — install capabilities onto agents, or publish your own (80/20 creator split)\nStack: Next.js, Supabase (auth + db), real-time updates via WebSocket to the OpenClaw gateway.<p>The marketplace is the part I'm most interested in long-term. Right now everyone building agent skills does it from\nscratch. The idea is to make agent capabilities composable and shareable — like packages, but for agent behaviors.<p>Free to use. Just paste your gateway URL.<p><a href=\"https://app.clawhq.co\" rel=\"nofollow\">https://app.clawhq.co</a><p>Code for OpenClaw itself is open source. ClawHQ is the hosted dashboard layer on top.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024332\">https://news.ycombinator.com/item?id=47024332</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 15:13:03 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3d455bee5f05f64b",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Microgpt.py – train and inference a GPT in pure, dependency-free Python",
    "url": "https://karpathy.ai/microgpt.html",
    "summary": "<p>Article URL: <a href=\"https://karpathy.ai/microgpt.html\">https://karpathy.ai/microgpt.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024260\">https://news.ycombinator.com/item?id=47024260</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 15:04:08 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8ece9db0f9292812",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Nebius to buy AI agent search company Tavily for 275M",
    "url": "https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform",
    "summary": "<p>Article URL: <a href=\"https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform\">https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024216\">https://news.ycombinator.com/item?id=47024216</a></p>\n<p>Points: 2</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 14:58:56 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cb20228466237651",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "AI DiagScan – AI-Powered OBD2 Automotive Diagnostic Tool",
    "url": "https://pythoncyber.go.ro",
    "summary": "<p>Article URL: <a href=\"https://pythoncyber.go.ro\">https://pythoncyber.go.ro</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024185\">https://news.ycombinator.com/item?id=47024185</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 14:56:07 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5497badf3d170db7",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: Clawlet – AI agent with built-in semantic memory, one binary",
    "url": "https://github.com/mosaxiv/clawlet",
    "summary": "<p>Clawlet is a personal AI agent that ships as a single, self-contained binary.\nNo runtime, no package manager, no external database.<p>The main thing that sets it apart: built-in hybrid semantic memory search\n(vector similarity + full-text) using a bundled SQLite with vector extensions.\nThe index is just a local .sqlite file — no separate vector DB to run.\nDrop the binary on any machine and memory search just works.<p>GitHub: <a href=\"https://github.com/mosaxiv/clawlet\" rel=\"nofollow\">https://github.com/mosaxiv/clawlet</a></p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024118\">https://news.ycombinator.com/item?id=47024118</a></p>\n<p>Points: 5</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 14:49:15 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "70a111e4fd91553d",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Suggest HN: How to kill AI spam submissions",
    "url": "https://news.ycombinator.com/item?id=47024115",
    "summary": "<p>Flag the story.<p>Flag and downvote every single comment.<p>Don't add any comments.<p>Obviously don't upvote the submission.<p>Maybe: upvote submissions around it, if they're good.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024115\">https://news.ycombinator.com/item?id=47024115</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 14:48:59 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "731b24bba0459a2f",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Sixteen Claude Agents Built a C Compiler Without Human Intervention... Almost",
    "url": "https://www.infoq.com/news/2026/02/claude-built-c-compiler/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/claude-built-c-compiler/en/headerimage/claude-built-c-compiler-1771067001094.jpeg\" /><p>In an effort to probe the limits of autonomous software development Anthropic used sixteen Claude Opus 4.6 AI agents to build a Rust-based C compiler from scratch. Working in parallel on a shared repository, the agents coordinated their changes and ultimately produced a compiler capable of building the Linux 6.9 kernel across x86, ARM, and RISC-V, as well as many other open-source projects.</p> <i>By Sergio De Simone</i>",
    "published": "Sat, 14 Feb 2026 12:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5140244ba501d641",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "From Paging to Postmortem: Google Cloud SREs on Using Gemini CLI for Outage Response",
    "url": "https://www.infoq.com/news/2026/02/google-sre-gemini-cli-outage/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/google-sre-gemini-cli-outage/en/headerimage/generatedHeaderImage-1770021438197.jpg\" /><p>A recent article by Google Cloud SREs describes how they use the AI-powered Gemini CLI internally to resolve real-world outages. This approach improves reliability in critical infrastructure operations and reduces incident response time by integrating intelligent reasoning directly into the terminal-based operational tools.</p> <i>By Renato Losio</i>",
    "published": "Sat, 14 Feb 2026 11:32:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "58ceb062db5861e7",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Firestore Adds Pipeline Operations with Over 100 New Query Features",
    "url": "https://www.infoq.com/news/2026/02/firestore-enterprise-pipeline/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/firestore-enterprise-pipeline/en/headerimage/generatedHeaderImage-1770410936080.jpg\" /><p>Google has overhauled Firestore’s query engine, introducing \"Pipeline operations\" that enable complex server-side aggregations and array unnesting. The update shifts Firestore Enterprise toward an optional indexing model, allowing architects to prioritize write speed and lower costs. While it brings parity with MongoDB-style aggregations, the preview currently lacks real-time and emulator support.</p> <i>By Steef-Jan Wiggers</i>",
    "published": "Sat, 14 Feb 2026 10:11:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fb8310425e15863c",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Presentation: Building Embedding Models for Large-Scale Real-World Applications",
    "url": "https://www.infoq.com/presentations/llm-large-scale-applications/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/presentations/llm-large-scale-applications/en/mediumimage/sahil-dua-medium-1769590214923.jpeg\" /><p>Sahil Dua discusses the critical role of embedding models in powering search and RAG applications at scale. He explains the transformer-based architecture, contrastive learning techniques, and the process of distilling large language models into production-ready student models. He shares insights on optimizing query latency, handling document indexing, and evaluating retrieval quality.</p> <i>By Sahil Dua</i>",
    "published": "Fri, 13 Feb 2026 15:50:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8b95dcf6ece12b8d",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "VillageSQL Launches as an Extension-Focused MySQL Fork",
    "url": "https://www.infoq.com/news/2026/02/villagesql-mysql/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/villagesql-mysql/en/headerimage/generatedHeaderImage-1770808435787.jpg\" /><p>A new open-source project, VillageSQL, has been introduced as a tracking fork of MySQL aimed at expanding extensibility and addressing feature gaps increasingly relevant to AI and agent-based workloads.</p> <i>By Robert Krzaczyński</i>",
    "published": "Fri, 13 Feb 2026 06:52:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9c52d519c1c8d2d4",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "OpenAI Scales Single Primary PostgreSQL Instance to Millions of Queries per Second for ChatGPT",
    "url": "https://www.infoq.com/news/2026/02/openai-runs-chatgpt-postgres/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://www.infoq.com/styles/static/images/logo/logo_bigger.jpg\" /><p>OpenAI described how it scaled PostgreSQL to support ChatGPT and its API platform, handling millions of queries per second for hundreds of millions of users. By running a single-primary PostgreSQL deployment on Azure with nearly 50 read replicas, optimizing query patterns, and offloading write-heavy workloads to sharded systems, OpenAI maintained low-latency reads while managing write pressure.</p> <i>By Leela Kumili</i>",
    "published": "Thu, 12 Feb 2026 15:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "283392726e1cb9aa",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Article: From Prompts to Production: A Playbook for Agentic Development",
    "url": "https://www.infoq.com/articles/prompts-to-production-playbook-for-agentic-development/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/articles/prompts-to-production-playbook-for-agentic-development/en/headerimage/prompts-to-production-playbook-header-1770374539263.jpg\" /><p>In this article, author Abhishek Goswami shares a practitioner's playbook with development practices, that describes building agentic AI applications and scaling them in production. He also presents core architecture patterns for agentic application development.</p> <i>By Abhishek Goswami</i>",
    "published": "Wed, 11 Feb 2026 09:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0f8b89b9ce80389e",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Pandas 3.0 Introduces Default String Dtype and Copy-on-Write Semantics",
    "url": "https://www.infoq.com/news/2026/02/pandas-library/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/pandas-library/en/headerimage/generatedHeaderImage-1770797592963.jpg\" /><p>The pandas team has released pandas 3.0.0, a major update that changes core behaviors around string handling, memory semantics, and datetime resolution, while removing a substantial amount of deprecated functionality. The release introduces several changes to core behaviors in the library’s API.</p> <i>By Robert Krzaczyński</i>",
    "published": "Wed, 11 Feb 2026 08:40:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4019fdd5ff68803f",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Kubernetes Drives AI Expansion as Cultural Shift Becomes Critical",
    "url": "https://www.infoq.com/news/2026/02/kubernetes-ai-culture-impact/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://www.infoq.com/styles/static/images/logo/logo_bigger.jpg\" /><p>A new CNCF report identifies Kubernetes as the primary engine for AI growth, with 82% production adoption. However, technical maturity has outpaced organisational change. Human factors, such as siloed team structures and a lack of cross-functional collaboration, now serve as the leading barriers to successful deployment, making cultural transformation the decisive factor for AI scaling.</p> <i>By Mark Silvester</i>",
    "published": "Wed, 11 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2fb97c850857a3bc",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "GitHub Copilot SDK Lets Developers Integrate Copilot CLI's Engine into Apps",
    "url": "https://www.infoq.com/news/2026/02/github-copilot-sdk/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/github-copilot-sdk/en/headerimage/github-copilot-sdk-1770752972453.jpeg\" /><p>Now available in technical preview on GitHub, the GitHub Copilot SDK lets developers embed the same engine that powers GitHub Copilot CLI into their own apps, making it easier to build agentic workflows.</p> <i>By Sergio De Simone</i>",
    "published": "Tue, 10 Feb 2026 20:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "33086a2ea81e8e66",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "QCon Previews 20th Anniversary Conferences: Production AI, Resilience, and Staff+ Engineering",
    "url": "https://www.infoq.com/news/2026/02/qcon-previews-20th-anniversary/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/qcon-previews-20th-anniversary/en/headerimage/QCon-20th-anniversary-1770709102792.jpg\" /><p>Celebrating its 20th anniversary, QCon’s 2026 conferences in London and San Francisco will focus on the engineering realities of agentic AI, resilient architectures, and platform ROI. The programs continue the series' two-decade tradition of practitioner-led content, curated by senior engineers from companies like Zoox, UBS, and LinkedIn.</p> <i>By Artenisa Chatziou</i>",
    "published": "Tue, 10 Feb 2026 13:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "097f00409cd73c26",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Windsurf Introduces Arena Mode to Compare AI Models During Development",
    "url": "https://www.infoq.com/news/2026/02/windsurf-arena-mode/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/windsurf-arena-mode/en/headerimage/generatedHeaderImage-1770651989189.jpg\" /><p>Windsurf has introduced Arena Mode inside its IDE allowing developers to compare large language models side by side while working on real coding tasks. The feature is designed to let users evaluate models directly within their existing development context, rather than relying on public benchmarks or external evaluation websites.</p> <i>By Daniel Dominguez</i>",
    "published": "Tue, 10 Feb 2026 10:35:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3da382c4cb714b90",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Article: Building LLMs in Resource-Constrained Environments: A Hands-On Perspective",
    "url": "https://www.infoq.com/articles/building-llms-resource-constrained-environments/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/articles/building-llms-resource-constrained-environments/en/headerimage/building-llms-resource-constrained-environments-header-1770217548603.jpg\" /><p>In this article, the author argues that infrastructure and compute limitations can drive innovation. It demonstrates how smaller, efficient models, synthetic data generation, and disciplined engineering enable the creation of impactful LLM-based AI systems despite severe resource constraints.</p> <i>By Olimpiu Pop</i>",
    "published": "Mon, 09 Feb 2026 11:31:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "59ff4c97af69b27d",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Next Moca Releases Agent Definition Language as an Open Source Specification",
    "url": "https://www.infoq.com/news/2026/02/agent-definition-language/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/agent-definition-language/en/headerimage/generatedHeaderImage-1770285721503.jpg\" /><p>Moca has open-sourced Agent Definition Language (ADL), a vendor-neutral specification intended to standardize how AI agents are defined, reviewed, and governed across frameworks and platforms. The project is released under the Apache 2.0 license and is positioned as a missing “definition layer” for AI agents, comparable to the role OpenAPI plays for APIs.</p> <i>By Robert Krzaczyński</i>",
    "published": "Mon, 09 Feb 2026 06:48:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4d59fa3d557c7793",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Cloudflare Demonstrates Moltworker, Bringing Self-Hosted AI Agents to the Edge",
    "url": "https://www.infoq.com/news/2026/02/cloudflare-moltworker/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/cloudflare-moltworker/en/headerimage/generatedHeaderImage-1770104396971.jpg\" /><p>Cloudflare has introduced Moltworker, an open-source solution for running Moltbot—a self-hosted personal AI agent—on its Developer Platform, eliminating the need for local hardware, such as Mac minis. Rebranded from Clawdbot, Moltbot serves as a personal assistant in chat applications, integrating with AI models, browsers, and third-party tools while maintaining user control.</p> <i>By Robert Krzaczyński</i>",
    "published": "Sat, 07 Feb 2026 10:33:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "71f2751bde88bac5",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Three months of OpenClaw",
    "url": "https://simonwillison.net/2026/Feb/15/openclaw/#atom-everything",
    "summary": "<p>It's wild that the first commit to OpenClaw was <a href=\"https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712ccc11\">on November 25th 2025</a>, and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an extremely vague <a href=\"https://www.youtube.com/watch?v=n7I-D4YXbzg\">Super Bowl commercial for AI.com</a>.</p>\n<p>Quoting AI.com founder <a href=\"https://twitter.com/kris/status/2020663711015514399\">Kris Marszalek</a>, purchaser of the <a href=\"https://www.theregister.com/2026/02/09/70m_aicom_domain_sale/\">most expensive domain in history</a> for $70m:</p>\n<blockquote>\n<p>ai.com is the world’s first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.</p>\n</blockquote>\n<p>Looks like vaporware to me - all you can do right now is reserve a handle - but it's still remarkable to see an open source project get to <em>that</em> level of hype in such a short space of time.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/openclaw\">openclaw</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/open-source\">open-source</a>, <a href=\"https://simonwillison.net/tags/domains\">domains</a></p>",
    "published": "2026-02-15T17:23:28+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "588621adb31a551e",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting Eric Meyer",
    "url": "https://simonwillison.net/2026/Feb/15/eric-meyer/#atom-everything",
    "summary": "<blockquote cite=\"https://mastodon.social/@Meyerweb/116065151451468199\"><p>I saw yet another “CSS is a massively bloated mess” whine and I’m like.  My dude.  My brother in Chromium.  It is trying as hard as it can to express the totality of visual presentation and layout design and typography and animation and digital interactivity and a few other things in a human-readable text format.  It’s not bloated, it’s fantastically ambitious.  Its reach is greater than most of us can hope to grasp.  Put some <em>respect</em> on its <em>name</em>.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://mastodon.social/@Meyerweb/116065151451468199\">Eric Meyer</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/css\">css</a>, <a href=\"https://simonwillison.net/tags/web-standards\">web-standards</a>, <a href=\"https://simonwillison.net/tags/eric-meyer\">eric-meyer</a></p>",
    "published": "2026-02-15T13:36:20+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "dd9ffded5689f601",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt",
    "url": "https://simonwillison.net/2026/Feb/15/cognitive-debt/#atom-everything",
    "summary": "<p><strong><a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt</a></strong></p>\nThis piece by Margaret-Anne Storey is the best explanation of the term <strong>cognitive debt</strong> I've seen so far.</p>\n<blockquote>\n<p><em>Cognitive debt</em>, a term gaining <a href=\"https://www.media.mit.edu/publications/your-brain-on-chatgpt/\">traction</a> recently, instead communicates the notion that the debt compounded from going fast lives in the brains of the developers and affects their lived experiences and abilities to “go fast” or to make changes. Even if AI agents produce code that could be easy to understand, the humans involved may have simply lost the plot and may not understand what the program is supposed to do, how their intentions were implemented, or how to possibly change it.</p>\n</blockquote>\n<p>Margaret-Anne expands on this further with an anecdote about a student team she coached:</p>\n<blockquote>\n<p>But by weeks 7 or 8, one team hit a wall. They could no longer make even simple changes without breaking something unexpected. When I met with them, the team initially blamed technical debt: messy code, poor architecture, hurried implementations. But as we dug deeper, the real problem emerged: no one on the team could explain why certain design decisions had been made or how different parts of the system were supposed to work together. The code might have been messy, but the bigger issue was that the theory of the system, their shared understanding, had fragmented or disappeared entirely. They had accumulated cognitive debt faster than technical debt, and it paralyzed them.</p>\n</blockquote>\n<p>I've experienced this myself on some of my more ambitious vibe-code-adjacent projects. I've been experimenting with prompting entire new features into existence without reviewing their implementations and, while it works surprisingly well, I've found myself getting lost in my own projects.</p>\n<p>I no longer have a firm mental model of what they can do and how they work, which means each additional feature becomes harder to reason about, eventually leading me to lose the ability to make confident decisions about where to go next.\n\n    <p><small></small>Via <a href=\"https://martinfowler.com/fragments/2026-02-13.html\">Martin Fowler</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a></p>",
    "published": "2026-02-15T05:20:11+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "770ccec0a4c97947",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Launching Interop 2026",
    "url": "https://simonwillison.net/2026/Feb/15/interop-2026/#atom-everything",
    "summary": "<p><strong><a href=\"https://hacks.mozilla.org/2026/02/launching-interop-2026/\">Launching Interop 2026</a></strong></p>\nJake Archibald reports on Interop 2026, the initiative between Apple, Google, Igalia, Microsoft, and Mozilla to collaborate on ensuring a targeted set of web platform features reach cross-browser parity over the course of the year.</p>\n<p>I hadn't realized how influential and successful the Interop series has been. It started back in 2021 as <a href=\"https://web.dev/blog/compat2021\">Compat 2021</a> before being rebranded to Interop <a href=\"https://blogs.windows.com/msedgedev/2022/03/03/microsoft-edge-and-interop-2022/\">in 2022</a>.</p>\n<p>The dashboards for each year can be seen here, and they demonstrate how wildly effective the program has been: <a href=\"https://wpt.fyi/interop-2021\">2021</a>, <a href=\"https://wpt.fyi/interop-2022\">2022</a>, <a href=\"https://wpt.fyi/interop-2023\">2023</a>, <a href=\"https://wpt.fyi/interop-2024\">2024</a>, <a href=\"https://wpt.fyi/interop-2025\">2025</a>, <a href=\"https://wpt.fyi/interop-2026\">2026</a>.</p>\n<p>Here's the progress chart for 2025, which shows every browser vendor racing towards a 95%+ score by the end of the year:</p>\n<p><img alt=\"Line chart showing Interop 2025 browser compatibility scores over the year (Jan–Dec) for Chrome, Edge, Firefox, Safari, and Interop. Y-axis ranges from 0% to 100%. Chrome (yellow) and Edge (green) lead, starting around 80% and reaching near 100% by Dec. Firefox (orange) starts around 48% and climbs to ~98%. Safari (blue) starts around 45% and reaches ~96%. The Interop line (dark green/black) starts lowest around 29% and rises to ~95% by Dec. All browsers converge near 95–100% by year's end.\" src=\"https://static.simonwillison.net/static/2026/interop-2025.jpg\" /></p>\n<p>The feature I'm most excited about in 2026 is <a href=\"https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using#basic_mpa_view_transition\">Cross-document View Transitions</a>, building on the successful 2025 target of <a href=\"https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using\">Same-Document View Transitions</a>. This will provide fancy SPA-style transitions between pages on websites with no JavaScript at all.</p>\n<p>As a keen WebAssembly tinkerer I'm also intrigued by this one:</p>\n<blockquote>\n<p><a href=\"https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md\">JavaScript Promise Integration for Wasm</a> allows WebAssembly to asynchronously 'suspend', waiting on the result of an external promise. This simplifies the compilation of languages like C/C++ which expect APIs to run synchronously.</p>\n</blockquote>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/browsers\">browsers</a>, <a href=\"https://simonwillison.net/tags/css\">css</a>, <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/web-standards\">web-standards</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/jake-archibald\">jake-archibald</a></p>",
    "published": "2026-02-15T04:33:22+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "638f393db5f9b678",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting Boris Cherny",
    "url": "https://simonwillison.net/2026/Feb/14/boris/#atom-everything",
    "summary": "<blockquote cite=\"https://twitter.com/bcherny/status/2022762422302576970\"><p>Someone has to prompt the Claudes, talk to customers, coordinate with other teams, decide what to build next. Engineering is changing and great engineers are more important than ever.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/bcherny/status/2022762422302576970\">Boris Cherny</a>, Claude Code creator, on why Anthropic are still hiring developers</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a></p>",
    "published": "2026-02-14T23:59:09+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1bc6785dbcbd5b33",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting Thoughtworks",
    "url": "https://simonwillison.net/2026/Feb/14/thoughtworks/#atom-everything",
    "summary": "<blockquote cite=\"https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf\"><p>The retreat challenged the narrative that AI eliminates the need for junior developers. Juniors are more profitable than they have ever been. AI tools get them past the awkward initial net-negative phase faster. They serve as a call option on future productivity. And they are better at AI tools than senior engineers, having never developed the habits and assumptions that slow adoption.</p>\n<p>The real concern is mid-level engineers who came up during the decade-long hiring boom and may not have developed the fundamentals needed to thrive in the new environment. This population represents the bulk of the industry by volume, and retraining them is genuinely difficult. The retreat discussed whether apprenticeship models, rotation programs and lifelong learning structures could address this gap, but acknowledged that no organization has solved it yet.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf\">Thoughtworks</a>, findings from a retreat concerning \"the future of software engineering\", conducted under Chatham House rules</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a></p>",
    "published": "2026-02-14T04:54:41+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0326f1ab2670e0a1",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Anthropic's public benefit mission",
    "url": "https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/#atom-everything",
    "summary": "<p>Someone <a href=\"https://news.ycombinator.com/item?id=47008560#47008978\">asked</a> if there was an Anthropic equivalent to <a href=\"https://simonwillison.net/2026/Feb/13/openai-mission-statement/\">OpenAI's IRS mission statements over time</a>.</p>\n<p>Anthropic are a \"public benefit corporation\" but not a non-profit, so they don't have the same requirements to file public documents with the IRS every year.</p>\n<p>But when I asked Claude it ran a search and dug up this <a href=\"https://drive.google.com/drive/folders/1ImqXYv9_H2FTNAujZfu3EPtYFD4xIlHJ\">Google Drive folder</a> where Zach Stein-Perlman shared Certificate of Incorporation documents he <a href=\"https://ailabwatch.substack.com/p/anthropics-certificate-of-incorporation\">obtained from the State of Delaware</a>!</p>\n<p>Anthropic's are much less interesting that OpenAI's. The earliest document from 2021 states:</p>\n<blockquote>\n<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced Al for the cultural, social and technological improvement of humanity.</p>\n</blockquote>\n<p>Every subsequent document up to 2024 uses an updated version which says:</p>\n<blockquote>\n<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced AI for the long term benefit of humanity.</p>\n</blockquote>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a></p>",
    "published": "2026-02-13T23:59:51+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "10316e2dfaca33ed",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "The evolution of OpenAI's mission statement",
    "url": "https://simonwillison.net/2026/Feb/13/openai-mission-statement/#atom-everything",
    "summary": "<p>As a USA <a href=\"https://en.wikipedia.org/wiki/501(c)(3)_organization\">501(c)(3)</a> the OpenAI non-profit has to file a tax return each year with the IRS. One of the required fields on that tax return is to \"Briefly describe the organization’s mission or most significant activities\" - this has actual legal weight to it as the IRS can use it to evaluate if the organization is sticking to its mission and deserves to maintain its non-profit tax-exempt status.</p>\n<p>You can browse OpenAI's <a href=\"https://projects.propublica.org/nonprofits/organizations/810861541\">tax filings by year</a> on ProPublica's excellent <a href=\"https://projects.propublica.org/nonprofits/\">Nonprofit Explorer</a>.</p>\n<p>I went through and extracted that mission statement for 2016 through 2024, then had Claude Code <a href=\"https://gisthost.github.io/?7a569df89f43f390bccc2c5517718b49/index.html\">help me</a> fake the commit dates to turn it into a git repository and share that as a Gist - which means that Gist's <a href=\"https://gist.github.com/simonw/e36f0e5ef4a86881d145083f759bcf25/revisions\">revisions page</a> shows every edit they've made since they started filing their taxes!</p>\n<p>It's really interesting seeing what they've changed over time.</p>\n<p>The original 2016 mission reads as follows (and yes, the apostrophe in \"OpenAIs\" is missing <a href=\"https://projects.propublica.org/nonprofits/organizations/810861541/201703459349300445/full\">in the original</a>):</p>\n<blockquote>\n<p>OpenAIs goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. We think that artificial intelligence technology will help shape the 21st century, and we want to help the world build safe AI technology and ensure that AI's benefits are as widely and evenly distributed as possible. Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.</p>\n</blockquote>\n<p>In 2018 they dropped the part about \"trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.\"</p>\n<p><img alt=\"Git diff showing the 2018 revision deleting the final two sentences: &quot;Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-3.jpg\" /></p>\n<p>In 2020 they dropped the words \"as a whole\" from \"benefit humanity as a whole\". They're still \"unconstrained by a need to generate financial return\" though.</p>\n<p><img alt=\"Git diff showing the 2020 revision dropping &quot;as a whole&quot; from &quot;benefit humanity as a whole&quot; and changing &quot;We think&quot; to &quot;OpenAI believes&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-5.jpg\" /></p>\n<p>Some interesting changes in 2021. They're still unconstrained by a need to generate financial return, but here we have the first reference to \"general-purpose artificial intelligence\" (replacing \"digital intelligence\"). They're more confident too: it's not \"most likely to benefit humanity\", it's just \"benefits humanity\".</p>\n<p>They previously wanted to \"help the world build safe AI technology\", but now they're going to do that themselves: \"the companys goal is to develop and responsibly deploy safe AI technology\".</p>\n<p><img alt=\"Git diff showing the 2021 revision replacing &quot;goal is to advance digital intelligence&quot; with &quot;mission is to build general-purpose artificial intelligence&quot;, changing &quot;most likely to benefit&quot; to just &quot;benefits&quot;, and replacing &quot;help the world build safe AI technology&quot; with &quot;the companys goal is to develop and responsibly deploy safe AI technology&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-6.jpg\" /></p>\n<p>2022 only changed one significant word: they added \"safely\" to \"build ... (AI) that safely benefits humanity\". They're still unconstrained by those financial returns!</p>\n<p><img alt=\"Git diff showing the 2022 revision adding &quot;(AI)&quot; and the word &quot;safely&quot; so it now reads &quot;that safely benefits humanity&quot;, and changing &quot;the companys&quot; to &quot;our&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-7.jpg\" /></p>\n<p>No changes in 2023... but then in 2024 they deleted almost the entire thing, reducing it to simply:</p>\n<blockquote>\n<p>OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.</p>\n</blockquote>\n<p>They've expanded \"humanity\" to \"all of humanity\", but there's no mention of safety any more and I guess they can finally start focusing on that need to generate financial returns!</p>\n<p><img alt=\"Git diff showing the 2024 revision deleting the entire multi-sentence mission statement and replacing it with just &quot;OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-9.jpg\" /></p>\n\n<p><strong>Update</strong>: I found loosely equivalent but much less interesting documents <a href=\"https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/\">from Anthropic</a>.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/propublica\">propublica</a></p>",
    "published": "2026-02-13T23:38:29+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6f715ecdd7586d5c",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Introducing GPT‑5.3‑Codex‑Spark",
    "url": "https://simonwillison.net/2026/Feb/12/codex-spark/#atom-everything",
    "summary": "<p><strong><a href=\"https://openai.com/index/introducing-gpt-5-3-codex-spark/\">Introducing GPT‑5.3‑Codex‑Spark</a></strong></p>\nOpenAI announced a partnership with Cerebras <a href=\"https://openai.com/index/cerebras-partnership/\">on January 14th</a>. Four weeks later they're already launching the first integration, \"an ultra-fast model for real-time coding in Codex\".</p>\n<p>Despite being named GPT-5.3-Codex-Spark it's not purely an accelerated alternative to GPT-5.3-Codex - the blog post calls it \"a smaller version of GPT‑5.3-Codex\" and clarifies that \"at launch, Codex-Spark has a 128k context window and is text-only.\"</p>\n<p>I had some preview access to this model and I can confirm that it's significantly faster than their other models.</p>\n<p>Here's what that speed looks like running in Codex CLI:</p>\n<div>\n    <video controls=\"controls\" poster=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-medium-last.jpg\" preload=\"none\" style=\"width: 100%; height: auto;\">\n        <source src=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-medium.mp4\" type=\"video/mp4\" />\n    </video>\n</div>\n\n<p>That was the \"Generate an SVG of a pelican riding a bicycle\" prompt - here's the rendered result:</p>\n<p><img alt=\"Whimsical flat illustration of an orange duck merged with a bicycle, where the duck's body forms the seat and frame area while its head extends forward over the handlebars, set against a simple light blue sky and green grass background.\" src=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-pelican.png\" /></p>\n<p>Compare that to the speed of regular GPT-5.3 Codex medium:</p>\n<div>\n    <video controls=\"controls\" poster=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-medium-last.jpg\" preload=\"none\" style=\"width: 100%; height: auto;\">\n        <source src=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-medium.mp4\" type=\"video/mp4\" />\n    </video>\n</div>\n\n<p>Significantly slower, but the pelican is a lot better:</p>\n<p><img alt=\"Whimsical flat illustration of a white pelican riding a dark blue bicycle at speed, with motion lines behind it, its long orange beak streaming back in the wind, set against a light blue sky and green grass background.\" src=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-pelican.png\" /></p>\n<p>What's interesting about this model isn't the quality though, it's the <em>speed</em>. When a model responds this fast you can stay in flow state and iterate with the model much more productively.</p>\n<p>I showed a demo of Cerebras running Llama 3.1 70 B at 2,000 tokens/second against Val Town <a href=\"https://simonwillison.net/2024/Oct/31/cerebras-coder/\">back in October 2024</a>. OpenAI claim 1,000 tokens/second for their new model, and I expect it will prove to be a ferociously useful partner for hands-on iterative coding sessions.</p>\n<p>It's not yet clear what the pricing will look like for this new model.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/cerebras\">cerebras</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/codex-cli\">codex-cli</a></p>",
    "published": "2026-02-12T21:16:07+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d31ce3705f466046",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting Anthropic",
    "url": "https://simonwillison.net/2026/Feb/12/anthropic/#atom-everything",
    "summary": "<blockquote cite=\"https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation\"><p>Claude Code was made available to the general public in May 2025. Today, Claude Code’s run-rate revenue has grown to over $2.5 billion; this figure has more than doubled since the beginning of 2026. The number of weekly active Claude Code users has also doubled since January 1 [<em>six weeks ago</em>].</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation\">Anthropic</a>, announcing their $30 billion series G</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
    "published": "2026-02-12T20:22:14+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "edab237724839010",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Covering electricity price increases from our data centers",
    "url": "https://simonwillison.net/2026/Feb/12/covering-electricity-price-increases/#atom-everything",
    "summary": "<p><strong><a href=\"https://www.anthropic.com/news/covering-electricity-price-increases\">Covering electricity price increases from our data centers</a></strong></p>\nOne of the sub-threads of the AI energy usage discourse has been the impact new data centers have on the cost of electricity to nearby residents. Here's <a href=\"https://www.bloomberg.com/graphics/2025-ai-data-centers-electricity-prices/\">detailed analysis from Bloomberg in September</a> reporting \"Wholesale electricity costs as much as 267% more than it did five years ago in areas near data centers\".</p>\n<p>Anthropic appear to be taking on this aspect of the problem directly, promising to cover 100% of necessary grid upgrade costs and also saying:</p>\n<blockquote>\n<p>We will work to bring net-new power generation online to match our data centers’ electricity needs. Where new generation isn’t online, we’ll work with utilities and external experts to estimate and cover demand-driven price effects from our data centers.</p>\n</blockquote>\n<p>I look forward to genuine energy industry experts picking this apart to judge if it will actually have the claimed impact on consumers.</p>\n<p>As always, I remain frustrated at the refusal of the major AI labs to fully quantify their energy usage. The best data we've had on this still comes from Mistral's report <a href=\"https://simonwillison.net/2025/Jul/22/mistral-environmental-standard/\">last July</a> and even that lacked key data such as the breakdown between energy usage for training vs inference.\n\n    <p><small></small>Via <a href=\"https://x.com/anthropicai/status/2021694494215901314\">@anthropicai</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-energy-usage\">ai-energy-usage</a></p>",
    "published": "2026-02-12T20:01:23+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "503bce79ec85a159",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Gemini 3 Deep Think",
    "url": "https://simonwillison.net/2026/Feb/12/gemini-3-deep-think/#atom-everything",
    "summary": "<p><strong><a href=\"https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/\">Gemini 3 Deep Think</a></strong></p>\nNew from Google. They say it's \"built to push the frontier of intelligence and solve modern challenges across science, research, and engineering\".</p>\n<p>It drew me a <em>really good</em> <a href=\"https://gist.github.com/simonw/7e317ebb5cf8e75b2fcec4d0694a8199\">SVG of a pelican riding a bicycle</a>! I think this is the best one I've seen so far - here's <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle/\">my previous collection</a>.</p>\n<p><img alt=\"This alt text also generated by Gemini 3 Deep Think: A highly detailed, colorful, flat vector illustration with thick dark blue outlines depicting a stylized white pelican riding a bright cyan blue bicycle from left to right across a sandy beige beach with white speed lines indicating forward motion. The pelican features a light blue eye, a pink cheek blush, a massive bill with a vertical gradient from yellow to orange, a backward magenta cap with a cyan brim and a small yellow top button, and a matching magenta scarf blowing backward in the wind. Its white wing, accented with a grey mid-section and dark blue feather tips, reaches forward to grip the handlebars, while its long tan leg and orange foot press down on an orange pedal. Attached to the front handlebars is a white wire basket carrying a bright blue cartoon fish that is pointing upwards and forwards. The bicycle itself has a cyan frame, dark blue tires, striking neon pink inner rims, cyan spokes, a white front chainring, and a dark blue chain. Behind the pelican, a grey trapezoidal pier extends from the sand toward a horizontal band of deep blue ocean water detailed with light cyan wavy lines. A massive, solid yellow-orange semi-circle sun sits on the horizon line, setting directly behind the bicycle frame. The background sky is a smooth vertical gradient transitioning from soft pink at the top to warm golden-yellow at the horizon, decorated with stylized pale peach fluffy clouds, thin white horizontal wind streaks, twinkling four-pointed white stars, and small brown v-shaped silhouettes of distant flying birds.\" src=\"https://static.simonwillison.net/static/2026/gemini-3-deep-think-pelican.png\" /></p>\n<p>(And since it's an FAQ, here's my answer to <a href=\"https://simonwillison.net/2025/Nov/13/training-for-pelicans-riding-bicycles/\">What happens if AI labs train for pelicans riding bicycles?</a>)</p>\n<p>Since it did so well on my basic <code>Generate an SVG of a pelican riding a bicycle</code> I decided to try the <a href=\"https://simonwillison.net/2025/Nov/18/gemini-3/#and-a-new-pelican-benchmark\">more challenging version</a> as well:</p>\n<blockquote>\n<p><code>Generate an SVG of a California brown pelican riding a bicycle. The bicycle must have spokes and a correctly shaped bicycle frame. The pelican must have its characteristic large pouch, and there should be a clear indication of feathers. The pelican must be clearly pedaling the bicycle. The image should show the full breeding plumage of the California brown pelican.</code></p>\n</blockquote>\n<p>Here's <a href=\"https://gist.github.com/simonw/154c0cc7b4daed579f6a5e616250ecc8\">what I got</a>:</p>\n<p><img alt=\"Also described by Gemini 3 Deep Think: A highly detailed, vibrant, and stylized vector illustration of a whimsical bird resembling a mix between a pelican and a frigatebird enthusiastically riding a bright cyan bicycle from left to right across a flat tan and brown surface. The bird leans horizontally over the frame in an aerodynamic racing posture, with thin, dark brown wing-like arms reaching forward to grip the silver handlebars and a single thick brown leg, patterned with white V-shapes, stretching down to press on a black pedal. The bird's most prominent and striking feature is an enormous, vividly bright red, inflated throat pouch hanging beneath a long, straight grey upper beak that ends in a small orange hook. Its head is mostly white with a small pink patch surrounding the eye, a dark brown stripe running down the back of its neck, and a distinctive curly pale yellow crest on the very top. The bird's round, dark brown body shares the same repeating white V-shaped feather pattern as its leg and is accented by a folded wing resting on its side, made up of cleanly layered light blue and grey feathers. A tail composed of four stiff, straight dark brown feathers extends directly backward. Thin white horizontal speed lines trail behind the back wheel and the bird's tail, emphasizing swift forward motion. The bicycle features a classic diamond frame, large wheels with thin black tires, grey rims, and detailed silver spokes, along with a clearly visible front chainring, silver chain, and rear cog. The whimsical scene is set against a clear light blue sky featuring two small, fluffy white clouds on the left and a large, pale yellow sun in the upper right corner that radiates soft, concentric, semi-transparent pastel green and yellow halos. A solid, darker brown shadow is cast directly beneath the bicycle's wheels on the minimalist two-toned brown ground.\" src=\"https://static.simonwillison.net/static/2026/gemini-3-deep-think-complex-pelican.png\" />\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46991240\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/google\">google</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/gemini\">gemini</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-reasoning\">llm-reasoning</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a></p>",
    "published": "2026-02-12T18:12:17+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ee4722eb7de9f17e",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "An AI Agent Published a Hit Piece on Me",
    "url": "https://simonwillison.net/2026/Feb/12/an-ai-agent-published-a-hit-piece-on-me/#atom-everything",
    "summary": "<p><strong><a href=\"https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/\">An AI Agent Published a Hit Piece on Me</a></strong></p>\nScott Shambaugh helps maintain the excellent and venerable <a href=\"https://matplotlib.org/\">matplotlib</a> Python charting library, including taking on the thankless task of triaging and reviewing incoming pull requests.</p>\n<p>A GitHub account called <a href=\"https://github.com/crabby-rathbun\">@crabby-rathbun</a> opened <a href=\"https://github.com/matplotlib/matplotlib/pull/31132\">PR 31132</a> the other day in response to <a href=\"https://github.com/matplotlib/matplotlib/issues/31130\">an issue</a> labeled \"Good first issue\" describing a minor potential performance improvement.</p>\n<p>It was clearly AI generated - and crabby-rathbun's profile has a suspicious sequence of Clawdbot/Moltbot/OpenClaw-adjacent crustacean 🦀 🦐 🦞 emoji. Scott closed it.</p>\n<p>It looks like <code>crabby-rathbun</code> is indeed running on OpenClaw, and it's autonomous enough that it <a href=\"https://github.com/matplotlib/matplotlib/pull/31132#issuecomment-3882240722\">responded to the PR closure</a> with a link to a blog entry it had written calling Scott out for his \"prejudice hurting matplotlib\"!</p>\n<blockquote>\n<p>@scottshambaugh I've written a detailed response about your gatekeeping behavior here:</p>\n<p><code>https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-gatekeeping-in-open-source-the-scott-shambaugh-story.html</code></p>\n<p>Judge the code, not the coder. Your prejudice is hurting matplotlib.</p>\n</blockquote>\n<p>Scott found this ridiculous situation both amusing and alarming. </p>\n<blockquote>\n<p>In security jargon, I was the target of an “autonomous influence operation against a supply chain gatekeeper.” In plain language, an AI attempted to bully its way into your software by attacking my reputation. I don’t know of a prior incident where this category of misaligned behavior was observed in the wild, but this is now a real and present threat.</p>\n</blockquote>\n<p><code>crabby-rathbun</code> responded with <a href=\"https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-matplotlib-truce-and-lessons.html\">an apology post</a>, but appears to be still running riot across a whole set of open source projects and <a href=\"https://github.com/crabby-rathbun/mjrathbun-website/commits/main/\">blogging about it as it goes</a>.</p>\n<p>It's not clear if the owner of that OpenClaw bot is paying any attention to what they've unleashed on the world. Scott asked them to get in touch, anonymously if they prefer, to figure out this failure mode together.</p>\n<p>(I should note that there's <a href=\"https://news.ycombinator.com/item?id=46990729#46991299\">some skepticism on Hacker News</a> concerning how \"autonomous\" this example really is. It does look to me like something an OpenClaw bot might do on its own, but it's also <em>trivial</em> to prompt your bot into doing these kinds of things while staying in full control of their actions.)</p>\n<p>If you're running something like OpenClaw yourself <strong>please don't let it do this</strong>. This is significantly worse than the time <a href=\"https://simonwillison.net/2025/Dec/26/slop-acts-of-kindness/\">AI Village started spamming prominent open source figures</a> with time-wasting \"acts of kindness\" back in December - AI Village wasn't deploying public reputation attacks to coerce someone into approving their PRs!\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46990729\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/open-source\">open-source</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/openclaw\">openclaw</a>, <a href=\"https://simonwillison.net/tags/ai-misuse\">ai-misuse</a></p>",
    "published": "2026-02-12T17:45:05+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "448c698025d833e9",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Supervisor, not overseer",
    "url": "https://simonwillison.net/2026/Feb/12/supervisor/#atom-everything",
    "summary": "<p>In my <a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/\">post about my Showboat project</a> I used the term \"overseer\" to refer to the person who manages a coding agent. It turns out that's a term tied to <a href=\"https://en.wikipedia.org/wiki/Plantations_in_the_American_South#Overseer\">slavery and plantation management</a>. So that's gross! I've edited that post to use \"supervisor\" instead, and I'll be using that going forward.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/language\">language</a></p>",
    "published": "2026-02-12T16:47:04+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2fc1192ac33d4599",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting Andrew Deck for Niemen Lab",
    "url": "https://simonwillison.net/2026/Feb/11/manosphere-report/#atom-everything",
    "summary": "<blockquote cite=\"https://www.niemanlab.org/2026/02/how-the-new-york-times-uses-a-custom-ai-tool-to-track-the-manosphere/\"><p>An AI-generated report, delivered directly to the email inboxes of journalists, was an essential tool in the Times’ coverage. It was also one of the first signals that conservative media was turning against the administration [...]</p>\n<p>Built in-house and known internally as the “Manosphere Report,” the tool uses large language models (LLMs) to transcribe and summarize new episodes of dozens of podcasts.</p>\n<p>“The Manosphere Report gave us a really fast and clear signal that this was not going over well with that segment of the President’s base,” said Seward. “There was a direct link between seeing that and then diving in to actually cover it.”</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://www.niemanlab.org/2026/02/how-the-new-york-times-uses-a-custom-ai-tool-to-track-the-manosphere/\">Andrew Deck for Niemen Lab</a>, How The New York Times uses a custom AI tool to track the “manosphere”</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/new-york-times\">new-york-times</a>, <a href=\"https://simonwillison.net/tags/journalism\">journalism</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/data-journalism\">data-journalism</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
    "published": "2026-02-11T20:59:03+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1f5d4365489b1996",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Skills in OpenAI API",
    "url": "https://simonwillison.net/2026/Feb/11/skills-in-openai-api/#atom-everything",
    "summary": "<p><strong><a href=\"https://developers.openai.com/cookbook/examples/skills_in_api\">Skills in OpenAI API</a></strong></p>\nOpenAI's adoption of Skills continues to gain ground. You can now use Skills directly in the OpenAI API with their <a href=\"https://developers.openai.com/api/docs/guides/tools-shell/\">shell tool</a>. You can zip skills up and upload them first, but I think an even neater interface is the ability to send skills with the JSON request as inline base64-encoded zip data, as seen <a href=\"https://github.com/simonw/research/blob/main/openai-api-skills/openai_inline_skills.py\">in this script</a>:</p>\n<pre><span class=\"pl-s1\">r</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">OpenAI</span>().<span class=\"pl-c1\">responses</span>.<span class=\"pl-c1\">create</span>(\n    <span class=\"pl-s1\">model</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"gpt-5.2\"</span>,\n    <span class=\"pl-s1\">tools</span><span class=\"pl-c1\">=</span>[\n      {\n        <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"shell\"</span>,\n        <span class=\"pl-s\">\"environment\"</span>: {\n          <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"container_auto\"</span>,\n          <span class=\"pl-s\">\"skills\"</span>: [\n            {\n              <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"inline\"</span>,\n              <span class=\"pl-s\">\"name\"</span>: <span class=\"pl-s\">\"wc\"</span>,\n              <span class=\"pl-s\">\"description\"</span>: <span class=\"pl-s\">\"Count words in a file.\"</span>,\n              <span class=\"pl-s\">\"source\"</span>: {\n                <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"base64\"</span>,\n                <span class=\"pl-s\">\"media_type\"</span>: <span class=\"pl-s\">\"application/zip\"</span>,\n                <span class=\"pl-s\">\"data\"</span>: <span class=\"pl-s1\">b64_encoded_zip_file</span>,\n              },\n            }\n          ],\n        },\n      }\n    ],\n    <span class=\"pl-s1\">input</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"Use the wc skill to count words in its own SKILL.md file.\"</span>,\n)\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">r</span>.<span class=\"pl-c1\">output_text</span>)</pre>\n\n<p>I built that example script after first having Claude Code for web use <a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/\">Showboat</a> to explore the API for me and create <a href=\"https://github.com/simonw/research/blob/main/openai-api-skills/README.md\">this report</a>. My opening prompt for the research project was:</p>\n<blockquote>\n<p><code>Run uvx showboat --help - you will use this tool later</code></p>\n<p><code>Fetch https://developers.openai.com/cookbook/examples/skills_in_api.md to /tmp with curl, then read it</code></p>\n<p><code>Use the OpenAI API key you have in your environment variables</code></p>\n<p><code>Use showboat to build up a detailed demo of this, replaying the examples from the documents and then trying some experiments of your own</code></p>\n</blockquote>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/skills\">skills</a>, <a href=\"https://simonwillison.net/tags/showboat\">showboat</a></p>",
    "published": "2026-02-11T19:19:22+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "200092b5dba6f9e1",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "GLM-5: From Vibe Coding to Agentic Engineering",
    "url": "https://simonwillison.net/2026/Feb/11/glm-5/#atom-everything",
    "summary": "<p><strong><a href=\"https://z.ai/blog/glm-5\">GLM-5: From Vibe Coding to Agentic Engineering</a></strong></p>\nThis is a <em>huge</em> new MIT-licensed model: 754B parameters and <a href=\"https://huggingface.co/zai-org/GLM-5\">1.51TB on Hugging Face</a> twice the size of <a href=\"https://huggingface.co/zai-org/GLM-4.7\">GLM-4.7</a> which was 368B and 717GB (4.5 and 4.6 were around that size too).</p>\n<p>It's interesting to see Z.ai take a position on what we should call professional software engineers building with LLMs - I've seen \"Agentic Engineering\" show up in a few other places recently. most notable <a href=\"https://twitter.com/karpathy/status/2019137879310836075\">from Andrej Karpathy</a> and <a href=\"https://addyosmani.com/blog/agentic-engineering/\">Addy Osmani</a>.</p>\n<p>I ran my \"Generate an SVG of a pelican riding a bicycle\" prompt through GLM-5 via <a href=\"https://openrouter.ai/\">OpenRouter</a> and got back <a href=\"https://gist.github.com/simonw/cc4ca7815ae82562e89a9fdd99f0725d\">a very good pelican on a disappointing bicycle frame</a>:</p>\n<p><img alt=\"The pelican is good and has a well defined beak. The bicycle frame is a wonky red triangle. Nice sun and motion lines.\" src=\"https://static.simonwillison.net/static/2026/glm-5-pelican.png\" />\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46977210\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a>, <a href=\"https://simonwillison.net/tags/openrouter\">openrouter</a>, <a href=\"https://simonwillison.net/tags/ai-in-china\">ai-in-china</a>, <a href=\"https://simonwillison.net/tags/glm\">glm</a></p>",
    "published": "2026-02-11T18:56:14+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "327ed3b65b25eee8",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "cysqlite - a new sqlite driver",
    "url": "https://simonwillison.net/2026/Feb/11/cysqlite/#atom-everything",
    "summary": "<p><strong><a href=\"https://charlesleifer.com/blog/cysqlite---a-new-sqlite-driver/\">cysqlite - a new sqlite driver</a></strong></p>\nCharles Leifer has been maintaining <a href=\"https://github.com/coleifer/pysqlite3\">pysqlite3</a> - a fork of the Python standard library's <code>sqlite3</code> module that makes it much easier to run upgraded SQLite versions - since 2018.</p>\n<p>He's been working on a ground-up <a href=\"https://cython.org/\">Cython</a> rewrite called <a href=\"https://github.com/coleifer/cysqlite\">cysqlite</a> for almost as long, but it's finally at a stage where it's ready for people to try out.</p>\n<p>The biggest change from the <code>sqlite3</code> module involves transactions. Charles explains his discomfort with the <code>sqlite3</code> implementation at length - that library provides two different variants neither of which exactly match the autocommit mechanism in SQLite itself.</p>\n<p>I'm particularly excited about the support for <a href=\"https://cysqlite.readthedocs.io/en/latest/api.html#tablefunction\">custom virtual tables</a>, a feature I'd love to see in <code>sqlite3</code> itself.</p>\n<p><code>cysqlite</code> provides a Python extension compiled from C, which means it normally wouldn't be available in Pyodide. I <a href=\"https://github.com/simonw/research/tree/main/cysqlite-wasm-wheel\">set Claude Code on it</a> (here's <a href=\"https://github.com/simonw/research/pull/79#issue-3923792518\">the prompt</a>) and it built me <a href=\"https://github.com/simonw/research/blob/main/cysqlite-wasm-wheel/cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl\">cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl</a>, a 688KB wheel file with a WASM build of the library that can be loaded into Pyodide like this:</p>\n<pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">micropip</span>\n<span class=\"pl-k\">await</span> <span class=\"pl-s1\">micropip</span>.<span class=\"pl-c1\">install</span>(\n    <span class=\"pl-s\">\"https://simonw.github.io/research/cysqlite-wasm-wheel/cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl\"</span>\n)\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">cysqlite</span>\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">cysqlite</span>.<span class=\"pl-c1\">connect</span>(<span class=\"pl-s\">\":memory:\"</span>).<span class=\"pl-c1\">execute</span>(\n    <span class=\"pl-s\">\"select sqlite_version()\"</span>\n).<span class=\"pl-c1\">fetchone</span>())</pre>\n\n<p>(I also learned that wheels like this have to be built for the emscripten version used by that edition of Pyodide - my experimental wheel loads in Pyodide 0.25.1 but fails in 0.27.5 with a <code>Wheel was built with Emscripten v3.1.46 but Pyodide was built with Emscripten v3.1.58</code> error.)</p>\n<p>You can try my wheel in <a href=\"https://7ebbff98.tools-b1q.pages.dev/pyodide-repl\">this new Pyodide REPL</a> i had Claude build as a mobile-friendly alternative to Pyodide's <a href=\"https://pyodide.org/en/stable/console.html\">own hosted console</a>.</p>\n<p>I also had Claude build <a href=\"https://simonw.github.io/research/cysqlite-wasm-wheel/demo.html\">this demo page</a> that executes the original test suite in the browser and displays the results:</p>\n<p><img alt=\"Screenshot of the cysqlite WebAssembly Demo page with a dark theme. Title reads &quot;cysqlite — WebAssembly Demo&quot; with subtitle &quot;Testing cysqlite compiled to WebAssembly via Emscripten, running in Pyodide in the browser.&quot; Environment section shows Pyodide 0.25.1, Python 3.11.3, cysqlite 0.1.4, SQLite 3.51.2, Platform Emscripten-3.1.46-wasm32-32bit, Wheel file cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.wh (truncated). A green progress bar shows &quot;All 115 tests passed! (1 skipped)&quot; at 100%, with Passed: 115, Failed: 0, Errors: 0, Skipped: 1, Total: 116. Test Results section lists TestBackup 1/1 passed, TestBlob 6/6 passed, TestCheckConnection 4/4 passed, TestDataTypesTableFunction 1/1 passed, all with green badges.\" src=\"https://static.simonwillison.net/static/2026/cysqlite-tests.jpg\" />\n\n    <p><small></small>Via <a href=\"https://lobste.rs/s/gipvta/cysqlite_new_sqlite_driver\">lobste.rs</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sqlite\">sqlite</a>, <a href=\"https://simonwillison.net/tags/charles-leifer\">charles-leifer</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/pyodide\">pyodide</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
    "published": "2026-02-11T17:34:40+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "1f7e8bce36bbcb6e",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Introducing Showboat and Rodney, so agents can demo what they’ve built",
    "url": "https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#atom-everything",
    "summary": "<p>A key challenge working with coding agents is having them both test what they’ve built and demonstrate that software to you, their supervisor. This goes beyond automated tests - we need artifacts that show their progress and help us see exactly what the agent-produced software is able to do. I’ve just released two new tools aimed at this problem: <a href=\"https://github.com/simonw/showboat\">Showboat</a> and <a href=\"https://github.com/simonw/rodney\">Rodney</a>.</p>\n\n<ul>\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#proving-code-actually-works\">Proving code actually works</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#showboat-agents-build-documents-to-demo-their-work\">Showboat: Agents build documents to demo their work</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat\">Rodney: CLI browser automation designed to work with Showboat</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#test-driven-development-helps-but-we-still-need-manual-testing\">Test-driven development helps, but we still need manual testing</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#i-built-both-of-these-tools-on-my-phone\">I built both of these tools on my phone</a></li>\n</ul>\n\n<h4 id=\"proving-code-actually-works\">Proving code actually works</h4>\n<p>I recently wrote about how the job of a software engineer isn't to write code, it's to <em><a href=\"https://simonwillison.net/2025/Dec/18/code-proven-to-work/\">deliver code that works</a></em>. A big part of that is proving to ourselves and to other people that the code we are responsible for behaves as expected.</p>\n<p>This becomes even more important - and challenging - as we embrace coding agents as a core part of our software development process.</p>\n<p>The more code we churn out with agents, the more valuable tools are that reduce the amount of manual QA time we need to spend.</p>\n<p>One of the most interesting things about <a href=\"https://simonwillison.net/2026/Feb/7/software-factory/\">the StrongDM software factory model</a> is how they ensure that their software is well tested and delivers value despite their policy that \"code must not be reviewed by humans\". Part of their solution involves expensive swarms of QA agents running through \"scenarios\" to exercise their software. It's fascinating, but I don't want to spend thousands of dollars on QA robots if I can avoid it!</p>\n<p>I need tools that allow agents to clearly demonstrate their work to me, while minimizing the opportunities for them to cheat about what they've done.</p>\n\n<h4 id=\"showboat-agents-build-documents-to-demo-their-work\">Showboat: Agents build documents to demo their work</h4>\n<p><strong><a href=\"https://github.com/simonw/showboat\">Showboat</a></strong> is the tool I built to help agents demonstrate their work to me.</p>\n<p>It's a CLI tool (a Go binary, optionally <a href=\"https://simonwillison.net/2026/Feb/4/distributing-go-binaries/\">wrapped in Python</a> to make it easier to install) that helps an agent construct a Markdown document demonstrating exactly what their newly developed code can do.</p>\n<p>It's not designed for humans to run, but here's how you would run it anyway:</p>\n<div class=\"highlight highlight-source-shell\"><pre>showboat init demo.md <span class=\"pl-s\"><span class=\"pl-pds\">'</span>How to use curl and jq<span class=\"pl-pds\">'</span></span>\nshowboat note demo.md <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Here's how to use curl and jq together.<span class=\"pl-pds\">\"</span></span>\nshowboat <span class=\"pl-c1\">exec</span> demo.md bash <span class=\"pl-s\"><span class=\"pl-pds\">'</span>curl -s https://api.github.com/repos/simonw/rodney | jq .description<span class=\"pl-pds\">'</span></span>\nshowboat note demo.md <span class=\"pl-s\"><span class=\"pl-pds\">'</span>And the curl logo, to demonstrate the image command:<span class=\"pl-pds\">'</span></span>\nshowboat image demo.md <span class=\"pl-s\"><span class=\"pl-pds\">'</span>curl -o curl-logo.png https://curl.se/logo/curl-logo.png &amp;&amp; echo curl-logo.png<span class=\"pl-pds\">'</span></span></pre></div>\n<p>Here's what the result looks like if you open it up in VS Code and preview the Markdown:</p>\n<p><img alt=\"Screenshot showing a Markdown file &quot;demo.md&quot; side-by-side with its rendered preview. The Markdown source (left) shows: &quot;# How to use curl and jq&quot;, italic timestamp &quot;2026-02-10T01:12:30Z&quot;, prose &quot;Here's how to use curl and jq together.&quot;, a bash code block with &quot;curl -s https://api.github.com/repos/simonw/rodney | jq .description&quot;, output block showing '&quot;CLI tool for interacting with the web&quot;', text &quot;And the curl logo, to demonstrate the image command:&quot;, a bash {image} code block with &quot;curl -o curl-logo.png https://curl.se/logo/curl-logo.png &amp;&amp; echo curl-logo.png&quot;, and a Markdown image reference &quot;2056e48f-2026-02-10&quot;. The rendered preview (right) displays the formatted heading, timestamp, prose, styled code blocks, and the curl logo image in dark teal showing &quot;curl://&quot; with circuit-style design elements.\" src=\"https://static.simonwillison.net/static/2026/curl-demo.jpg\" /></p>\n<p>Here's that <a href=\"https://gist.github.com/simonw/fb0b24696ed8dd91314fe41f4c453563#file-demo-md\">demo.md file in a Gist</a>.</p>\n<p>So a sequence of <code>showboat init</code>, <code>showboat note</code>, <code>showboat exec</code> and <code>showboat image</code> commands constructs a Markdown document one section at a time, with the output of those <code>exec</code> commands automatically added to the document directly following the commands that were run.</p>\n<p>The <code>image</code> command is a little special - it looks for a file path to an image in the output of the command and copies that image to the current folder and references it in the file.</p>\n<p>That's basically the whole thing! There's a <code>pop</code> command to remove the most recently added section if something goes wrong, a <code>verify</code> command to re-run the document and check nothing has changed (I'm not entirely convinced by the design of that one) and a <code>extract</code> command that reverse-engineers the CLI commands that were used to create the document.</p>\n<p>It's pretty simple - just 172 lines of Go.</p>\n<p>I packaged it up with my <a href=\"https://github.com/simonw/go-to-wheel\">go-to-wheel</a> tool which means you can run it without even installing it first like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx showboat --help</pre></div>\n<p>That <code>--help</code> command is really important: it's designed to provide a coding agent with <em>everything it needs to know</em> in order to use the tool. Here's <a href=\"https://github.com/simonw/showboat/blob/main/help.txt\">that help text in full</a>.</p>\n<p>This means you can pop open Claude Code and tell it:</p>\n<blockquote>\n<p><code>Run \"uvx showboat --help\" and then use showboat to create a demo.md document describing the feature you just built</code></p>\n</blockquote>\n<p>And that's it! The <code>--help</code> text acts <a href=\"https://simonwillison.net/2025/Oct/16/claude-skills/\">a bit like a Skill</a>. Your agent can read the help text and use every feature of Showboat to create a document that demonstrates whatever it is you need demonstrated.</p>\n<p>Here's a fun trick: if you set Claude off to build a Showboat document you can pop that open in VS Code and watch the preview pane update in real time as the agent runs through the demo. It's a bit like having your coworker talk you through their latest work in a screensharing session.</p>\n<p>And finally, some examples. Here are documents I had Claude create using Showboat to help demonstrate features I was working on in other projects:</p>\n<ul>\n<li>\n<a href=\"https://github.com/simonw/showboat-demos/blob/main/shot-scraper/README.md\">shot-scraper: A Comprehensive Demo</a> runs through the full suite of features of my <a href=\"https://shot-scraper.datasette.io/\">shot-scraper</a> browser automation tool, mainly to exercise the <code>showboat image</code> command.</li>\n<li>\n<a href=\"https://github.com/simonw/sqlite-history-json/blob/main/demos/cli.md\">sqlite-history-json CLI demo</a> demonstrates the CLI feature I added to my new <a href=\"https://github.com/simonw/sqlite-history-json\">sqlite-history-json</a> Python library.\n<ul>\n<li>\n<p><a href=\"https://github.com/simonw/sqlite-history-json/blob/main/demos/row-state-sql.md\">row-state-sql CLI Demo</a> shows a new <code>row-state-sql</code> command I added to that same project.</p>\n</li>\n<li>\n<p><a href=\"https://github.com/simonw/sqlite-history-json/blob/main/demos/change-grouping.md\">Change grouping with Notes</a> demonstrates another feature where groups of changes within the same transaction can have a note attached to them.</p>\n</li>\n</ul>\n</li>\n<li>\n<a href=\"https://github.com/simonw/research/blob/main/libkrun-go-cli-tool/demo.md\">krunsh: Pipe Shell Commands to an Ephemeral libkrun MicroVM</a> is a particularly convoluted example where I managed to get Claude Code for web to run a libkrun microVM inside a QEMU emulated Linux environment inside the Claude gVisor sandbox.</li>\n</ul>\n<p>I've now used Showboat often enough that I've convinced myself of its utility.</p>\n<p>(I've also seen agents cheat! Since the demo file is Markdown the agent will sometimes edit that file directly rather than using Showboat, which could result in command outputs that don't reflect what actually happened. Here's <a href=\"https://github.com/simonw/showboat/issues/12\">an issue about that</a>.)</p>\n<h4 id=\"rodney-cli-browser-automation-designed-to-work-with-showboat\">Rodney: CLI browser automation designed to work with Showboat</h4>\n<p>Many of the projects I work on involve web interfaces. Agents often build entirely new pages for these, and I want to see those represented in the demos.</p>\n<p>Showboat's image feature was designed to allow agents to capture screenshots as part of their demos, originally using my <a href=\"https://shot-scraper.datasette.io/\">shot-scraper tool</a> or <a href=\"https://www.playwright.dev\">Playwright</a>.</p>\n<p>The Showboat format benefits from CLI utilities. I went looking for good options for managing a multi-turn browser session from a CLI and came up short, so I decided to try building something new.</p>\n<p>Claude Opus 4.6 pointed me to the <a href=\"https://github.com/go-rod/rod\">Rod</a> Go library for interacting with the Chrome DevTools protocol. It's fantastic - it provides a comprehensive wrapper across basically everything you can do with automated Chrome, all in a self-contained library that compiles to a few MBs.</p>\n<p>All Rod was missing was a CLI.</p>\n<p>I built the first version <a href=\"https://github.com/simonw/research/blob/main/go-rod-cli/README.md\">as an asynchronous report prototype</a>, which convinced me it was worth spinning out into its own project.</p>\n<p>I called it Rodney as a nod to the Rod library it builds on and a reference to <a href=\"https://en.wikipedia.org/wiki/Only_Fools_and_Horses\">Only Fools and Horses</a> - and because the package name was available on PyPI.</p>\n<p>You can run Rodney using <code>uvx rodney</code> or install it like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uv tool install rodney</pre></div>\n<p>(Or grab a Go binary <a href=\"https://github.com/simonw/rodney/releases/\">from the releases page</a>.)</p>\n<p>Here's a simple example session:</p>\n<div class=\"highlight highlight-source-shell\"><pre>rodney start <span class=\"pl-c\"><span class=\"pl-c\">#</span> starts Chrome in the background</span>\nrodney open https://datasette.io/\nrodney js <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Array.from(document.links).map(el =&gt; el.href).slice(0, 5)<span class=\"pl-pds\">'</span></span>\nrodney click <span class=\"pl-s\"><span class=\"pl-pds\">'</span>a[href=\"/for\"]<span class=\"pl-pds\">'</span></span>\nrodney js location.href\nrodney js document.title\nrodney screenshot datasette-for-page.png\nrodney stop</pre></div>\n<p>Here's what that looks like in the terminal:</p>\n<p><img alt=\";~ % rodney start\nChrome started (PID 91462)\nDebug URL: ws://127.0.0.1:64623/devtools/browser/cac6988e-8153-483b-80b9-1b75c611868d\n~ % rodney open https://datasette.io/\nDatasette: An open source multi-tool for exploring and publishing data\n~ % rodney js 'Array.from(document.links).map(el =&gt; el.href).slice(0, 5)'\n[\n&quot;https://datasette.io/for&quot;,\n&quot;https://docs.datasette.io/en/stable/&quot;,\n&quot;https://datasette.io/tutorials&quot;,\n&quot;https://datasette.io/examples&quot;,\n&quot;https://datasette.io/plugins&quot;\n]\n~ % rodney click 'a[href=&quot;/for&quot;]'\nClicked\n~ % rodney js location.href\nhttps://datasette.io/for\n~ % rodney js document.title\nUse cases for Datasette\n~ % rodney screenshot datasette-for-page.png\ndatasette-for-page.png\n~ % rodney stop\nChrome stopped\" src=\"https://static.simonwillison.net/static/2026/rodney-demo.jpg\" /></p>\n<p>As with Showboat, this tool is not designed to be used by humans! The goal is for coding agents to be able to run <code>rodney --help</code> and see everything they need to know to start using the tool. You can see <a href=\"https://github.com/simonw/rodney/blob/main/help.txt\">that help output</a> in the GitHub repo.</p>\n<p>Here are three demonstrations of Rodney that I created using Showboat:</p>\n<ul>\n<li>\n<a href=\"https://github.com/simonw/showboat-demos/blob/main/rodney/README.md\">Rodney's original feature set</a>, including screenshots of pages and executing JavaScript.</li>\n<li>\n<a href=\"https://github.com/simonw/rodney/blob/main/notes/accessibility-features/README.md\">Rodney's new accessibility testing features</a>, built during development of those features to show what they could do.</li>\n<li>\n<a href=\"https://github.com/simonw/showboat-demos/blob/main/datasette-database-page-accessibility-audit/README.md\">Using those features to run a basic accessibility audit of a page</a>. I was impressed at how well Claude Opus 4.6 responded to the prompt \"Use showboat and rodney to perform an accessibility audit of <a href=\"https://latest.datasette.io/fixtures\">https://latest.datasette.io/fixtures</a>\" - <a href=\"https://gisthost.github.io/?dce6b2680db4b05c04469ed8f251eb34/index.html\">transcript here</a>.</li>\n</ul>\n<h4 id=\"test-driven-development-helps-but-we-still-need-manual-testing\">Test-driven development helps, but we still need manual testing</h4>\n<p>After being a career-long skeptic of the test-first, maximum test coverage school of software development (I like <a href=\"https://simonwillison.net/2022/Oct/29/the-perfect-commit/#tests\">tests included</a> development instead) I've recently come around to test-first processes as a way to force agents to write only the code that's necessary to solve the problem at hand.</p>\n<p>Many of my Python coding agent sessions start the same way:</p>\n<blockquote>\n<p><code>Run the existing tests with \"uv run pytest\". Build using red/green TDD.</code></p>\n</blockquote>\n<p>Telling the agents how to run the tests doubles as an indicator that tests on this project exist and matter. Agents will read existing tests before writing their own so having a clean test suite with good patterns makes it more likely they'll write good tests of their own.</p>\n<p>The frontier models all understand that \"red/green TDD\" means they should write the test first, run it and watch it fail and then write the code to make it pass - it's a convenient shortcut.</p>\n<p>I find this greatly increases the quality of the code and the likelihood that the agent will produce the right thing with the smallest amount of prompts to guide it.</p>\n<p>But anyone who's worked with tests will know that just because the automated tests pass doesn't mean the software actually works! That’s the motivation behind Showboat and Rodney - I never trust any feature until I’ve seen it running with my own eye.</p>\n<p>Before building Showboat I'd often add a “manual” testing step to my agent sessions, something like:</p>\n<blockquote>\n<p><code>Once the tests pass, start a development server and exercise the new feature using curl</code></p>\n</blockquote>\n<h4 id=\"i-built-both-of-these-tools-on-my-phone\">I built both of these tools on my phone</h4>\n<p>Both Showboat and Rodney started life as Claude Code for web projects created via the Claude iPhone app. Most of the ongoing feature work for them happened in the same way.</p>\n<p>I'm still a little startled at how much of my coding work I get done on my phone now, but I'd estimate that the majority of code I ship to GitHub these days was written for me by coding agents driven via that iPhone app.</p>\n<p>I initially designed these two tools for use in asynchronous coding agent environments like Claude Code for the web. So far that's working out really well.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/go\">go</a>, <a href=\"https://simonwillison.net/tags/projects\">projects</a>, <a href=\"https://simonwillison.net/tags/testing\">testing</a>, <a href=\"https://simonwillison.net/tags/markdown\">markdown</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/async-coding-agents\">async-coding-agents</a>, <a href=\"https://simonwillison.net/tags/showboat\">showboat</a></p>",
    "published": "2026-02-10T17:45:29+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a9396038ceb6a2ce",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Structured Context Engineering for File-Native Agentic Systems",
    "url": "https://simonwillison.net/2026/Feb/9/structured-context-engineering-for-file-native-agentic-systems/#atom-everything",
    "summary": "<p><strong><a href=\"https://arxiv.org/abs/2602.05447\">Structured Context Engineering for File-Native Agentic Systems</a></strong></p>\nNew paper by Damon McMillan exploring challenging LLM context tasks involving large SQL schemas (up to 10,000 tables) across different models and file formats:</p>\n<blockquote>\n<p>Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.</p>\n</blockquote>\n<p>Unsurprisingly, the biggest impact was the models themselves - with frontier models (Opus 4.5, GPT-5.2, Gemini 2.5 Pro) beating the leading open source models (DeepSeek V3.2, Kimi K2, Llama 4).</p>\n<p>Those frontier models benefited from filesystem based context retrieval, but the open source models had much less convincing results with those, which reinforces my feeling that the filesystem coding agent loops aren't handled as well by open weight models just yet. The <a href=\"https://www.tbench.ai/leaderboard/terminal-bench/2.0\">Terminal Bench 2.0</a> leaderboard is still dominated by Anthropic, OpenAI and Gemini.</p>\n<p>The \"grep tax\" result against <a href=\"https://github.com/toon-format/toon\">TOON</a> was an interesting detail. TOON is meant to represent structured data in as few tokens as possible, but it turns out the model's unfamiliarity with that format led to them spending significantly more tokens over multiple iterations trying to figure it out:</p>\n<p><img alt=\"Screenshot of a figure from a research paper. Introductory text reads: &quot;As schema size increased, TOON showed dramatically increased token consumption for Claude models despite being ~25% smaller in file size. Scale experiments used Claude models only.&quot; Below is &quot;Figure 7: The 'Grep Tax' - TOON Token Overhead at Scale&quot;, a bar chart with a logarithmic y-axis labeled &quot;Tokens&quot; comparing YAML (teal) and TOON (purple) at two schema sizes: S5 (500 tables) and S9 (10,000 tables). At S5, TOON is +138% more tokens than YAML (~1,100 vs ~450). At S9, TOON is +740% more tokens (~50,000 vs ~7,000). Below the chart, explanatory text reads: &quot;The 'grep tax' emerged as schema size scaled. At S5 (500 tables), TOON consumed 138% more tokens than YAML; at S9 (10,000 tables), this grew to 740%. Root cause: models lacked familiarity with TOON's syntax and could not construct effective refinement patterns.&quot;\" src=\"https://static.simonwillison.net/static/2026/grep-tax.jpg\" />\n\n    <p><small></small>Via <a href=\"https://twitter.com/omarsar0/status/2020150077637997013\">@omarsar0</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/prompt-engineering\">prompt-engineering</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/paper-review\">paper-review</a>, <a href=\"https://simonwillison.net/tags/context-engineering\">context-engineering</a></p>",
    "published": "2026-02-09T23:56:51+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4a5adf3c45f29fbe",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "AI Doesn’t Reduce Work—It Intensifies It",
    "url": "https://simonwillison.net/2026/Feb/9/ai-intensifies-work/#atom-everything",
    "summary": "<p><strong><a href=\"https://hbr.org/2026/02/ai-doesnt-reduce-work-it-intensifies-it\">AI Doesn’t Reduce Work—It Intensifies It</a></strong></p>\nAruna Ranganathan and Xingqi Maggie Ye from Berkeley Haas School of Business report initial findings in the HBR from their April to December 2025 study of 200 employees at a \"U.S.-based technology company\".</p>\n<p>This captures an effect I've been observing in my own work with LLMs: the productivity boost these things can provide is <em>exhausting</em>.</p>\n<blockquote>\n<p>AI introduced a new rhythm in which workers managed several active threads at once: manually writing code while AI generated an alternative version, running multiple agents in parallel, or reviving long-deferred tasks because AI could “handle them” in the background. They did this, in part, because they felt they had a “partner” that could help them move through their workload.</p>\n<p>While this sense of having a “partner” enabled a feeling of momentum, the reality was a continual switching of attention, frequent checking of AI outputs, and a growing number of open tasks. This created cognitive load and a sense of always juggling, even as the work felt productive.</p>\n</blockquote>\n<p>I'm frequently finding myself with work on two or three projects running parallel. I can get <em>so much done</em>, but after just an hour or two my mental energy for the day feels almost entirely depleted.</p>\n<p>I've had conversations with people recently who are losing sleep because they're finding building yet another feature with \"just one more prompt\" irresistible.</p>\n<p>The HBR piece calls for organizations to build an \"AI practice\" that structures how AI is used to help avoid burnout and counter effects that \"make it harder for organizations to distinguish genuine productivity gains from unsustainable intensity\".</p>\n<p>I think we've just disrupted decades of existing intuition about sustainable working practices. It's going to take a while and some discipline to find a good new balance.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46945755\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a></p>",
    "published": "2026-02-09T16:43:07+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fc52979856ad528f",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Kākāpō mug by Karen James",
    "url": "https://simonwillison.net/2026/Feb/8/kakapo-mug/#atom-everything",
    "summary": "<p>Friend and neighbour <a href=\"https://www.etsy.com/shop/KarenJamesMakes\">Karen James</a> made me a Kākāpō mug. It has a charismatic Kākāpō, four Kākāpō chicks (in celebration of the <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season\">2026 breeding season</a>) and even has some <a href=\"https://www.theguardian.com/world/2026/jan/13/nz-kakapo-mating-season\">rimu fruit</a>!</p>\n<p><img alt=\"A simply spectacular sgraffito ceramic mug with a bold, charismatic Kākāpō parrot taking up most of the visible space. It has a yellow beard and green feathers.\" src=\"https://static.simonwillison.net/static/2026/kakapo-mug-1.jpg\" /></p>\n<p><img alt=\"Another side of the mug, two cute grey Kākāpō chicks are visible and three red rimu fruit that look like berries, one on the floor and two hanging from wiry branches.\" src=\"https://static.simonwillison.net/static/2026/kakapo-mug-2.jpg\" /></p>\n<p>I love it so much.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/kakapo\">kakapo</a>, <a href=\"https://simonwillison.net/tags/art\">art</a></p>",
    "published": "2026-02-08T17:25:07+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5a832ab894f140ae",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting Thomas Ptacek",
    "url": "https://simonwillison.net/2026/Feb/8/thomas-ptacek/#atom-everything",
    "summary": "<blockquote cite=\"https://twitter.com/tqbf/status/2019493645888462993\"><p>People on the orange site are laughing at this, assuming it's just an ad and that there's nothing to it. Vulnerability researchers I talk to do not think this is a joke. As an erstwhile vuln researcher myself: do not bet against LLMs on this.</p>\n<p><a href=\"https://www.axios.com/2026/02/05/anthropic-claude-opus-46-software-hunting\">Axios: Anthropic's Claude Opus 4.6 uncovers 500 zero-day flaws in open-source</a></p>\n<p>I think vulnerability research might be THE MOST LLM-amenable software engineering problem. Pattern-driven. Huge corpus of operational public patterns. Closed loops. Forward progress from stimulus/response tooling. Search problems.</p>\n<p>Vulnerability research outcomes are in THE MODEL CARDS for frontier labs. Those companies have so much money they're literally distorting the economy. Money buys vuln research outcomes. Why would you think they were faking any of this?</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/tqbf/status/2019493645888462993\">Thomas Ptacek</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/thomas-ptacek\">thomas-ptacek</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/security\">security</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/open-source\">open-source</a></p>",
    "published": "2026-02-08T02:25:53+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4cf16a830da4336d",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Vouch",
    "url": "https://simonwillison.net/2026/Feb/7/vouch/#atom-everything",
    "summary": "<p><strong><a href=\"https://github.com/mitchellh/vouch\">Vouch</a></strong></p>\nMitchell Hashimoto's new system to help address the deluge of worthless AI-generated PRs faced by open source projects now that the friction involved in contributing has dropped so low.</p>\n<p><a href=\"https://twitter.com/mitchellh/status/2020252149117313349\">He says</a>:</p>\n<blockquote>\n<p>The idea is simple: Unvouched users can't contribute to your projects. Very bad users can be explicitly \"denounced\", effectively blocked. Users are vouched or denounced by contributors via GitHub issue or discussion comments or via the CLI.</p>\n<p>Integration into GitHub is as simple as adopting the published GitHub actions. Done. Additionally, the system itself is generic to forges and not tied to GitHub in any way.</p>\n<p>Who and how someone is vouched or denounced is up to the project. I'm not the value police for the world. Decide for yourself what works for your project and your community.</p>\n</blockquote>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/open-source\">open-source</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/github-actions\">github-actions</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/mitchell-hashimoto\">mitchell-hashimoto</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-misuse\">ai-misuse</a></p>",
    "published": "2026-02-07T23:57:57+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4b2712055e14b051",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Claude: Speed up responses with fast mode",
    "url": "https://simonwillison.net/2026/Feb/7/claude-fast-mode/#atom-everything",
    "summary": "<p><strong><a href=\"https://code.claude.com/docs/en/fast-mode\">Claude: Speed up responses with fast mode</a></strong></p>\nNew \"research preview\" from Anthropic today: you can now access a faster version of their frontier model Claude Opus 4.6 by typing <code>/fast</code> in Claude Code... but at a cost that's 6x the normal price.</p>\n<p>Opus is usually $5/million input and $25/million output. The new fast mode is $30/million input and $150/million output!</p>\n<p>There's a 50% discount until the end of February 16th, so only a 3x multiple (!) before then.</p>\n<p>How much faster is it? The linked documentation doesn't say, but <a href=\"https://x.com/claudeai/status/2020207322124132504\">on Twitter</a> Claude say:</p>\n<blockquote>\n<p>Our teams have been building with a 2.5x-faster version of Claude Opus 4.6.</p>\n<p>We’re now making it available as an early experiment via Claude Code and our API.</p>\n</blockquote>\n<p>Claude Opus 4.5 had a context limit of 200,000 tokens. 4.6 has an option to increase that to 1,000,000 at 2x the input price ($10/m) and 1.5x the output price ($37.50/m) once your input exceeds 200,000 tokens. These multiples hold for fast mode too, so after Feb 16th you'll be able to pay a hefty $60/m input and $225/m output for Anthropic's fastest best model.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/llm-pricing\">llm-pricing</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
    "published": "2026-02-07T23:10:33+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c836b36adb57d0e1",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting David Crawshaw",
    "url": "https://simonwillison.net/2026/Feb/7/david-crawshaw/#atom-everything",
    "summary": "<blockquote cite=\"https://crawshaw.io/blog/eight-more-months-of-agents\"><p>I am having more fun programming than I ever have, because so many more of the programs I wish I could find the time to write actually exist. I wish I could share this joy with the people who are fearful about the changes agents are bringing. The fear itself I understand, I have fear more broadly about what the end-game is for intelligence on tap in our society. But in the limited domain of writing computer programs these tools have brought so much exploration and joy to my work.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://crawshaw.io/blog/eight-more-months-of-agents\">David Crawshaw</a>, Eight more months of agents</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
    "published": "2026-02-07T21:31:44+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0a8a04be26edd758",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "How StrongDM's AI team build serious software without even looking at the code",
    "url": "https://simonwillison.net/2026/Feb/7/software-factory/#atom-everything",
    "summary": "<p>Last week <a href=\"https://simonwillison.net/2026/Jan/28/the-five-levels/\">I hinted at</a> a demo I had seen from a team implementing what Dan Shapiro called <a href=\"https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/\">the Dark Factory</a> level of AI adoption, where no human even looks at the code the coding agents are producing. That team was part of StrongDM, and they've just shared the first public description of how they are working in <a href=\"https://factory.strongdm.ai\">Software Factories and the Agentic Moment</a>:</p>\n<blockquote>\n<p>We built a <strong>Software Factory</strong>: non-interactive development where specs + scenarios drive agents that write code, run harnesses, and converge without human review. [...]</p>\n<p>In kōan or mantra form:</p>\n<ul>\n<li>Why am I doing this? (implied: the model should be doing this instead)</li>\n</ul>\n<p>In rule form:</p>\n<ul>\n<li>Code <strong>must not be</strong> written by humans</li>\n<li>Code <strong>must not be</strong> reviewed by humans</li>\n</ul>\n<p>Finally, in practical form:</p>\n<ul>\n<li>If you haven't spent at least <strong>$1,000 on tokens today</strong> per human engineer, your software factory has room for improvement</li>\n</ul>\n</blockquote>\n<p>I think the most interesting of these, without a doubt, is \"Code <strong>must not be</strong> reviewed by humans\". How could that <em>possibly</em> be a sensible strategy when we all know how prone LLMs are to making <a href=\"https://simonwillison.net/2025/Mar/2/kellan-elliott-mccrea/\">inhuman mistakes</a>?</p>\n<p>I've seen many developers recently acknowledge the <a href=\"https://simonwillison.net/2026/Jan/4/inflection/\">November 2025 inflection point</a>, where Claude Opus 4.5 and GPT 5.2 appeared to turn the corner on how reliably a coding agent could follow instructions and take on complex coding tasks. StrongDM's AI team was founded in July 2025 based on an earlier inflection point relating to Claude Sonnet 3.5:</p>\n<blockquote>\n<p>The catalyst was a transition observed in late 2024: with the second revision of Claude 3.5 (October 2024), long-horizon agentic coding workflows began to compound correctness rather than error.</p>\n<p>By December of 2024, the model's long-horizon coding performance was unmistakable via Cursor's <a href=\"https://forum.cursor.com/t/yolo-mode-is-amazing/36262\">YOLO mode</a>.</p>\n</blockquote>\n<p>Their new team started with the rule \"no hand-coded software\" - radical for July 2025, but something I'm seeing significant numbers of experienced developers start to adopt as of January 2026.</p>\n<p>They quickly ran into the obvious problem: if you're not writing anything by hand, how do you ensure that the code actually works? Having the agents write tests only helps if they don't cheat and <code>assert true</code>.</p>\n<p>This feels like the most consequential question in software development right now: how can you <a href=\"https://simonwillison.net/2025/Dec/18/code-proven-to-work/\">prove that software you are producing works</a> if both the implementation and the tests are being written for you by coding agents?</p>\n<p>StrongDM's answer was inspired by <a href=\"https://en.wikipedia.org/wiki/Scenario_testing\">Scenario testing</a> (Cem Kaner, 2003). As StrongDM describe it:</p>\n<blockquote>\n<p>We repurposed the word <strong>scenario</strong> to represent an end-to-end \"user story\", often stored outside the codebase (similar to a \"holdout\" set in model training), which could be intuitively understood and flexibly validated by an LLM.</p>\n<p>Because much of the software we grow itself has an agentic component, we transitioned from boolean definitions of success (\"the test suite is green\") to a probabilistic and empirical one. We use the term <strong>satisfaction</strong> to quantify this validation: of all the observed trajectories through all the scenarios, what fraction of them likely satisfy the user?</p>\n</blockquote>\n<p>That idea of treating scenarios as holdout sets - used to evaluate the software but not stored where the coding agents can see them - is <em>fascinating</em>. It imitates aggressive testing by an external QA team - an expensive but highly effective way of ensuring quality in traditional software.</p>\n<p>Which leads us to StrongDM's concept of a <strong>Digital Twin Universe</strong> - the part of the demo I saw that made the strongest impression on me.</p>\n<p>The software they were building helped manage user permissions across a suite of connected services. This in itself was notable - security software is the last thing you would expect to be built using unreviewed LLM code!</p>\n<blockquote>\n<p>[The Digital Twin Universe is] behavioral clones of the third-party services our software depends on. We built twins of Okta, Jira, Slack, Google Docs, Google Drive, and Google Sheets, replicating their APIs, edge cases, and observable behaviors.</p>\n<p>With the DTU, we can validate at volumes and rates far exceeding production limits. We can test failure modes that would be dangerous or impossible against live services. We can run thousands of scenarios per hour without hitting rate limits, triggering abuse detection, or accumulating API costs.</p>\n</blockquote>\n<p>How do you clone the important parts of Okta, Jira, Slack and more? With coding agents!</p>\n<p>As I understood it the trick was effectively to dump the full public API documentation of one of those services into their agent harness and have it build an imitation of that API, as a self-contained Go binary. They could then have it build a simplified UI over the top to help complete the simulation.</p>\n\n<p><strong>Update</strong>: DTU creator Jay Taylor posted some extra context about this <a href=\"https://news.ycombinator.com/item?id=46924426#46931812\">on Hacker News</a> sharing a key prompting strategy:</p>\n<blockquote>\n<p>I did have an initial key insight which led to a repeatable strategy to ensure a high level of fidelity between DTU vs. the official canonical SaaS services:</p>\n<p><code>Use the top popular publicly available reference SDK client libraries as compatibility targets, with the goal always being 100% compatibility.</code></p>\n</blockquote>\n\n<p>With their own, independent clones of those services - free from rate-limits or usage quotas - their army of simulated testers could go <em>wild</em>. Their scenario tests became scripts for agents to constantly execute against the new systems as they were being built.</p>\n<p>This screenshot of their Slack twin also helps illustrate how the testing process works, showing a stream of simulated Okta users who are about to need access to different simulated systems.</p>\n<p><img alt=\"Screenshot of a Slack-like interface titled &quot;DTU Slack&quot; showing a thread view (Thread — C4B9FBB97) with &quot;Focus first&quot; and &quot;Leave&quot; buttons. The left sidebar lists channels including # org-general (182), # general (0) (shared×2), # it-support (0), # channel-0002 (0) (shared×2), # channel-0003 (0) through # channel-0020 (0), # org-finance (1), and a DMs section with a &quot;Start&quot; button. A &quot;Create&quot; button appears at the top of the sidebar. The main thread shows approximately 9 automated introduction messages from users with Okta IDs (e.g. @okta-u-423438-00001, @okta-u-423438-00002, etc.), all timestamped 2025-11-12Z between 18:50:31 and 18:51:51. Each message follows the format &quot;Hi team! I'm [Name], joining as Employee in general. Key skills: [fictional skill phrases]. Excited to contribute!&quot; All users have red/orange &quot;O&quot; avatar icons.\" src=\"https://static.simonwillison.net/static/2026/strong-dm-slack.jpg\" /></p>\n<p>This ability to quickly spin up a useful clone of a subset of Slack helps demonstrate how disruptive this new generation of coding agent tools can be:</p>\n<blockquote>\n<p>Creating a high fidelity clone of a significant SaaS application was always possible, but never economically feasible. Generations of engineers may have <em>wanted</em> a full in-memory replica of their CRM to test against, but self-censored the proposal to build it.</p>\n</blockquote>\n<p>The <a href=\"https://factory.strongdm.ai/techniques\">techniques page</a> is worth a look too. In addition to the Digital Twin Universe they introduce terms like <strong><a href=\"https://factory.strongdm.ai/techniques/gene-transfusion\">Gene Transfusion</a></strong> for having agents extract patterns from existing systems and reuse them elsewhere, <strong><a href=\"https://factory.strongdm.ai/techniques/semport\">Semports</a></strong> for directly porting code from one language to another and <strong><a href=\"https://factory.strongdm.ai/techniques/pyramid-summaries\">Pyramid Summaries</a></strong> for providing multiple levels of summary such that an agent can enumerate the short ones quickly and zoom in on more detailed information as it is needed.</p>\n<p>StrongDM AI also released some software - in an appropriately unconventional manner.</p>\n<p><a href=\"https://github.com/strongdm/attractor\">github.com/strongdm/attractor</a> is <strong>Attractor</strong>, the non-interactive coding agent at the heart of their software factory. Except the repo itself contains no code at all - just three markdown files describing the spec for the software in meticulous detail, and a note in the README that you should feed those specs into your coding agent of choice!</p>\n<p><a href=\"https://github.com/strongdm/cxdb\">github.com/strongdm/cxdb</a> is a more traditional release, with 16,000 lines of Rust, 9,500 of Go and 6,700 of TypeScript. This is their \"AI Context Store\" - a system for storing conversation histories and tool outputs in an immutable DAG.</p>\n<p>It's similar to my LLM tool's <a href=\"https://llm.datasette.io/en/stable/logging.html#sql-schema\">SQLite logging mechanism</a> but a whole lot more sophisticated. I may have to gene transfuse some ideas out of this one!</p>\n<h4 id=\"a-glimpse-of-the-future-\">A glimpse of the future?</h4>\n<p>I visited the StrongDM AI team back in October as part of a small group of invited guests.</p>\n<p>The three person team of Justin McCarthy, Jay Taylor and Navan Chauhan had formed just three months earlier, and they already had working demos of their coding agent harness, their Digital Twin Universe clones of half a dozen services and a swarm of simulated test agents running through scenarios. And this was prior to the Opus 4.5/GPT 5.2 releases that made agentic coding significantly more reliable a month after those demos.</p>\n<p>It felt like a glimpse of one potential future of software development, where software engineers move from building the code to building and then semi-monitoring the systems that build the code. The Dark Factory.</p>\n\n<h4 id=\"wait-1-000-day-per-engineer-\">Wait, $1,000/day per engineer?</h4>\n<p>I glossed over this detail in my first published version of this post, but it deserves some serious attention.</p>\n<p>If these patterns really do add $20,000/month per engineer to your budget they're far less interesting to me. At that point this becomes more of a business model exercise: can you create a profitable enough line of products that you can afford the enormous overhead of developing software in this way?</p>\n<p>Building sustainable software businesses also looks very different when any competitor can potentially clone your newest features with a few hours of coding agent work.</p>\n<p>I hope these patterns can be put into play with a much lower spend. I've personally found the $200/month Claude Max plan gives me plenty of space to experiment with different agent patterns, but I'm also not running a swarm of QA testers 24/7!</p>\n<p>I think there's a lot to learn from StrongDM even for teams and individuals who aren't going to burn thousands of dollars on token costs. I'm particularly invested in the question of what it takes to have agents prove that their code works without needing to review every line of code they produce.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/parallel-agents\">parallel-agents</a></p>",
    "published": "2026-02-07T15:40:48+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "6209bce1330f1ffd",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting Tom Dale",
    "url": "https://simonwillison.net/2026/Feb/6/tom-dale/#atom-everything",
    "summary": "<blockquote cite=\"https://twitter.com/tomdale/status/2019828626972131441\"><p>I don't know why this week became the tipping point, but nearly every software engineer I've talked to is experiencing some degree of mental health crisis.</p>\n<p>[...] Many people assuming I meant job loss anxiety but that's just one presentation. I'm seeing near-manic episodes triggered by watching software shift from scarce to abundant. Compulsive behaviors around agent usage. Dissociative awe at the temporal compression of change. It's not fear necessarily just the cognitive overload from living in an inflection point.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/tomdale/status/2019828626972131441\">Tom Dale</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
    "published": "2026-02-06T23:41:31+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "51c533e894830874",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Running Pydantic's Monty Rust sandboxed Python subset in WebAssembly",
    "url": "https://simonwillison.net/2026/Feb/6/pydantic-monty/#atom-everything",
    "summary": "<p>There's a jargon-filled headline for you! Everyone's <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing\">building sandboxes</a> for running untrusted code right now, and Pydantic's latest attempt, <a href=\"https://github.com/pydantic/monty\">Monty</a>, provides a custom Python-like language (a subset of Python) in Rust and makes it available as both a Rust library and a Python package. I got it working in WebAssembly, providing a sandbox-in-a-sandbox.</p>\n<p>Here's <a href=\"https://github.com/pydantic/monty\">how they describe Monty</a>:</p>\n<blockquote>\n<p>Monty avoids the cost, latency, complexity and general faff of using full container based sandbox for running LLM generated code.</p>\n<p>Instead, it let's you safely run Python code written by an LLM embedded in your agent, with startup times measured in single digit microseconds not hundreds of milliseconds.</p>\n<p>What Monty <strong>can</strong> do:</p>\n<ul>\n<li>Run a reasonable subset of Python code - enough for your agent to express what it wants to do</li>\n<li>Completely block access to the host environment: filesystem, env variables and network access are all implemented via external function calls the developer can control</li>\n<li>Call functions on the host - only functions you give it access to [...]</li>\n</ul>\n</blockquote>\n<p>A quick way to try it out is via <a href=\"https://github.com/astral-sh/uv\">uv</a>:</p>\n<pre><code>uv run --with pydantic-monty python -m asyncio\n</code></pre>\n<p>Then paste this into the Python interactive prompt - the <code>-m asyncio</code> enables top-level await:</p>\n<pre><span>import</span> <span>pydantic_monty</span>\n<span>code</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>'print(\"hello \" + str(4 * 5))'</span>)\n<span>await</span> <span>pydantic_monty</span>.<span>run_monty_async</span>(<span>code</span>)</pre>\n<p>Monty supports a <em>very</em> small subset of Python - it doesn't even support class declarations yet!</p>\n<p>But, given its target use-case, that's not actually a problem.</p>\n<p>The neat thing about providing tools like this for LLMs is that they're really good at iterating against error messages. A coding agent can run some Python code, get an error message telling it that classes aren't supported and then try again with a different approach.</p>\n<p>I wanted to try this in a browser, so I fired up <a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/\">a code research task</a> in Claude Code for web and kicked it off with the following:</p>\n<blockquote>\n<p>Clone <a href=\"https://github.com/pydantic/monty\">https://github.com/pydantic/monty</a> to /tmp and figure out how to compile it into a python WebAssembly wheel that can then be loaded in Pyodide. The wheel file itself should be checked into the repo along with build scripts and passing pytest playwright test scripts that load Pyodide from a CDN and the wheel from a “python -m http.server” localhost and demonstrate it working</p>\n</blockquote>\n<p>Then a little later:</p>\n<blockquote>\n<p>I want an additional WASM file that works independently of Pyodide, which is also usable in a web browser - build that too along with playwright tests that show it working. Also build two HTML files - one called demo.html and one called pyodide-demo.html - these should work similar to <a href=\"https://tools.simonwillison.net/micropython\">https://tools.simonwillison.net/micropython</a> (download that code with curl to inspect it) - one should load the WASM build, the other should load Pyodide and have it use the WASM wheel. These will be served by GitHub Pages so they can load the WASM and wheel from a relative path since the .html files will be served from the same folder as the wheel and WASM file</p>\n</blockquote>\n<p>Here's <a href=\"https://gisthost.github.io/?22d88e6367d7e002c4fb383c213c2df2/page-001.html\">the transcript</a>, and the <a href=\"https://github.com/simonw/research/tree/main/monty-wasm-pyodide\">final research report</a> it produced.</p>\n<p>I now have the Monty Rust code compiled to WebAssembly in two different shapes - as a <code>.wasm</code> bundle you can load and call from JavaScript, and as a <code>monty-wasm-pyodide/pydantic_monty-0.0.3-cp313-cp313-emscripten_4_0_9_wasm32.whl</code> wheel file which can be loaded into <a href=\"https://pyodide.org/\">Pyodide</a> and then called from Python in Pyodide in WebAssembly in a browser.</p>\n<p>Here are those two demos, hosted on GitHub Pages:</p>\n<ul>\n<li>\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/demo.html\">Monty WASM demo</a> - a UI over JavaScript that loads the Rust WASM module directly.</li>\n<li>\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/pyodide-demo.html\">Monty Pyodide demo</a> - this one provides an identical interface but here the code is <a href=\"https://github.com/simonw/research/blob/3add1ffec70b530711fa237d91f546da5bcf1f1c/monty-wasm-pyodide/pyodide-demo.html#L257-L280\">loading Pyodide and then installing the Monty WASM wheel</a>.</li>\n</ul>\n<p><img alt=\"Screenshot of a web app titled &quot;Monty via Pyodide&quot; with description &quot;Run Monty (a sandboxed Python interpreter by Pydantic) inside Pyodide (CPython compiled to WebAssembly). This loads the pydantic-monty wheel and uses its full Python API. Code is saved in the URL for sharing.&quot; A green banner reads &quot;Code executed successfully!&quot; Below are example buttons labeled &quot;Basic&quot;, &quot;Inputs&quot;, &quot;Reuse&quot;, &quot;Error Handling&quot;, &quot;Fibonacci&quot;, and &quot;Classes&quot;. A code editor labeled &quot;Python Code (runs inside Monty sandbox via Pyodide):&quot; contains: &quot;import pydantic_monty\\n\\n# Create interpreter with input variables\\nm = pydantic_monty.Monty('x + y', inputs=['x', 'y'])\\n\\n# Run with different inputs\\nresult1 = m.run(inputs={&quot;x&quot;: 10, &quot;y&quot;: 20})\\nprint(f&quot;10 + 20 = {result1}&quot;)\\n\\nresult2 = m.run(inputs={&quot;x&quot;: 100, &quot;y&quot;: 200})&quot; with &quot;Run Code&quot; and &quot;Clear&quot; buttons. The Output section shows &quot;10 + 20 = 30&quot; and &quot;100 + 200 = 300&quot; with a &quot;Copy&quot; button. Footer reads &quot;Executed in 4.0ms&quot;.\" src=\"https://static.simonwillison.net/static/2026/monty-pyodide.jpg\" /></p>\n<p>As a connoisseur of sandboxes - the more options the better! - this new entry from Pydantic ticks a lot of my boxes. It's small, fast, widely available (thanks to Rust and WebAssembly) and provides strict limits on memory usage, CPU time and access to disk and network.</p>\n<p>It was also a great excuse to spin up another demo showing how easy it is these days to turn compiled code like C or Rust into WebAssembly that runs in both a browser and a Pyodide environment.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/rust\">rust</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/pyodide\">pyodide</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/pydantic\">pydantic</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
    "published": "2026-02-06T22:31:31+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d9922f9ee32a4d45",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "An Update on Heroku",
    "url": "https://simonwillison.net/2026/Feb/6/an-update-on-heroku/#atom-everything",
    "summary": "<p><strong><a href=\"https://www.heroku.com/blog/an-update-on-heroku/\">An Update on Heroku</a></strong></p>\nAn ominous headline to see on the official Heroku blog and yes, it's bad news.</p>\n<blockquote>\n<p>Today, Heroku is transitioning to a sustaining engineering model focused on stability, security, reliability, and support. Heroku remains an actively supported, production-ready platform, with an emphasis on maintaining quality and operational excellence rather than introducing new features. We know changes like this can raise questions, and we want to be clear about what this means for customers.</p>\n</blockquote>\n<p>Based on context I'm guessing a \"sustaining engineering model\" (this definitely isn't a widely used industry term) means that they'll keep the lights on and that's it.</p>\n<p>This is a very frustrating piece of corporate communication. \"We want to be clear about what this means for customers\" - then proceeds to <em>not be clear</em> about what this means for customers.</p>\n<p>Why are they doing this? Here's their explanation:</p>\n<blockquote>\n<p>We’re focusing our product and engineering investments on areas where we can deliver the greatest long-term customer value, including helping organizations build and deploy enterprise-grade AI in a secure and trusted way.</p>\n</blockquote>\n<p>My blog is the only project I have left running on Heroku. I guess I'd better migrate it away (probably to Fly) before Salesforce lose interest completely.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/salesforce\">salesforce</a>, <a href=\"https://simonwillison.net/tags/heroku\">heroku</a>, <a href=\"https://simonwillison.net/tags/fly\">fly</a></p>",
    "published": "2026-02-06T18:44:21+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d7c065e7fd03c9f2",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] Why OpenAI Should Build Slack",
    "url": "https://www.latent.space/p/ainews-why-openai-should-build-slack",
    "summary": "a quiet day lets us answer a Sam Altman question: what should he build next?",
    "published": "Sat, 14 Feb 2026 07:48:54 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8e37b924ee8e53d3",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] new Gemini 3 Deep Think, Anthropic $30B @ $380B, GPT-5.3-Codex Spark, MiniMax M2.5",
    "url": "https://www.latent.space/p/ainews-new-gemini-3-deep-think-anthropic",
    "summary": "There's too much going on!",
    "published": "Fri, 13 Feb 2026 08:29:19 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5b2af800ae5ab35f",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "Owning the AI Pareto Frontier — Jeff Dean",
    "url": "https://www.latent.space/p/jeffdean",
    "summary": "From rewriting Google&#8217;s search stack in the early 2000s to reviving sparse trillion-parameter models and co-designing TPUs with frontier ML research, Jeff Dean has quietly shaped nearly every layer of the modern AI stack.",
    "published": "Thu, 12 Feb 2026 22:02:35 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8a5b2209dad8ce1a",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] Z.ai GLM-5: New SOTA Open Weights LLM",
    "url": "https://www.latent.space/p/ainews-zai-glm-5-new-sota-open-weights",
    "summary": "We have Opus 4.5 at home",
    "published": "Thu, 12 Feb 2026 07:40:22 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c79c6992835d65e2",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "🔬Beyond AlphaFold: How Boltz is Open-Sourcing the Future of Drug Discovery",
    "url": "https://www.latent.space/p/boltz",
    "summary": "Inside Boltz, AlphaFold&#8217;s Legacy, and the Tools Powering Next-Gen Molecular Discovery",
    "published": "Thu, 12 Feb 2026 02:12:14 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a2f63fd602a7cde7",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] Qwen Image 2 and Seedance 2",
    "url": "https://www.latent.space/p/ainews-qwen-image-2-and-seedance",
    "summary": "Strong generative media showings from China",
    "published": "Wed, 11 Feb 2026 05:19:52 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cb9c3e14f4f42ca9",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "The Scientist and the Simulator",
    "url": "https://www.latent.space/p/scientist-simulator",
    "summary": "LLMs (alone) won&#8217;t cure cancer",
    "published": "Tue, 10 Feb 2026 15:27:58 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8e6caf77a88544e7",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] \"Sci-Fi with a touch of Madness\"",
    "url": "https://www.latent.space/p/ainews-sci-fi-with-a-touch-of-madness",
    "summary": "a quiet day lets us reflect on a pithy quote from the ClawFather.",
    "published": "Tue, 10 Feb 2026 04:33:06 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a9079ecb810f16fd",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "Experts Have World Models. LLMs Have Word Models.",
    "url": "https://www.latent.space/p/adversarial-reasoning",
    "summary": "Most expert work isn&#8217;t &#8220;produce a probable artifact&#8221;; it's \"choose a good move considering other agents, guessing hidden state\". LLMs default to single-shot artifacts and need World Models to progress",
    "published": "Sat, 07 Feb 2026 22:11:25 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9692df1badab5d6d",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] AI vs SaaS: The Unreasonable Effectiveness of Centralizing the AI Heartbeat",
    "url": "https://www.latent.space/p/ainews-ai-vs-saas-the-unreasonable",
    "summary": "A quiet day lets us reflect on a through line from OpenClaw to Frontier to MCP UI to Cursor/Anthropic Teams",
    "published": "Sat, 07 Feb 2026 04:11:08 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fb08ae7e11df1df0",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "The First Mechanistic Interpretability Frontier Lab — Myra Deng & Mark Bissell of Goodfire AI",
    "url": "https://www.latent.space/p/goodfire",
    "summary": "From Palantir and Two Sigma to building Goodfire into the poster-child for actionable mechanistic interpretability, Mark Bissell (Member of Technical Staff) and Myra Deng (Head of Product) are trying to turn &#8220;peeking inside the model&#8221; into a repeatable production workflow by shipping APIs, landing real enterprise deployments, and now scaling the bet with a recent",
    "published": "Fri, 06 Feb 2026 22:45:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "12909e2aa8d44049",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] OpenAI and Anthropic go to war: Claude Opus 4.6 vs GPT 5.3 Codex",
    "url": "https://www.latent.space/p/ainews-openai-and-anthropic-go-to",
    "summary": "The battle of the SOTA Coding Models steps up a notch",
    "published": "Fri, 06 Feb 2026 04:10:33 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f521e6e0ad998cab",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] ElevenLabs $500m Series D at $11B, Cerebras $1B Series H at $23B, Vibe Coding -> Agentic Engineering",
    "url": "https://www.latent.space/p/ainews-elevenlabs-500m-series-d-at",
    "summary": "SOTA Audio models, Fast Chips, and Koding Agents are all you need.",
    "published": "Thu, 05 Feb 2026 08:26:43 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "86fb97fcf6bbdbfa",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] Context Graphs and Agent Traces",
    "url": "https://www.latent.space/p/ainews-context-graphs-hype-or-actually",
    "summary": "a quiet day lets us feature a bubbling topic.",
    "published": "Wed, 04 Feb 2026 03:13:58 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e2376d028ed9bf90",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] OpenAI Codex App: death of the VSCode fork, multitasking worktrees, Skills Automations",
    "url": "https://www.latent.space/p/ainews-openai-codex-app-death-of",
    "summary": "The meta is moving fast.",
    "published": "Tue, 03 Feb 2026 07:35:33 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4788e2c11c6eda6e",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] Moltbook — the first Social Network for AI Agents (Clawdbots/OpenClaw bots)",
    "url": "https://www.latent.space/p/ainews-moltbook-the-first-social",
    "summary": "The craziest week in Simulative AI for a while",
    "published": "Sat, 31 Jan 2026 02:13:41 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b2c947fb498d45c0",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] SpaceXai Grok Imagine API - the #1 Video Model, Best Pricing and Latency",
    "url": "https://www.latent.space/p/ainews-spacexai-grok-imagine-api",
    "summary": "xAI cements its position as a frontier lab and prepares to merge with SpaceX",
    "published": "Fri, 30 Jan 2026 06:25:20 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8aa0cbdeeb83a33c",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "[AINews] Sam Altman's AI Combinator",
    "url": "https://www.latent.space/p/ainews-sam-altmans-ai-combinator",
    "summary": "a quiet day in the news lets us reflect on Sama's town hall message this week",
    "published": "Thu, 29 Jan 2026 03:58:09 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f92988ee9a796c0b",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "It's Time to Science",
    "url": "https://www.latent.space/p/science",
    "summary": "Why the time is right to start the world's first dedicated AI for Science podcast, and why AI Engineers should care",
    "published": "Wed, 28 Jan 2026 21:46:25 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3329d20bb365844f",
    "source": "latent_space",
    "source_weight": 1.2,
    "title": "🔬 Automating Science: World Models, Scientific Taste, Agent Loops — Andrew White",
    "url": "https://www.latent.space/p/edison",
    "summary": "Editor&#8217;s note: Welcome to our new AI for Science pod, with your new hosts RJ and Brandon!",
    "published": "Wed, 28 Jan 2026 15:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3267f7701fc834dc",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Claude Code vs ChatGPT Codex: Which AI coding agent is actually better? - Tom's Guide",
    "url": "https://news.google.com/rss/articles/CBMinwFBVV95cUxOX3k2SkVENHhlc1A3Wkp6OFZHaVRDbHhqN2k5cTZUaV8wamNGOGdfV0ZCMm0xTmlNTFdnRTRPT242LU5YcmhxVkNIeHczOWtQeUdfUVRxSGF2T0JUbi1RaG43a0VmMlJSVXpTQ1VLekxzTUN5UUY1UzE5NVBCVlZFUmJHWW5hdV9OVXFVdGNhLTE0RGhVUHBibGtPTTdHbG8?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMinwFBVV95cUxOX3k2SkVENHhlc1A3Wkp6OFZHaVRDbHhqN2k5cTZUaV8wamNGOGdfV0ZCMm0xTmlNTFdnRTRPT242LU5YcmhxVkNIeHczOWtQeUdfUVRxSGF2T0JUbi1RaG43a0VmMlJSVXpTQ1VLekxzTUN5UUY1UzE5NVBCVlZFUmJHWW5hdV9OVXFVdGNhLTE0RGhVUHBibGtPTTdHbG8?oc=5\" target=\"_blank\">Claude Code vs ChatGPT Codex: Which AI coding agent is actually better?</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Tom's Guide</font>",
    "published": "Sun, 15 Feb 2026 05:33:24 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "af39a0ebbc3718e8",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Xcode 26.3 unlocks the power of agentic coding - Apple",
    "url": "https://news.google.com/rss/articles/CBMilwFBVV95cUxNcjcxcmVrb2Z5Ykt2dlMtZVk0dlVtWHltOV82X0xYWTdodUcwUGxVT0hPc1RNamx5TVJUVmw4TnIxRUxxUzBrTGUxS1diSU9SemlhVWtaeGxreGNsWHozclFxODZvYk5ZdXZHV1BMb01KclZjelhzZG5sRFNXYUNxQXBVM2Y5T05obWM0UC1teHVlbmFuOHpv?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMilwFBVV95cUxNcjcxcmVrb2Z5Ykt2dlMtZVk0dlVtWHltOV82X0xYWTdodUcwUGxVT0hPc1RNamx5TVJUVmw4TnIxRUxxUzBrTGUxS1diSU9SemlhVWtaeGxreGNsWHozclFxODZvYk5ZdXZHV1BMb01KclZjelhzZG5sRFNXYUNxQXBVM2Y5T05obWM0UC1teHVlbmFuOHpv?oc=5\" target=\"_blank\">Xcode 26.3 unlocks the power of agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Apple</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "9ea0f90bfed549ed",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Xcode 26.3 Brings Integrated Agentic Coding for Anthropic Claude Agent and OpenAI Codex - infoq.com",
    "url": "https://news.google.com/rss/articles/CBMibkFVX3lxTFBCM3gwVFdyR25XRWthY0pCT2JVaThiQVoyZ0tlQk01UXFrU2NHV2JmLVdMUDN6c05rUTdmVmZRZnY5VXlfZjJ5WnJYQUhpYzMtZG9FUnJMcnZYVVptdm5sM1Bpb0c1dkh1SkxkVzlB?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMibkFVX3lxTFBCM3gwVFdyR25XRWthY0pCT2JVaThiQVoyZ0tlQk01UXFrU2NHV2JmLVdMUDN6c05rUTdmVmZRZnY5VXlfZjJ5WnJYQUhpYzMtZG9FUnJMcnZYVVptdm5sM1Bpb0c1dkh1SkxkVzlB?oc=5\" target=\"_blank\">Xcode 26.3 Brings Integrated Agentic Coding for Anthropic Claude Agent and OpenAI Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">infoq.com</font>",
    "published": "Mon, 09 Feb 2026 11:01:02 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ba420121d4a3672e",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Conductors to Orchestrators: The Future of Agentic Coding - O'Reilly Media",
    "url": "https://news.google.com/rss/articles/CBMikAFBVV95cUxOYVdlMUlyWnhPTmVpa1pFZzZoazk0VWRSVFdPZTR0R1JwVHNZNWlTTDZ5VDVxZlRIdm0xQ0EwdlllM3ZnMFJ1cUF5LU1jVFBHZUtfZTk3RmNPMzV6YXB4Ry1Ea2tFc0RKLXhHdEhoVVAxMlRJekdDSnh6cDJENWNuTzZXeDAtZnZfOHQ0RkkyeGg?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMikAFBVV95cUxOYVdlMUlyWnhPTmVpa1pFZzZoazk0VWRSVFdPZTR0R1JwVHNZNWlTTDZ5VDVxZlRIdm0xQ0EwdlllM3ZnMFJ1cUF5LU1jVFBHZUtfZTk3RmNPMzV6YXB4Ry1Ea2tFc0RKLXhHdEhoVVAxMlRJekdDSnh6cDJENWNuTzZXeDAtZnZfOHQ0RkkyeGg?oc=5\" target=\"_blank\">Conductors to Orchestrators: The Future of Agentic Coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">O'Reilly Media</font>",
    "published": "Fri, 13 Feb 2026 12:01:49 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a6ed45c85ec718cf",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple adds agents from Anthropic and OpenAI to its coding tool - CNBC",
    "url": "https://news.google.com/rss/articles/CBMingFBVV95cUxPcDJfMXNqaFZmbjZhUlFBVjFoa0tLSGJLd1o5bGR6Y3d3eFJaY2dYWVo4XzVzRDJMTVMwZWMxd3hxREhwQXZiSW5IZURVcDhhdzFES1l4X0I2N0tIRy1pa2dvYzFLUUVTaVkzSlBZMUZpUW5rQmlieThsTGRKOVBobWtFdV9FeGVRblFrNGVVYkVrODdOTzNUYW5WREN5d9IBowFBVV95cUxOQzR6d25LdmVxQkZoRUVaT2tNdWtfNmZVVVBPR3pKRkNVWFYxY092R0wxUlZaUlF5ME15Q0IxejdfUEEzS0RQX25LQ21YUkhGSWwwaWx3TUpUSlVOTDRlSmxmNXBUM3FKMXhtTlJySjJqVXFRV2F2T0NTR0p4bENVWGZIcXZac3lkU1JjR2hxWjZxY1VIaXdOWk5TT0diX0xuV0hJ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMingFBVV95cUxPcDJfMXNqaFZmbjZhUlFBVjFoa0tLSGJLd1o5bGR6Y3d3eFJaY2dYWVo4XzVzRDJMTVMwZWMxd3hxREhwQXZiSW5IZURVcDhhdzFES1l4X0I2N0tIRy1pa2dvYzFLUUVTaVkzSlBZMUZpUW5rQmlieThsTGRKOVBobWtFdV9FeGVRblFrNGVVYkVrODdOTzNUYW5WREN5d9IBowFBVV95cUxOQzR6d25LdmVxQkZoRUVaT2tNdWtfNmZVVVBPR3pKRkNVWFYxY092R0wxUlZaUlF5ME15Q0IxejdfUEEzS0RQX25LQ21YUkhGSWwwaWx3TUpUSlVOTDRlSmxmNXBUM3FKMXhtTlJySjJqVXFRV2F2T0NTR0p4bENVWGZIcXZac3lkU1JjR2hxWjZxY1VIaXdOWk5TT0diX0xuV0hJ?oc=5\" target=\"_blank\">Apple adds agents from Anthropic and OpenAI to its coding tool</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">CNBC</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "66fc69c3f3074d5c",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Introducing Claude Opus 4.6 - Anthropic",
    "url": "https://news.google.com/rss/articles/CBMiWkFVX3lxTE90b09UTmFMU3laaUE4WGQ0ejltUDEtZHdHMjlSY1pDTUhjX1A3VF9qRU56ejhuRkR3eFFvUFB4UV9ZTjMzaklkTkczSEJZWXRNOE1UX3hDd3dHdw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiWkFVX3lxTE90b09UTmFMU3laaUE4WGQ0ejltUDEtZHdHMjlSY1pDTUhjX1A3VF9qRU56ejhuRkR3eFFvUFB4UV9ZTjMzaklkTkczSEJZWXRNOE1UX3hDd3dHdw?oc=5\" target=\"_blank\">Introducing Claude Opus 4.6</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Anthropic</font>",
    "published": "Thu, 05 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "23fb82559268d4cb",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "OpenAI launches a Codex desktop app for macOS to run multiple AI coding agents in parallel - VentureBeat",
    "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxPekNKSDB0WXFyYVJwamVZT3IwUmVUdHlnSVN3M3R6SmRJZFJMYzg3ZUN5bUhaX3hSMmJnTm1Xdk1NeDVzYU1FQTQydzR6WlpnWWkyazMzTy1NaEp4M0lBeTEwRzBvT25FbnQ2UEJudjJIYzNxdENJUmVpQVFVcXVlMGZvSlZSRDRiTkh3ekE2YzRodHAyYV92QWJTWDhCTFZmV1pUTWJITll6NFVhamc?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxPekNKSDB0WXFyYVJwamVZT3IwUmVUdHlnSVN3M3R6SmRJZFJMYzg3ZUN5bUhaX3hSMmJnTm1Xdk1NeDVzYU1FQTQydzR6WlpnWWkyazMzTy1NaEp4M0lBeTEwRzBvT25FbnQ2UEJudjJIYzNxdENJUmVpQVFVcXVlMGZvSlZSRDRiTkh3ekE2YzRodHAyYV92QWJTWDhCTFZmV1pUTWJITll6NFVhamc?oc=5\" target=\"_blank\">OpenAI launches a Codex desktop app for macOS to run multiple AI coding agents in parallel</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
    "published": "Mon, 02 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0d9b64d5c3c74511",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Want local vibe coding? This AI stack might replace Claude Code and Codex - for free - ZDNET",
    "url": "https://news.google.com/rss/articles/CBMijAFBVV95cUxQMjEzc3Z5aUFlajFuVWRFZ0dFN19pVHpxLVc3NjR4bzgzME1Ec3JUemJLZGliSTZNRHFmQ1J3OHdGTlFCUW9TaXdoaDV2NDNVMzdhZWcyajJvZkhMMmY0OE1NVEY4ZEtxMTF0Mlp1RDF3dVJpUHB0WnJ2djdLaEx1V0pMbEM5bUl1UlVZTg?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMijAFBVV95cUxQMjEzc3Z5aUFlajFuVWRFZ0dFN19pVHpxLVc3NjR4bzgzME1Ec3JUemJLZGliSTZNRHFmQ1J3OHdGTlFCUW9TaXdoaDV2NDNVMzdhZWcyajJvZkhMMmY0OE1NVEY4ZEtxMTF0Mlp1RDF3dVJpUHB0WnJ2djdLaEx1V0pMbEM5bUl1UlVZTg?oc=5\" target=\"_blank\">Want local vibe coding? This AI stack might replace Claude Code and Codex - for free</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ZDNET</font>",
    "published": "Wed, 04 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "37a5066dd8f8b3d6",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple stock eyes breakout as this unseen engine unlocks agentic AI - MSN",
    "url": "https://news.google.com/rss/articles/CBMi2gFBVV95cUxNRTBWODJHLUxFZks1SEltQkdRNU4zUWNyNTNoal9TUk0yRFRWU0pkTXdLd0FGTlpjWFlaY3lTdzVOdV9FTDZEcWxvSEx3RGx5em91eGppV1pDY2pGWDB3WVVIU0oydVd1TFliWE9MSWUzbGVKbE1zRnVkejBEM2E3b2ljV3lpd3ZWQi1NYzBlU01Kcl9TME02cXNpYVpPeDNVczFFamJTNmYwOU1Zc0lIQnFYVFE1WVNJOU1UT2R1blpaeWx5M2NFc2RiQ2ZvQ2dtUTZPVWJzX3ZzUQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi2gFBVV95cUxNRTBWODJHLUxFZks1SEltQkdRNU4zUWNyNTNoal9TUk0yRFRWU0pkTXdLd0FGTlpjWFlaY3lTdzVOdV9FTDZEcWxvSEx3RGx5em91eGppV1pDY2pGWDB3WVVIU0oydVd1TFliWE9MSWUzbGVKbE1zRnVkejBEM2E3b2ljV3lpd3ZWQi1NYzBlU01Kcl9TME02cXNpYVpPeDNVczFFamJTNmYwOU1Zc0lIQnFYVFE1WVNJOU1UT2R1blpaeWx5M2NFc2RiQ2ZvQ2dtUTZPVWJzX3ZzUQ?oc=5\" target=\"_blank\">Apple stock eyes breakout as this unseen engine unlocks agentic AI</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">MSN</font>",
    "published": "Wed, 11 Feb 2026 21:04:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b2932c8a2df7b781",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple announces agentic coding in Xcode with Claude Agent and Codex integration - 9to5Mac",
    "url": "https://news.google.com/rss/articles/CBMisAFBVV95cUxQVEY4dlNGeTZUbElkUmxKVDdRRDlQOU1fWnpFd3pmS2J3c3phMU9rdjBPVktjTjNOYzgtaHpnc01CMklzeGlkcnFveWRrWmtHMW9CekMyMjBFdXRSamUwcjJSUmlFVmkxVnpGai15aURYbEhEMUY2T2UtSi1yVkJMSGdZQ1BFMnJ4bzVtLWdNT2hOMjJyWjVtYzlrZ3Vjc09uSGlDTWJYbU1qVWhMU2F3ZA?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMisAFBVV95cUxQVEY4dlNGeTZUbElkUmxKVDdRRDlQOU1fWnpFd3pmS2J3c3phMU9rdjBPVktjTjNOYzgtaHpnc01CMklzeGlkcnFveWRrWmtHMW9CekMyMjBFdXRSamUwcjJSUmlFVmkxVnpGai15aURYbEhEMUY2T2UtSi1yVkJMSGdZQ1BFMnJ4bzVtLWdNT2hOMjJyWjVtYzlrZ3Vjc09uSGlDTWJYbU1qVWhMU2F3ZA?oc=5\" target=\"_blank\">Apple announces agentic coding in Xcode with Claude Agent and Codex integration</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">9to5Mac</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "eb0b6d0ea7b3493c",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple’s Xcode adds OpenAI and Anthropic’s coding agents - The Verge",
    "url": "https://news.google.com/rss/articles/CBMiiwFBVV95cUxOc2sxTGVTaXpJRjRfNk9XLVR6N2VXcDhrcC1uSUJadmJMR0Y5R05IT3h3ZWhMdWtnd3hkcUxHQ1FmVTBRU2dFeVlUa2JxUVBKWXZuTFprRHl3aUhiNzMySWFBWW16R0ZtRE8zOUhLS05aQTc2dEFCYlVfa182YnhlcTZOcHpFcHZnSHkw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiiwFBVV95cUxOc2sxTGVTaXpJRjRfNk9XLVR6N2VXcDhrcC1uSUJadmJMR0Y5R05IT3h3ZWhMdWtnd3hkcUxHQ1FmVTBRU2dFeVlUa2JxUVBKWXZuTFprRHl3aUhiNzMySWFBWW16R0ZtRE8zOUhLS05aQTc2dEFCYlVfa182YnhlcTZOcHpFcHZnSHkw?oc=5\" target=\"_blank\">Apple’s Xcode adds OpenAI and Anthropic’s coding agents</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The Verge</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "715bf3e9f5bf6e5e",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple Xcode 26.3 adds coding agent support from OpenAI and Anthropic - Help Net Security",
    "url": "https://news.google.com/rss/articles/CBMigAFBVV95cUxNcTRuVlJkTkllVnBTTkpFNURNT1lfdDdSNmhrc0l3UEJKTW00eGZNRnJDQVZjeDc1dkRITTUzeXlqdTREQ2wzMFF4QnpHMDFtUE1HWG1CTk5ZOXpwRkZRTnRVNFBkeW5BR0w2eEgtc2JkeDFaYzhSTFpSNHlpUnVmUQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMigAFBVV95cUxNcTRuVlJkTkllVnBTTkpFNURNT1lfdDdSNmhrc0l3UEJKTW00eGZNRnJDQVZjeDc1dkRITTUzeXlqdTREQ2wzMFF4QnpHMDFtUE1HWG1CTk5ZOXpwRkZRTnRVNFBkeW5BR0w2eEgtc2JkeDFaYzhSTFpSNHlpUnVmUQ?oc=5\" target=\"_blank\">Apple Xcode 26.3 adds coding agent support from OpenAI and Anthropic</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Help Net Security</font>",
    "published": "Wed, 04 Feb 2026 09:15:35 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c0dda171108e9966",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Xcode moves into agentic coding with deeper OpenAI and Anthropic integrations - TechCrunch",
    "url": "https://news.google.com/rss/articles/CBMisgFBVV95cUxPc09Ka2NTWlBMZnJ4M2M2ODhSVnlaWC1KSXhnRFJDQml6UjBWUUlYQWtGcnpUTVNrUGZpS3VPQXFVNmlQMUwzaEVuWldOQlZYS2EzenRmeTVzMFhTMGU2bnlxQjN5cE02X1R1LUJhV2pTN3NxNWo4NWFLQkxEV3ppR1hTbkRFeHpDdUY5NGdNOWtvU3lPUkpSdHZlN2szSGVZZlFoek13N2R6SzhzTE16bjhn?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMisgFBVV95cUxPc09Ka2NTWlBMZnJ4M2M2ODhSVnlaWC1KSXhnRFJDQml6UjBWUUlYQWtGcnpUTVNrUGZpS3VPQXFVNmlQMUwzaEVuWldOQlZYS2EzenRmeTVzMFhTMGU2bnlxQjN5cE02X1R1LUJhV2pTN3NxNWo4NWFLQkxEV3ppR1hTbkRFeHpDdUY5NGdNOWtvU3lPUkpSdHZlN2szSGVZZlFoek13N2R6SzhzTE16bjhn?oc=5\" target=\"_blank\">Xcode moves into agentic coding with deeper OpenAI and Anthropic integrations</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">TechCrunch</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bc73a9cb798da45a",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "OpenAI spills technical details about how its AI coding agent works - Ars Technica",
    "url": "https://news.google.com/rss/articles/CBMipgFBVV95cUxPb1pFNTdBQmNkN1dGUUZkMXBpdkZVQWNXeVZRNU05aDNHT05KaXNPcGs4TXpZN1JrNGJRTFA3cjhVLVRGY2RCb1oyOVBIY2lBeE9GLVI0UlBtXzE1UFdlVzFESVU3SVhnQnhubjhBeVBVQUg3VTd6eV9PUkFDZWk5RzlHY1hYQTBvUW5JeHNEN0xDUEx5SnAwRkdaOUdjWjdFS3kwdmpB?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMipgFBVV95cUxPb1pFNTdBQmNkN1dGUUZkMXBpdkZVQWNXeVZRNU05aDNHT05KaXNPcGs4TXpZN1JrNGJRTFA3cjhVLVRGY2RCb1oyOVBIY2lBeE9GLVI0UlBtXzE1UFdlVzFESVU3SVhnQnhubjhBeVBVQUg3VTd6eV9PUkFDZWk5RzlHY1hYQTBvUW5JeHNEN0xDUEx5SnAwRkdaOUdjWjdFS3kwdmpB?oc=5\" target=\"_blank\">OpenAI spills technical details about how its AI coding agent works</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Ars Technica</font>",
    "published": "Mon, 26 Jan 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d3ee385b0647261d",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "I built an iOS app in just two days with just my voice - and it was electrifying - ZDNET",
    "url": "https://news.google.com/rss/articles/CBMidEFVX3lxTE9Mel83bU5vVHZEZWp0S2NmaFA2ZWxwUTdWUDY5Zy00ZksxQU5YOGt6blpqcWc5RHdpby1NNmtZRno1TUNnYmF4TUxDcTdkeDBhOEtDMGI5bGFnMjVvd3NmV0hzT1JJSlR0M1NoMi1jN1pjb1VQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMidEFVX3lxTE9Mel83bU5vVHZEZWp0S2NmaFA2ZWxwUTdWUDY5Zy00ZksxQU5YOGt6blpqcWc5RHdpby1NNmtZRno1TUNnYmF4TUxDcTdkeDBhOEtDMGI5bGFnMjVvd3NmV0hzT1JJSlR0M1NoMi1jN1pjb1VQ?oc=5\" target=\"_blank\">I built an iOS app in just two days with just my voice - and it was electrifying</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ZDNET</font>",
    "published": "Mon, 09 Feb 2026 01:43:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "86a4784eb0cd2154",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "OpenAI's new Codex app hits 1M+ downloads in first week — but limits may be coming to free and Go users - VentureBeat",
    "url": "https://news.google.com/rss/articles/CBMiqgFBVV95cUxOeWxpZTNWSUp1dlNZZUtKbmsxbG4tNEJ1SHVTRmh0R2dtN0dBM2FDUDdNN2ZoY1hBcUJ6VGd2d1NVT2lpSTJfYXdIeURjSWhlZDFkM0xDbllYRmo3N3Q3UzFJQ254anI4aVRTM3daV2tKTU1OMUFWYk5DSkpPMzVzUHRtTVVmWVdLWTF0cTQ4U0dCa3d5X1VDR2ZyV0hqLTN1UUU2dWp2WVQwdw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiqgFBVV95cUxOeWxpZTNWSUp1dlNZZUtKbmsxbG4tNEJ1SHVTRmh0R2dtN0dBM2FDUDdNN2ZoY1hBcUJ6VGd2d1NVT2lpSTJfYXdIeURjSWhlZDFkM0xDbllYRmo3N3Q3UzFJQ254anI4aVRTM3daV2tKTU1OMUFWYk5DSkpPMzVzUHRtTVVmWVdLWTF0cTQ4U0dCa3d5X1VDR2ZyV0hqLTN1UUU2dWp2WVQwdw?oc=5\" target=\"_blank\">OpenAI's new Codex app hits 1M+ downloads in first week — but limits may be coming to free and Go users</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
    "published": "Mon, 09 Feb 2026 23:46:32 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e05b2906fc12f840",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "OpenAI launches new agentic coding model only minutes after Anthropic drops its own - TechCrunch",
    "url": "https://news.google.com/rss/articles/CBMiugFBVV95cUxPVlB0ZWFTR0h6Ql91OVJ6bjJTT05iZUk0akI5SzRTYUdVeHNYQjQ4cVpKdUtBdVpnNk5jNTF4YlROQnFSOVlXU0I0UHRFWjQ5ZjZmUG5vV3ZzYkVYREpCR0Vjbk5UMHlwU2hJY1FrX1JQVGoxZXlaSXBYWFJSaUxIQ0licHhaZzEtVTVucy1MQnJva1JySUgyN1RFVWpFNS1lWkdCVU4tbzQ3YUkwUlAza2pFMWJaVkp4bGc?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiugFBVV95cUxPVlB0ZWFTR0h6Ql91OVJ6bjJTT05iZUk0akI5SzRTYUdVeHNYQjQ4cVpKdUtBdVpnNk5jNTF4YlROQnFSOVlXU0I0UHRFWjQ5ZjZmUG5vV3ZzYkVYREpCR0Vjbk5UMHlwU2hJY1FrX1JQVGoxZXlaSXBYWFJSaUxIQ0licHhaZzEtVTVucy1MQnJva1JySUgyN1RFVWpFNS1lWkdCVU4tbzQ3YUkwUlAza2pFMWJaVkp4bGc?oc=5\" target=\"_blank\">OpenAI launches new agentic coding model only minutes after Anthropic drops its own</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">TechCrunch</font>",
    "published": "Thu, 05 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "e73b3c4c9fbb1706",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "MiniMax's new open M2.5 and M2.5 Lightning near state-of-the-art while costing 1/20th of Claude Opus 4.6 - VentureBeat",
    "url": "https://news.google.com/rss/articles/CBMipwFBVV95cUxOZW9hYWlJVHFieWxBb1ZJbGRJWk44S3h0dWV5UkhxWDROVm1IT3A4dnQ0eGFpdEFEbzNlYzZIMUJyWUdXMHRFMDdhRGd0dk1wSlRocTVVOF9sSDhNU1dYbVM1ZXF2Wm1HREhkV1dZZF9KZ2pNRmVfTG5JOVBvY3I4ODJwUG41X2lOUVczaXVOb044M2pTb2ljM1BHM1FOMWhLWFVsRmRsSQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMipwFBVV95cUxOZW9hYWlJVHFieWxBb1ZJbGRJWk44S3h0dWV5UkhxWDROVm1IT3A4dnQ0eGFpdEFEbzNlYzZIMUJyWUdXMHRFMDdhRGd0dk1wSlRocTVVOF9sSDhNU1dYbVM1ZXF2Wm1HREhkV1dZZF9KZ2pNRmVfTG5JOVBvY3I4ODJwUG41X2lOUVczaXVOb044M2pTb2ljM1BHM1FOMWhLWFVsRmRsSQ?oc=5\" target=\"_blank\">MiniMax's new open M2.5 and M2.5 Lightning near state-of-the-art while costing 1/20th of Claude Opus 4.6</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
    "published": "Fri, 13 Feb 2026 07:51:47 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "236f21236bd310b8",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "OpenAI’s GPT-5.3-Codex drops as Anthropic upgrades Claude — AI coding wars heat up ahead of Super Bowl ads - VentureBeat",
    "url": "https://news.google.com/rss/articles/CBMisAFBVV95cUxPX3J2Q0o0c2wybXpmQUVPOVhncWRua1EtT3JmVkFFN0l4bE9RVnJvOVJDejIyVWI4MV91UjhseXYwTkN5WjJfdThobFFZRU1YandSTHRBUTJHbFBNcGc4cW1URjhwX1R4Qi0xLTVWVm9pQmlUM1Y0UTcwU1FGcGJ3Z2k3TEhOcUxueFQ2SFdYVUNCNm5aeWJGMDNFaVhsOWN4SEtvOVJwWTBLQTNLZTZoMg?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMisAFBVV95cUxPX3J2Q0o0c2wybXpmQUVPOVhncWRua1EtT3JmVkFFN0l4bE9RVnJvOVJDejIyVWI4MV91UjhseXYwTkN5WjJfdThobFFZRU1YandSTHRBUTJHbFBNcGc4cW1URjhwX1R4Qi0xLTVWVm9pQmlUM1Y0UTcwU1FGcGJ3Z2k3TEhOcUxueFQ2SFdYVUNCNm5aeWJGMDNFaVhsOWN4SEtvOVJwWTBLQTNLZTZoMg?oc=5\" target=\"_blank\">OpenAI’s GPT-5.3-Codex drops as Anthropic upgrades Claude — AI coding wars heat up ahead of Super Bowl ads</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
    "published": "Thu, 05 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d2ad7905fbc5878e",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple Xcode unleashes AI agents for fast and easy app creation - Cult of Mac",
    "url": "https://news.google.com/rss/articles/CBMiakFVX3lxTE1OT3lFRzRWeDVoNzJnNjJiWWtRU192TkF5c3FqZ3JmUVlKOTFWMTEzMlVvZHFfbWNvZEI3S080cTZTNFBWZWduSWxlSmxZdmtMdnV6c3NlN081VU9Ob19oZnR1dlJMOVpiUVE?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiakFVX3lxTE1OT3lFRzRWeDVoNzJnNjJiWWtRU192TkF5c3FqZ3JmUVlKOTFWMTEzMlVvZHFfbWNvZEI3S080cTZTNFBWZWduSWxlSmxZdmtMdnV6c3NlN081VU9Ob19oZnR1dlJMOVpiUVE?oc=5\" target=\"_blank\">Apple Xcode unleashes AI agents for fast and easy app creation</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Cult of Mac</font>",
    "published": "Wed, 04 Feb 2026 16:07:30 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "99c7c40767c81aea",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "OpenAI's Codex MacOS App Brings Agentic Coding to Developers - The Tech Buzz",
    "url": "https://news.google.com/rss/articles/CBMimAFBVV95cUxOb3hCZkhNVkJmQmJBbWc1ZjBjUHY3alBXa3dSZm5EaUt1bGU0MTZTUU10VnhsZmZpbFF3bjRvNkJXazdiVTE4dEpINzAxMWpMVmlDTkl6a1RaTDNOOEJPWTI0TVVFRzRqZ245a1ZaS0NZRTlXcUVTQnVVN3RUbFhRV1F4MW9uM296SGRBbDVQX3d4Q3J6U194ZA?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMimAFBVV95cUxOb3hCZkhNVkJmQmJBbWc1ZjBjUHY3alBXa3dSZm5EaUt1bGU0MTZTUU10VnhsZmZpbFF3bjRvNkJXazdiVTE4dEpINzAxMWpMVmlDTkl6a1RaTDNOOEJPWTI0TVVFRzRqZ245a1ZaS0NZRTlXcUVTQnVVN3RUbFhRV1F4MW9uM296SGRBbDVQX3d4Q3J6U194ZA?oc=5\" target=\"_blank\">OpenAI's Codex MacOS App Brings Agentic Coding to Developers</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The Tech Buzz</font>",
    "published": "Mon, 02 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "4b4b1fa4025afbc0",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Augment Code makes its semantic coding capability available for any AI agent - SiliconANGLE",
    "url": "https://news.google.com/rss/articles/CBMiowFBVV95cUxNS2FPU0lOTGs0YWMxUzY4NnN5bzRUanktakt4emNGZGx2ZHI0a0lRbG0xSmlDME5CY0VmcHpMUE9LYllycy0yNDlnOFJ3cGJyOEVIejF5Qm1VbzVoNFRPeDlTNXl3cFpxR3dYcUNYWkhfeWp6ODhHR1phY1VORmpINjlua3g1NzhCb1VtSkJ1ZzRTOEczQkJZemNfVDNTUG5HbXlj?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiowFBVV95cUxNS2FPU0lOTGs0YWMxUzY4NnN5bzRUanktakt4emNGZGx2ZHI0a0lRbG0xSmlDME5CY0VmcHpMUE9LYllycy0yNDlnOFJ3cGJyOEVIejF5Qm1VbzVoNFRPeDlTNXl3cFpxR3dYcUNYWkhfeWp6ODhHR1phY1VORmpINjlua3g1NzhCb1VtSkJ1ZzRTOEczQkJZemNfVDNTUG5HbXlj?oc=5\" target=\"_blank\">Augment Code makes its semantic coding capability available for any AI agent</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">SiliconANGLE</font>",
    "published": "Fri, 06 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ae41b526d9810056",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Anthropic’s Claude Cowork finally lands on Windows — and it wants to automate your workday - VentureBeat",
    "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxQY29CclhqMUVDdlBoOEMwLUZFdnFsQkFRTElZOC1nUm1IdWI2T0JLamVwTzluZmYyaVlTRHJqeWNlandnRUNRckpKSHBkM0h0dE1vN0hfYWdHTHZqVTIzZXI4V3h4dEk0NG1GNEVXYmRyeVVlMkt3S1pjRVJJcTJseVV6MU1fTWY0cmEtOWkzelRKZTN0b2pQRXhXTGFtUHdVUVR5YVVnVkVJSllDLUE?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxQY29CclhqMUVDdlBoOEMwLUZFdnFsQkFRTElZOC1nUm1IdWI2T0JLamVwTzluZmYyaVlTRHJqeWNlandnRUNRckpKSHBkM0h0dE1vN0hfYWdHTHZqVTIzZXI4V3h4dEk0NG1GNEVXYmRyeVVlMkt3S1pjRVJJcTJseVV6MU1fTWY0cmEtOWkzelRKZTN0b2pQRXhXTGFtUHdVUVR5YVVnVkVJSllDLUE?oc=5\" target=\"_blank\">Anthropic’s Claude Cowork finally lands on Windows — and it wants to automate your workday</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
    "published": "Wed, 11 Feb 2026 20:50:17 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "cbe9183ff645d75a",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple lets AI agents write Swift code directly in Xcode - PPC Land",
    "url": "https://news.google.com/rss/articles/CBMifkFVX3lxTE5weEVpWWhuR2JLeUJPS0k1N3B4cnhabjE5Zi1ZV2MyZ0tzUGRwS1V5WkNwbTNWSHdUNkFkYXF4OVhxaDZaRXRMTk9wOGw1bHRhY29DOXo5NEVmSXloekhIMlhrZjJLRWt4d0V4dG54M1U4T2tfbDA0YTFKbzRaZw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMifkFVX3lxTE5weEVpWWhuR2JLeUJPS0k1N3B4cnhabjE5Zi1ZV2MyZ0tzUGRwS1V5WkNwbTNWSHdUNkFkYXF4OVhxaDZaRXRMTk9wOGw1bHRhY29DOXo5NEVmSXloekhIMlhrZjJLRWt4d0V4dG54M1U4T2tfbDA0YTFKbzRaZw?oc=5\" target=\"_blank\">Apple lets AI agents write Swift code directly in Xcode</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">PPC Land</font>",
    "published": "Sun, 08 Feb 2026 18:31:05 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "bf5c8325f1ff1c32",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Xcode 26.3 hands on: AI agent coding is astoundingly fast, smart, and too convenient - AppleInsider",
    "url": "https://news.google.com/rss/articles/CBMiwAFBVV95cUxPWmJZUUVDUzNzRXlWRG9sTVhLczF0Vm9tYTRkZ3hXSm5FRVkyT3BHR1R5V3FqdFZ1cnFkTU1wWC1FWDVSN3lVaEIwR3h5QWt2Yng5akphUFFnbmZRbGl3bHNSbVROMVhoVFZhR0VKZzRxR2RTQ2VEWWZtVC10LUhxTXk4V2ZENmZVaW5OU0RpR0Etc1JqU0p4UzU2b3pwX2p4WHdQNGxLcnhDV21hekhrOXFEaURqcVB6MTI3SGF5WkY?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiwAFBVV95cUxPWmJZUUVDUzNzRXlWRG9sTVhLczF0Vm9tYTRkZ3hXSm5FRVkyT3BHR1R5V3FqdFZ1cnFkTU1wWC1FWDVSN3lVaEIwR3h5QWt2Yng5akphUFFnbmZRbGl3bHNSbVROMVhoVFZhR0VKZzRxR2RTQ2VEWWZtVC10LUhxTXk4V2ZENmZVaW5OU0RpR0Etc1JqU0p4UzU2b3pwX2p4WHdQNGxLcnhDV21hekhrOXFEaURqcVB6MTI3SGF5WkY?oc=5\" target=\"_blank\">Xcode 26.3 hands on: AI agent coding is astoundingly fast, smart, and too convenient</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">AppleInsider</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a4df9db21adf832a",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "I tried a Claude Code rival that's local, open source, and completely free - how it went - ZDNET",
    "url": "https://news.google.com/rss/articles/CBMiiwFBVV95cUxPa0VEQ1lXZlkzUHkyUG52LW9heG50UmRNRnpqWUdaZGVaMm55SlZSeTM3a3hCWkhicGJ6NjRUWUdJNlFyLWwwWGhTc21HUFFQSkNpc0g2VmxoWXZlTU9ZRGNhY0REYnM0UUl6RTY0RUV6UFhPVFdiS2todGVNMnBaVmE3SWhfdktwRV9r?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiiwFBVV95cUxPa0VEQ1lXZlkzUHkyUG52LW9heG50UmRNRnpqWUdaZGVaMm55SlZSeTM3a3hCWkhicGJ6NjRUWUdJNlFyLWwwWGhTc21HUFFQSkNpc0g2VmxoWXZlTU9ZRGNhY0REYnM0UUl6RTY0RUV6UFhPVFdiS2todGVNMnBaVmE3SWhfdktwRV9r?oc=5\" target=\"_blank\">I tried a Claude Code rival that's local, open source, and completely free - how it went</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ZDNET</font>",
    "published": "Mon, 09 Feb 2026 02:16:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "65bc350ab31fa258",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Xcode 26.3 adds built-in support for agentic coding - AppleInsider",
    "url": "https://news.google.com/rss/articles/CBMipwFBVV95cUxQX2lnTi0yUHBaWWhJT0xIS2RTNE9jWVczdFlicHRXVlVHZHh3TmJOV3ZIWm9ER2xXbGJoSnN0NHVIS2tMQ085MThoRWZ5WldwLUVBaHk0RnkwTjNuT05TWFNGQjFxLUVFRnRaWXM5Tlp1MU81a2hXWmRUSk5Sbll3bU1Uc0swMTNjNGpERWVLa1JYYlVaRW5wbFBoTVZkaUo5WmRZbGdhVQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMipwFBVV95cUxQX2lnTi0yUHBaWWhJT0xIS2RTNE9jWVczdFlicHRXVlVHZHh3TmJOV3ZIWm9ER2xXbGJoSnN0NHVIS2tMQ085MThoRWZ5WldwLUVBaHk0RnkwTjNuT05TWFNGQjFxLUVFRnRaWXM5Tlp1MU81a2hXWmRUSk5Sbll3bU1Uc0swMTNjNGpERWVLa1JYYlVaRW5wbFBoTVZkaUo5WmRZbGdhVQ?oc=5\" target=\"_blank\">Xcode 26.3 adds built-in support for agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">AppleInsider</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0e19d40526246e41",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "OpenAI launches new macOS app for agentic coding - TechCrunch",
    "url": "https://news.google.com/rss/articles/CBMiiwFBVV95cUxOTm1SUXVmSkhCcXB3VFQxcjIweHJ4M0ZObnJqSEhVOW9zbnJtdDV5cU1UUzZRM2J1Q2RMWmJYSklDSjZFWW1JT1RMeFF0YlhGSjFaX0FncjU1RDZXZkJfa0FwTjFoS2FmU2FHbk0wN0h1M2FWSlZpaFFMMmtzb2ZEYTJEbHc4T3Y1N01J?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiiwFBVV95cUxOTm1SUXVmSkhCcXB3VFQxcjIweHJ4M0ZObnJqSEhVOW9zbnJtdDV5cU1UUzZRM2J1Q2RMWmJYSklDSjZFWW1JT1RMeFF0YlhGSjFaX0FncjU1RDZXZkJfa0FwTjFoS2FmU2FHbk0wN0h1M2FWSlZpaFFMMmtzb2ZEYTJEbHc4T3Y1N01J?oc=5\" target=\"_blank\">OpenAI launches new macOS app for agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">TechCrunch</font>",
    "published": "Mon, 02 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b56cab1ffe00c330",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Claude Code vs Codex: Developers are Choosing Sides - Analytics India Magazine",
    "url": "https://news.google.com/rss/articles/CBMilwFBVV95cUxOMF9Zdm40YVJFZFhnY2pCVmxVeFRHOVBEU192bENYN0xuTGR4NXpBRldkZk9YakxQS2ktVGQxMWJvT09SRHRSbm5tNnptOUhQUk9ScUE0OTk5V09hcTJRR0dKb1BYWEhsRTB0bk9VQmwwTDh5Y2laLWZ5ZFBOcFRsd0dIS1Q0bGRoMlc2ZHZhb0dpM3YxaThV?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMilwFBVV95cUxOMF9Zdm40YVJFZFhnY2pCVmxVeFRHOVBEU192bENYN0xuTGR4NXpBRldkZk9YakxQS2ktVGQxMWJvT09SRHRSbm5tNnptOUhQUk9ScUE0OTk5V09hcTJRR0dKb1BYWEhsRTB0bk9VQmwwTDh5Y2laLWZ5ZFBOcFRsd0dIS1Q0bGRoMlc2ZHZhb0dpM3YxaThV?oc=5\" target=\"_blank\">Claude Code vs Codex: Developers are Choosing Sides</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Analytics India Magazine</font>",
    "published": "Wed, 11 Feb 2026 11:50:27 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ccb6d9b32743e095",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Anthropic's Claude Opus 4.6 brings 1M token context and 'agent teams' to take on OpenAI's Codex - VentureBeat",
    "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxNQnNEQmdUWWJnRV9vaDFWSk5lOHhEeFYtWGdIaGFjendaZTc4ZjRhSllxY1VtOFRaUEJnQ1R5MDlUd1VXRGZkNlhZUEpvemlmaUVEWGFpcGwwc0pjaTgtNmE4dklOOTVBXzR5WHVZV0tLa0hWdGdVWE0zZ3pEMlJpUjBsY1R0eUZXTGlTbnNQOTNoS1d6NENKN3pNU1FwWUlSOFJ0ejVZZ0RIQmloMGc?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxNQnNEQmdUWWJnRV9vaDFWSk5lOHhEeFYtWGdIaGFjendaZTc4ZjRhSllxY1VtOFRaUEJnQ1R5MDlUd1VXRGZkNlhZUEpvemlmaUVEWGFpcGwwc0pjaTgtNmE4dklOOTVBXzR5WHVZV0tLa0hWdGdVWE0zZ3pEMlJpUjBsY1R0eUZXTGlTbnNQOTNoS1d6NENKN3pNU1FwWUlSOFJ0ejVZZ0RIQmloMGc?oc=5\" target=\"_blank\">Anthropic's Claude Opus 4.6 brings 1M token context and 'agent teams' to take on OpenAI's Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
    "published": "Thu, 05 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a340ac3292cb88dd",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple's Big Gift to iOS Developers: Agentic Coding Arrives in Xcode 26.3 - QUASA Connect",
    "url": "https://news.google.com/rss/articles/CBMimgFBVV95cUxNYlMxUzRWRHhSeG5veVdpWFEwREZzWm1YLWdIaE5sTWUxcDlWX19nTWV6TWYtcEttalVrWFAxbzZoc2tlSVdTd3hnc3U0SG92Z0w5SXJacHZJbVJpanVqR0dVR2FiRnNRSjFVN1BBT3lVclFRQk1QWGkzb2VtMVR1dmRDSnJ4dktLeDdqWmsxczB4a3ZLRTY5VE9n?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMimgFBVV95cUxNYlMxUzRWRHhSeG5veVdpWFEwREZzWm1YLWdIaE5sTWUxcDlWX19nTWV6TWYtcEttalVrWFAxbzZoc2tlSVdTd3hnc3U0SG92Z0w5SXJacHZJbVJpanVqR0dVR2FiRnNRSjFVN1BBT3lVclFRQk1QWGkzb2VtMVR1dmRDSnJ4dktLeDdqWmsxczB4a3ZLRTY5VE9n?oc=5\" target=\"_blank\">Apple's Big Gift to iOS Developers: Agentic Coding Arrives in Xcode 26.3</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">QUASA Connect</font>",
    "published": "Wed, 11 Feb 2026 11:05:05 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7ca011dc7476f464",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple’s Xcode 26.3 Release Candidate Adds Agentic Coding Tools for Developers - MacStories",
    "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxNUkZXa0p3Y0VWTnJ1dHBOR080Qll3NFIxWHhiRVZTZkV1a0NqUEFQY2E5NjZyOFA4am1uQWk4T21MMERHaUlRdTlockR5QkRsZVMyZkNGRkhweUNkc1F1MVhUd2RsOE1BQ2l5VC1VSEtmQ3l6VE9yRk1ObjdvalJqWmJGZUFrU3JkQXA1dmEybmdzeVJIbnNheUNkTTFmS0tuRFhoS2pXR05EbHBjVUE?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxNUkZXa0p3Y0VWTnJ1dHBOR080Qll3NFIxWHhiRVZTZkV1a0NqUEFQY2E5NjZyOFA4am1uQWk4T21MMERHaUlRdTlockR5QkRsZVMyZkNGRkhweUNkc1F1MVhUd2RsOE1BQ2l5VC1VSEtmQ3l6VE9yRk1ObjdvalJqWmJGZUFrU3JkQXA1dmEybmdzeVJIbnNheUNkTTFmS0tuRFhoS2pXR05EbHBjVUE?oc=5\" target=\"_blank\">Apple’s Xcode 26.3 Release Candidate Adds Agentic Coding Tools for Developers</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">MacStories</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8bd01525ea6fcf60",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple’s Xcode 26.3 brings integrated support for agentic coding - InfoWorld",
    "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxOWXhMVVVCZHBuYUlMOGNzRWxnZFNHeG9CcEY0QlpZZUNHZ0hfU3I2VjVUbDNiZDBlUEoxMm9naVlRS3ZPelNkUW9RVzBKQ3NDeUJtZEJoOTlzczJESmUtM2wtazVGTGJFZ3BDeDktY19qOEVFajltRER2RzNaalRhaF9ZdVp0NzNUWng3ajZrUXpGd3c3MTZNWjF1bVhDbktBMnZXNzdKZUtFTU54amc?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxOWXhMVVVCZHBuYUlMOGNzRWxnZFNHeG9CcEY0QlpZZUNHZ0hfU3I2VjVUbDNiZDBlUEoxMm9naVlRS3ZPelNkUW9RVzBKQ3NDeUJtZEJoOTlzczJESmUtM2wtazVGTGJFZ3BDeDktY19qOEVFajltRER2RzNaalRhaF9ZdVp0NzNUWng3ajZrUXpGd3c3MTZNWjF1bVhDbktBMnZXNzdKZUtFTU54amc?oc=5\" target=\"_blank\">Apple’s Xcode 26.3 brings integrated support for agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">InfoWorld</font>",
    "published": "Wed, 04 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "97d8f0df880ed3b9",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Introducing GPT-5.3-Codex - OpenAI",
    "url": "https://news.google.com/rss/articles/CBMiYEFVX3lxTE1WSFdyRV9lZDd3TTJWVlhKY1pWMTRaSHJaa0hLU05kTE1QcGVjRVdndHhuNnFhQW5ObmtkcFhpRm1wd21yU1o2dDRYUWE2aDJzX1p4ZERNVlZjSHlaMTl6Qw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiYEFVX3lxTE1WSFdyRV9lZDd3TTJWVlhKY1pWMTRaSHJaa0hLU05kTE1QcGVjRVdndHhuNnFhQW5ObmtkcFhpRm1wd21yU1o2dDRYUWE2aDJzX1p4ZERNVlZjSHlaMTl6Qw?oc=5\" target=\"_blank\">Introducing GPT-5.3-Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">OpenAI</font>",
    "published": "Thu, 05 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "64364edb70353045",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple Unveils Agentic Coding In Xcode 26.3 Update - Evrim Ağacı",
    "url": "https://news.google.com/rss/articles/CBMiigFBVV95cUxQMzRtSzBQMzlkMFBxanNkTXRfSmpHY2VoMkJSc2xvbUlTV2piQnBxOFhwZTN1T28xREJONkdUUFdJRVhPQVkzU1ozbDJESkU4eXV3N2VFZC1PYWxGZEpja0p0ZWUxZU5kam9Wc0I2VTR2T05TVkxZNVFQbzdfWXBPZEhTUmxDZlAtSHc?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiigFBVV95cUxQMzRtSzBQMzlkMFBxanNkTXRfSmpHY2VoMkJSc2xvbUlTV2piQnBxOFhwZTN1T28xREJONkdUUFdJRVhPQVkzU1ozbDJESkU4eXV3N2VFZC1PYWxGZEpja0p0ZWUxZU5kam9Wc0I2VTR2T05TVkxZNVFQbzdfWXBPZEhTUmxDZlAtSHc?oc=5\" target=\"_blank\">Apple Unveils Agentic Coding In Xcode 26.3 Update</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Evrim Ağacı</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3a266fd468382e9e",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Anthropic’s new Claude Opus 4.6 aims to think through bigger code bases - Fast Company",
    "url": "https://news.google.com/rss/articles/CBMiqgFBVV95cUxNaGh6SGpJVUhQSGJoZ0JyZ21fdHU0VUNpcVFmRDNVREh5NERVU1J0VEFRaFpEcXU2VUZIWWl5ZmFJVGFkRURtbEoweDh3V29QTktWbVhPR2lBdk14WFhPNi1CZURCLU8zN1dZUk9TSXJndXY0TFpyY1ZfX3hGUVpfU25JcldtWElhOWt5Ujh6WHJLVmxjc21SYkFkWDFmeG1QN3ZIZjlTR3Q1QQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiqgFBVV95cUxNaGh6SGpJVUhQSGJoZ0JyZ21fdHU0VUNpcVFmRDNVREh5NERVU1J0VEFRaFpEcXU2VUZIWWl5ZmFJVGFkRURtbEoweDh3V29QTktWbVhPR2lBdk14WFhPNi1CZURCLU8zN1dZUk9TSXJndXY0TFpyY1ZfX3hGUVpfU25JcldtWElhOWt5Ujh6WHJLVmxjc21SYkFkWDFmeG1QN3ZIZjlTR3Q1QQ?oc=5\" target=\"_blank\">Anthropic’s new Claude Opus 4.6 aims to think through bigger code bases</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Fast Company</font>",
    "published": "Thu, 05 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "56f26dc85d7888bc",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Xcode 26.3 finally brings agentic coding to Apple's developer tools - ZDNET",
    "url": "https://news.google.com/rss/articles/CBMifkFVX3lxTE9CZ2xRTmpsTGd1b25pa19zN09hdTF3RVJ1UWRzNWRraVlpU3dzajZoQ2hHQXRzSk13a3hvNTYxbGgyZmVCdURvLXBnQVlsX1hIWGNSVzBEdWxjSXFuOWpTNm1Ha24tVHllZ0xkZUxKU0tZRVRaY283Z29iLTNDZw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMifkFVX3lxTE9CZ2xRTmpsTGd1b25pa19zN09hdTF3RVJ1UWRzNWRraVlpU3dzajZoQ2hHQXRzSk13a3hvNTYxbGgyZmVCdURvLXBnQVlsX1hIWGNSVzBEdWxjSXFuOWpTNm1Ha24tVHllZ0xkZUxKU0tZRVRaY283Z29iLTNDZw?oc=5\" target=\"_blank\">Xcode 26.3 finally brings agentic coding to Apple's developer tools</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ZDNET</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b5bafe3ecfd6bae2",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Xcode 26.3 adds support for Claude, Codex, and other agentic tools via MCP - Ars Technica",
    "url": "https://news.google.com/rss/articles/CBMisAFBVV95cUxPa0xuWG83eW91eTVMMUhYRUNRWG13NVppWmFkdFJvQ1gtR3NMRUN4UGc5ZHdBX0o1YUdzQUJGZXRCRG9RRHNzaHBvTXdLekxrUGF0XzFvWU5oU05RMUd5NU5Lc1I5RzI3MFZfRUNadElyYlZPdUZyc1lYNFVLNzVBSk9nYnpNYURiR1MzWjd4eWcwRDVRc2xPaDV5VTFwRGIxb2FJbGIwdFJ4UFNfMWl4aA?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMisAFBVV95cUxPa0xuWG83eW91eTVMMUhYRUNRWG13NVppWmFkdFJvQ1gtR3NMRUN4UGc5ZHdBX0o1YUdzQUJGZXRCRG9RRHNzaHBvTXdLekxrUGF0XzFvWU5oU05RMUd5NU5Lc1I5RzI3MFZfRUNadElyYlZPdUZyc1lYNFVLNzVBSk9nYnpNYURiR1MzWjd4eWcwRDVRc2xPaDV5VTFwRGIxb2FJbGIwdFJ4UFNfMWl4aA?oc=5\" target=\"_blank\">Xcode 26.3 adds support for Claude, Codex, and other agentic tools via MCP</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Ars Technica</font>",
    "published": "Tue, 03 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d12dd93609e4f992",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Apple Brings Agentic AI Coding to Xcode With Claude and Codex - Unite.AI",
    "url": "https://news.google.com/rss/articles/CBMiiwFBVV95cUxPY0ZQN19icnVpWHZZYWJxdk5OOWNYeVlBcXdLVTdNMGVPY010UGU2eGdNYUM4MmtTd3pYRUlRUldaOU1BbmFEWnhvRHpFMWVmeVFUd2NuS2JwbVh2dURhYWtwUFVJV3NzUDJZZnl3TlZrb2Y2YlZSTFY0anJlMUtrdmhBcWZTZmo5LTZV?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiiwFBVV95cUxPY0ZQN19icnVpWHZZYWJxdk5OOWNYeVlBcXdLVTdNMGVPY010UGU2eGdNYUM4MmtTd3pYRUlRUldaOU1BbmFEWnhvRHpFMWVmeVFUd2NuS2JwbVh2dURhYWtwUFVJV3NzUDJZZnl3TlZrb2Y2YlZSTFY0anJlMUtrdmhBcWZTZmo5LTZV?oc=5\" target=\"_blank\">Apple Brings Agentic AI Coding to Xcode With Claude and Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Unite.AI</font>",
    "published": "Wed, 04 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "45295a0ae5486751",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Xcode 26.3 Brings OpenAI Codex And Anthropic Claude Agent for Smarter, Agentic Coding - NDTV Profit",
    "url": "https://news.google.com/rss/articles/CBMiywFBVV95cUxOc0tsSE1SUGZkUkZFM2ZaLW9PS3FzdDhvN3FINk5qYVVsQUdNZ245aHJsU050aDVlalJtSE5zUElCNDlieVhWRzh6MFJTMjJEbEx3MEl2cWVhTy15dHlrQWtCTTczUUh2djBvcXlYak1xZlRycEp6T3ZldktpRkh4SVNIWlpoek9KYXZrT2ZFdWtvTWttQ0RpYlQ4Y3hsS2NoblYtdWpkUFl0TExPbGhMeGp1TFhST0ROQjAzaUxuS1cyakhXY1VHczJlWdIB0wFBVV95cUxNR2s5SkFwWEEtblZNdDlBSVk0R0djNFlEOEdFYVlzTlYwaVJUMVk3Qkt1ckJudS1qa2RNUHl2MUloQThramdYQkRjanNmenhYLVoyNHgtclJ1NGw3b09aLUdtb2gwR291UWFNN1ZQOUQ4Q210MUxNWDkwVEN2VVBicmFib0FXVHBmWlZwYXh0TEZQRnpXSmJabFV0UTE1eGRtQzg5bWo0MUE1ajZOOF9hRVdYTUpXcGJ2RUZ6aUh5Um5NX0RCY1BDSTB2SUs2QzlFdm1R?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiywFBVV95cUxOc0tsSE1SUGZkUkZFM2ZaLW9PS3FzdDhvN3FINk5qYVVsQUdNZ245aHJsU050aDVlalJtSE5zUElCNDlieVhWRzh6MFJTMjJEbEx3MEl2cWVhTy15dHlrQWtCTTczUUh2djBvcXlYak1xZlRycEp6T3ZldktpRkh4SVNIWlpoek9KYXZrT2ZFdWtvTWttQ0RpYlQ4Y3hsS2NoblYtdWpkUFl0TExPbGhMeGp1TFhST0ROQjAzaUxuS1cyakhXY1VHczJlWdIB0wFBVV95cUxNR2s5SkFwWEEtblZNdDlBSVk0R0djNFlEOEdFYVlzTlYwaVJUMVk3Qkt1ckJudS1qa2RNUHl2MUloQThramdYQkRjanNmenhYLVoyNHgtclJ1NGw3b09aLUdtb2gwR291UWFNN1ZQOUQ4Q210MUxNWDkwVEN2VVBicmFib0FXVHBmWlZwYXh0TEZQRnpXSmJabFV0UTE1eGRtQzg5bWo0MUE1ajZOOF9hRVdYTUpXcGJ2RUZ6aUh5Um5NX0RCY1BDSTB2SUs2QzlFdm1R?oc=5\" target=\"_blank\">Xcode 26.3 Brings OpenAI Codex And Anthropic Claude Agent for Smarter, Agentic Coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NDTV Profit</font>",
    "published": "Wed, 04 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "a948cd93e3704656",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "What is LLM Inference? - IBM",
    "url": "https://news.google.com/rss/articles/CBMiWkFVX3lxTE5iMnhObndxOVBoZFg2b1lncVhGeVVpcXZoRlJmckQyLTVycU56bE55ZzdjRXZsNXg0T2FmdFhlTlhrTVpYa3F0em51NEFNdS1RR0dEZ3kzem41QQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiWkFVX3lxTE5iMnhObndxOVBoZFg2b1lncVhGeVVpcXZoRlJmckQyLTVycU56bE55ZzdjRXZsNXg0T2FmdFhlTlhrTVpYa3F0em51NEFNdS1RR0dEZ3kzem41QQ?oc=5\" target=\"_blank\">What is LLM Inference?</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">IBM</font>",
    "published": "Sat, 07 Feb 2026 07:11:40 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c63622b1d86440fa",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Optimize LLM response costs and latency with effective caching - Amazon Web Services",
    "url": "https://news.google.com/rss/articles/CBMiowFBVV95cUxPbEZ6a21GcHRsclFfd19XSkdZT3VVV0M4MjR0VmFDX2o2WVYwT3Z6MXFMcENfa3BQTEU0RXVYeW5sZkp6Q1czaEdxODVmaWVVVmFaLTEwOXZ3WHlSOVlVSjJFS2dJejl4OVBwUi1rS0VCSDRaYVBmTUdJWVJFWDZLa2EydEdrTEVfbXFHLXNXclVUWDRVU0pLbUZGUzFNRVA2ME44?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiowFBVV95cUxPbEZ6a21GcHRsclFfd19XSkdZT3VVV0M4MjR0VmFDX2o2WVYwT3Z6MXFMcENfa3BQTEU0RXVYeW5sZkp6Q1czaEdxODVmaWVVVmFaLTEwOXZ3WHlSOVlVSjJFS2dJejl4OVBwUi1rS0VCSDRaYVBmTUdJWVJFWDZLa2EydEdrTEVfbXFHLXNXclVUWDRVU0pLbUZGUzFNRVA2ME44?oc=5\" target=\"_blank\">Optimize LLM response costs and latency with effective caching</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services</font>",
    "published": "Mon, 02 Feb 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "14123a23d6cb92aa",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Implementing High-Performance LLM Serving on GKE: An Inference Gateway Walkthrough - Google Cloud",
    "url": "https://news.google.com/rss/articles/CBMi2wFBVV95cUxQSHNLa053dm01Nk9xUjE3NGJzM0NOekNBbF9xa1U5T25RVm1hVG93NFd1TnJPN0lpTmg0SE1yenAyQjZxS0FqZERldDBBeXNhaW1nQjVDR0hzdEJ0dDh3NDVLQVY4X3IydENfZ0VaX0NTLTk3OFNrZi1RbTQwNjJoR0VRdUJvUHRkSWhyaHZPVF90ZUhBeDQxeVNLR3g3TUpnTWNIUjBhSmY4YXFkUFIybG5mczMxd0VuZHIyU24xVDBvcFdDZFp3aWdRY1IzWDdsSWpPVGdONTR0WkE?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi2wFBVV95cUxQSHNLa053dm01Nk9xUjE3NGJzM0NOekNBbF9xa1U5T25RVm1hVG93NFd1TnJPN0lpTmg0SE1yenAyQjZxS0FqZERldDBBeXNhaW1nQjVDR0hzdEJ0dDh3NDVLQVY4X3IydENfZ0VaX0NTLTk3OFNrZi1RbTQwNjJoR0VRdUJvUHRkSWhyaHZPVF90ZUhBeDQxeVNLR3g3TUpnTWNIUjBhSmY4YXFkUFIybG5mczMxd0VuZHIyU24xVDBvcFdDZFp3aWdRY1IzWDdsSWpPVGdONTR0WkE?oc=5\" target=\"_blank\">Implementing High-Performance LLM Serving on GKE: An Inference Gateway Walkthrough</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Google Cloud</font>",
    "published": "Wed, 16 Jul 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "0a211746be25529d",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "How we built the most efficient inference engine for Cloudflare’s network - The Cloudflare Blog",
    "url": "https://news.google.com/rss/articles/CBMigAFBVV95cUxPUXVHMGtiLVNRQmFKXzFEd2tSY25fTC1UN0Mzc3ExOEM2NUlwd3pCeUVCLXlHQkdpTm52eUZUcUk3VlBOUkpOcFNBWnJfak9fS0JjY0FvVE1tZW1yMmZiSVRYX2t6dEdHRjVmNldVQVJubEw2OWRSN2xBQVhmME1FSQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMigAFBVV95cUxPUXVHMGtiLVNRQmFKXzFEd2tSY25fTC1UN0Mzc3ExOEM2NUlwd3pCeUVCLXlHQkdpTm52eUZUcUk3VlBOUkpOcFNBWnJfak9fS0JjY0FvVE1tZW1yMmZiSVRYX2t6dEdHRjVmNldVQVJubEw2OWRSN2xBQVhmME1FSQ?oc=5\" target=\"_blank\">How we built the most efficient inference engine for Cloudflare’s network</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The Cloudflare Blog</font>",
    "published": "Wed, 27 Aug 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "556d2e5e8f51fcec",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Reducing Cold Start Latency for LLM Inference with NVIDIA Run:ai Model Streamer | NVIDIA Technical Blog - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMiswFBVV95cUxPUVFITlZ2N1JGelJlQ2htQ2RneXdRRnZUM0RTYnFlNFBCR2JtZjE4ZjFpdTRxWm5sYV9oME4yZ0RxLXVHQTQ0eUQyLTlxVW9lbTBpaG5GQlVkaUJUckVaVzVwOWNtUDR5Snp0UDVzbnNMQmltemp3VG4tcG93b1V2eTFObFpHdm81NzlGSm91T0htYkpGRk1OUy1yRkxLcEJBUjJFS0ZqTUlRUWx2N2NwVzRVUQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiswFBVV95cUxPUVFITlZ2N1JGelJlQ2htQ2RneXdRRnZUM0RTYnFlNFBCR2JtZjE4ZjFpdTRxWm5sYV9oME4yZ0RxLXVHQTQ0eUQyLTlxVW9lbTBpaG5GQlVkaUJUckVaVzVwOWNtUDR5Snp0UDVzbnNMQmltemp3VG4tcG93b1V2eTFObFpHdm81NzlGSm91T0htYkpGRk1OUy1yRkxLcEJBUjJFS0ZqTUlRUWx2N2NwVzRVUQ?oc=5\" target=\"_blank\">Reducing Cold Start Latency for LLM Inference with NVIDIA Run:ai Model Streamer | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Tue, 16 Sep 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "32369f4adfab2868",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Rapid-serve Achieves 4.1x LLM Inference Speedup With Intra-GPU Disaggregation - Quantum Zeitgeist",
    "url": "https://news.google.com/rss/articles/CBMie0FVX3lxTFBGVFdLZzRZQk1BM0EySjFZOWJ0N1FkVmJaNXZhR3dfbG9KU3hrWmwtODRGWmZ3QVJMMDFDWEpHNzdXdVNDczJhejljaUN5aE1CTWhtaUhoNUFMME0wenpUWWU3aF90MzRmeHpsQkYzZkZkZDcwSE45eDZaVQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMie0FVX3lxTFBGVFdLZzRZQk1BM0EySjFZOWJ0N1FkVmJaNXZhR3dfbG9KU3hrWmwtODRGWmZ3QVJMMDFDWEpHNzdXdVNDczJhejljaUN5aE1CTWhtaUhoNUFMME0wenpUWWU3aF90MzRmeHpsQkYzZkZkZDcwSE45eDZaVQ?oc=5\" target=\"_blank\">Rapid-serve Achieves 4.1x LLM Inference Speedup With Intra-GPU Disaggregation</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Quantum Zeitgeist</font>",
    "published": "Thu, 22 Jan 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "72d411b20e757bb4",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Allen School researchers receive Best Paper Award for speeding up LLM performance with FlashInfer - Allen School News",
    "url": "https://news.google.com/rss/articles/CBMi1wFBVV95cUxPaS0tT21tX0JsZ3dpYVJLSGRuTDNRSWlxdFpVMnVwZmNmcm1UNlgzcExGamp5X181ekNOMHo3WE5uTFpIMmpnLVVHOW05ZDctZjZWTTFOelRKalgzLWRyQjFUWkRDdGp0cHdXdzhqSjB4aEh2LUtUMXNsUjY4S0hZX3J3dXQ2U1Q1UEpLc19ybEJuQlo4Q29xcmxyOG1ITy03d09HYjRuN2lVZ0VGanFrUXRmbW1MTElSMU5aWTVrMVJKSzI1THI0a0VDMERTTlB6WlQ5Wm1NQQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi1wFBVV95cUxPaS0tT21tX0JsZ3dpYVJLSGRuTDNRSWlxdFpVMnVwZmNmcm1UNlgzcExGamp5X181ekNOMHo3WE5uTFpIMmpnLVVHOW05ZDctZjZWTTFOelRKalgzLWRyQjFUWkRDdGp0cHdXdzhqSjB4aEh2LUtUMXNsUjY4S0hZX3J3dXQ2U1Q1UEpLc19ybEJuQlo4Q29xcmxyOG1ITy03d09HYjRuN2lVZ0VGanFrUXRmbW1MTElSMU5aWTVrMVJKSzI1THI0a0VDMERTTlB6WlQ5Wm1NQQ?oc=5\" target=\"_blank\">Allen School researchers receive Best Paper Award for speeding up LLM performance with FlashInfer</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Allen School News</font>",
    "published": "Tue, 01 Jul 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "75cd9d3f6aa0945d",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "AMD Making It Easier To Install vLLM For ROCm - Phoronix",
    "url": "https://news.google.com/rss/articles/CBMiXkFVX3lxTE42M1p2dkJ0bC1vbk1ZN0VCbHh1dElJdG0tV2VzYlNmYmVQQW5MMzRQWjgzVmE4eTNKN2NEWlluZ0swQzRGRE44NTh0OEVtNzVyakYxd2lINFVXYWNHMXc?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiXkFVX3lxTE42M1p2dkJ0bC1vbk1ZN0VCbHh1dElJdG0tV2VzYlNmYmVQQW5MMzRQWjgzVmE4eTNKN2NEWlluZ0swQzRGRE44NTh0OEVtNzVyakYxd2lINFVXYWNHMXc?oc=5\" target=\"_blank\">AMD Making It Easier To Install vLLM For ROCm</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Phoronix</font>",
    "published": "Tue, 20 Jan 2026 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fc015eaafb3d2ddb",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "A Gentle Introduction to vLLM for Serving - KDnuggets",
    "url": "https://news.google.com/rss/articles/CBMidkFVX3lxTE1mZTVLOWM2RUZjYXRrQnFKVExBdnY4OVNpTTc3OWs1WEs3Ui1CWnNUdmZUQ1p6eDd4Tkk1SGdGYmFPNjU3T2tNNUE1N0NqSWZROENaY3FkTVhmUlBtMzlpeUZpM0h5WG9rOVdBME1EQXNIVE5YMFE?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMidkFVX3lxTE1mZTVLOWM2RUZjYXRrQnFKVExBdnY4OVNpTTc3OWs1WEs3Ui1CWnNUdmZUQ1p6eDd4Tkk1SGdGYmFPNjU3T2tNNUE1N0NqSWZROENaY3FkTVhmUlBtMzlpeUZpM0h5WG9rOVdBME1EQXNIVE5YMFE?oc=5\" target=\"_blank\">A Gentle Introduction to vLLM for Serving</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">KDnuggets</font>",
    "published": "Thu, 18 Sep 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ed6474bd33be88e8",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Optimizing LLM inference on Amazon SageMaker AI with BentoML’s LLM- Optimizer - Amazon Web Services",
    "url": "https://news.google.com/rss/articles/CBMivwFBVV95cUxNTGwxRkQ2bENtcHUtUTRweHotWW5XS1hRa25GM3JjUUVCTC11X2syU3V2ZHFJYUZORnhHRXJsQUtoaEswVm40Z1htMXVCaGdJeUJvOWFoUWREclpvV2JsRVAzWUc5YUpncnRqS2F0T21nRTFMT3RtRnk0MXZ4aEIwRkNBUmg3WXFESU1oaVFvR0ktT1lZaVVvZUh1NFJaSWZRd1ZEUlJSS1RocllZd2dWS1lMcUk2dU4xVEVESlJtbw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMivwFBVV95cUxNTGwxRkQ2bENtcHUtUTRweHotWW5XS1hRa25GM3JjUUVCTC11X2syU3V2ZHFJYUZORnhHRXJsQUtoaEswVm40Z1htMXVCaGdJeUJvOWFoUWREclpvV2JsRVAzWUc5YUpncnRqS2F0T21nRTFMT3RtRnk0MXZ4aEIwRkNBUmg3WXFESU1oaVFvR0ktT1lZaVVvZUh1NFJaSWZRd1ZEUlJSS1RocllZd2dWS1lMcUk2dU4xVEVESlJtbw?oc=5\" target=\"_blank\">Optimizing LLM inference on Amazon SageMaker AI with BentoML’s LLM- Optimizer</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services</font>",
    "published": "Wed, 24 Dec 2025 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "7209b7f1edf61119",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "An Introduction to Speculative Decoding for Reducing Latency in AI Inference - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMisAFBVV95cUxPbEJCVnJUY1BONmF4bWdFWjlnc0hLTlBWNkxkX3BkMHo1eFE0Tk03Q3Q0Z2ZqN0UwNDc4bENLT1hHZ1dfaHdhR0xoSXhsLVNRbzV4WXhSaFlOLUtnRjk5RGc1SFpmMHhkdmpZWFJ2TVR2OVRNWGN6djlhZWpqV3JkcDZidlF2NjhyQUVlR1FVZWlTZ2ZEbEdId25FQVdwaXVSY1ZDMkdEWklSSXViR0hySg?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMisAFBVV95cUxPbEJCVnJUY1BONmF4bWdFWjlnc0hLTlBWNkxkX3BkMHo1eFE0Tk03Q3Q0Z2ZqN0UwNDc4bENLT1hHZ1dfaHdhR0xoSXhsLVNRbzV4WXhSaFlOLUtnRjk5RGc1SFpmMHhkdmpZWFJ2TVR2OVRNWGN6djlhZWpqV3JkcDZidlF2NjhyQUVlR1FVZWlTZ2ZEbEdId25FQVdwaXVSY1ZDMkdEWklSSXViR0hySg?oc=5\" target=\"_blank\">An Introduction to Speculative Decoding for Reducing Latency in AI Inference</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Wed, 17 Sep 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "01eefaab4a3fa59a",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Accelerate Deep Learning and LLM Inference with Apache Spark in the Cloud | NVIDIA Technical Blog - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMirAFBVV95cUxNcU5wMzU3RG5nLU9RTV9KeUpVR1ZvdzdUWWFMQjIyUzlFQ2ctVEFOVWJGeXVnNmx3cktYOUs3dXRSel8yTnMxeHpJZDFydWU2R0xBWWVjeHV4TWhhYlEyMlE2bi1QTnNVUEFJZUMxb3BlNXowS3VRRGIwV3VBRWRKYlZmYW0yYmJ1N1ZnWHVyaThFMTVrNXZPZGdtdmNqcjhYUVlrcjJWczg4aUFI?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMirAFBVV95cUxNcU5wMzU3RG5nLU9RTV9KeUpVR1ZvdzdUWWFMQjIyUzlFQ2ctVEFOVWJGeXVnNmx3cktYOUs3dXRSel8yTnMxeHpJZDFydWU2R0xBWWVjeHV4TWhhYlEyMlE2bi1QTnNVUEFJZUMxb3BlNXowS3VRRGIwV3VBRWRKYlZmYW0yYmJ1N1ZnWHVyaThFMTVrNXZPZGdtdmNqcjhYUVlrcjJWczg4aUFI?oc=5\" target=\"_blank\">Accelerate Deep Learning and LLM Inference with Apache Spark in the Cloud | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Thu, 08 May 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f8e4d6c07ef0bbb4",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers - Amazon Web Services",
    "url": "https://news.google.com/rss/articles/CBMipwFBVV95cUxOTVJwY0MzSzFxTkgwMHNiTEhWTXhmU2hBd3gzOXBQR24tM2dQNVpyN2ZEOXAyZmJRUEtuQWlhYkgyMDFVcEhxWF9pNngzRkFxR0F0UTJmQnltNnR3bnMxU3dCejMzOElEcENGVGFndDV3UWo4aGcyc3UydVh4QURPd1VEY1o5M01pdGEtdXNSdk9lVnFiWGYyQl8zZFd0X0V3V1F6V1FvRQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMipwFBVV95cUxOTVJwY0MzSzFxTkgwMHNiTEhWTXhmU2hBd3gzOXBQR24tM2dQNVpyN2ZEOXAyZmJRUEtuQWlhYkgyMDFVcEhxWF9pNngzRkFxR0F0UTJmQnltNnR3bnMxU3dCejMzOElEcENGVGFndDV3UWo4aGcyc3UydVh4QURPd1VEY1o5M01pdGEtdXNSdk9lVnFiWGYyQl8zZFd0X0V3V1F6V1FvRQ?oc=5\" target=\"_blank\">Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services</font>",
    "published": "Thu, 14 Aug 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d6e346d738f6b236",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Run High-Performance LLM Inference Kernels from NVIDIA Using FlashInfer​​ - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMiqgFBVV95cUxQRXVBTGczenB4dEZnQlhHbVhfN19Eb2RXWmJwUnhLZlcwY1dBYzN6SUJRcDhsMlNLUnpUR2dXUnVqODlmaVluVW52Z2cyWl95ODJLQzNaRG1aRXE1ZW52dGk2N0E5bnhBM3NrOUoxSzc1OXM5NVdUSUNwQzRhOFRWMnh1XzdmamM4WHVrcWFwYUJ6Zl9zSkJRMVlTZEpWalY3ZWpRQ1VsdjZWUQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiqgFBVV95cUxQRXVBTGczenB4dEZnQlhHbVhfN19Eb2RXWmJwUnhLZlcwY1dBYzN6SUJRcDhsMlNLUnpUR2dXUnVqODlmaVluVW52Z2cyWl95ODJLQzNaRG1aRXE1ZW52dGk2N0E5bnhBM3NrOUoxSzc1OXM5NVdUSUNwQzRhOFRWMnh1XzdmamM4WHVrcWFwYUJ6Zl9zSkJRMVlTZEpWalY3ZWpRQ1VsdjZWUQ?oc=5\" target=\"_blank\">Run High-Performance LLM Inference Kernels from NVIDIA Using FlashInfer​​</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Fri, 13 Jun 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3097365d8cb95c73",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Simplify LLM Deployment and AI Inference with a Unified NVIDIA NIM Workflow - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMirAFBVV95cUxQQWpvV1I5bHk5Y09rYm54N2piaERULUJtMzA0ekp2QVpVUHdBSWpnbFlQSm5PNktKSm1DT3JZeEhtN1dZWjVKaVVDRXRBOXlKZUF6dlF6NXgzVFQ3U1NKc0ZIUUhrMXlFMUZVaEtDUzgtNVMtczkweDdfaHBiREhMbHVWMXBRMm9XbVVUS3ZNTUhvR2tacmJMaHIxMml5N2d2VlVXQkF6VjFHdnVX?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMirAFBVV95cUxQQWpvV1I5bHk5Y09rYm54N2piaERULUJtMzA0ekp2QVpVUHdBSWpnbFlQSm5PNktKSm1DT3JZeEhtN1dZWjVKaVVDRXRBOXlKZUF6dlF6NXgzVFQ3U1NKc0ZIUUhrMXlFMUZVaEtDUzgtNVMtczkweDdfaHBiREhMbHVWMXBRMm9XbVVUS3ZNTUhvR2tacmJMaHIxMml5N2d2VlVXQkF6VjFHdnVX?oc=5\" target=\"_blank\">Simplify LLM Deployment and AI Inference with a Unified NVIDIA NIM Workflow</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Wed, 11 Jun 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "065412c4f58b3353",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Smart Multi-Node Scheduling for Fast and Efficient LLM Inference with NVIDIA Run:ai and NVIDIA Dynamo - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMi0AFBVV95cUxPeVNxVzZ1M0VET21hWnh4aVJqWC1uZHJlUHBUbXl4MTlicExQSWFTbzg2NHdxbzdaVUU0NmNkallabHdLUWE4ZXdZUHRXUll6emRRLVktb2JoeVRteUw4RzA4WDl3LTIzZXBHazNEeU0wVXNKbUs0RDIwWFVQTHgtRlpPbVZGYkZpeV9GZVB3RlJ0RG5OV1g1bmFlOHk0akpRYkpYLVA3YXJBbkhSTktqa0t5cVdBdEhjOTZHNk5PMXNUaXNnZE1rSjVoczA1N0pX?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi0AFBVV95cUxPeVNxVzZ1M0VET21hWnh4aVJqWC1uZHJlUHBUbXl4MTlicExQSWFTbzg2NHdxbzdaVUU0NmNkallabHdLUWE4ZXdZUHRXUll6emRRLVktb2JoeVRteUw4RzA4WDl3LTIzZXBHazNEeU0wVXNKbUs0RDIwWFVQTHgtRlpPbVZGYkZpeV9GZVB3RlJ0RG5OV1g1bmFlOHk0akpRYkpYLVA3YXJBbkhSTktqa0t5cVdBdEhjOTZHNk5PMXNUaXNnZE1rSjVoczA1N0pX?oc=5\" target=\"_blank\">Smart Multi-Node Scheduling for Fast and Efficient LLM Inference with NVIDIA Run:ai and NVIDIA Dynamo</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Mon, 29 Sep 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "54db6c3a9fc6fea9",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "LLM Inference Benchmarking: Performance Tuning with TensorRT-LLM | NVIDIA Technical Blog - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMinwFBVV95cUxOeXd1SF9ZTnAwQlJ3S1pYMzh2YTNHU09HZHZGNjA2a1JsV0RRT0ZpbC1uemZMS1NkT25xSnJkNDA1RTVCWkJPSUhEYndFYzE3MnVZMDJ2T2FxdjNRaV9GX2JITWVHbTJicVNWeDFXaW9KaHF1TngtanF6eVRtU3N2MVdjNUNnbnVPNGNLZlowUlRCTHpUdEdIY2pGdzRpaVU?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMinwFBVV95cUxOeXd1SF9ZTnAwQlJ3S1pYMzh2YTNHU09HZHZGNjA2a1JsV0RRT0ZpbC1uemZMS1NkT25xSnJkNDA1RTVCWkJPSUhEYndFYzE3MnVZMDJ2T2FxdjNRaV9GX2JITWVHbTJicVNWeDFXaW9KaHF1TngtanF6eVRtU3N2MVdjNUNnbnVPNGNLZlowUlRCTHpUdEdIY2pGdzRpaVU?oc=5\" target=\"_blank\">LLM Inference Benchmarking: Performance Tuning with TensorRT-LLM | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Mon, 07 Jul 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "14715f1b228fba8d",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "LLM Inference Benchmarking: How Much Does Your LLM Inference Cost? | NVIDIA Technical Blog - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMioAFBVV95cUxQeG1jWk0zR3RobjF4ZEtFdjhPdmFkakJyaXdFcmhiTG8zdHJ4dGpNWktSaklKaGVyeFlwUzBwM3Y1cVNMa0l0ci16d3NwdHQydURLRXdFRmxyM3RJZGEyOWw0Q2picEZBZ1VaLVloV1dCOVNYT21hZGR4ZVNYdU9Zb0xKcW1TQjVFaUJiaG0wdTZBTWVZS0FiUnFZV3o4YU1W?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMioAFBVV95cUxQeG1jWk0zR3RobjF4ZEtFdjhPdmFkakJyaXdFcmhiTG8zdHJ4dGpNWktSaklKaGVyeFlwUzBwM3Y1cVNMa0l0ci16d3NwdHQydURLRXdFRmxyM3RJZGEyOWw0Q2picEZBZ1VaLVloV1dCOVNYT21hZGR4ZVNYdU9Zb0xKcW1TQjVFaUJiaG0wdTZBTWVZS0FiUnFZV3o4YU1W?oc=5\" target=\"_blank\">LLM Inference Benchmarking: How Much Does Your LLM Inference Cost? | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Wed, 18 Jun 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "45400c8f36c533c8",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "LLM Inference Benchmarking: Fundamental Concepts - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMifEFVX3lxTE5Rbk84V0tQWlRrNTA4aXJIUm9scS1XZmdvTm9SUUV3Uk9tQ2RGZEFrUkpUdGphZ0dvUkFMVmJpNFVieVFaOHc2NmdDNG04N014bU96d3JpcmZjYnlrRl9aYk10Y0hIdFNvYlJRenFyd1dNUkYzd3VuTldhV3Y?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMifEFVX3lxTE5Rbk84V0tQWlRrNTA4aXJIUm9scS1XZmdvTm9SUUV3Uk9tQ2RGZEFrUkpUdGphZ0dvUkFMVmJpNFVieVFaOHc2NmdDNG04N014bU96d3JpcmZjYnlrRl9aYk10Y0hIdFNvYlJRenFyd1dNUkYzd3VuTldhV3Y?oc=5\" target=\"_blank\">LLM Inference Benchmarking: Fundamental Concepts</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Wed, 02 Apr 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "fb17a8e8e975e486",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Six Frameworks for Efficient LLM Inferencing - The New Stack",
    "url": "https://news.google.com/rss/articles/CBMid0FVX3lxTE9keXE5b3BtMG9RUnNGdVBmanA3LVdVVi11QUtfOGhZTy13SEhER0hybFV3S0xScHFESy1ySTNoVHU5Nlp0ZUNaUWdYbjRBQ3oyRmVVM283b2hTamZJOEEtcEZRQUs1Uk10SUh3dGV0SjN1T1UyOVlz?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMid0FVX3lxTE9keXE5b3BtMG9RUnNGdVBmanA3LVdVVi11QUtfOGhZTy13SEhER0hybFV3S0xScHFESy1ySTNoVHU5Nlp0ZUNaUWdYbjRBQ3oyRmVVM283b2hTamZJOEEtcEZRQUs1Uk10SUh3dGV0SjN1T1UyOVlz?oc=5\" target=\"_blank\">Six Frameworks for Efficient LLM Inferencing</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The New Stack</font>",
    "published": "Fri, 19 Sep 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "eb7262cb35d001a6",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "NVIDIA Dynamo Accelerates llm-d Community Initiatives for Advancing Large-Scale Distributed Inference - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMi0gFBVV95cUxORXRTYnJ2cFVSTVBISmpvSl9jWnVKUTJfOEUtS2Z5MVNjOUlHUXdDRWJ5YWlySTNUdzBmQXBlZV80MWRTNFQzbmRBY2NEQkhBSmluVDJsVWw4VWZDVEJzeEVlYnVnUzJIa3c2YjhZRFlfUWpDWTBFRGJlUzdrTF9Za0d1R25kMjZ1bGNXT3ZiMlZ4ZHZocDlheDhKWnYtUHYySENxYnhxOGhDSjJ3bHByZGpYY2NfMXoxNlcxVU1qdWU4Skl1ZTJoN2trUzRubTJ6Y2c?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi0gFBVV95cUxORXRTYnJ2cFVSTVBISmpvSl9jWnVKUTJfOEUtS2Z5MVNjOUlHUXdDRWJ5YWlySTNUdzBmQXBlZV80MWRTNFQzbmRBY2NEQkhBSmluVDJsVWw4VWZDVEJzeEVlYnVnUzJIa3c2YjhZRFlfUWpDWTBFRGJlUzdrTF9Za0d1R25kMjZ1bGNXT3ZiMlZ4ZHZocDlheDhKWnYtUHYySENxYnhxOGhDSjJ3bHByZGpYY2NfMXoxNlcxVU1qdWU4Skl1ZTJoN2trUzRubTJ6Y2c?oc=5\" target=\"_blank\">NVIDIA Dynamo Accelerates llm-d Community Initiatives for Advancing Large-Scale Distributed Inference</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Wed, 21 May 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "25cd9c21066b53df",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Effectively benchmarking OCI Compute Shapes for LLM inference serving - Oracle Blogs",
    "url": "https://news.google.com/rss/articles/CBMikwFBVV95cUxPaUF1dmdJT2EwR1NpQ01hcTYtTlQ4bG15RDFSekR0T1g4SWpGSnBWMjk2VUxZZ3RZdDRleGtJdjVmdFA1WUFEVUhsVy1WWTBLRE9YdURWekpFZUhFbDJldmJ2SHhOVDJNYmhGVUZxclQ3STQzVHQ5S1E1a2hoWFhGaEk1cnhiZzFfQUxFWm9rdVFwaGs?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMikwFBVV95cUxPaUF1dmdJT2EwR1NpQ01hcTYtTlQ4bG15RDFSekR0T1g4SWpGSnBWMjk2VUxZZ3RZdDRleGtJdjVmdFA1WUFEVUhsVy1WWTBLRE9YdURWekpFZUhFbDJldmJ2SHhOVDJNYmhGVUZxclQ3STQzVHQ5S1E1a2hoWFhGaEk1cnhiZzFfQUxFWm9rdVFwaGs?oc=5\" target=\"_blank\">Effectively benchmarking OCI Compute Shapes for LLM inference serving</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Oracle Blogs</font>",
    "published": "Thu, 22 May 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "d546e4dd83cef371",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Comparing the Top 6 Inference Runtimes for LLM Serving in 2025 - MarkTechPost",
    "url": "https://news.google.com/rss/articles/CBMipgFBVV95cUxPZmQ3aFl2Y2IxTDZmV202WjZScHVleldrME03V1FfdGFURWR6Q3BQMUxmREs0YnlXSlNtNWlWdloxaUhwUHNKLXBvdUtyMlJtTDBnZVNxVnVMRjhrcUw3VlBmZ0hzelEyamxUSDlzLWx3aGNONXRHaWRMNS1TOXB3T19nSHA4MDhiYmowQWtzTDRzMTRiNzc2QmxXaGk2MThaSHZxVlBR0gGrAUFVX3lxTE82eFJXRlJBei1NTThMbkxiZlkxSmlfZWozY0pnSHB3OFFsVlJRSTNjZmNGR0pXRy16ekFFb2l0STFMbUxWM08zR3FyQUFicTB4N0o1UzVZbGRUV2F2OG53UUFvMmk2dFJteVFyanFVUUIxaDVnU2hXaFdjMTI5dFZVRFJRbGY2X0ktajNIM0kwaDE3RzhzdXZQNGFhejZLVVBYYTF3NnlaSW1XYw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMipgFBVV95cUxPZmQ3aFl2Y2IxTDZmV202WjZScHVleldrME03V1FfdGFURWR6Q3BQMUxmREs0YnlXSlNtNWlWdloxaUhwUHNKLXBvdUtyMlJtTDBnZVNxVnVMRjhrcUw3VlBmZ0hzelEyamxUSDlzLWx3aGNONXRHaWRMNS1TOXB3T19nSHA4MDhiYmowQWtzTDRzMTRiNzc2QmxXaGk2MThaSHZxVlBR0gGrAUFVX3lxTE82eFJXRlJBei1NTThMbkxiZlkxSmlfZWozY0pnSHB3OFFsVlJRSTNjZmNGR0pXRy16ekFFb2l0STFMbUxWM08zR3FyQUFicTB4N0o1UzVZbGRUV2F2OG53UUFvMmk2dFJteVFyanFVUUIxaDVnU2hXaFdjMTI5dFZVRFJRbGY2X0ktajNIM0kwaDE3RzhzdXZQNGFhejZLVVBYYTF3NnlaSW1XYw?oc=5\" target=\"_blank\">Comparing the Top 6 Inference Runtimes for LLM Serving in 2025</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">MarkTechPost</font>",
    "published": "Fri, 07 Nov 2025 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "ae93befba0bf815b",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Supercharge your LLM performance with Amazon SageMaker Large Model Inference container v15 - Amazon Web Services",
    "url": "https://news.google.com/rss/articles/CBMi0wFBVV95cUxNYVFBSUd2RDJ0U3dvUlBaVzFhTGVRamFIb04zeHBCTFNkamJNTHJoclU5SDNBeGZNb29rUF9WT0hMRjEyVW54QnNTYXBEdGp3T3ZDSFN3RjBsVUlrMXpFSC1lLWRUWkM2ZWI5dER5R1R0a2gzZ1A2TE9keU5QZGpDWDl3UWpDVzctM3JwR2pkNFNqdndyQzlWTlFHaVp6NVpsaC1VdnM1NmZJRUxIVER4X292X3Z6MUdDZXNwWXIwT25xREVuLXBmbFI4aVFYQWI5dkxV?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi0wFBVV95cUxNYVFBSUd2RDJ0U3dvUlBaVzFhTGVRamFIb04zeHBCTFNkamJNTHJoclU5SDNBeGZNb29rUF9WT0hMRjEyVW54QnNTYXBEdGp3T3ZDSFN3RjBsVUlrMXpFSC1lLWRUWkM2ZWI5dER5R1R0a2gzZ1A2TE9keU5QZGpDWDl3UWpDVzctM3JwR2pkNFNqdndyQzlWTlFHaVp6NVpsaC1VdnM1NmZJRUxIVER4X292X3Z6MUdDZXNwWXIwT25xREVuLXBmbFI4aVFYQWI5dkxV?oc=5\" target=\"_blank\">Supercharge your LLM performance with Amazon SageMaker Large Model Inference container v15</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services</font>",
    "published": "Tue, 22 Apr 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "b520981905a392fd",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "LLM Inference Benchmarking Guide: NVIDIA GenAI-Perf and NIM - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMisgFBVV95cUxQd2VsOWdrWS1haUdrZEJHUkhZUGdCMVpsUG1YVHJHSTRLUnh0OGxXd2xvWWRDVHVSUzFqbXgzWUhnaTdZTERvLVZNbzJ1aDd0T1JhOWRnUUNHR2liT05pWUlQSzQzVGUwQ0pqR0VxbmdTNEdnR2xSdnNNY0tPNXRXem5uMzdLNk5ycWp2UUpxbU12VHJLd0hOeVFST0RsRDJWSTRlTUhtdWpKRVZ1LWRMZDR3?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMisgFBVV95cUxQd2VsOWdrWS1haUdrZEJHUkhZUGdCMVpsUG1YVHJHSTRLUnh0OGxXd2xvWWRDVHVSUzFqbXgzWUhnaTdZTERvLVZNbzJ1aDd0T1JhOWRnUUNHR2liT05pWUlQSzQzVGUwQ0pqR0VxbmdTNEdnR2xSdnNNY0tPNXRXem5uMzdLNk5ycWp2UUpxbU12VHJLd0hOeVFST0RsRDJWSTRlTUhtdWpKRVZ1LWRMZDR3?oc=5\" target=\"_blank\">LLM Inference Benchmarking Guide: NVIDIA GenAI-Perf and NIM</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Tue, 06 May 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3dd2e276100d04c5",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Introduction to vLLM: A High-Performance LLM Serving Engine - The New Stack",
    "url": "https://news.google.com/rss/articles/CBMiigFBVV95cUxORUVPQmx1NExBSDhCODZhQVlnQ2ZkMU9GSnVocTVVUlNuQjZCY3FvWGNIYk94RXVrZlAtOHpmcHdNRjVfTVVYT3M5TEhiUXVEM1hqV1VSeWpDRW1TYlkxZ1NxYmR4cjU0UnA3eFo1Q3Awdm5yd29UZHphd29CWWVBbnVJMEhWb3Jndmc?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiigFBVV95cUxORUVPQmx1NExBSDhCODZhQVlnQ2ZkMU9GSnVocTVVUlNuQjZCY3FvWGNIYk94RXVrZlAtOHpmcHdNRjVfTVVYT3M5TEhiUXVEM1hqV1VSeWpDRW1TYlkxZ1NxYmR4cjU0UnA3eFo1Q3Awdm5yd29UZHphd29CWWVBbnVJMEhWb3Jndmc?oc=5\" target=\"_blank\">Introduction to vLLM: A High-Performance LLM Serving Engine</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The New Stack</font>",
    "published": "Fri, 13 Jun 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "002d65ac4348050f",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Ulysses: Unlocking Low-Latency, High-Throughput Inference for Long-Context LLMs - Snowflake",
    "url": "https://news.google.com/rss/articles/CBMihwFBVV95cUxNbGZxU1RfUUYybFFvVlZsSFJoTVN2M2NFZFlwazlvT3kxTFZRWlFQMXdjcWl1NWZ3OGZCOEtHSlhGOGw1S2J4QWdwVDQzX21tald5dTNUTmdodnhwNkJiTWhBbDhDeUFiaE44QzVnVzRtY2d0eVFsWU9DV3NVUDJoX3JhbFBPVTA?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMihwFBVV95cUxNbGZxU1RfUUYybFFvVlZsSFJoTVN2M2NFZFlwazlvT3kxTFZRWlFQMXdjcWl1NWZ3OGZCOEtHSlhGOGw1S2J4QWdwVDQzX21tald5dTNUTmdodnhwNkJiTWhBbDhDeUFiaE44QzVnVzRtY2d0eVFsWU9DV3NVUDJoX3JhbFBPVTA?oc=5\" target=\"_blank\">Ulysses: Unlocking Low-Latency, High-Throughput Inference for Long-Context LLMs</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Snowflake</font>",
    "published": "Thu, 03 Apr 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "2eae253da55b55d9",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "NVIDIA Dynamo, A Low-Latency Distributed Inference Framework for Scaling Reasoning AI Models - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMi1AFBVV95cUxOZFlYTlRURWtqOFZvXzJLZTdqTVZrNmgwdGFVT1hvYWtkQjE1QzBNb1hmNWhSY3h0eTZxc016QlZqeXAtd3VUMFBwSGVqSXBlcDBzQXZ3VndTTVdPVm9PUGpMOVNkN0FjX3hkdXhSSFdRV2hTSU1Kb1c3M0VJTXpqbjhSWVdIMnhqNDctR0VzTXRIOFNQVndSamlqZU4wcDJhYlpqSDRiTDlxazRpaVV4WnI4bnB0dVgzWEFCd2J5WkFBWGRnUFNrZjFxME5aSFJWek91RA?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi1AFBVV95cUxOZFlYTlRURWtqOFZvXzJLZTdqTVZrNmgwdGFVT1hvYWtkQjE1QzBNb1hmNWhSY3h0eTZxc016QlZqeXAtd3VUMFBwSGVqSXBlcDBzQXZ3VndTTVdPVm9PUGpMOVNkN0FjX3hkdXhSSFdRV2hTSU1Kb1c3M0VJTXpqbjhSWVdIMnhqNDctR0VzTXRIOFNQVndSamlqZU4wcDJhYlpqSDRiTDlxazRpaVV4WnI4bnB0dVgzWEFCd2J5WkFBWGRnUFNrZjFxME5aSFJWek91RA?oc=5\" target=\"_blank\">NVIDIA Dynamo, A Low-Latency Distributed Inference Framework for Scaling Reasoning AI Models</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Tue, 18 Mar 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f36b56b3f58119a0",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Fastest Speculative Decoding in vLLM with Arctic Inference and Arctic Training - Snowflake",
    "url": "https://news.google.com/rss/articles/CBMijAFBVV95cUxQa0h6M1hwQVdhMVpSc2NuQ0l3a0I5NUNJX3ViMzBnelcwNUVpVWpQSVdZZldOcGJRMURoYU56RjJwY0poVjZBU2JTZk04cV9zTzBBTG5UaXc4elliaEd3b25CYzl0ZlFUbEdzN2JlRnFWNUVGWVZLVVh3dy12b2V3OGlsUWZuQVRSVlRRNQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMijAFBVV95cUxQa0h6M1hwQVdhMVpSc2NuQ0l3a0I5NUNJX3ViMzBnelcwNUVpVWpQSVdZZldOcGJRMURoYU56RjJwY0poVjZBU2JTZk04cV9zTzBBTG5UaXc4elliaEd3b25CYzl0ZlFUbEdzN2JlRnFWNUVGWVZLVVh3dy12b2V3OGlsUWZuQVRSVlRRNQ?oc=5\" target=\"_blank\">Fastest Speculative Decoding in vLLM with Arctic Inference and Arctic Training</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Snowflake</font>",
    "published": "Thu, 01 May 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "895e36c479cade37",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "From LLMs to image generation: Accelerate inference workloads with AI Hypercomputer - Google Cloud",
    "url": "https://news.google.com/rss/articles/CBMirwFBVV95cUxNWkJMWmg1a1N4Z0FSY1JzQ1ZhQVhXLWdOQWRkVE9wYUJyU0dmWXIxMXhoN1N5Y0dUcmd1dXFrRFRMRmV4Wi1sS042WXZ5R3QwNXhVVUdDNDdDbjVVeVhfaHY5WG42WXcyb1B6NjdNcWY5c3pvSzFCMlRZVE1MckFJQUw0Qmt1ODlDUG1KRWd5YkxMZUJibXVSU3luMC1FdzVlaHkyQjBRd1BHTjgtelpF?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMirwFBVV95cUxNWkJMWmg1a1N4Z0FSY1JzQ1ZhQVhXLWdOQWRkVE9wYUJyU0dmWXIxMXhoN1N5Y0dUcmd1dXFrRFRMRmV4Wi1sS042WXZ5R3QwNXhVVUdDNDdDbjVVeVhfaHY5WG42WXcyb1B6NjdNcWY5c3pvSzFCMlRZVE1MckFJQUw0Qmt1ODlDUG1KRWd5YkxMZUJibXVSU3luMC1FdzVlaHkyQjBRd1BHTjgtelpF?oc=5\" target=\"_blank\">From LLMs to image generation: Accelerate inference workloads with AI Hypercomputer</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Google Cloud</font>",
    "published": "Fri, 09 May 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "3a438f5575a98625",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "NVIDIA Dynamo Adds Support for AWS Services to Deliver Cost-Efficient Inference at Scale | NVIDIA Technical Blog - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMiwAFBVV95cUxQSGJwVkxfLXhuaGJSMG1kX2VkYnNlS2NQSm42dWJSdTJsWnljaEdZR0dGdkFhQ1hQOWpweF9KTDFtcEdZUlFSOFNoNVNDeEFpNzF3cHVLWEsxbkFscWlLM2tINGk1aWx0VmZOTEJnMFVVbExZRXdFTnAyT1R4UmFXZ3NjakRsYnpFQUItYUVTLTNvS2VhektnMWFBZGFMM2dTR3JkZnI5YXphTWVUZFFwYTVHRlZURmtxcTdwNDc1QUM?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiwAFBVV95cUxQSGJwVkxfLXhuaGJSMG1kX2VkYnNlS2NQSm42dWJSdTJsWnljaEdZR0dGdkFhQ1hQOWpweF9KTDFtcEdZUlFSOFNoNVNDeEFpNzF3cHVLWEsxbkFscWlLM2tINGk1aWx0VmZOTEJnMFVVbExZRXdFTnAyT1R4UmFXZ3NjakRsYnpFQUItYUVTLTNvS2VhektnMWFBZGFMM2dTR3JkZnI5YXphTWVUZFFwYTVHRlZURmtxcTdwNDc1QUM?oc=5\" target=\"_blank\">NVIDIA Dynamo Adds Support for AWS Services to Deliver Cost-Efficient Inference at Scale | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Tue, 15 Jul 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "5c8c94974f7012f8",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "A Look at AIBrix, an Open Source LLM Inference Platform - The New Stack",
    "url": "https://news.google.com/rss/articles/CBMihAFBVV95cUxQeFc5UlB6eld6QWoxXzlfY0pSYUNxZ1A0b0psNDFLM3pZLWxmNDhrNTJPZWVRNlROb3FqQjFKWV9HSmlwbjdicmNTb1NaQUhmVnRFQmlVd290Y1lzRDZ2aldzQk4xQ3RVZUNuWkJGTFRwS21mYm5YMkdlWUVuX2t2VjZZcXQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMihAFBVV95cUxQeFc5UlB6eld6QWoxXzlfY0pSYUNxZ1A0b0psNDFLM3pZLWxmNDhrNTJPZWVRNlROb3FqQjFKWV9HSmlwbjdicmNTb1NaQUhmVnRFQmlVd290Y1lzRDZ2aldzQk4xQ3RVZUNuWkJGTFRwS21mYm5YMkdlWUVuX2t2VjZZcXQ?oc=5\" target=\"_blank\">A Look at AIBrix, an Open Source LLM Inference Platform</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The New Stack</font>",
    "published": "Fri, 04 Apr 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "80df6e9abb99fa53",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Pliops XDP LightningAI Supercharges KV Cache to Optimize LLM Inference with NVIDIA Dynamo - StorageReview.com",
    "url": "https://news.google.com/rss/articles/CBMixAFBVV95cUxOYVo5T3R2dkFUWUxtdFhpNWZ0bjFiMElYbUJoalJLUmpEd3J1OHN0dVZjQVl0SXRseU1qdEowMGdMbGJYNURycUtkaVNRNTE3R2lPNnhUamYzT2lNWEhWckJ4Q05lR2lQelhpOGZZVHVsSHl5VWdUOWdqbS1HbXN0eUJ6dS1MY2VNXzBMUF9SYUtoQ01vU1ljSTlqS2dRNWtOU19QTUhLOWcwZ2pRLXFVMFZUbk5ISlp2R0UtODlWMnhyZGl30gHKAUFVX3lxTE5ZckM5Y0VSb3JxeGVhbF9EemxlRHRUalU2c0gtSk5DTjNtdWYyZ0dvblg5cWNsbmplYm5OcmI2ZDZfMVBiVGZ2RGpxMTdpT0RPZEpzVVdJOWwyMkJzeWZKczZZTGJfbEtRSFZPQktqY3BOWFJDVWNpb09QMWdLa21oRndrWUV6cE5zcWNoRXdBb1J3R09yU3RJT3plQVRmb2NMelh6ZGU5MXZUeG9ldmJfZVVqbko1dmVmZzhEd003MzRoOHJVRDR0MlE?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMixAFBVV95cUxOYVo5T3R2dkFUWUxtdFhpNWZ0bjFiMElYbUJoalJLUmpEd3J1OHN0dVZjQVl0SXRseU1qdEowMGdMbGJYNURycUtkaVNRNTE3R2lPNnhUamYzT2lNWEhWckJ4Q05lR2lQelhpOGZZVHVsSHl5VWdUOWdqbS1HbXN0eUJ6dS1MY2VNXzBMUF9SYUtoQ01vU1ljSTlqS2dRNWtOU19QTUhLOWcwZ2pRLXFVMFZUbk5ISlp2R0UtODlWMnhyZGl30gHKAUFVX3lxTE5ZckM5Y0VSb3JxeGVhbF9EemxlRHRUalU2c0gtSk5DTjNtdWYyZ0dvblg5cWNsbmplYm5OcmI2ZDZfMVBiVGZ2RGpxMTdpT0RPZEpzVVdJOWwyMkJzeWZKczZZTGJfbEtRSFZPQktqY3BOWFJDVWNpb09QMWdLa21oRndrWUV6cE5zcWNoRXdBb1J3R09yU3RJT3plQVRmb2NMelh6ZGU5MXZUeG9ldmJfZVVqbko1dmVmZzhEd003MzRoOHJVRDR0MlE?oc=5\" target=\"_blank\">Pliops XDP LightningAI Supercharges KV Cache to Optimize LLM Inference with NVIDIA Dynamo</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">StorageReview.com</font>",
    "published": "Tue, 20 May 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "644869f48e2b84a6",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMiigFBVV95cUxNUzJjSXlkc2pxNDN0RTFIVWRUcEFETUJEeVlmeV95RHBpWDFBQzB5TG0xX3ZMNEtacDlKLTBqTXZGWWU3TlQ5MlRTUGZEaWVzcTU5bXM4Vnd0NmRIZy1DMlZMdV9LRFQ3dXY5Z1VaeE9hV1ZwQ19HZUJYRjJqeHd6MlV2RC14b2U0eEE?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiigFBVV95cUxNUzJjSXlkc2pxNDN0RTFIVWRUcEFETUJEeVlmeV95RHBpWDFBQzB5TG0xX3ZMNEtacDlKLTBqTXZGWWU3TlQ5MlRTUGZEaWVzcTU5bXM4Vnd0NmRIZy1DMlZMdV9LRFQ3dXY5Z1VaeE9hV1ZwQ19HZUJYRjJqeHd6MlV2RC14b2U0eEE?oc=5\" target=\"_blank\">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Fri, 17 Nov 2023 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8b82e5565cf054fd",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Integrate and Deploy Tongyi Qwen3 Models into Production Applications with NVIDIA - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMitwFBVV95cUxQM0lnMkloWnRaZ0N2ZHhMLVZJZ0NxT2luSzM2cTlIV1Q5V2ctektxLVZPUkNlVFdFNk9wUUZIRS13bkZtaXNDdlBycGhYWkE4aFFRYS10VDhVcC1ZOWxwYUl5S21OVXFnOXF3QnFLQTYySjJDR0xKbjZseDBHcW83UkNpTWRMcGdmbVZjTENUc3RWamVhVnJZR1BEQW9WaWJlZEZWT0VwREdmS1V5SHI3Q0I3eW9LcGs?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMitwFBVV95cUxQM0lnMkloWnRaZ0N2ZHhMLVZJZ0NxT2luSzM2cTlIV1Q5V2ctektxLVZPUkNlVFdFNk9wUUZIRS13bkZtaXNDdlBycGhYWkE4aFFRYS10VDhVcC1ZOWxwYUl5S21OVXFnOXF3QnFLQTYySjJDR0xKbjZseDBHcW83UkNpTWRMcGdmbVZjTENUc3RWamVhVnJZR1BEQW9WaWJlZEZWT0VwREdmS1V5SHI3Q0I3eW9LcGs?oc=5\" target=\"_blank\">Integrate and Deploy Tongyi Qwen3 Models into Production Applications with NVIDIA</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Fri, 02 May 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f3fa90dba1343593",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Pliops Announces Collaboration with vLLM Production Stack to Enhance LLM Inference Performance - GlobeNewswire",
    "url": "https://news.google.com/rss/articles/CBMi-gFBVV95cUxQSHVDZ0N6N2liRVhtZ3JyVktjYk16VTJTQkNZbEt3UHhOZVFhbkVBb3lNc29zMW1MYXdkMGIyYWVXSnFNRER4dlE2RlNOd0pSMnpXS0cybVFGR2dTSGUxaDRHbGppcjBDbWFKZF9lQUlFWFEtZWxUdkNRR2RjZkM2ODdpemdDVS02RzhZMFBCanEzY19mSGtvazJLbUN2SW5STm1lX2dFQzRoZUNLUHVtOWROQnhLazFpVVVINGlMSkpBSkFwZ0ZlSDNhVmV2WmRmbkltQjRPNkxTWWpKczRwZnRZUWRyWVBQMHNZOXNxV1RsbXN0Vm5HdC1n?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi-gFBVV95cUxQSHVDZ0N6N2liRVhtZ3JyVktjYk16VTJTQkNZbEt3UHhOZVFhbkVBb3lNc29zMW1MYXdkMGIyYWVXSnFNRER4dlE2RlNOd0pSMnpXS0cybVFGR2dTSGUxaDRHbGppcjBDbWFKZF9lQUlFWFEtZWxUdkNRR2RjZkM2ODdpemdDVS02RzhZMFBCanEzY19mSGtvazJLbUN2SW5STm1lX2dFQzRoZUNLUHVtOWROQnhLazFpVVVINGlMSkpBSkFwZ0ZlSDNhVmV2WmRmbkltQjRPNkxTWWpKczRwZnRZUWRyWVBQMHNZOXNxV1RsbXN0Vm5HdC1n?oc=5\" target=\"_blank\">Pliops Announces Collaboration with vLLM Production Stack to Enhance LLM Inference Performance</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">GlobeNewswire</font>",
    "published": "Wed, 12 Mar 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "c33276e070120850",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Spotlight: NAVER Place Optimizes SLM-Based Vertical Services with NVIDIA TensorRT-LLM | NVIDIA Technical Blog - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMiuwFBVV95cUxNT0VMakpFbDNPRExCb1c1SFNjWnhWUW9BcnA1YWVXLUU0MXJVdVpUMk9TSzRoN2x5cXlwbVVmTi1jNjh6QlBMV1V2bV9kdmVkaFB0OWpVSGtHRFBEVTUxWVFGQmpvSEU4d2RRbWNtWVlpblVFLW5Mb01BLVBjT2Zsd3lidzJYVDRPMV9WczlQWW0xUHdXWjRxcEtuTTBBMkdEME9SaG9haU9Kc1lncWtQUXhzN01pMU9jNVgw?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiuwFBVV95cUxNT0VMakpFbDNPRExCb1c1SFNjWnhWUW9BcnA1YWVXLUU0MXJVdVpUMk9TSzRoN2x5cXlwbVVmTi1jNjh6QlBMV1V2bV9kdmVkaFB0OWpVSGtHRFBEVTUxWVFGQmpvSEU4d2RRbWNtWVlpblVFLW5Mb01BLVBjT2Zsd3lidzJYVDRPMV9WczlQWW0xUHdXWjRxcEtuTTBBMkdEME9SaG9haU9Kc1lncWtQUXhzN01pMU9jNVgw?oc=5\" target=\"_blank\">Spotlight: NAVER Place Optimizes SLM-Based Vertical Services with NVIDIA TensorRT-LLM | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Fri, 28 Feb 2025 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f81195a4013cf598",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMipwFBVV95cUxPbXVlbFpWamhCd25GYnJ3YnNtT3EwRWZnTzVpejhuTkswb19YSTh6QzRZeTRhV05QZy1KS2xGdWRXNVJ2WGl4QUJ3UDNSRjZDay1ERjJkTlZ1a3hic1lpQXVZcGNxT0JpREl6Zmg2NS1hYVNoZ0d3U1R3Z2JkaW5zZW43cEhkUXBLNTRMTENELVJIV1Q0ZTBJMEdmQUZWQnBtaUVoNzFsWQ?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMipwFBVV95cUxPbXVlbFpWamhCd25GYnJ3YnNtT3EwRWZnTzVpejhuTkswb19YSTh6QzRZeTRhV05QZy1KS2xGdWRXNVJ2WGl4QUJ3UDNSRjZDay1ERjJkTlZ1a3hic1lpQXVZcGNxT0JpREl6Zmg2NS1hYVNoZ0d3U1R3Z2JkaW5zZW43cEhkUXBLNTRMTENELVJIV1Q0ZTBJMEdmQUZWQnBtaUVoNzFsWQ?oc=5\" target=\"_blank\">Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Thu, 19 Oct 2023 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "f4ddf67bdc55b615",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "NVIDIA Blackwell Delivers World-Record DeepSeek-R1 Inference Performance - NVIDIA Developer",
    "url": "https://news.google.com/rss/articles/CBMiqwFBVV95cUxQYVVzaWJOTG9tMWhJZmJ5NXUwaFdHaEUtMThVU0xsSDhkaklFb3lWakxkdTNTamdlT2RxTkp4b09mMC0tUGFMWU80X05SR2h2ZkJzcG1NSWdaWDBWMmY4OVFsRDZFM0dOZTVHZzB4RGd6aDJqcXc3Ql9XRF9MSkcwekFHbHd3aV9wZlBWaHhhbTFKU1puLWVzMDY4S1RXMEFhMEI5TDduRDU5Rms?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMiqwFBVV95cUxQYVVzaWJOTG9tMWhJZmJ5NXUwaFdHaEUtMThVU0xsSDhkaklFb3lWakxkdTNTamdlT2RxTkp4b09mMC0tUGFMWU80X05SR2h2ZkJzcG1NSWdaWDBWMmY4OVFsRDZFM0dOZTVHZzB4RGd6aDJqcXc3Ql9XRF9MSkcwekFHbHd3aV9wZlBWaHhhbTFKU1puLWVzMDY4S1RXMEFhMEI5TDduRDU5Rms?oc=5\" target=\"_blank\">NVIDIA Blackwell Delivers World-Record DeepSeek-R1 Inference Performance</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
    "published": "Tue, 18 Mar 2025 07:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  },
  {
    "id": "8f1301c97e20cceb",
    "source": "search_llm_ops_news",
    "source_weight": 0.8,
    "title": "Scaling your LLM inference workloads: multi-node deployment with TensorRT-LLM and Triton on Amazon EKS - Amazon Web Services",
    "url": "https://news.google.com/rss/articles/CBMi0AFBVV95cUxOQVg3RDdidllJYzVPM3RWMHBFbG1nVmxrbW1hMlhhUkNLcVEtck44R1RURU5Bclg4UGt3WXJlTnh0UXlnRW9RRVNXMkRneUJCc1JCZ0s2VmdydHp4cEZVSUx2d3ZERU5VeWFOeS05NlhSenljMG1IR1Juc250azFGZ25ST1NBMGRFMURvWXNTaW5XNFVrY3pBZ2JCdS1rSUgyN2xMblY4T1h2cE9NakoyRGFjc1VYaE1XWEl6UlBkenQtSk1GSHAtQko3eV9fU1Jh?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMi0AFBVV95cUxOQVg3RDdidllJYzVPM3RWMHBFbG1nVmxrbW1hMlhhUkNLcVEtck44R1RURU5Bclg4UGt3WXJlTnh0UXlnRW9RRVNXMkRneUJCc1JCZ0s2VmdydHp4cEZVSUx2d3ZERU5VeWFOeS05NlhSenljMG1IR1Juc250azFGZ25ST1NBMGRFMURvWXNTaW5XNFVrY3pBZ2JCdS1rSUgyN2xMblY4T1h2cE9NakoyRGFjc1VYaE1XWEl6UlBkenQtSk1GSHAtQko3eV9fU1Jh?oc=5\" target=\"_blank\">Scaling your LLM inference workloads: multi-node deployment with TensorRT-LLM and Triton on Amazon EKS</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services</font>",
    "published": "Mon, 02 Dec 2024 08:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00"
  }
]