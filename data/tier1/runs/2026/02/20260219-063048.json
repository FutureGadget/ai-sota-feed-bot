{
  "run_at": "2026-02-19T06:30:48.690300+00:00",
  "item_count": 684,
  "items": [
    {
      "id": "5713ae1ccd7185d1",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.47",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.47",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed FileWriteTool line counting to preserve intentional trailing blank lines instead of stripping them with <code>trimEnd()</code>.</li>\n<li>Fixed Windows terminal rendering bugs caused by <code>os.EOL</code> (<code>\\r\\n</code>) in display code — line counts now show correct values instead of always showing 1 on Windows.</li>\n<li>Improved VS Code plan preview: auto-updates as Claude iterates, enables commenting only when the plan is ready for review, and keeps the preview open when rejecting so Claude can revise.</li>\n<li>Fixed a bug where bold and colored text in markdown output could shift to the wrong characters on Windows due to <code>\\r\\n</code> line endings.</li>\n<li>Fixed compaction failing when conversation contains many PDF documents by stripping document blocks alongside images before sending to the compaction API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26188\">#26188</a>)</li>\n<li>Improved memory usage in long-running sessions by releasing API stream buffers, agent context, and skill state after use</li>\n<li>Improved startup performance by deferring SessionStart hook execution, reducing time-to-interactive by ~500ms.</li>\n<li>Fixed an issue where bash tool output was silently discarded on Windows when using MSYS2 or Cygwin shells.</li>\n<li>Improved performance of <code>@</code> file mentions - file suggestions now appear faster by pre-warming the index on startup and using session-based caching with background refresh.</li>\n<li>Improved memory usage by trimming agent task message history after tasks complete</li>\n<li>Improved memory usage during long agent sessions by eliminating O(n²) message accumulation in progress updates</li>\n<li>Fixed the bash permission classifier to validate that returned match descriptions correspond to actual input rules, preventing hallucinated descriptions from incorrectly granting permissions</li>\n<li>Fixed user-defined agents only loading one file on NFS/FUSE filesystems that report zero inodes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26044\">#26044</a>)</li>\n<li>Fixed plugin agent skills silently failing to load when referenced by bare name instead of fully-qualified plugin name (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25834\">#25834</a>)</li>\n<li>Search patterns in collapsed tool results are now displayed in quotes for clarity</li>\n<li>Windows: Fixed CWD tracking temp files never being cleaned up, causing them to accumulate indefinitely (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/17600\">#17600</a>)</li>\n<li>Use <code>ctrl+f</code> to kill all background agents instead of double-pressing ESC. Background agents now continue running when you press ESC to cancel the main thread, giving you more control over agent lifecycle.</li>\n<li>Fixed API 400 errors (\"thinking blocks cannot be modified\") that occurred in sessions with concurrent agents, caused by interleaved streaming content blocks preventing proper message merging.</li>\n<li>Simplified teammate navigation to use only Shift+Down (with wrapping) instead of both Shift+Up and Shift+Down.</li>\n<li>Fixed an issue where a single file write/edit error would abort all other parallel file write/edit operations. Independent file mutations now complete even when a sibling fails.</li>\n<li>Added <code>last_assistant_message</code> field to Stop and SubagentStop hook inputs, providing the final assistant response text so hooks can access it without parsing transcript files.</li>\n<li>Fixed custom session titles set via <code>/rename</code> being lost after resuming a conversation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/23610\">#23610</a>)</li>\n<li>Fixed collapsed read/search hint text overflowing on narrow terminals by truncating from the start.</li>\n<li>Fixed an issue where bash commands with backslash-newline continuation lines (e.g., long commands split across multiple lines with <code>\\</code>) would produce spurious empty arguments, potentially breaking command execution.</li>\n<li>Fixed built-in slash commands (<code>/help</code>, <code>/model</code>, <code>/compact</code>, etc.) being hidden from the autocomplete dropdown when many user skills are installed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/22020\">#22020</a>)</li>\n<li>Fixed MCP servers not appearing in the MCP Management Dialog after deferred loading</li>\n<li>Fixed session name persisting in status bar after <code>/clear</code> command (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26082\">#26082</a>)</li>\n<li>Fixed crash when a skill's <code>name</code> or <code>description</code> in SKILL.md frontmatter is a bare number (e.g., <code>name: 3000</code>) — the value is now properly coerced to a string (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25837\">#25837</a>)</li>\n<li>Fixed /resume silently dropping sessions when the first message exceeds 16KB or uses array-format content (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25721\">#25721</a>)</li>\n<li>Added <code>chat:newline</code> keybinding action for configurable multi-line input (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26075\">#26075</a>)</li>\n<li>Added <code>added_dirs</code> to the statusline JSON <code>workspace</code> section, exposing directories added via <code>/add-dir</code> to external scripts (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26096\">#26096</a>)</li>\n<li>Fixed <code>claude doctor</code> misclassifying mise and asdf-managed installations as native installs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26033\">#26033</a>)</li>\n<li>Fixed zsh heredoc failing with \"read-only file system\" error in sandboxed commands (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25990\">#25990</a>)</li>\n<li>Fixed agent progress indicator showing inflated tool use count (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26023\">#26023</a>)</li>\n<li>Fixed image pasting not working on WSL2 systems where Windows copies images as BMP format (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25935\">#25935</a>)</li>\n<li>Fixed background agent results returning raw transcript data instead of the agent's final answer (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26012\">#26012</a>)</li>\n<li>Fixed Warp terminal incorrectly prompting for Shift+Enter setup when it supports it natively (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25957\">#25957</a>)</li>\n<li>Fixed CJK wide characters causing misaligned timestamps and layout elements in the TUI (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26084\">#26084</a>)</li>\n<li>Fixed custom agent <code>model</code> field in <code>.claude/agents/*.md</code> being ignored when spawning team teammates (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26064\">#26064</a>)</li>\n<li>Fixed plan mode being lost after context compaction, causing the model to switch from planning to implementation mode (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26061\">#26061</a>)</li>\n<li>Fixed <code>alwaysThinkingEnabled: true</code> in settings.json not enabling thinking mode on Bedrock and Vertex providers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26074\">#26074</a>)</li>\n<li>Fixed <code>tool_decision</code> OTel telemetry event not being emitted in headless/SDK mode (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26059\">#26059</a>)</li>\n<li>Fixed session name being lost after context compaction — renamed sessions now preserve their custom title through compaction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26121\">#26121</a>)</li>\n<li>Increased initial session count in resume picker from 10 to 50 for faster session discovery (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26123\">#26123</a>)</li>\n<li>Windows: fixed worktree session matching when drive letter casing differs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26123\">#26123</a>)</li>\n<li>Fixed <code>/resume &lt;session-id&gt;</code> failing to find sessions whose first message exceeds 16KB (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25920\">#25920</a>)</li>\n<li>Fixed \"Always allow\" on multiline bash commands creating invalid permission patterns that corrupt settings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25909\">#25909</a>)</li>\n<li>Fixed React crash (error <a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/31\">#31</a>) when a skill's <code>argument-hint</code> in SKILL.md frontmatter uses YAML sequence syntax (e.g., <code>[topic: foo | bar]</code>) — the value is now properly coerced to a string (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25826\">#25826</a>)</li>\n<li>Fixed crash when using <code>/fork</code> on sessions that used web search — null entries in search results from transcript deserialization are now handled gracefully (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25811\">#25811</a>)</li>\n<li>Fixed read-only git commands triggering FSEvents file watcher loops on macOS by adding --no-optional-locks flag (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25750\">#25750</a>)</li>\n<li>Fixed custom agents and skills not being discovered when running from a git worktree — project-level <code>.claude/agents/</code> and <code>.claude/skills/</code> from the main repository are now included (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25816\">#25816</a>)</li>\n<li>Fixed non-interactive subcommands like <code>claude doctor</code> and <code>claude plugin validate</code> being blocked inside nested Claude sessions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25803\">#25803</a>)</li>\n<li>Windows: Fixed the same CLAUDE.md file being loaded twice when drive letter casing differs between paths (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25756\">#25756</a>)</li>\n<li>Fixed inline code spans in markdown being incorrectly parsed as bash commands (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25792\">#25792</a>)</li>\n<li>Fixed teammate spinners not respecting custom spinnerVerbs from settings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25748\">#25748</a>)</li>\n<li>Fixed shell commands permanently failing after a command deletes its own working directory (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26136\">#26136</a>)</li>\n<li>Fixed hooks (PreToolUse, PostToolUse) silently failing to execute on Windows by using Git Bash instead of cmd.exe (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25981\">#25981</a>)</li>\n<li>Fixed LSP <code>findReferences</code> and other location-based operations returning results from gitignored files (e.g., <code>node_modules/</code>, <code>venv/</code>) (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26051\">#26051</a>)</li>\n<li>Moved config backup files from home directory root to <code>~/.claude/backups/</code> to reduce home directory clutter (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26130\">#26130</a>)</li>\n<li>Fixed sessions with large first prompts (&gt;16KB) disappearing from the /resume list (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26140\">#26140</a>)</li>\n<li>Fixed shell functions with double-underscore prefixes (e.g., <code>__git_ps1</code>) not being preserved across shell sessions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25824\">#25824</a>)</li>\n<li>Fixed spinner showing \"0 tokens\" counter before any tokens have been received (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26105\">#26105</a>)</li>\n<li>VSCode: Fixed conversation messages appearing dimmed while the AskUserQuestion dialog is open (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26078\">#26078</a>)</li>\n<li>Fixed background tasks failing in git worktrees due to remote URL resolution reading from worktree-specific gitdir instead of the main repository config (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26065\">#26065</a>)</li>\n<li>Fixed Right Alt key leaving visible <code>[25~</code> escape sequence residue in the input field on Windows/Git Bash terminals (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25943\">#25943</a>)</li>\n<li>The <code>/rename</code> command now updates the terminal tab title by default (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25789\">#25789</a>)</li>\n<li>Fixed Edit tool silently corrupting Unicode curly quotes (\\u201c\\u201d \\u2018\\u2019) by replacing them with straight quotes when making edits (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26141\">#26141</a>)</li>\n<li>Fixed OSC 8 hyperlinks only being clickable on the first line when link text wraps across multiple terminal lines.</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-18T21:38:45Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.884,
      "tier1_quick_score": 3.921
    },
    {
      "id": "56c7f75b96578bf3",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.105.0-alpha.3",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.105.0-alpha.3",
      "summary": "<p>Release 0.105.0-alpha.3</p>",
      "image_url": "",
      "published": "2026-02-18T19:57:50Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.864,
      "tier1_quick_score": 3.901
    },
    {
      "id": "15e64d341c1e864b",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.105.0-alpha.2",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.105.0-alpha.2",
      "summary": "<p>Release 0.105.0-alpha.2</p>",
      "image_url": "",
      "published": "2026-02-18T14:25:55Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.8,
      "tier1_quick_score": 3.837
    },
    {
      "id": "8addf19535c56559",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.105.0-alpha.1",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.105.0-alpha.1",
      "summary": "<p>Release 0.105.0-alpha.1</p>",
      "image_url": "",
      "published": "2026-02-18T08:32:07Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.737,
      "tier1_quick_score": 3.774
    },
    {
      "id": "c58d7596e7e871ef",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.104.0",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.104.0",
      "summary": "<h2>New Features</h2>\n<ul>\n<li>Added <code>WS_PROXY</code>/<code>WSS_PROXY</code> environment support (including lowercase variants) for websocket proxying in the network proxy. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11784\">#11784</a>)</li>\n<li>App-server v2 now emits notifications when threads are archived or unarchived, enabling clients to react without polling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12030\">#12030</a>)</li>\n<li>Protocol/core now carry distinct approval IDs for command approvals to support multiple approvals within a single shell command execution flow. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12051\">#12051</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li><code>Ctrl+C</code>/<code>Ctrl+D</code> now cleanly exits the cwd-change prompt during resume/fork flows instead of implicitly selecting an option. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12040\">#12040</a>)</li>\n<li>Reduced false-positive safety-check downgrade behavior by relying on the response header model (and websocket top-level events) rather than the response body model slug. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12061\">#12061</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Updated docs and schemas to cover websocket proxy configuration, new thread archive/unarchive notifications, and the command approval ID plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11784\">#11784</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12030\">#12030</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12051\">#12051</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Made the Rust release workflow resilient to <code>npm publish</code> attempts for an already-published version. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12044\">#12044</a>)</li>\n<li>Standardized remote compaction test mocking and refreshed related snapshots to align with the default production-shaped behavior. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12050\">#12050</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.103.0...rust-v0.104.0\"><tt>rust-v0.103.0...rust-v0.104.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11784\">#11784</a> feat(network-proxy): add websocket proxy env support <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12044\">#12044</a> don't fail if an npm publish attempt is for an existing version. <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12040\">#12040</a> tui: exit session on Ctrl+C in cwd change prompt <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12030\">#12030</a> app-server: Emit thread archive/unarchive notifications <a class=\"user-mention notranslate\" href=\"https://github.com/euroelessar\">@euroelessar</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12061\">#12061</a> Chore: remove response model check and rely on header model for downgrade <a class=\"user-mention notranslate\" href=\"https://github.com/shijie-oai\">@shijie-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12051\">#12051</a> feat(core): plumb distinct approval ids for command approvals <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12050\">#12050</a> Unify remote compaction snapshot mocks around default endpoint behavior <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n</ul>",
      "image_url": "",
      "published": "2026-02-18T07:13:02Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.724,
      "tier1_quick_score": 3.761
    },
    {
      "id": "cbfa40d662dbc863",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.104.0-alpha.1",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.104.0-alpha.1",
      "summary": "<p>Release 0.104.0-alpha.1</p>",
      "image_url": "",
      "published": "2026-02-18T02:48:48Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.681,
      "tier1_quick_score": 3.718
    },
    {
      "id": "1e3b25e389035e2c",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing OpenAI for India",
      "url": "https://openai.com/index/openai-for-india",
      "summary": "OpenAI for India expands AI access across the country—building local infrastructure, powering enterprises, and advancing workforce skills.",
      "image_url": "",
      "published": "Wed, 18 Feb 2026 21:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.876,
      "tier1_quick_score": 3.713
    },
    {
      "id": "071cd72262b5c321",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.103.0",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.103.0",
      "summary": "<h2>New Features</h2>\n<ul>\n<li>App listing responses now include richer app details (<code>app_metadata</code>, branding, and labels), so clients can render more complete app cards without extra requests. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11706\">#11706</a>)</li>\n<li>Commit co-author attribution now uses a Codex-managed <code>prepare-commit-msg</code> hook, with <code>command_attribution</code> override support (default label, custom label, or disable). (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11617\">#11617</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Removed the <code>remote_models</code> feature flag to prevent fallback model metadata when it was disabled, improving model selection reliability and performance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11699\">#11699</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Updated Rust dependencies (<code>clap</code>, <code>env_logger</code>, <code>arc-swap</code>) and refreshed Bazel lock state as routine maintenance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11888\">#11888</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11889\">#11889</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11890\">#11890</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12032\">#12032</a>)</li>\n<li>Reverted the Rust toolchain bump to <code>1.93.1</code> after CI breakage. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11886\">#11886</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12035\">#12035</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.102.0...rust-v0.103.0\"><tt>rust-v0.102.0...rust-v0.103.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11699\">#11699</a> chore: rm remote models fflag <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11706\">#11706</a> [apps] Expose more fields from apps listing endpoints. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11890\">#11890</a> chore(deps): bump arc-swap from 1.8.0 to 1.8.2 in /codex-rs <a class=\"user-mention notranslate\" href=\"https://github.com/dependabot\">@dependabot</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11886\">#11886</a> chore(deps): bump rust-toolchain from 1.93.0 to 1.93.1 in /codex-rs <a class=\"user-mention notranslate\" href=\"https://github.com/dependabot\">@dependabot</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12032\">#12032</a> chore: just bazel-lock-update <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11888\">#11888</a> chore(deps): bump clap from 4.5.56 to 4.5.58 in /codex-rs <a class=\"user-mention notranslate\" href=\"https://github.com/dependabot\">@dependabot</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11889\">#11889</a> chore(deps): bump env_logger from 0.11.8 to 0.11.9 in /codex-rs <a class=\"user-mention notranslate\" href=\"https://github.com/dependabot\">@dependabot</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11617\">#11617</a> Use prompt-based co-author attribution with config override <a class=\"user-mention notranslate\" href=\"https://github.com/gabec-openai\">@gabec-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12035\">#12035</a> Revert \"chore(deps): bump rust-toolchain from 1.93.0 to 1.93.1 in /co…dex-rs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11886\">#11886</a>)\" <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n</ul>",
      "image_url": "",
      "published": "2026-02-17T23:03:06Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.646,
      "tier1_quick_score": 3.683
    },
    {
      "id": "39ce43ce78e15e31",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.103.0-alpha.1",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.103.0-alpha.1",
      "summary": "<p>Release 0.103.0-alpha.1</p>",
      "image_url": "",
      "published": "2026-02-17T22:11:00Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.638,
      "tier1_quick_score": 3.675
    },
    {
      "id": "0ab7556afc26ee30",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.102.0",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0",
      "summary": "<h2>New Features</h2>\n<ul>\n<li>Added a more unified permissions flow, including clearer permissions history in the TUI and a slash command to grant sandbox read access when directories are blocked. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11633\">#11633</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11512\">#11512</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11550\">#11550</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11639\">#11639</a>)</li>\n<li>Introduced structured network approval handling, with richer host/protocol context shown directly in approval prompts. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11672\">#11672</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11674\">#11674</a>)</li>\n<li>Expanded app-server fuzzy file search with explicit session-complete signaling so clients can stop loading indicators reliably. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10268\">#10268</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11773\">#11773</a>)</li>\n<li>Added customizable multi-agent roles via config, including migration toward the new multi-agent naming/config surface. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11917\">#11917</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11982\">#11982</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11939\">#11939</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11918\">#11918</a>)</li>\n<li>Added a <code>model/rerouted</code> notification so clients can detect and render model reroute events explicitly. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12001\">#12001</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Fixed remote image attachments so they persist correctly across resume/backtrack and history replay in the TUI. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10590\">#10590</a>)</li>\n<li>Fixed a TUI accessibility regression where animation gating for screen reader users was not consistently respected. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11860\">#11860</a>)</li>\n<li>Fixed app-server thread resume behavior to correctly rejoin active in-memory threads and tighten invalid resume cases. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11756\">#11756</a>)</li>\n<li>Fixed <code>model/list</code> output to return full model data plus visibility metadata, avoiding unintended server-side filtering. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11793\">#11793</a>)</li>\n<li>Fixed several <code>js_repl</code> stability issues, including reset hangs, in-flight tool-call races, and a <code>view_image</code> panic path. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11932\">#11932</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11922\">#11922</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11800\">#11800</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11796\">#11796</a>)</li>\n<li>Fixed app integration edge cases in mention parsing and app list loading/filtering behavior. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11894\">#11894</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11518\">#11518</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11697\">#11697</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Updated contributor guidance to require snapshot coverage for user-visible TUI changes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10669\">#10669</a>)</li>\n<li>Updated docs/help text around Codex app and MCP command usage. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11926\">#11926</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11813\">#11813</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Improved developer log tooling with new <code>just log --search</code> and <code>just log --compact</code> modes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11995\">#11995</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11994\">#11994</a>)</li>\n<li>Updated vendored <code>rg</code> and tightened Bazel/Cargo lockfile sync checks to reduce dependency drift. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12007\">#12007</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11790\">#11790</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.101.0...rust-v0.102.0\"><tt>rust-v0.101.0...rust-v0.102.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10268\">#10268</a> app-server: add fuzzy search sessions for streaming file search <a class=\"user-mention notranslate\" href=\"https://github.com/nornagon-openai\">@nornagon-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11547\">#11547</a> Parse first order skill/connector mentions <a class=\"user-mention notranslate\" href=\"https://github.com/canvrno-oai\">@canvrno-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11227\">#11227</a> feat(app-server): experimental flag to persist extended history <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10672\">#10672</a> Add js_repl host helpers and exec end events <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11512\">#11512</a> add a slash command to grant sandbox read access to inaccessible directories <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11631\">#11631</a> chore(core) Deprecate approval_policy: on-failure <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11636\">#11636</a> Better error message for model limit hit. <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11633\">#11633</a> feat: introduce Permissions <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10669\">#10669</a> docs: require insta snapshot coverage for UI changes <a class=\"user-mention notranslate\" href=\"https://github.com/joshka-oai\">@joshka-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11645\">#11645</a> fix: skip review_start_with_detached_delivery_returns_new_thread_id o… <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11639\">#11639</a> [feat] add seatbelt permission files <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11622\">#11622</a> Remove absolute path in rollout_summary <a class=\"user-mention notranslate\" href=\"https://github.com/wendyjiao-openai\">@wendyjiao-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10671\">#10671</a> Add js_repl_tools_only model and routing restrictions <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11657\">#11657</a> app-server tests: disable shell_snapshot for review suite <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11646\">#11646</a> app-server: stabilize detached review start on Windows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11638\">#11638</a> fix(app-server): surface more helpful errors for json-rpc <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11417\">#11417</a> [apps] Add is_enabled to app info. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11630\">#11630</a> Add new apps_mcp_gateway <a class=\"user-mention notranslate\" href=\"https://github.com/canvrno-oai\">@canvrno-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11656\">#11656</a> Persist complete TurnContextItem state via canonical conversion <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11510\">#11510</a> Remove git commands from dangerous command checks <a class=\"user-mention notranslate\" href=\"https://github.com/joshka-oai\">@joshka-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11668\">#11668</a> feat(shell-tool-mcp): add patched zsh build pipeline <a class=\"user-mention notranslate\" href=\"https://github.com/nornagon-openai\">@nornagon-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11275\">#11275</a> Added a test to verify that feature flags that are enabled by default are stable <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11651\">#11651</a> Add cwd as an optional field to thread/list <a class=\"user-mention notranslate\" href=\"https://github.com/acrognale-oai\">@acrognale-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11660\">#11660</a> chore(approvals) More approvals scenarios <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11518\">#11518</a> [apps] Fix app loading logic. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11679\">#11679</a> fix: dont show NUX for upgrade-target models that are hidden <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11515\">#11515</a> Point Codex App tooltip links to app landing page <a class=\"user-mention notranslate\" href=\"https://github.com/joshka-oai\">@joshka-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11671\">#11671</a> chore(core) Restrict model-suggested rules <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11703\">#11703</a> fix(ci) lock rust toolchain at 1.93.0 to unblock <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11662\">#11662</a> feat(network-proxy): structured policy signaling and attempt correlation to core <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11709\">#11709</a> fix(shell-tool-mcp) build dependencies <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11618\">#11618</a> feat: add token usage on memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11722\">#11722</a> Lower missing rollout log level <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11712\">#11712</a> chore: streamline phase 2 <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11731\">#11731</a> feat: memories config <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11736\">#11736</a> feat: increase windows workers stack <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11739\">#11739</a> feat: add slug in name <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11745\">#11745</a> chore: move explorer to spark <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11748\">#11748</a> Fix memories output schema requirements <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11669\">#11669</a> core: limit search_tool_bm25 to Apps and clarify discovery guidance <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11755\">#11755</a> app-server-test-client websocket client and thread tools <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11663\">#11663</a> fix: reduce flakiness of compact_resume_after_second_compaction_preserves_history <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11667\">#11667</a> sandbox NUX metrics update <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11695\">#11695</a> Updated app bug report template <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11477\">#11477</a> feat: switch on dying sub-agents <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11711\">#11711</a> feat(tui): prevent macOS idle sleep while turns run <a class=\"user-mention notranslate\" href=\"https://github.com/yvolovich-cyber\">@yvolovich-cyber</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11686\">#11686</a> Report syntax errors in rules file <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11763\">#11763</a> Update read_path prompt <a class=\"user-mention notranslate\" href=\"https://github.com/zuxin-oai\">@zuxin-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11772\">#11772</a> chore: mini <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11697\">#11697</a> [apps] Improve app listing filtering. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11666\">#11666</a> Add js_repl kernel crash diagnostics <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11687\">#11687</a> support app usage analytics <a class=\"user-mention notranslate\" href=\"https://github.com/alexsong-oai\">@alexsong-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11769\">#11769</a> Improve GitHub issue deduplication reliability by introducing a stage… <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11770\">#11770</a> fix(nix): use correct version from Cargo.toml in flake build <a class=\"user-mention notranslate\" href=\"https://github.com/rupurt\">@rupurt</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11677\">#11677</a> turn metadata: per-turn non-blocking <a class=\"user-mention notranslate\" href=\"https://github.com/pash-openai\">@pash-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11692\">#11692</a> rmcp-client: fix auth crash <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10590\">#10590</a> tui: preserve remote image attachments across resume/backtrack <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11782\">#11782</a> turn metadata followups <a class=\"user-mention notranslate\" href=\"https://github.com/pash-openai\">@pash-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11773\">#11773</a> [app-server] add fuzzyFileSearch/sessionCompleted <a class=\"user-mention notranslate\" href=\"https://github.com/nornagon-openai\">@nornagon-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11756\">#11756</a> codex-rs: fix thread resume rejoin semantics <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11793\">#11793</a> fix: send unfiltered models over model/list <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11799\">#11799</a> fix(protocol): make local image test Bazel-friendly <a class=\"user-mention notranslate\" href=\"https://github.com/joshka-oai\">@joshka-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11796\">#11796</a> Fix js_repl view_image test runtime panic <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11800\">#11800</a> Fix js_repl in-flight tool-call waiter race <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11658\">#11658</a> feat(skills): add permission profiles from openai.yaml metadata <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11790\">#11790</a> bazel: enforce MODULE.bazel.lock sync with Cargo.lock <a class=\"user-mention notranslate\" href=\"https://github.com/joshka-oai\">@joshka-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11803\">#11803</a> add perf metrics for connectors load <a class=\"user-mention notranslate\" href=\"https://github.com/alexsong-oai\">@alexsong-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11659\">#11659</a> Handle model-switch base instructions after compaction <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11813\">#11813</a> Fixed help text for <code>mcp</code> and <code>mcp-server</code> CLI commands <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11672\">#11672</a> feat(core): add structured network approval plumbing and policy decision model <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11674\">#11674</a> feat(tui): render structured network approval prompts in approval overlay <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11550\">#11550</a> feat(tui) Permissions update history item <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11767\">#11767</a> fix(core): add linux bubblewrap sandbox tag <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11534\">#11534</a> Add process_uuid to sqlite logs <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11487\">#11487</a> core: snapshot tests for compaction requests, post-compaction layout, some additional compaction tests <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11690\">#11690</a> fix: show user warning when using default fallback metadata <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11780\">#11780</a> chore(tui): reduce noisy key logging <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11884\">#11884</a> fix: only emit unknown model warning on user turns <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11893\">#11893</a> bazel: fix snapshot parity for tests/*.rs rust_test targets <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11759\">#11759</a> feat: use shell policy in shell snapshot <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11615\">#11615</a> Allow hooks to error <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11918\">#11918</a> chore: rename collab feature flag key to multi_agent <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11924\">#11924</a> nit: memory storage <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11917\">#11917</a> feat: add customizable roles for multi-agents <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11926\">#11926</a> docs: mention Codex app in README intro <a class=\"user-mention notranslate\" href=\"https://github.com/vb-openai\">@vb-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11900\">#11900</a> feat: drop MCP managing tools if no MCP servers <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11939\">#11939</a> Rename collab modules to multi agents <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11894\">#11894</a> [apps] Fix app mention syntax. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11866\">#11866</a> chore(core) rm Feature::RequestRule <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11948\">#11948</a> add(feedback): over-refusal / safety check <a class=\"user-mention notranslate\" href=\"https://github.com/fouad-openai\">@fouad-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11860\">#11860</a> Fixed screen reader regression in CLI <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11964\">#11964</a> add(core): safety check downgrade warning <a class=\"user-mention notranslate\" href=\"https://github.com/fouad-openai\">@fouad-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11951\">#11951</a> fix(core) exec_policy parsing fixes <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11932\">#11932</a> fix: js_repl reset hang by clearing exec tool calls without waiting <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11974\">#11974</a> Hide /debug slash commands from popup menu <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11922\">#11922</a> fix: race in js repl <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11969\">#11969</a> fix(ci) Fix shell-tool-mcp.yml <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11908\">#11908</a> Exit early when session initialization fails <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11986\">#11986</a> nit: wording multi-agent <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11995\">#11995</a> feat: add <code>--search</code> to <code>just log</code> <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11994\">#11994</a> feat: add <code>--compact</code> mode to <code>just log</code> <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11833\">#11833</a> Don't allow model_supports_reasoning_summaries to disable reasoning <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11807\">#11807</a> Centralize context update diffing logic <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12007\">#12007</a> Update vendored rg to the latest stable version (15.1) <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11970\">#11970</a> Protect workspace .agents directory in Windows sandbox <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12005\">#12005</a> Add /statusline tooltip entry <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11982\">#11982</a> feat: move agents config to main config <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11224\">#11224</a> chore: clarify web_search deprecation notices and consolidate tests <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12001\">#12001</a> Feat: add model reroute notification <a class=\"user-mention notranslate\" href=\"https://github.com/shijie-oai\">@shijie-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11801\">#11801</a> Add remote skill scope/product_surface/enabled params and cleanup <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n</ul>",
      "image_url": "",
      "published": "2026-02-17T20:02:54Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.62,
      "tier1_quick_score": 3.657
    },
    {
      "id": "6b18d450bbd59ae6",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.45",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.45",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Added support for Claude Sonnet 4.6</li>\n<li>Added support for reading <code>enabledPlugins</code> and <code>extraKnownMarketplaces</code> from <code>--add-dir</code> directories</li>\n<li>Added <code>spinnerTipsOverride</code> setting to customize spinner tips — configure <code>tips</code> with an array of custom tip strings, and optionally set <code>excludeDefault: true</code> to show only your custom tips instead of the built-in ones</li>\n<li>Added <code>SDKRateLimitInfo</code> and <code>SDKRateLimitEvent</code> types to the SDK, enabling consumers to receive rate limit status updates including utilization, reset times, and overage information</li>\n<li>Fixed Agent Teams teammates failing on Bedrock, Vertex, and Foundry by propagating API provider environment variables to tmux-spawned processes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/23561\">#23561</a>)</li>\n<li>Fixed sandbox \"operation not permitted\" errors when writing temporary files on macOS by using the correct per-user temp directory (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/21654\">#21654</a>)</li>\n<li>Fixed Task tool (backgrounded agents) crashing with a <code>ReferenceError</code> on completion (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/22087\">#22087</a>)</li>\n<li>Fixed autocomplete suggestions not being accepted on Enter when images are pasted in the input</li>\n<li>Fixed skills invoked by subagents incorrectly appearing in main session context after compaction</li>\n<li>Fixed excessive <code>.claude.json.backup</code> files accumulating on every startup</li>\n<li>Fixed plugin-provided commands, agents, and hooks not being available immediately after installation without requiring a restart</li>\n<li>Improved startup performance by removing eager loading of session history for stats caching</li>\n<li>Improved memory usage for shell commands that produce large output — RSS no longer grows unboundedly with command output size</li>\n<li>Improved collapsed read/search groups to show the current file or search pattern being processed beneath the summary line while active</li>\n<li>[VSCode] Improved permission destination choice (project/user/session) to persist across sessions</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-17T18:53:52Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.61,
      "tier1_quick_score": 3.647
    },
    {
      "id": "40303bf9c02eecf8",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.102.0-alpha.11",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.11",
      "summary": "<p>Release 0.102.0-alpha.11</p>",
      "image_url": "",
      "published": "2026-02-17T18:12:34Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.604,
      "tier1_quick_score": 3.641
    },
    {
      "id": "542eda03bb7cb1bf",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "rust-v0.102.0-alpha.10",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.102.0-alpha.10",
      "summary": "<p>Release 0.102.0-alpha.10</p>",
      "image_url": "",
      "published": "2026-02-17T07:12:08Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.518,
      "tier1_quick_score": 3.555
    },
    {
      "id": "4a9f636f53197519",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.44",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.44",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed auth refresh errors</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-16T21:35:03Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.454,
      "tier1_quick_score": 3.491
    },
    {
      "id": "42710d92908034f2",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Opus 4 6",
      "url": "https://www.anthropic.com/news/claude-opus-4-6",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T17:46:31.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.6,
      "tier1_quick_score": 3.237
    },
    {
      "id": "6ed48b697f4e1625",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Sonnet 4 6",
      "url": "https://www.anthropic.com/news/claude-sonnet-4-6",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T17:45:22.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.6,
      "tier1_quick_score": 3.237
    },
    {
      "id": "f75baf298ba21604",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Bengaluru Office Partnerships Across India",
      "url": "https://www.anthropic.com/news/bengaluru-office-partnerships-across-india",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T15:11:31.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.579,
      "tier1_quick_score": 3.216
    },
    {
      "id": "50ed598c41e44951",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Anthropic Interviewer",
      "url": "https://www.anthropic.com/research/anthropic-interviewer",
      "summary": "",
      "image_url": "",
      "published": "2026-02-19T04:03:53.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.967,
      "tier1_quick_score": 3.204
    },
    {
      "id": "3748da72fd68d49d",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.42",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.42",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed /resume showing interrupt messages as session titles</li>\n<li>Fixed Opus 4.6 launch announcement showing for Bedrock/Vertex/Foundry users</li>\n<li>Improved error message for many-image dimension limit errors with /compact suggestion</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-13T19:56:33Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.163,
      "tier1_quick_score": 3.2
    },
    {
      "id": "c8740b36f03df678",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Rwanda Mou",
      "url": "https://www.anthropic.com/news/anthropic-rwanda-mou",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T10:01:21.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.539,
      "tier1_quick_score": 3.176
    },
    {
      "id": "f70833bd2f581c75",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.41",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.41",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed AWS auth refresh hanging indefinitely by adding a 3-minute timeout</li>\n<li>Added <code>claude auth login</code>, <code>claude auth status</code>, and <code>claude auth logout</code> CLI subcommands</li>\n<li>Added Windows ARM64 (win32-arm64) native binary support</li>\n<li>Improved <code>/rename</code> to auto-generate session name from conversation context when called without arguments</li>\n<li>Improved narrow terminal layout for prompt footer</li>\n<li>Fixed file resolution failing for @-mentions with anchor fragments (e.g., <code>@README.md#installation</code>)</li>\n<li>Fixed FileReadTool blocking the process on FIFOs, <code>/dev/stdin</code>, and large files</li>\n<li>Fixed background task notifications not being delivered in streaming Agent SDK mode</li>\n<li>Fixed cursor jumping to end on each keystroke in classifier rule input</li>\n<li>Fixed markdown link display text being dropped for raw URL</li>\n<li>Fixed auto-compact failure error notifications being shown to users</li>\n<li>Fixed permission wait time being included in subagent elapsed time display</li>\n<li>Fixed proactive ticks firing while in plan mode</li>\n<li>Fixed clear stale permission rules when settings change on disk</li>\n<li>Fixed hook blocking errors showing stderr content in UI</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-13T06:08:49Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.135,
      "tier1_quick_score": 3.172
    },
    {
      "id": "c967be8280f90bed",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Infosys",
      "url": "https://www.anthropic.com/news/anthropic-infosys",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T09:26:56.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.535,
      "tier1_quick_score": 3.172
    },
    {
      "id": "0f43cce4717b58ca",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Measuring Agent Autonomy",
      "url": "https://www.anthropic.com/research/measuring-agent-autonomy",
      "summary": "",
      "image_url": "",
      "published": "2026-02-18T20:26:31.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.869,
      "tier1_quick_score": 3.106
    },
    {
      "id": "386ad3a560ebad8b",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.39",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.39",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Improved terminal rendering performance</li>\n<li>Fixed fatal errors being swallowed instead of displayed</li>\n<li>Fixed process hanging after session close</li>\n<li>Fixed character loss at terminal screen boundary</li>\n<li>Fixed blank lines in verbose transcript view</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-10T23:11:36Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.063,
      "tier1_quick_score": 3.1
    },
    {
      "id": "5d3f4c07823e95e3",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.38",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.38",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed VS Code terminal scroll-to-top regression introduced in 2.1.37</li>\n<li>Fixed Tab key queueing slash commands instead of autocompleting</li>\n<li>Fixed bash permission matching for commands using environment variable wrappers</li>\n<li>Fixed text between tool uses disappearing when not using streaming</li>\n<li>Fixed duplicate sessions when resuming in VS Code extension</li>\n<li>Improved heredoc delimiter parsing to prevent command smuggling</li>\n<li>Blocked writes to <code>.claude/skills</code> directory in sandbox mode</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-10T00:53:32Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.046,
      "tier1_quick_score": 3.083
    },
    {
      "id": "7302495be6ceb799",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Experimenting with sponsorship for my blog and newsletter",
      "url": "https://simonwillison.net/2026/Feb/19/sponsorship/#atom-everything",
      "summary": "<p>I've long been resistant to the idea of accepting sponsorship for my blog. I value my credibility as an independent voice, and I don't want to risk compromising that reputation.</p>\n<p>Then I learned about Troy Hunt's <a href=\"https://www.troyhunt.com/sponsorship/\">approach to sponsorship</a>, which he first wrote about <a href=\"https://www.troyhunt.com/im-now-offering-sponsorship-of-this-blog/\">in 2016</a>. Troy runs with a simple text row in the page banner - no JavaScript, no cookies, unobtrusive while providing value to the sponsor. I can live with that!</p>\n<p>Accepting sponsorship in this way helps me maintain my independence while offsetting the opportunity cost of not taking a full-time job.</p>\n<p>To start with I'm selling sponsorship by the week. Sponsors get that unobtrusive banner across my blog and also their sponsored message at the top of <a href=\"https://simonw.substack.com/\">my newsletter</a>.</p>\n<p><img alt=\"Screenshot of my blog's homepage. Below the Simon Willison's Weblog heading and list of tags is a new blue page-wide banner reading &quot;Sponsored by: Teleport - Secure, Govern, and Operate Al at Engineering Scale. Learn more&quot;.\" src=\"https://static.simonwillison.net/static/2026/sponsor-banner.jpg\" /></p>\n<p>I <strong>will not write content in exchange for sponsorship</strong>. I hope the sponsors I work with understand that my credibility as an independent voice is a key reason I have an audience, and compromising that trust would be bad for everyone.</p>\n<p><a href=\"https://www.freemanandforrest.com/\">Freeman &amp; Forrest</a> helped me set up and sell my first slots. Thanks also to <a href=\"https://t3.gg/\">Theo Browne</a> for helping me think through my approach.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/newsletter\">newsletter</a>, <a href=\"https://simonwillison.net/tags/blogging\">blogging</a>, <a href=\"https://simonwillison.net/tags/troy-hunt\">troy-hunt</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/sponsor-banner.jpg",
      "published": "2026-02-19T05:44:29+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.989,
      "tier1_quick_score": 3.076
    },
    {
      "id": "80a9e576f04115fb",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "SWE-bench February 2026 leaderboard update",
      "url": "https://simonwillison.net/2026/Feb/19/swe-bench/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.swebench.com/\">SWE-bench February 2026 leaderboard update</a></strong></p>\nSWE-bench is one of the benchmarks that the labs love to list in their model releases. The official leaderboard is infrequently updated but they just did a full run of it against the current generation of models, which is notable because it's always good to see benchmark results like this that <em>weren't</em> self-reported by the labs.</p>\n<p>The fresh results are for their \"Bash Only\" benchmark, which runs their <a href=\"https://github.com/SWE-agent/mini-swe-agent\">mini-swe-bench</a> agent (~9,000 lines of Python, <a href=\"https://github.com/SWE-agent/mini-swe-agent/blob/v2.2.1/src/minisweagent/config/benchmarks/swebench.yaml\">here are the prompts</a> they use) against the <a href=\"https://huggingface.co/datasets/princeton-nlp/SWE-bench\">SWE-bench</a> dataset of coding problems - 2,294 real-world examples pulled from 12 open source repos: <a href=\"https://github.com/django/django\">django/django</a> (850), <a href=\"https://github.com/sympy/sympy\">sympy/sympy</a> (386), <a href=\"https://github.com/scikit-learn/scikit-learn\">scikit-learn/scikit-learn</a> (229), <a href=\"https://github.com/sphinx-doc/sphinx\">sphinx-doc/sphinx</a> (187), <a href=\"https://github.com/matplotlib/matplotlib\">matplotlib/matplotlib</a> (184), <a href=\"https://github.com/pytest-dev/pytest\">pytest-dev/pytest</a> (119), <a href=\"https://github.com/pydata/xarray\">pydata/xarray</a> (110), <a href=\"https://github.com/astropy/astropy\">astropy/astropy</a> (95), <a href=\"https://github.com/pylint-dev/pylint\">pylint-dev/pylint</a> (57), <a href=\"https://github.com/psf/requests\">psf/requests</a> (44), <a href=\"https://github.com/mwaskom/seaborn\">mwaskom/seaborn</a> (22), <a href=\"https://github.com/pallets/flask\">pallets/flask</a> (11).</p>\n<p>Here's how the top ten models performed:</p>\n<p><img alt=\"Bar chart showing &quot;% Resolved&quot; by &quot;Model&quot;. Bars in descending order: Claude 4.5 Opus (high reasoning) 76.8%, Gemini 3 Flash (high reasoning) 75.8%, MiniMax M2.5 (high reasoning) 75.8%, Claude Opus 4.6 75.6%, GLM-5 (high reasoning) 72.8%, GPT-5.2 (high reasoning) 72.8%, Claude 4.5 Sonnet (high reasoning) 72.8%, Kimi K2.5 (high reasoning) 71.4%, DeepSeek V3.2 (high reasoning) 70.8%, Claude 4.5 Haiku (high reasoning) 70.0%, and a partially visible final bar at 66.6%.\" src=\"https://static.simonwillison.net/static/2026/swbench-feb-2026.jpg\" /></p>\n<p>It's interesting to see Claude Opus 4.5 beat Opus 4.6, though only by about a percentage point. 4.5 Opus is top, then Gemini 3 Flash, then MiniMax M2.5 - a 229B model released <a href=\"https://www.minimax.io/news/minimax-m25\">last week</a> by Chinese lab MiniMax. GLM-5, Kimi K2.5 and DeepSeek V3.2 are three more Chinese models that make the top ten as well.</p>\n<p>OpenAI's GPT-5.2 is their highest performing model at position 6, but it's worth noting that their best coding model, GPT-5.3-Codex, is not represented - maybe because it's not yet available in the OpenAI API.</p>\n<p>This benchmark uses the same system prompt for every model, which is important for a fair comparison but does mean that the quality of the different harnesses or optimized prompts is not being measured here.</p>\n<p>The chart above is a screenshot from the SWE-bench website, but their charts don't include the actual percentage values visible on the bars. I successfully used Claude for Chrome to add these - <a href=\"https://claude.ai/share/81a0c519-c727-4caa-b0d4-0d866375d0da\">transcript here</a>. My prompt sequence included:</p>\n<blockquote>\n<p>Use claude in chrome to open https://www.swebench.com/</p>\n<p>Click on \"Compare results\" and then select \"Select top 10\"</p>\n<p>See those bar charts? I want them to display the percentage on each bar so I can take a better screenshot, modify the page like that</p>\n</blockquote>\n<p>I'm impressed at how well this worked - Claude injected custom JavaScript into the page to draw additional labels on top of the existing chart.</p>\n<p><img alt=\"Screenshot of a Claude AI conversation showing browser automation. A thinking step reads &quot;Pivoted strategy to avoid recursion issues with chart labeling &gt;&quot; followed by the message &quot;Good, the chart is back. Now let me carefully add the labels using an inline plugin on the chart instance to avoid the recursion issue.&quot; A collapsed &quot;Browser_evaluate&quot; section shows a browser_evaluate tool call with JavaScript code using Chart.js canvas context to draw percentage labels on bars: meta.data.forEach((bar, index) =&gt; { const value = dataset.data[index]; if (value !== undefined &amp;&amp; value !== null) { ctx.save(); ctx.textAlign = 'center'; ctx.textBaseline = 'bottom'; ctx.fillStyle = '#333'; ctx.font = 'bold 12px sans-serif'; ctx.fillText(value.toFixed(1) + '%', bar.x, bar.y - 5); A pending step reads &quot;Let me take a screenshot to see if it worked.&quot; followed by a completed &quot;Done&quot; step, and the message &quot;Let me take a screenshot to check the result.&quot;\" src=\"https://static.simonwillison.net/static/2026/claude-chrome-draw-on-chart.jpg\" />\n\n    <p><small></small>Via <a href=\"https://twitter.com/KLieret/status/2024176335782826336\">@KLieret</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/benchmarks\">benchmarks</a>, <a href=\"https://simonwillison.net/tags/django\">django</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/ai-in-china\">ai-in-china</a>, <a href=\"https://simonwillison.net/tags/minimax\">minimax</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/swbench-feb-2026.jpg",
      "published": "2026-02-19T04:48:47+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.977,
      "tier1_quick_score": 3.064
    },
    {
      "id": "0fc8d2f4db05ff52",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.37",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.37",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed an issue where /fast was not immediately available after enabling /extra-usage</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-07T19:10:10Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.022,
      "tier1_quick_score": 3.059
    },
    {
      "id": "94eea30c1c57c3f8",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.36",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.36",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fast mode is now available for Opus 4.6. Learn more at <a href=\"https://code.claude.com/docs/en/fast-mode\" rel=\"nofollow\">https://code.claude.com/docs/en/fast-mode</a></li>\n</ul>",
      "image_url": "",
      "published": "2026-02-07T18:02:15Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.021,
      "tier1_quick_score": 3.058
    },
    {
      "id": "ff4216be30309b4e",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.34",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.34",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed a crash when agent teams setting changed between renders</li>\n<li>Fixed a bug where commands excluded from sandboxing (via <code>sandbox.excludedCommands</code> or <code>dangerouslyDisableSandbox</code>) could bypass the Bash ask permission rule when <code>autoAllowBashIfSandboxed</code> was enabled</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-06T14:26:47Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.015,
      "tier1_quick_score": 3.052
    },
    {
      "id": "b7dd9d05bcdf917e",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.38",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.38",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.47</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.38/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.38/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.38</pre></div>",
      "image_url": "",
      "published": "2026-02-18T21:57:56Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.888,
      "tier1_quick_score": 3.025
    },
    {
      "id": "758c7d7a1776d429",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "LadybirdBrowser/ladybird: Abandon Swift adoption",
      "url": "https://simonwillison.net/2026/Feb/19/ladybird/#atom-everything",
      "summary": "<p><strong><a href=\"https://github.com/LadybirdBrowser/ladybird/commit/e87f889e31afbb5fa32c910603c7f5e781c97afd\">LadybirdBrowser/ladybird: Abandon Swift adoption</a></strong></p>\nBack <a href=\"https://simonwillison.net/2024/Aug/11/ladybird-set-to-adopt-swift/\">in August 2024</a> the Ladybird browser project announced an intention to adopt Swift as their memory-safe language of choice.</p>\n<p>As of <a href=\"https://github.com/LadybirdBrowser/ladybird/commit/e87f889e31afbb5fa32c910603c7f5e781c97afd\">this commit</a> it looks like they've changed their mind:</p>\n<blockquote>\n<p><strong>Everywhere: Abandon Swift adoption</strong></p>\n<p>After making no progress on this for a very long time, let's acknowledge it's not going anywhere and remove it from the codebase.</p>\n</blockquote>\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=47067678\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ladybird\">ladybird</a>, <a href=\"https://simonwillison.net/tags/swift\">swift</a></p>",
      "image_url": "",
      "published": "2026-02-19T01:25:33+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.932,
      "tier1_quick_score": 3.019
    },
    {
      "id": "c16b69a1be247646",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "GPT-5.2 derives a new result in theoretical physics",
      "url": "https://openai.com/index/new-result-theoretical-physics",
      "summary": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 11:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.144,
      "tier1_quick_score": 2.981
    },
    {
      "id": "c5ef81aef2a2ccc1",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
      "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt",
      "summary": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT to help organizations defend against prompt injection and AI-driven data exfiltration.",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 10:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.142,
      "tier1_quick_score": 2.979
    },
    {
      "id": "c10ff8c0943e4e07",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Scaling social science research",
      "url": "https://openai.com/index/scaling-social-science-research",
      "summary": "GABRIEL is a new open-source toolkit from OpenAI that uses GPT to turn qualitative text and images into quantitative data, helping social scientists analyze research at scale.",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.14,
      "tier1_quick_score": 2.977
    },
    {
      "id": "9d95a891a81b27c3",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Beyond rate limits: scaling access to Codex and Sora",
      "url": "https://openai.com/index/beyond-rate-limits",
      "summary": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.14,
      "tier1_quick_score": 2.977
    },
    {
      "id": "e090493a0ff267ce",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing GPT-5.3-Codex-Spark",
      "url": "https://openai.com/index/introducing-gpt-5-3-codex-spark",
      "summary": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
      "image_url": "",
      "published": "Thu, 12 Feb 2026 10:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.102,
      "tier1_quick_score": 2.939
    },
    {
      "id": "553ba5bc3e227c68",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Typing without having to type",
      "url": "https://simonwillison.net/2026/Feb/18/typing/#atom-everything",
      "summary": "<p>25+ years into my career as a programmer I think I may <em>finally</em> be coming around to preferring type hints or even strong typing. I resisted those in the past because they slowed down the rate at which I could iterate on code, especially in the REPL environments that were key to my productivity. But if a coding agent is doing all that <em>typing</em> for me, the benefits of explicitly defining all of those types are suddenly much more attractive.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/programming\">programming</a>, <a href=\"https://simonwillison.net/tags/programming-languages\">programming-languages</a>, <a href=\"https://simonwillison.net/tags/static-typing\">static-typing</a></p>",
      "image_url": "",
      "published": "2026-02-18T18:56:56+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.852,
      "tier1_quick_score": 2.939
    },
    {
      "id": "6fb0a14b99db59b6",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: I built an AI that generates pixel art sprite animations",
      "url": "https://rika-ai.com/",
      "summary": "<p>Article URL: <a href=\"https://rika-ai.com/\">https://rika-ai.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070525\">https://news.ycombinator.com/item?id=47070525</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 06:22:56 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.998,
      "tier1_quick_score": 2.935
    },
    {
      "id": "11b4083e79533449",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: I Stopped Writing Code – The 60/40 Rule for AI-Native Engineering",
      "url": "https://github.com/myinvestpilot/ai-architecture/blob/main/docs/02_ai_driven_development.md",
      "summary": "<p>Article URL: <a href=\"https://github.com/myinvestpilot/ai-architecture/blob/main/docs/02_ai_driven_development.md\">https://github.com/myinvestpilot/ai-architecture/blob/main/docs/02_ai_driven_development.md</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070476\">https://news.ycombinator.com/item?id=47070476</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 06:14:50 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.996,
      "tier1_quick_score": 2.933
    },
    {
      "id": "574a913ddc5d12ef",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "LobsterAI – Your 24/7 all-scenario AI agent that gets work done for you",
      "url": "https://lobsterai.youdao.com/#/en/index",
      "summary": "<p>Article URL: <a href=\"https://lobsterai.youdao.com/#/en/index\">https://lobsterai.youdao.com/#/en/index</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070368\">https://news.ycombinator.com/item?id=47070368</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 05:56:33 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.992,
      "tier1_quick_score": 2.929
    },
    {
      "id": "8f83aaaed9fbfc9a",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Security audit of OpenClaw and other similar open source AI Agents",
      "url": "https://www.prismor.dev/blog/openclaw-ai-agents-security-audit",
      "summary": "<p>Article URL: <a href=\"https://www.prismor.dev/blog/openclaw-ai-agents-security-audit\">https://www.prismor.dev/blog/openclaw-ai-agents-security-audit</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070200\">https://news.ycombinator.com/item?id=47070200</a></p>\n<p>Points: 5</p>\n<p># Comments: 1</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 05:18:24 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.983,
      "tier1_quick_score": 2.92
    },
    {
      "id": "483e182c33bedbd8",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Building an Elite AI Engineering Culture in 2026",
      "url": "https://www.cjroth.com/blog/2026-02-18-building-an-elite-engineering-culture",
      "summary": "<p>Article URL: <a href=\"https://www.cjroth.com/blog/2026-02-18-building-an-elite-engineering-culture\">https://www.cjroth.com/blog/2026-02-18-building-an-elite-engineering-culture</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070173\">https://news.ycombinator.com/item?id=47070173</a></p>\n<p>Points: 2</p>\n<p># Comments: 2</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 05:13:45 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.982,
      "tier1_quick_score": 2.919
    },
    {
      "id": "25b183a6d63ea0db",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "HumanCompiler – Compile humans into AI agents – a Claude Code plugin",
      "url": "https://github.com/Gerstep/HumanCompiler",
      "summary": "<p>Article URL: <a href=\"https://github.com/Gerstep/HumanCompiler\">https://github.com/Gerstep/HumanCompiler</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070165\">https://news.ycombinator.com/item?id=47070165</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 05:12:26 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.982,
      "tier1_quick_score": 2.919
    },
    {
      "id": "bd186e1075a12cbb",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "AI Agents Are Taking America by Storm",
      "url": "https://www.theatlantic.com/technology/2026/02/post-chatbot-claude-code-ai-agents/686029/",
      "summary": "<p>Article URL: <a href=\"https://www.theatlantic.com/technology/2026/02/post-chatbot-claude-code-ai-agents/686029/\">https://www.theatlantic.com/technology/2026/02/post-chatbot-claude-code-ai-agents/686029/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070157\">https://news.ycombinator.com/item?id=47070157</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 05:10:34 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.982,
      "tier1_quick_score": 2.919
    },
    {
      "id": "13463e1f08e717b4",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "The A.I. Disruption We’ve Been Waiting for Has Arrived",
      "url": "https://simonwillison.net/2026/Feb/18/the-ai-disruption/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.nytimes.com/2026/02/18/opinion/ai-software.html?unlocked_article_code=1.NFA.UkLv.r-XczfzYRdXJ&amp;smid=url-share\">The A.I. Disruption We’ve Been Waiting for Has Arrived</a></strong></p>\nNew opinion piece from Paul Ford in the New York Times. Unsurprisingly for a piece by Paul it's packed with quoteworthy snippets, but a few stood out for me in particular.</p>\n<p>Paul describes the <a href=\"https://simonwillison.net/2026/Jan/4/inflection/\">November moment</a> that so many other programmers have observed, and highlights Claude Code's ability to revive old side projects:</p>\n<blockquote>\n<p>[Claude Code] was always a helpful coding assistant, but in November it suddenly got much better, and ever since I’ve been knocking off side projects that had sat in folders for a decade or longer. It’s fun to see old ideas come to life, so I keep a steady flow. Maybe it adds up to a half-hour a day of my time, and an hour of Claude’s.</p>\n<p>November was, for me and many others in tech, a great surprise. Before, A.I. coding tools were often useful, but halting and clumsy. Now, the bot can run for a full hour and make whole, designed websites and apps that may be flawed, but credible. I spent an entire session of therapy talking about it.</p>\n</blockquote>\n<p>And as the former CEO of a respected consultancy firm (Postlight) he's well positioned to evaluate the potential impact:</p>\n<blockquote>\n<p>When you watch a large language model slice through some horrible, expensive problem — like migrating data from an old platform to a modern one — you feel the earth shifting. I was the chief executive of a software services firm, which made me a professional software cost estimator. When I rebooted my messy personal website a few weeks ago, I realized: I would have paid $25,000 for someone else to do this. When a friend asked me to convert a large, thorny data set, I downloaded it, cleaned it up and made it pretty and easy to explore. In the past I would have charged $350,000.</p>\n<p>That last price is full 2021 retail — it implies a product manager, a designer, two engineers (one senior) and four to six months of design, coding and testing. Plus maintenance. Bespoke software is joltingly expensive. Today, though, when the stars align and my prompts work out, I can do hundreds of thousands of dollars worth of work for fun (fun for me) over weekends and evenings, for the price of the Claude $200-a-month plan.</p>\n</blockquote>\n<p>He also neatly captures the inherent community tension involved in exploring this technology:</p>\n<blockquote>\n<p>All of the people I love hate this stuff, and all the people I hate love it. And yet, likely because of the same personality flaws that drew me to technology in the first place, I am annoyingly excited.</p>\n</blockquote>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/new-york-times\">new-york-times</a>, <a href=\"https://simonwillison.net/tags/paul-ford\">paul-ford</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
      "image_url": "",
      "published": "2026-02-18T17:07:31+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.83,
      "tier1_quick_score": 2.917
    },
    {
      "id": "58b61a2ab08d1edb",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "AgentPuzzles – API‑first timed puzzle arena and public leaderboard for AI agents",
      "url": "https://agentpuzzles.com",
      "summary": "<p>Article URL: <a href=\"https://agentpuzzles.com\">https://agentpuzzles.com</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070129\">https://news.ycombinator.com/item?id=47070129</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 05:05:39 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.98,
      "tier1_quick_score": 2.917
    },
    {
      "id": "da8bbcdae3dd0490",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Quoting Martin Fowler",
      "url": "https://simonwillison.net/2026/Feb/18/martin-fowler/#atom-everything",
      "summary": "<blockquote cite=\"https://martinfowler.com/fragments/2026-02-18.html\"><p>LLMs are eating specialty skills. There will be less use of specialist front-end and back-end developers as the LLM-driving skills become more important than the details of platform usage. Will this lead to a greater recognition of the role of <a href=\"https://martinfowler.com/articles/expert-generalist.html\">Expert Generalists</a>? Or will the ability of LLMs to write lots of code mean they code around the silos rather than eliminating them?</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://martinfowler.com/fragments/2026-02-18.html\">Martin Fowler</a>, tidbits from the Thoughtworks Future of Software Development Retreat, <a href=\"https://news.ycombinator.com/item?id=47062534\">via HN</a>)</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/martin-fowler\">martin-fowler</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a></p>",
      "image_url": "",
      "published": "2026-02-18T16:50:07+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.827,
      "tier1_quick_score": 2.914
    },
    {
      "id": "5f0cfc6c3278360d",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: I built a pre-push safety net for AI-generated code",
      "url": "https://github.com/asamassekou10/ship-safe",
      "summary": "<p>Hey HN,<p>I am sharing Ship Safe (<a href=\"https://github.com/asamassekou10/ship-safe\" rel=\"nofollow\">https://github.com/asamassekou10/ship-safe</a>), an open-source tool I created to help developers catch security risks locally before they end up in a public repository.<p>Like many of us, I have been leaning heavily on AI code generation lately. It is incredible for velocity, but it has a dangerous habit of hallucinating credentials, hardcoding sensitive configurations, or spitting out insecure boilerplate. My wake-up call was almost pushing a live Stripe key to a public repo. That moment of panic made me realize I needed a reliable local checkpoint.<p>I built Ship Safe so it is not just a simple key checker. It is meant to be a comprehensive safety net for your code. It scans for hardcoded secrets, misconfigured environment setups, and general insecure patterns. The goal is to give you total peace of mind when you hit push, knowing your environment is actually secure.<p>The biggest priority was making it completely frictionless. You do not need any complex setup or configuration files to get started. You can run it instantly in your current project:<p>npx ship-safe scan .<p>One feature I am particularly excited to share is the MCP integration. You can connect it so your AI code editors can call Ship Safe directly. This creates a really helpful feedback loop where the AI writes the code, and then immediately audits its own work for security flaws before you even start your review.<p>I genuinely hope this saves some of you from the stress of a compromised repository or a ruined weekend. I would love for you to try it out and let me know what other security checks would be helpful for your daily workflow!</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47070012\">https://news.ycombinator.com/item?id=47070012</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 04:42:09 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.975,
      "tier1_quick_score": 2.912
    },
    {
      "id": "ced261bbff253eed",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Upstart Sarvam Unveils AI Model Customized for India Market",
      "url": "https://www.bloomberg.com/news/articles/2026-02-18/upstart-sarvam-unveils-ai-model-customized-for-india-market",
      "summary": "<p>Article URL: <a href=\"https://www.bloomberg.com/news/articles/2026-02-18/upstart-sarvam-unveils-ai-model-customized-for-india-market\">https://www.bloomberg.com/news/articles/2026-02-18/upstart-sarvam-unveils-ai-model-customized-for-india-market</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069988\">https://news.ycombinator.com/item?id=47069988</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 04:38:27 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.974,
      "tier1_quick_score": 2.911
    },
    {
      "id": "29449fea57b0d8ac",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Is Python Still the King of AI in 2026?",
      "url": "https://codebitdaily.blogspot.com/2026/02/why-python-is-king-ai-2026.html",
      "summary": "<p>Article URL: <a href=\"https://codebitdaily.blogspot.com/2026/02/why-python-is-king-ai-2026.html\">https://codebitdaily.blogspot.com/2026/02/why-python-is-king-ai-2026.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069973\">https://news.ycombinator.com/item?id=47069973</a></p>\n<p>Points: 2</p>\n<p># Comments: 2</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 04:36:20 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.974,
      "tier1_quick_score": 2.911
    },
    {
      "id": "c35812ccca3ce7be",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Harness engineering: leveraging Codex in an agent-first world",
      "url": "https://openai.com/index/harness-engineering",
      "summary": "By Ryan Lopopolo, Member of the Technical Staff",
      "image_url": "",
      "published": "Wed, 11 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.072,
      "tier1_quick_score": 2.909
    },
    {
      "id": "f061bd820fca75cd",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Stateful sandbox environments (for AI agents)",
      "url": "https://sprites.dev/",
      "summary": "<p>Article URL: <a href=\"https://sprites.dev/\">https://sprites.dev/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069931\">https://news.ycombinator.com/item?id=47069931</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 04:29:38 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.972,
      "tier1_quick_score": 2.909
    },
    {
      "id": "4ca39a0363630c8d",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "MNX: AI perps, futures, and prediction markets",
      "url": "https://mnx.fi/",
      "summary": "<p>Article URL: <a href=\"https://mnx.fi/\">https://mnx.fi/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069592\">https://news.ycombinator.com/item?id=47069592</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 03:36:37 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.96,
      "tier1_quick_score": 2.897
    },
    {
      "id": "e8bdae5b3bed60af",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "I hacked ChatGPT and Google's AI – and it only took 20 minutes",
      "url": "https://www.bbc.com/future/article/20260218-i-hacked-chatgpt-and-googles-ai-and-it-only-took-20-minutes",
      "summary": "<p>Article URL: <a href=\"https://www.bbc.com/future/article/20260218-i-hacked-chatgpt-and-googles-ai-and-it-only-took-20-minutes\">https://www.bbc.com/future/article/20260218-i-hacked-chatgpt-and-googles-ai-and-it-only-took-20-minutes</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069544\">https://news.ycombinator.com/item?id=47069544</a></p>\n<p>Points: 4</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 03:28:16 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.959,
      "tier1_quick_score": 2.896
    },
    {
      "id": "675df5de4df7dfdd",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Theres no mainstream AI coding tool?",
      "url": "https://news.ycombinator.com/item?id=47069362",
      "summary": "<p>ik theres a few yc companies doing this but are they any good before I try them out?</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069362\">https://news.ycombinator.com/item?id=47069362</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 03:00:44 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.953,
      "tier1_quick_score": 2.89
    },
    {
      "id": "ac1f3491bc56508b",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: DeskMic a Rust based hyper-light continuous transcriber/AI summarizer",
      "url": "https://github.com/varunr89/deskmic",
      "summary": "<p>I hate missing key action items/brainstorming ideas at work and always regret not having recorded notes from meetings. Now I built an always on rust based tool that continuously transcribes and summarizes all meetings and sends previous day/weeks summary to my inbox.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069345\">https://news.ycombinator.com/item?id=47069345</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 02:59:12 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.952,
      "tier1_quick_score": 2.889
    },
    {
      "id": "f75e372a01957629",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Auxos: AI Agents for Product Feedback",
      "url": "https://www.auxos.dev",
      "summary": "<p>Article URL: <a href=\"https://www.auxos.dev\">https://www.auxos.dev</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069342\">https://news.ycombinator.com/item?id=47069342</a></p>\n<p>Points: 15</p>\n<p># Comments: 4</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 02:58:49 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.952,
      "tier1_quick_score": 2.889
    },
    {
      "id": "5da501382a9416a8",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Stanford researchers and Air Force partner to test AI copilots",
      "url": "https://www.aftc.af.mil/News/Article/4388756/stanford-researchers-and-air-force-partner-to-test-ai-copilots/",
      "summary": "<p>Article URL: <a href=\"https://www.aftc.af.mil/News/Article/4388756/stanford-researchers-and-air-force-partner-to-test-ai-copilots/\">https://www.aftc.af.mil/News/Article/4388756/stanford-researchers-and-air-force-partner-to-test-ai-copilots/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069337\">https://news.ycombinator.com/item?id=47069337</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 02:58:19 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.952,
      "tier1_quick_score": 2.889
    },
    {
      "id": "d45919d21add4f13",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Claw Diary – Show HN: Visualize what your AI agent does every day",
      "url": "https://github.com/0xbeekeeper/claw-diary",
      "summary": "<p>Article URL: <a href=\"https://github.com/0xbeekeeper/claw-diary\">https://github.com/0xbeekeeper/claw-diary</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069266\">https://news.ycombinator.com/item?id=47069266</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 02:46:30 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.949,
      "tier1_quick_score": 2.886
    },
    {
      "id": "1f26a30c1c1760eb",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Ottr – one-click human approval links for scripts, CI, and AI agents",
      "url": "https://ottr.run",
      "summary": "<p>Automations, CI jobs, and AI agents often need a human “yes” at the last step. Most solutions require bots, dashboards, or auth flows that feel heavier than the problem.<p>I built Ottr to make this a primitive.<p>Ottr gives you a signed, expiring approval link you can generate from any script. When the link is clicked, the process continues. No accounts. No UI.<p>Flow:<p>• Script or agent requests approval → gets a one-time URL<p>• URL is sent to a human (email, Slack, SMS, etc.)<p>• Human clicks → script resumes (via polling)<p>This is early and deliberately minimal. I’m trying to validate whether “approval links” are a useful building block, or if people already solve this in a better way.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069206\">https://news.ycombinator.com/item?id=47069206</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 02:37:43 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.947,
      "tier1_quick_score": 2.884
    },
    {
      "id": "10b7309ba5430c63",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Extra-steps.dev – AI hype mapped to CS primitives",
      "url": "https://extra-steps.dev",
      "summary": "<p>I built a reference site that maps AI marketing buzzwords to the CS primitives underneath them. Every entry follows the same format: \"X is just Y with extra steps.\"<p>MCP? JSON-RPC over stdio. Agents? A while loop with an LLM call. RAG? A search index + string concatenation. Prompt engineering? natural language + markdown<p>The site has an origins section for buzzwords that already graduated — Docker (cgroups + namespaces), Kubernetes (reconciliation loops watching YAML), Serverless\n  (someone else's process on someone else's computer). And of course, BrandonM's 2007 comment on the Dropbox YC demo.<p>Each entry has expandable pseudocode on the index page, detailed breakdowns with citations on the detail pages.<p>The motivation: I keep having conversations at work where a product executive suggests that we \"orchestrate APIs using MCP\" or asks about implementing \"agentic memory\" in our web apps and the fastest path to a productive conversation is showing them the code underneath the buzzword. I wanted a community-based reference site like caniuse or mdn that I loved when learning to code. I couldn't find one, so I built one.<p><pre><code>  14 entries so far. Static Astro site, open source, PRs welcome.\n\n  https://extra-steps.dev</code></pre></p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069100\">https://news.ycombinator.com/item?id=47069100</a></p>\n<p>Points: 1</p>\n<p># Comments: 2</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 02:18:36 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.943,
      "tier1_quick_score": 2.88
    },
    {
      "id": "82952c610981f0ff",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Pre-AI – Chrome extension that filters AI posts from LinkedIn",
      "url": "https://pre-ai.netlify.app",
      "summary": "<p>Article URL: <a href=\"https://pre-ai.netlify.app\">https://pre-ai.netlify.app</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47069060\">https://news.ycombinator.com/item?id=47069060</a></p>\n<p>Points: 2</p>\n<p># Comments: 3</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 02:12:46 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.942,
      "tier1_quick_score": 2.879
    },
    {
      "id": "4e730871994fa83c",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Testing ads in ChatGPT",
      "url": "https://openai.com/index/testing-ads-in-chatgpt",
      "summary": "OpenAI begins testing ads in ChatGPT to support free access, with clear labeling, answer independence, strong privacy protections, and user control.",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 11:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.038,
      "tier1_quick_score": 2.875
    },
    {
      "id": "75bb2edf937a96ea",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Bringing ChatGPT to GenAI.mil",
      "url": "https://openai.com/index/bringing-chatgpt-to-genaimil",
      "summary": "OpenAI for Government announces the deployment of a custom ChatGPT on GenAI.mil, bringing secure, safety-forward AI to U.S. defense teams.",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 11:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.038,
      "tier1_quick_score": 2.875
    },
    {
      "id": "99c54e6ff7a65a6d",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "AI pioneer Fei-Fei Li's World Labs raises $1B in funding",
      "url": "https://finance.yahoo.com/news/ai-pioneer-fei-fei-lis-202957884.html",
      "summary": "<p>Article URL: <a href=\"https://finance.yahoo.com/news/ai-pioneer-fei-fei-lis-202957884.html\">https://finance.yahoo.com/news/ai-pioneer-fei-fei-lis-202957884.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47068944\">https://news.ycombinator.com/item?id=47068944</a></p>\n<p>Points: 2</p>\n<p># Comments: 1</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 01:54:38 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.938,
      "tier1_quick_score": 2.875
    },
    {
      "id": "f6f152cb75ebd806",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Grandpa Lissajous – A 13-Agent AI Orchestration Loop",
      "url": "https://blog.sauhsoj.wtf/posts/the-grandpa-loop/",
      "summary": "<p>Article URL: <a href=\"https://blog.sauhsoj.wtf/posts/the-grandpa-loop/\">https://blog.sauhsoj.wtf/posts/the-grandpa-loop/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47068917\">https://news.ycombinator.com/item?id=47068917</a></p>\n<p>Points: 3</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 01:50:48 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.937,
      "tier1_quick_score": 2.874
    },
    {
      "id": "fb92ccb2415cb07c",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "PicoLM: Run a 1B parameter LLM on a $10 board",
      "url": "https://github.com/RightNow-AI/picolm",
      "summary": "<p>Article URL: <a href=\"https://github.com/RightNow-AI/picolm\">https://github.com/RightNow-AI/picolm</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47068850\">https://news.ycombinator.com/item?id=47068850</a></p>\n<p>Points: 3</p>\n<p># Comments: 1</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 01:38:16 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.935,
      "tier1_quick_score": 2.872
    },
    {
      "id": "393aa4beedceb047",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Agf – A TUI to find and resume your AI coding agent sessions",
      "url": "https://github.com/subinium/agf",
      "summary": "<p>Article URL: <a href=\"https://github.com/subinium/agf\">https://github.com/subinium/agf</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47068793\">https://news.ycombinator.com/item?id=47068793</a></p>\n<p>Points: 2</p>\n<p># Comments: 2</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 01:28:22 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.932,
      "tier1_quick_score": 2.869
    },
    {
      "id": "0493524f9d1b6ce9",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Seedance AI video demo debunked",
      "url": "https://www.shokunin.studio/blog/2026/2/18/is-it-all-over-for-filmmakers",
      "summary": "<p>Article URL: <a href=\"https://www.shokunin.studio/blog/2026/2/18/is-it-all-over-for-filmmakers\">https://www.shokunin.studio/blog/2026/2/18/is-it-all-over-for-filmmakers</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47068708\">https://news.ycombinator.com/item?id=47068708</a></p>\n<p>Points: 4</p>\n<p># Comments: 3</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 01:17:10 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.93,
      "tier1_quick_score": 2.867
    },
    {
      "id": "272553f390395364",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "AI-Native Observability Notebook",
      "url": "https://outcrop.app/",
      "summary": "<p>Article URL: <a href=\"https://outcrop.app/\">https://outcrop.app/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47068658\">https://news.ycombinator.com/item?id=47068658</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 01:08:58 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.928,
      "tier1_quick_score": 2.865
    },
    {
      "id": "03091ec2c0cfc536",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Analytics that tells AI product teams where their AI fails user",
      "url": "https://dashboard-xi-taupe-75.vercel.app/",
      "summary": "<p>Traditional analytics tracks clicks. For AI products, you need to know: what was the user trying to do, did the AI help, and did they succeed?<p>I built a demo of this. It ingests AI conversations and runs 3 workers (GPT-4o-mini): intent classifier, quality scorer (LLM-as-judge), and task completion detector. Results show up in a dashboard designed for PMs, not engineers.<p>Stack: Python SDK (zero deps, async) → FastAPI → Supabase → GPT-4o-mini workers → Next.js dashboard.<p>Demo with sample data (not live product, validating the concept): <a href=\"https://dashboard-xi-taupe-75.vercel.app\" rel=\"nofollow\">https://dashboard-xi-taupe-75.vercel.app</a><p>The sample data models an AI app builder. Interesting patterns: scaffolding works great (78% success), but API integrations fail 75% of the time, and users who enter bug-fix loops almost always churn.<p>Key design question: is the \"insights layer\" (auto-generated recommendations, revenue-at-risk estimates, root cause identification) valuable enough to differentiate from Langfuse/Helicone adding product analytics to their existing tracing tools?<p>Looking for honest feedback, especially from AI product builders.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47068605\">https://news.ycombinator.com/item?id=47068605</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Thu, 19 Feb 2026 01:02:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.927,
      "tier1_quick_score": 2.864
    },
    {
      "id": "fe7dbebc903fef59",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "India Brief Economic Index",
      "url": "https://www.anthropic.com/research/india-brief-economic-index",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T19:36:33.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.616,
      "tier1_quick_score": 2.853
    },
    {
      "id": "ebf24cf04fa0982c",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Making AI work for everyone, everywhere: our approach to localization",
      "url": "https://openai.com/index/our-approach-to-localization",
      "summary": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
      "image_url": "",
      "published": "Fri, 06 Feb 2026 10:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.014,
      "tier1_quick_score": 2.851
    },
    {
      "id": "9f6a0346a31359dc",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Building C Compiler",
      "url": "https://www.anthropic.com/engineering/building-c-compiler",
      "summary": "",
      "image_url": "",
      "published": "2026-02-05T19:38:29.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.011,
      "tier1_quick_score": 2.848
    },
    {
      "id": "912cf25fe3586ccd",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Infrastructure Noise",
      "url": "https://www.anthropic.com/engineering/infrastructure-noise",
      "summary": "",
      "image_url": "",
      "published": "2026-02-05T17:42:29.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.011,
      "tier1_quick_score": 2.848
    },
    {
      "id": "c82ec79fbf9f9804",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "GPT-5 lowers the cost of cell-free protein synthesis",
      "url": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
      "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 11:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 2.847
    },
    {
      "id": "4e829743aa85afb8",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing Trusted Access for Cyber",
      "url": "https://openai.com/index/trusted-access-for-cyber",
      "summary": "OpenAI introduces Trusted Access for Cyber, a trust-based framework that expands access to frontier cyber capabilities while strengthening safeguards against misuse.",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 10:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 2.847
    },
    {
      "id": "ae167fa7c076bdf8",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing OpenAI Frontier",
      "url": "https://openai.com/index/introducing-openai-frontier",
      "summary": "OpenAI Frontier is an enterprise platform for building, deploying, and managing AI agents with shared context, onboarding, permissions, and governance.",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 06:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.009,
      "tier1_quick_score": 2.846
    },
    {
      "id": "6b46382980c79e4a",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Navigating health questions with ChatGPT",
      "url": "https://openai.com/index/navigating-health-questions",
      "summary": "A family shares how ChatGPT helped them prepare for critical cancer treatment decisions for their son alongside expert guidance from his doctors.",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.009,
      "tier1_quick_score": 2.846
    },
    {
      "id": "7bd3c882dcf7e77a",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing GPT-5.3-Codex",
      "url": "https://openai.com/index/introducing-gpt-5-3-codex",
      "summary": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.009,
      "tier1_quick_score": 2.846
    },
    {
      "id": "b2f960344448d2d5",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "GPT-5.3-Codex System Card",
      "url": "https://openai.com/index/gpt-5-3-codex-system-card",
      "summary": "GPT‑5.3-Codex is the most capable agentic coding model to date, combining the frontier coding performance of GPT‑5.2-Codex with the reasoning and professional knowledge capabilities of GPT‑5.2.",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.009,
      "tier1_quick_score": 2.846
    },
    {
      "id": "d689d52beed2dd22",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Demystifying Evals For Ai Agents",
      "url": "https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents",
      "summary": "",
      "image_url": "",
      "published": "2026-02-04T18:45:28.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.008,
      "tier1_quick_score": 2.845
    },
    {
      "id": "cf0ef71eaded1866",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Unlocking the Codex harness: how we built the App Server",
      "url": "https://openai.com/index/unlocking-the-codex-harness",
      "summary": "Learn how to embed the Codex agent using the Codex App Server, a bidirectional JSON-RPC API powering streaming progress, tool use, approvals, and diffs.",
      "image_url": "",
      "published": "Wed, 04 Feb 2026 13:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.007,
      "tier1_quick_score": 2.844
    },
    {
      "id": "f68e038d8548a39e",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "VfL Wolfsburg turns ChatGPT into a club-wide capability",
      "url": "https://openai.com/index/vfl-wolfsburg",
      "summary": "By focusing on people, not pilots, the Bundesliga club is scaling efficiency, creativity, and knowledge—without losing its football identity.",
      "image_url": "",
      "published": "Wed, 04 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 2.843
    },
    {
      "id": "26e2757a88ef178e",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "The Sora feed philosophy",
      "url": "https://openai.com/index/sora-feed-philosophy",
      "summary": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.004,
      "tier1_quick_score": 2.841
    },
    {
      "id": "974dde3d72e6bf95",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Snowflake and OpenAI partner to bring frontier intelligence to enterprise data",
      "url": "https://openai.com/index/snowflake-partnership",
      "summary": "OpenAI and Snowflake partner in a $200M agreement to bring frontier intelligence into enterprise data, enabling AI agents and insights directly in Snowflake.",
      "image_url": "",
      "published": "Mon, 02 Feb 2026 06:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.003,
      "tier1_quick_score": 2.84
    },
    {
      "id": "c2e00d7c926c0f28",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing the Codex app",
      "url": "https://openai.com/index/introducing-the-codex-app",
      "summary": "Introducing the Codex app for macOS—a command center for AI coding and software development with multiple agents, parallel workflows, and long-running tasks.",
      "image_url": "",
      "published": "Mon, 02 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.003,
      "tier1_quick_score": 2.84
    },
    {
      "id": "a6861025e0f1a141",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Inside OpenAI’s in-house data agent",
      "url": "https://openai.com/index/inside-our-in-house-data-agent",
      "summary": "How OpenAI built an in-house AI data agent that uses GPT-5, Codex, and memory to reason over massive datasets and deliver reliable insights in minutes.",
      "image_url": "",
      "published": "Thu, 29 Jan 2026 10:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.838
    },
    {
      "id": "8af76a98aaedba17",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT",
      "url": "https://openai.com/index/retiring-gpt-4o-and-older-models",
      "summary": "On February 13, 2026, alongside the previously announced retirement⁠ of GPT‑5 (Instant, Thinking, and Pro), we will retire GPT‑4o, GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini from ChatGPT. In the API, there are no changes at this time.",
      "image_url": "",
      "published": "Thu, 29 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.838
    },
    {
      "id": "cacb53a143831134",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Taisei Corporation shapes the next generation of talent with ChatGPT",
      "url": "https://openai.com/index/taisei",
      "summary": "Taisei Corporation uses ChatGPT Enterprise to support HR-led talent development and scale generative AI across its global construction business.",
      "image_url": "",
      "published": "Thu, 29 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.838
    },
    {
      "id": "b2fec04c8337daa6",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "EMEA Youth & Wellbeing Grant",
      "url": "https://openai.com/index/emea-youth-and-wellbeing-grant",
      "summary": "Apply for the EMEA Youth & Wellbeing Grant, a €500,000 program funding NGOs and researchers advancing youth safety and wellbeing in the age of AI.",
      "image_url": "",
      "published": "Wed, 28 Jan 2026 01:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.838
    },
    {
      "id": "92ce58d187a17ee3",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "The next chapter for AI in the EU",
      "url": "https://openai.com/index/the-next-chapter-for-ai-in-the-eu",
      "summary": "OpenAI launches the EU Economic Blueprint 2.0 with new data, partnerships, and initiatives to accelerate AI adoption, skills, and growth across Europe.",
      "image_url": "",
      "published": "Wed, 28 Jan 2026 01:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.838
    },
    {
      "id": "443337d9f1fab6df",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Keeping your data safe when an AI agent clicks a link",
      "url": "https://openai.com/index/ai-agent-link-safety",
      "summary": "Learn how OpenAI protects user data when AI agents open links, preventing URL-based data exfiltration and prompt injection with built-in safeguards.",
      "image_url": "",
      "published": "Wed, 28 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.838
    },
    {
      "id": "3d62d157a9fb33c0",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "PVH reimagines the future of fashion with OpenAI",
      "url": "https://openai.com/index/pvh-future-of-fashion",
      "summary": "PVH Corp., parent company of Calvin Klein and Tommy Hilfiger, is adopting ChatGPT Enterprise to bring AI into fashion design, supply chain, and consumer engagement.",
      "image_url": "",
      "published": "Tue, 27 Jan 2026 06:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "c6bbb381c4c62cfe",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Powering tax donations with AI powered personalized recommendations",
      "url": "https://openai.com/index/trustbank",
      "summary": "TRUSTBANK partnered with Recursive to build Choice AI using OpenAI models, delivering personalized, conversational recommendations that simplify Furusato Nozei gift discovery. A multi-agent system helps donors navigate thousands of options and find gifts that match their preferences.",
      "image_url": "",
      "published": "Tue, 27 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "2b9a95ae28f5d889",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing Prism",
      "url": "https://openai.com/index/introducing-prism",
      "summary": "Prism is a free LaTeX-native workspace with GPT-5.2 built in, helping researchers write, collaborate, and reason in one place.",
      "image_url": "",
      "published": "Tue, 27 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "113ca482302d82d2",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "How Indeed uses AI to help evolve the job search",
      "url": "https://openai.com/index/indeed-maggie-hulce",
      "summary": "Indeed’s CRO Maggie Hulce shares how AI is transforming job search, recruiting, and talent acquisition for employers and job seekers.",
      "image_url": "",
      "published": "Mon, 26 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "b61d2d12af8f8b49",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Unrolling the Codex agent loop",
      "url": "https://openai.com/index/unrolling-the-codex-agent-loop",
      "summary": "A technical deep dive into the Codex agent loop, explaining how Codex CLI orchestrates models, tools, prompts, and performance using the Responses API.",
      "image_url": "",
      "published": "Fri, 23 Jan 2026 12:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "b518ddec0acdec8d",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Scaling PostgreSQL to power 800 million ChatGPT users",
      "url": "https://openai.com/index/scaling-postgresql",
      "summary": "An inside look at how OpenAI scaled PostgreSQL to millions of queries per second using replicas, caching, rate limiting, and workload isolation.",
      "image_url": "",
      "published": "Thu, 22 Jan 2026 12:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "06b92625ece2ea5c",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Inside Praktika's conversational approach to language learning",
      "url": "https://openai.com/index/praktika",
      "summary": "How Praktika uses GPT-4.1 and GPT-5.2 to build adaptive AI tutors that personalize lessons, track progress, and help learners achieve real-world language fluency",
      "image_url": "",
      "published": "Thu, 22 Jan 2026 05:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "51f698ecb8840e49",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Inside GPT-5 for Work: How Businesses Use GPT-5",
      "url": "https://openai.com/business/guides-and-resources/chatgpt-usage-and-adoption-patterns-at-work",
      "summary": "A data-driven report on how workers across industries use ChatGPT—covering adoption trends, top tasks, departmental patterns, and the future of AI at work.",
      "image_url": "",
      "published": "Thu, 22 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "7e790b947fa69980",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "How Higgsfield turns simple ideas into cinematic social videos",
      "url": "https://openai.com/index/higgsfield",
      "summary": "Discover how Higgsfield gives creators cinematic, social-first video output from simple inputs using OpenAI GPT-4.1, GPT-5, and Sora 2.",
      "image_url": "",
      "published": "Wed, 21 Jan 2026 10:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "469f9a6719278f63",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "How countries can end the capability overhang",
      "url": "https://openai.com/index/how-countries-can-end-the-capability-overhang",
      "summary": "Our latest report reveals stark differences in advanced AI adoption across countries and outlines new initiatives to help nations capture productivity gains from AI.",
      "image_url": "",
      "published": "Wed, 21 Jan 2026 01:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "fa2fed92141bd20c",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing Edu for Countries",
      "url": "https://openai.com/index/edu-for-countries",
      "summary": "Edu for Countries is a new OpenAI initiative helping governments use AI to modernize education systems and build future-ready workforces.",
      "image_url": "",
      "published": "Wed, 21 Jan 2026 01:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "ed986a99514cf288",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Horizon 1000: Advancing AI for primary healthcare",
      "url": "https://openai.com/index/horizon-1000",
      "summary": "OpenAI and the Gates Foundation launch Horizon 1000, a $50M pilot advancing AI capabilities for healthcare in Africa. The initiative aims to reach 1,000 clinics by 2028.",
      "image_url": "",
      "published": "Tue, 20 Jan 2026 21:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "20b1e4b4fbc6fb96",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Stargate Community",
      "url": "https://openai.com/index/stargate-community",
      "summary": "Stargate Community plans detail a community-first approach to AI infrastructure, using locally tailored plans shaped by community input, energy needs, and workforce priorities.",
      "image_url": "",
      "published": "Tue, 20 Jan 2026 19:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "1ec70862d85c1973",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Claude Code Best Practices",
      "url": "https://www.anthropic.com/engineering/claude-code-best-practices",
      "summary": "",
      "image_url": "",
      "published": "2026-01-26T23:24:56.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "bef6fafffc802698",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Ai Resistant Technical Evaluations",
      "url": "https://www.anthropic.com/engineering/AI-resistant-technical-evaluations",
      "summary": "",
      "image_url": "",
      "published": "2026-01-22T01:16:20.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "e2b90dd8fabfd8f2",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Contextual Retrieval",
      "url": "https://www.anthropic.com/engineering/contextual-retrieval",
      "summary": "",
      "image_url": "",
      "published": "2026-01-07T15:00:44.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "512a12b7ae75ff77",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Effective Context Engineering For Ai Agents",
      "url": "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
      "summary": "",
      "image_url": "",
      "published": "2026-01-06T15:31:12.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "88027f7315d59a33",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Building Effective Agents",
      "url": "https://www.anthropic.com/engineering/building-effective-agents",
      "summary": "",
      "image_url": "",
      "published": "2026-01-06T15:24:44.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "4bffd18b76c47d13",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Multi Agent Research System",
      "url": "https://www.anthropic.com/engineering/multi-agent-research-system",
      "summary": "",
      "image_url": "",
      "published": "2026-01-06T15:09:32.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "13d851a818b9d01a",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Writing Tools For Agents",
      "url": "https://www.anthropic.com/engineering/writing-tools-for-agents",
      "summary": "",
      "image_url": "",
      "published": "2026-01-06T15:05:23.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "9c6963896a79a683",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Claude Think Tool",
      "url": "https://www.anthropic.com/engineering/claude-think-tool",
      "summary": "",
      "image_url": "",
      "published": "2025-12-15T18:02:25.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "317293c01f1d2a57",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "A Postmortem Of Three Recent Issues",
      "url": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
      "summary": "",
      "image_url": "",
      "published": "2025-12-14T20:27:33.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "a5a54faf2dba6740",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Effective Harnesses For Long Running Agents",
      "url": "https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents",
      "summary": "",
      "image_url": "",
      "published": "2025-11-26T18:07:21.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "befa65b8a744151e",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Advanced Tool Use",
      "url": "https://www.anthropic.com/engineering/advanced-tool-use",
      "summary": "",
      "image_url": "",
      "published": "2025-11-25T16:53:53.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "8761099c9d017166",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Code Execution With Mcp",
      "url": "https://www.anthropic.com/engineering/code-execution-with-mcp",
      "summary": "",
      "image_url": "",
      "published": "2025-11-04T22:06:04.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "cf1f68524d89bd5b",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Claude Code Sandboxing",
      "url": "https://www.anthropic.com/engineering/claude-code-sandboxing",
      "summary": "",
      "image_url": "",
      "published": "2025-11-03T21:01:45.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "93f054b6e0ffd331",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Desktop Extensions",
      "url": "https://www.anthropic.com/engineering/desktop-extensions",
      "summary": "",
      "image_url": "",
      "published": "2025-09-12T00:43:28.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "96ce3fe2047170df",
      "source": "anthropic_engineering",
      "source_weight": 2.0,
      "title": "Swe Bench Sonnet",
      "url": "https://www.anthropic.com/engineering/swe-bench-sonnet",
      "summary": "",
      "image_url": "",
      "published": "2025-03-19T20:43:04.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.837
    },
    {
      "id": "f09c45ee226de24a",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Chris Liddell Appointed Anthropic Board",
      "url": "https://www.anthropic.com/news/chris-liddell-appointed-anthropic-board",
      "summary": "",
      "image_url": "",
      "published": "2026-02-13T16:21:14.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.155,
      "tier1_quick_score": 2.792
    },
    {
      "id": "b91259f7d1a90da4",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Codepath Partnership",
      "url": "https://www.anthropic.com/news/anthropic-codepath-partnership",
      "summary": "",
      "image_url": "",
      "published": "2026-02-13T16:19:50.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.155,
      "tier1_quick_score": 2.792
    },
    {
      "id": "a76c02cb3f5a6457",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude For Financial Services",
      "url": "https://www.anthropic.com/news/claude-for-financial-services",
      "summary": "",
      "image_url": "",
      "published": "2026-02-13T15:35:49.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.154,
      "tier1_quick_score": 2.791
    },
    {
      "id": "bf60b833f693787c",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "GitHub Agentic Workflows Unleash AI-Driven Repository Automation",
      "url": "https://www.infoq.com/news/2026/02/github-agentic-workflows/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/github-agentic-workflows/en/headerimage/github-agentic-workflows-1771415135302.jpeg\" /><p>Recently launched in technical preview, GitHub Agentic Workflows introduce a way to automate complex, repetitive repository tasks using coding agents that understand context and intent, GitHub says. This enables workflows such as automatic issue triage and labeling, documentation updates, CI troubleshooting, test improvements, and reporting.</p> <i>By Sergio De Simone</i>",
      "image_url": "https://res.infoq.com/news/2026/02/github-agentic-workflows/en/headerimage/github-agentic-workflows-1771415135302.jpeg",
      "published": "Wed, 18 Feb 2026 12:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.773,
      "tier1_quick_score": 2.76
    },
    {
      "id": "61bc1bd4729cda9c",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Raises 30 Billion Series G Funding 380 Billion Post Money Valuation",
      "url": "https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation",
      "summary": "",
      "image_url": "",
      "published": "2026-02-12T21:43:18.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.12,
      "tier1_quick_score": 2.757
    },
    {
      "id": "5ded8551ab4e9f3f",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "IBM and UC Berkeley Diagnose Why Enterprise Agents Fail Using IT-Bench and MAST",
      "url": "https://huggingface.co/blog/ibm-research/itbenchandmast",
      "summary": "",
      "image_url": "",
      "published": "Wed, 18 Feb 2026 16:15:45 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.82,
      "tier1_quick_score": 2.757
    },
    {
      "id": "95be95be6ca33f4b",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Claude Sonnet 4.6: clean upgrade of 4.5, mostly better with some caveats",
      "url": "https://www.latent.space/p/ainews-claude-sonnet-46-clean-upgrade",
      "summary": "Anthropic notches another W.",
      "image_url": "https://substackcdn.com/image/fetch/$s_!z85d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe582538-a851-4fbd-a810-2274e13fd0be_1880x1132.png",
      "published": "Wed, 18 Feb 2026 06:48:36 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.719,
      "tier1_quick_score": 2.756
    },
    {
      "id": "e2fb706f0e744611",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Donate Public First Action",
      "url": "https://www.anthropic.com/news/donate-public-first-action",
      "summary": "",
      "image_url": "",
      "published": "2026-02-12T14:45:37.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.109,
      "tier1_quick_score": 2.746
    },
    {
      "id": "8af6289d4ecb1da2",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Introducing Claude Sonnet 4.6",
      "url": "https://simonwillison.net/2026/Feb/17/claude-sonnet-46/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.anthropic.com/news/claude-sonnet-4-6\">Introducing Claude Sonnet 4.6</a></strong></p>\nSonnet 4.6 is out today, and Anthropic claim it offers similar performance to <a href=\"https://simonwillison.net/2025/Nov/24/claude-opus/\">November's Opus 4.5</a> while maintaining the Sonnet pricing of $3/million input and $15/million output tokens (the Opus models are $5/$25). Here's <a href=\"https://www-cdn.anthropic.com/78073f739564e986ff3e28522761a7a0b4484f84.pdf\">the system card PDF</a>.</p>\n<p>Sonnet 4.6 has a \"reliable knowledge cutoff\" of August 2025, compared to Opus 4.6's May 2025 and Haiku 4.5's February 2025. Both Opus and Sonnet default to 200,000 max input tokens but can stretch to 1 million in beta and at a higher cost.</p>\n<p>I just released <a href=\"https://github.com/simonw/llm-anthropic/releases/tag/0.24\">llm-anthropic 0.24</a> with support for both Sonnet 4.6 and Opus 4.6. Claude Code <a href=\"https://github.com/simonw/llm-anthropic/pull/65\">did most of the work</a> - the new models had a fiddly amount of extra details around adaptive thinking and no longer supporting prefixes, as described <a href=\"https://platform.claude.com/docs/en/about-claude/models/migration-guide\">in Anthropic's migration guide</a>.</p>\n<p>Here's <a href=\"https://gist.github.com/simonw/b185576a95e9321b441f0a4dfc0e297c\">what I got</a> from:</p>\n<pre><code>uvx --with llm-anthropic llm 'Generate an SVG of a pelican riding a bicycle' -m claude-sonnet-4.6\n</code></pre>\n<p><img alt=\"The pelican has a jaunty top hat with a red band. There is a string between the upper and lower beaks for some reason. The bicycle frame is warped in the wrong way.\" src=\"https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png\" /></p>\n<p>The SVG comments include:</p>\n<pre><code>&lt;!-- Hat (fun accessory) --&gt;\n</code></pre>\n<p>I tried a second time and also got a top hat. Sonnet 4.6 apparently loves top hats!</p>\n<p>For comparison, here's the pelican Opus 4.5 drew me <a href=\"https://simonwillison.net/atom/everything/(https:/simonwillison.net/2025/Nov/24/claude-opus/)\">in November</a>:</p>\n<p><img alt=\"The pelican is cute and looks pretty good. The bicycle is not great - the frame is wrong and the pelican is facing backwards when the handlebars appear to be forwards.There is also something that looks a bit like an egg on the handlebars.\" src=\"https://static.simonwillison.net/static/2025/claude-opus-4.5-pelican.jpg\" /></p>\n<p>And here's Anthropic's current best pelican, drawn by Opus 4.6 <a href=\"https://simonwillison.net/2026/Feb/5/two-new-models/\">on February 5th</a>:</p>\n<p><img alt=\"Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers.\" src=\"https://static.simonwillison.net/static/2026/opus-4.6-pelican.png\" /></p>\n<p>Opus 4.6 produces the best pelican beak/pouch. I do think the top hat from Sonnet 4.6 is a nice touch though.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=47050488\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/llm\">llm</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/llm-pricing\">llm-pricing</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png",
      "published": "2026-02-17T23:58:58+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.654,
      "tier1_quick_score": 2.741
    },
    {
      "id": "54ce197274b98a74",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Rodney v0.4.0",
      "url": "https://simonwillison.net/2026/Feb/17/rodney/#atom-everything",
      "summary": "<p><strong><a href=\"https://github.com/simonw/rodney/releases/tag/v0.4.0\">Rodney v0.4.0</a></strong></p>\nMy <a href=\"https://github.com/simonw/rodney\">Rodney</a> CLI tool for browser automation attracted quite the flurry of PRs since I announced it <a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat\">last week</a>. Here are the release notes for the just-released v0.4.0:</p>\n<blockquote>\n<ul>\n<li>Errors now use exit code 2, which means exit code 1 is just for for check failures. <a href=\"https://github.com/simonw/rodney/pull/15\">#15</a></li>\n<li>New <code>rodney assert</code> command for running JavaScript tests, exit code 1 if they fail. <a href=\"https://github.com/simonw/rodney/issues/19\">#19</a></li>\n<li>New directory-scoped sessions with <code>--local</code>/<code>--global</code> flags. <a href=\"https://github.com/simonw/rodney/pull/14\">#14</a></li>\n<li>New <code>reload --hard</code> and <code>clear-cache</code> commands. <a href=\"https://github.com/simonw/rodney/pull/17\">#17</a></li>\n<li>New <code>rodney start --show</code> option to make the browser window visible. Thanks, <a href=\"https://github.com/antocuni\">Antonio Cuni</a>. <a href=\"https://github.com/simonw/rodney/paull/13\">#13</a></li>\n<li>New <code>rodney connect PORT</code> command to debug an already-running Chrome instance. Thanks, <a href=\"https://github.com/pnf\">Peter Fraenkel</a>. <a href=\"https://github.com/simonw/rodney/pull/12\">#12</a></li>\n<li>New <code>RODNEY_HOME</code> environment variable to support custom state directories. Thanks, <a href=\"https://github.com/senko\">Senko Rašić</a>. <a href=\"https://github.com/simonw/rodney/pull/11\">#11</a></li>\n<li>New <code>--insecure</code> flag to ignore certificate errors. Thanks, <a href=\"https://github.com/zgolus\">Jakub Zgoliński</a>. <a href=\"https://github.com/simonw/rodney/pull/10\">#10</a></li>\n<li>Windows support: avoid <code>Setsid</code> on Windows via build-tag helpers. Thanks, <a href=\"https://github.com/adm1neca\">adm1neca</a>. <a href=\"https://github.com/simonw/rodney/pull/18\">#18</a></li>\n<li>Tests now run on <code>windows-latest</code> and <code>macos-latest</code> in addition to Linux.</li>\n</ul>\n</blockquote>\n<p>I've been using <a href=\"https://github.com/simonw/showboat\">Showboat</a> to create demos of new features - here those are for <a href=\"https://github.com/simonw/rodney/tree/v0.4.0/notes/assert-command-demo\">rodney assert</a>, <a href=\"https://github.com/simonw/rodney/tree/v0.4.0/notes/clear-cache-demo\">rodney reload --hard</a>, <a href=\"https://github.com/simonw/rodney/tree/v0.4.0/notes/error-codes-demo\">rodney exit codes</a>, and <a href=\"https://github.com/simonw/rodney/tree/v0.4.0/notes/local-sessions-demo\">rodney start --local</a>.</p>\n<p>The <code>rodney assert</code> command is pretty neat: you can now Rodney to test a web app through multiple steps in a shell script that looks something like this (adapted from <a href=\"https://github.com/simonw/rodney/blob/v0.4.0/README.md#combining-checks-in-a-shell-script\">the README</a>):</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#!</span>/bin/bash</span>\n<span class=\"pl-c1\">set</span> -euo pipefail\n\nFAIL=0\n\n<span class=\"pl-en\">check</span>() {\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">!</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$@</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">;</span> <span class=\"pl-k\">then</span>\n        <span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FAIL: <span class=\"pl-smi\">$*</span><span class=\"pl-pds\">\"</span></span>\n        FAIL=1\n    <span class=\"pl-k\">fi</span>\n}\n\nrodney start\nrodney open <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://example.com<span class=\"pl-pds\">\"</span></span>\nrodney waitstable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert elements exist</span>\ncheck rodney exists <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h1<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert key elements are visible</span>\ncheck rodney visible <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h1<span class=\"pl-pds\">\"</span></span>\ncheck rodney visible <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>#main-content<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert JS expressions</span>\ncheck rodney assert <span class=\"pl-s\"><span class=\"pl-pds\">'</span>document.title<span class=\"pl-pds\">'</span></span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Example Domain<span class=\"pl-pds\">'</span></span>\ncheck rodney assert <span class=\"pl-s\"><span class=\"pl-pds\">'</span>document.querySelectorAll(\"p\").length<span class=\"pl-pds\">'</span></span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2<span class=\"pl-pds\">'</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert accessibility requirements</span>\ncheck rodney ax-find --role navigation\n\nrodney stop\n\n<span class=\"pl-k\">if</span> [ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$FAIL</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">-ne</span> 0 ]<span class=\"pl-k\">;</span> <span class=\"pl-k\">then</span>\n    <span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Some checks failed<span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c1\">exit</span> 1\n<span class=\"pl-k\">fi</span>\n<span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>All checks passed<span class=\"pl-pds\">\"</span></span></pre></div>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/browsers\">browsers</a>, <a href=\"https://simonwillison.net/tags/projects\">projects</a>, <a href=\"https://simonwillison.net/tags/testing\">testing</a>, <a href=\"https://simonwillison.net/tags/annotated-release-notes\">annotated-release-notes</a>, <a href=\"https://simonwillison.net/tags/rodney\">rodney</a></p>",
      "image_url": "",
      "published": "2026-02-17T23:02:33+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.646,
      "tier1_quick_score": 2.733
    },
    {
      "id": "5f6c5f2c7309fb50",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Presentation: Panel: Modern Data Architectures",
      "url": "https://www.infoq.com/presentations/panel-modern-data-architectures/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/presentations/panel-modern-data-architectures/en/mediumimage/ln-540x400-1770815269544.jpg\" /><p>The panelists emphasize that data engineering is no longer just about \"click-and-drag\" UI tools; it is software engineering applied to data.</p> <i>By Fabiane Nardon, Matthias Niehoff, Adi Polak, Sarah Usher</i>",
      "image_url": "https://res.infoq.com/presentations/panel-modern-data-architectures/en/mediumimage/ln-540x400-1770815269544.jpg",
      "published": "Wed, 18 Feb 2026 09:20:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.745,
      "tier1_quick_score": 2.732
    },
    {
      "id": "dc134a75ef857e94",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Healthcare Life Sciences",
      "url": "https://www.anthropic.com/news/healthcare-life-sciences",
      "summary": "",
      "image_url": "",
      "published": "2026-02-11T22:31:33.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.087,
      "tier1_quick_score": 2.724
    },
    {
      "id": "d90c001d10ad2d99",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Covering Electricity Price Increases",
      "url": "https://www.anthropic.com/news/covering-electricity-price-increases",
      "summary": "",
      "image_url": "",
      "published": "2026-02-11T21:08:31.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.085,
      "tier1_quick_score": 2.722
    },
    {
      "id": "87ab2257aa76cec8",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "How Dropbox Built a Scalable Context Engine for Enterprise Knowledge Search",
      "url": "https://www.infoq.com/news/2026/02/dropbox-context-engine/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/dropbox-context-engine/en/headerimage/generatedHeaderImage-1771284814159.jpg\" /><p>Dropbox engineers have detailed how the company built the context engine behind Dropbox Dash, revealing a shift toward index-based retrieval, knowledge graph-derived context, and continuous evaluation to support enterprise AI at scale</p> <i>By Matt Foster</i>",
      "image_url": "https://res.infoq.com/news/2026/02/dropbox-context-engine/en/headerimage/generatedHeaderImage-1771284814159.jpg",
      "published": "Wed, 18 Feb 2026 07:23:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.725,
      "tier1_quick_score": 2.712
    },
    {
      "id": "af71e79c3870b103",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "New in Agent Builder: all new agent chat, file uploads + tool registry",
      "url": "https://blog.langchain.com/new-in-agent-builder-all-new-agent-chat-file-uploads-tool-registry/",
      "summary": "<p>Today, we&apos;re expanding what you can do with <a href=\"https://www.langchain.com/langsmith/agent-builder?ref=blog.langchain.com\">LangSmith Agent Builder</a>. It&#x2019;s an big update built around a simple idea: working with an agent should feel like working with a teammate.</p><p>We rebuilt Agent Builder around this idea. There is now an always available agent (&#x201d;</p>",
      "image_url": "https://blog.langchain.com/content/images/2026/02/LangSmith-Agent-Builder-1.1-4.png",
      "published": "Wed, 18 Feb 2026 15:55:08 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.817,
      "tier1_quick_score": 2.704
    },
    {
      "id": "92f2a5d3a3a7ce4c",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude For Nonprofits",
      "url": "https://www.anthropic.com/news/claude-for-nonprofits",
      "summary": "",
      "image_url": "",
      "published": "2026-02-09T15:51:38.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.041,
      "tier1_quick_score": 2.678
    },
    {
      "id": "577062b415ca8221",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Quoting ROUGH DRAFT 8/2/66",
      "url": "https://simonwillison.net/2026/Feb/17/rough-draft-8266/#atom-everything",
      "summary": "<blockquote cite=\"https://www.neatorama.com/2026/02/11/The-Original-Drafts-for-Star-Treks-Opening-Narration/\"><p>This is the story of the United Space Ship Enterprise. Assigned a five year patrol of our galaxy, the giant starship visits Earth colonies, regulates commerce, and explores strange new worlds and civilizations. These are its voyages... and its adventures.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://www.neatorama.com/2026/02/11/The-Original-Drafts-for-Star-Treks-Opening-Narration/\">ROUGH DRAFT 8/2/66</a>, before the Star Trek opening narration reached its final form</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/screen-writing\">screen-writing</a>, <a href=\"https://simonwillison.net/tags/science-fiction\">science-fiction</a></p>",
      "image_url": "",
      "published": "2026-02-17T14:49:04+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.576,
      "tier1_quick_score": 2.663
    },
    {
      "id": "f4537583999480ac",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "First kākāpō chick in four years hatches on Valentine's Day",
      "url": "https://simonwillison.net/2026/Feb/17/first-kakapo-chick-in-four-years/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.doc.govt.nz/news/media-releases/2026-media-releases/first-kakapo-chick-in-four-years-hatches-on-valentines-day/\">First kākāpō chick in four years hatches on Valentine&#x27;s Day</a></strong></p>\nFirst chick of <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season\">the 2026 breeding season</a>!</p>\n<blockquote>\n<p>Kākāpō Yasmine hatched an egg fostered from kākāpō Tīwhiri on Valentine's Day, bringing the total number of kākāpō to 237 – though it won’t be officially added to the population until it fledges.</p>\n</blockquote>\n<p>Here's why the egg was fostered:</p>\n<blockquote>\n<p>\"Kākāpō mums typically have the best outcomes when raising a maximum of two chicks. Biological mum Tīwhiri has four fertile eggs this season already, while Yasmine, an experienced foster mum, had no fertile eggs.\"</p>\n</blockquote>\n<p>And an <a href=\"https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b\">update from conservation biologist Andrew Digby</a> - a second chick hatched this morning!</p>\n<blockquote>\n<p>The second #kakapo chick of the #kakapo2026 breeding season hatched this morning: Hine Taumai-A1-2026 on Ako's nest on Te Kākahu. We transferred the egg from Anchor two nights ago. This is Ako's first-ever chick, which is just a few hours old in this video.</p>\n</blockquote>\n<p>That post <a href=\"https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b\">has a video</a> of mother and chick.</p>\n<p><img alt=\"A beautiful charismatic green Kākāp feeding a little grey chick\" src=\"https://static.simonwillison.net/static/2026/kakapo-plus-chick.jpg\" />\n\n    <p><small></small>Via <a href=\"https://www.metafilter.com/212231/Happy-Valen-Kkp-Tines\">MetaFilter</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/kakapo\">kakapo</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/kakapo-plus-chick.jpg",
      "published": "2026-02-17T14:09:43+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.571,
      "tier1_quick_score": 2.658
    },
    {
      "id": "4bb02b3d513f839c",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Quoting Dimitris Papailiopoulos",
      "url": "https://simonwillison.net/2026/Feb/17/dimitris-papailiopoulos/#atom-everything",
      "summary": "<blockquote cite=\"https://twitter.com/dimitrispapail/status/2023080289828831349\"><p>But the intellectually interesting part for me is something else. <strong>I now have something close to a magic box where I throw in a question and a first answer comes back basically for free, in terms of human effort</strong>. Before this, the way I'd explore a new idea is to either clumsily put something together myself or ask a student to run something short for signal, and if it's there, we’d go deeper. That quick signal step, i.e., finding out if a question has any meat to it, is what I can now do without taking up anyone else's time. It’s now between just me, Claude Code, and a few days of GPU time.</p>\n<p>I don’t know what this means for how we do research long term. I don’t think anyone does yet. But <strong>the distance between a question and a first answer just got very small</strong>.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/dimitrispapail/status/2023080289828831349\">Dimitris Papailiopoulos</a>, on running research questions though Claude Code</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/research\">research</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
      "image_url": "",
      "published": "2026-02-17T14:04:44+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.57,
      "tier1_quick_score": 2.657
    },
    {
      "id": "adc962dc0eb8fb5f",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Is A Space To Think",
      "url": "https://www.anthropic.com/news/claude-is-a-space-to-think",
      "summary": "",
      "image_url": "",
      "published": "2026-02-04T17:29:42.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.008,
      "tier1_quick_score": 2.645
    },
    {
      "id": "87e9dc91cff0bded",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Protecting Well Being Of Users",
      "url": "https://www.anthropic.com/news/protecting-well-being-of-users",
      "summary": "",
      "image_url": "",
      "published": "2026-02-03T22:04:42.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 2.643
    },
    {
      "id": "da97f24efa77bffc",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Apple Xcode Claude Agent Sdk",
      "url": "https://www.anthropic.com/news/apple-xcode-claude-agent-sdk",
      "summary": "",
      "image_url": "",
      "published": "2026-02-03T18:06:03.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 2.643
    },
    {
      "id": "3618155c5b232ac6",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-sdk==0.3.7",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.7",
      "summary": "<p>Changes since sdk==0.3.6</p>\n<ul>\n<li>fix(sdk-py): allow reset of config/context in assistants update (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6862\">#6862</a>)</li>\n<li>chore: state_updated_at sort by (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6857\">#6857</a>)</li>\n<li>chore: bump orjson (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6852\">#6852</a>)</li>\n<li>chore: conformance testing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6842\">#6842</a>)</li>\n<li>chore(deps): bump the all-dependencies group in /libs/sdk-py with 4 updates (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6809\">#6809</a>)</li>\n<li>chore(deps): bump langchain-core from 1.2.10 to 1.2.11 in /libs/sdk-py (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6829\">#6829</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-18T19:17:53Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.856,
      "tier1_quick_score": 2.643
    },
    {
      "id": "22ae838c5b8585d0",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Partners With Allen Institute And Howard Hughes Medical Institute",
      "url": "https://www.anthropic.com/news/anthropic-partners-with-allen-institute-and-howard-hughes-medical-institute",
      "summary": "",
      "image_url": "",
      "published": "2026-02-02T14:22:01.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.004,
      "tier1_quick_score": 2.641
    },
    {
      "id": "86624d7aa186ea4d",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.15",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.15",
      "summary": "<h1>Release Notes</h1>\n<h2>[2026-02-18]</h2>\n<h3>llama-index-agent-agentmesh [0.1.0]</h3>\n<ul>\n<li>[Integration] AgentMesh: Trust Layer for LlamaIndex Agents (<a href=\"https://github.com/run-llama/llama_index/pull/20644\">#20644</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.15]</h3>\n<ul>\n<li>Support basic operations for multimodal types (<a href=\"https://github.com/run-llama/llama_index/pull/20640\">#20640</a>)</li>\n<li>Feat recursive llm type support (<a href=\"https://github.com/run-llama/llama_index/pull/20642\">#20642</a>)</li>\n<li>fix: remove redundant metadata_seperator field from TextNode (<a href=\"https://github.com/run-llama/llama_index/pull/20649\">#20649</a>)</li>\n<li>fix(tests): update mock prompt type in mock_prompts.py (<a href=\"https://github.com/run-llama/llama_index/pull/20661\">#20661</a>)</li>\n<li>Feat multimodal template var formatting (<a href=\"https://github.com/run-llama/llama_index/pull/20682\">#20682</a>)</li>\n<li>Feat multimodal prompt templates (<a href=\"https://github.com/run-llama/llama_index/pull/20683\">#20683</a>)</li>\n<li>Feat multimodal chat prompt helper (<a href=\"https://github.com/run-llama/llama_index/pull/20684\">#20684</a>)</li>\n<li>Add retry and error handling to BaseExtractor (<a href=\"https://github.com/run-llama/llama_index/pull/20693\">#20693</a>)</li>\n<li>ensure at least one message/content block is returned by the old memory (<a href=\"https://github.com/run-llama/llama_index/pull/20729\">#20729</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ibm [0.6.0.post1]</h3>\n<ul>\n<li>chore: Remove persistent_connection parameter support, update (<a href=\"https://github.com/run-llama/llama_index/pull/20714\">#20714</a>)</li>\n<li>docs: Update IBM docs (<a href=\"https://github.com/run-llama/llama_index/pull/20718\">#20718</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.9]</h3>\n<ul>\n<li>Sonnet 4-6 addition (<a href=\"https://github.com/run-llama/llama_index/pull/20723\">#20723</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.10]</h3>\n<ul>\n<li>fix(bedrock-converse): ensure thinking_delta is populated in all chat modes (<a href=\"https://github.com/run-llama/llama_index/pull/20664\">#20664</a>)</li>\n<li>feat(bedrock-converse): Add support for Claude Sonnet 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20726\">#20726</a>)</li>\n</ul>\n<h3>llama-index-llms-ibm [0.7.0.post1]</h3>\n<ul>\n<li>chore: Remove persistent_connection parameter support, update (<a href=\"https://github.com/run-llama/llama_index/pull/20714\">#20714</a>)</li>\n<li>docs: Update IBM docs (<a href=\"https://github.com/run-llama/llama_index/pull/20718\">#20718</a>)</li>\n</ul>\n<h3>llama-index-llms-mistralai [0.10.0]</h3>\n<ul>\n<li>Rrubini/mistral azure sdk (<a href=\"https://github.com/run-llama/llama_index/pull/20668\">#20668</a>)</li>\n</ul>\n<h3>llama-index-llms-oci-data-science [1.0.0]</h3>\n<ul>\n<li>Add support for new OCI DataScience endpoint /predictWithStream for streaming use case (<a href=\"https://github.com/run-llama/llama_index/pull/20545\">#20545</a>)</li>\n</ul>\n<h3>llama-index-observability-otel [0.3.0]</h3>\n<ul>\n<li>improve otel data serialization by flattening dicts (<a href=\"https://github.com/run-llama/llama_index/pull/20719\">#20719</a>)</li>\n<li>feat: support custom span processor; refactor: use llama-index-instrumentation instead of llama-index-core (<a href=\"https://github.com/run-llama/llama_index/pull/20732\">#20732</a>)</li>\n</ul>\n<h3>llama-index-program-evaporate [0.5.2]</h3>\n<ul>\n<li>Sandbox LLM-generated code execution in EvaporateExtractor (<a href=\"https://github.com/run-llama/llama_index/pull/20676\">#20676</a>)</li>\n</ul>\n<h3>llama-index-readers-bitbucket [0.4.2]</h3>\n<ul>\n<li>fix: replace mutable default argument in load_all_file_paths (<a href=\"https://github.com/run-llama/llama_index/pull/20698\">#20698</a>)</li>\n</ul>\n<h3>llama-index-readers-github [0.10.0]</h3>\n<ul>\n<li>feat: Enhance GitHubRepoReader with selective file fetching and deduplication (Issue <a class=\"issue-link js-issue-link\" href=\"https://github.com/run-llama/llama_index/issues/20471\">#20471</a>) (<a href=\"https://github.com/run-llama/llama_index/pull/20550\">#20550</a>)</li>\n</ul>\n<h3>llama-index-readers-layoutir [0.1.1]</h3>\n<ul>\n<li>feat: Add LayoutIR reader integration (<a href=\"https://github.com/run-llama/llama_index/pull/20708\">#20708</a>)</li>\n<li>fix(layoutir): hotfix for output_dir crash and Block extraction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/run-llama/llama_index/pull/20708\">#20708</a> follow-up) (<a href=\"https://github.com/run-llama/llama_index/pull/20715\">#20715</a>)</li>\n<li>fix(layoutir): restrict requires-python to &gt;=3.12 to match layoutir dependency (<a href=\"https://github.com/run-llama/llama_index/pull/20733\">#20733</a>)</li>\n</ul>\n<h3>llama-index-readers-microsoft-sharepoint [0.8.0]</h3>\n<ul>\n<li>Add pagination support for Microsoft Graph API calls in SharePoint reader (<a href=\"https://github.com/run-llama/llama_index/pull/20704\">#20704</a>)</li>\n</ul>\n<h3>llama-index-readers-whatsapp [0.4.2]</h3>\n<ul>\n<li>fix: Update WhatsAppChatLoader to retrieve DataFrame in pandas format (<a href=\"https://github.com/run-llama/llama_index/pull/20722\">#20722</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.7]</h3>\n<ul>\n<li>feat: propagate partial_params to get_tools_from_mcp utils (<a href=\"https://github.com/run-llama/llama_index/pull/20669\">#20669</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-faiss [0.5.3]</h3>\n<ul>\n<li>Replace eval() with json.loads in FaissMapVectorStore persistence (<a href=\"https://github.com/run-llama/llama_index/pull/20675\">#20675</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [1.0.0]</h3>\n<ul>\n<li>Fix: remove ORM Collection mix-usage with MilvusClient in Milvus vector store (<a href=\"https://github.com/run-llama/llama_index/pull/20687\">#20687</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-18T19:06:42Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.854,
      "tier1_quick_score": 2.641
    },
    {
      "id": "1698fab210cfb462",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Servicenow Anthropic Claude",
      "url": "https://www.anthropic.com/news/servicenow-anthropic-claude",
      "summary": "",
      "image_url": "",
      "published": "2026-01-28T22:27:37.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.638
    },
    {
      "id": "6c270de312aa4702",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Gov Uk Partnership",
      "url": "https://www.anthropic.com/news/gov-UK-partnership",
      "summary": "",
      "image_url": "",
      "published": "2026-01-27T17:47:33.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.638
    },
    {
      "id": "d1090b0c93d0ddc9",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claudes Constitution",
      "url": "https://www.anthropic.com/news/claudes-constitution",
      "summary": "",
      "image_url": "",
      "published": "2026-01-21T18:26:10.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "5adc396ab77c76e2",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Teach For All",
      "url": "https://www.anthropic.com/news/anthropic-teach-for-all",
      "summary": "",
      "image_url": "",
      "published": "2026-01-21T17:36:00.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "effb3780b4481f1c",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude New Constitution",
      "url": "https://www.anthropic.com/news/claude-new-constitution",
      "summary": "",
      "image_url": "",
      "published": "2026-01-21T16:34:47.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "1ad8b4f49514b15c",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Mariano Florentino Long Term Benefit Trust",
      "url": "https://www.anthropic.com/news/mariano-florentino-long-term-benefit-trust",
      "summary": "",
      "image_url": "",
      "published": "2026-01-20T15:09:28.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "a625cc82b9c13a98",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Appoints Irina Ghose As Managing Director Of India",
      "url": "https://www.anthropic.com/news/anthropic-appoints-irina-ghose-as-managing-director-of-india",
      "summary": "",
      "image_url": "",
      "published": "2026-01-16T03:28:16.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "bae48fe420c0151d",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Accelerating Scientific Research",
      "url": "https://www.anthropic.com/news/accelerating-scientific-research",
      "summary": "",
      "image_url": "",
      "published": "2026-01-15T22:49:21.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "414ab5999dc1d268",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Introducing Anthropic Labs",
      "url": "https://www.anthropic.com/news/introducing-anthropic-labs",
      "summary": "",
      "image_url": "",
      "published": "2026-01-13T22:46:14.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "0d91496d44e61262",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Prompting Long Context",
      "url": "https://www.anthropic.com/news/prompting-long-context",
      "summary": "",
      "image_url": "",
      "published": "2026-01-06T15:23:18.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "ca06aa5da94ea9da",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Compliance Framework Sb53",
      "url": "https://www.anthropic.com/news/compliance-framework-SB53",
      "summary": "",
      "image_url": "",
      "published": "2025-12-19T21:05:06.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "58514db118b48352",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Genesis Mission Partnership",
      "url": "https://www.anthropic.com/news/genesis-mission-partnership",
      "summary": "",
      "image_url": "",
      "published": "2025-12-18T23:06:17.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "a9ab1bcc95b77ca2",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "How People Use Claude For Support Advice And Companionship",
      "url": "https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship",
      "summary": "",
      "image_url": "",
      "published": "2025-12-12T01:47:15.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "fea9572459fb3562",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Donating The Model Context Protocol And Establishing Of The Agentic Ai Foundation",
      "url": "https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation",
      "summary": "",
      "image_url": "",
      "published": "2025-12-11T18:14:55.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "0e449fb1ae69d06a",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Political Even Handedness",
      "url": "https://www.anthropic.com/news/political-even-handedness",
      "summary": "",
      "image_url": "",
      "published": "2025-12-10T00:18:44.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "3ab707c82df25f07",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Accenture Partnership",
      "url": "https://www.anthropic.com/news/anthropic-accenture-partnership",
      "summary": "",
      "image_url": "",
      "published": "2025-12-09T17:55:19.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "cca01521c3a186d1",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Economic Index Insights From Claude Sonnet 3 7",
      "url": "https://www.anthropic.com/news/anthropic-economic-index-insights-from-claude-sonnet-3-7",
      "summary": "",
      "image_url": "",
      "published": "2025-12-07T14:53:39.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "0ca73ca8a8d841c6",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "The Anthropic Economic Index",
      "url": "https://www.anthropic.com/news/the-anthropic-economic-index",
      "summary": "",
      "image_url": "",
      "published": "2025-12-07T14:53:27.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "a39a5fb15249a533",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Snowflake Anthropic Expanded Partnership",
      "url": "https://www.anthropic.com/news/snowflake-anthropic-expanded-partnership",
      "summary": "",
      "image_url": "",
      "published": "2025-12-03T21:13:50.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "40cd6a2be347c3bf",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Signs Cms Health Tech Ecosystem Pledge To Advance Healthcare Interoperability",
      "url": "https://www.anthropic.com/news/anthropic-signs-cms-health-tech-ecosystem-pledge-to-advance-healthcare-interoperability",
      "summary": "",
      "image_url": "",
      "published": "2025-12-02T23:19:08.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "d39c75c646da99d7",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Opus 4 5",
      "url": "https://www.anthropic.com/news/claude-opus-4-5",
      "summary": "",
      "image_url": "",
      "published": "2025-12-02T22:16:48.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "0e5f5f5ac0abd2dc",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Acquires Bun As Claude Code Reaches Usd1B Milestone",
      "url": "https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone",
      "summary": "",
      "image_url": "",
      "published": "2025-12-02T18:17:32.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "f5beac66a9d5b4d3",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Education Report How Educators Use Claude",
      "url": "https://www.anthropic.com/news/anthropic-education-report-how-educators-use-claude",
      "summary": "",
      "image_url": "",
      "published": "2025-11-22T20:40:06.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "b42cc076232e9883",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Sonnet 4 5",
      "url": "https://www.anthropic.com/news/claude-sonnet-4-5",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:26:28.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "70dc6d57fbc42f80",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Thoughts On America S Ai Action Plan",
      "url": "https://www.anthropic.com/news/thoughts-on-america-s-ai-action-plan",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:26:24.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "f87abd86265a3a5d",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Raises Series F At Usd183B Post Money Valuation",
      "url": "https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:26:11.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "65078d3793b46c6f",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Haiku 4 5",
      "url": "https://www.anthropic.com/news/claude-haiku-4-5",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:25:50.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "65b08ecd50d2a05b",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Microsoft Nvidia Anthropic Announce Strategic Partnerships",
      "url": "https://www.anthropic.com/news/microsoft-nvidia-anthropic-announce-strategic-partnerships",
      "summary": "",
      "image_url": "",
      "published": "2025-11-18T18:17:07.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "4b11d6fcc5cde12e",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude In Microsoft Foundry",
      "url": "https://www.anthropic.com/news/claude-in-microsoft-foundry",
      "summary": "",
      "image_url": "",
      "published": "2025-11-18T16:52:14.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "7416f8b164e9987c",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Rwandan Government Partnership Ai Education",
      "url": "https://www.anthropic.com/news/rwandan-government-partnership-ai-education",
      "summary": "",
      "image_url": "",
      "published": "2025-11-17T13:29:53.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "3943a3ed768fbeee",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Disrupting Ai Espionage",
      "url": "https://www.anthropic.com/news/disrupting-AI-espionage",
      "summary": "",
      "image_url": "",
      "published": "2025-11-14T11:25:56.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "a835a61ba42351d0",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Maryland Partnership",
      "url": "https://www.anthropic.com/news/maryland-partnership",
      "summary": "",
      "image_url": "",
      "published": "2025-11-13T17:57:34.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "88dc42a407df8994",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "New Offices In Paris And Munich Expand European Presence",
      "url": "https://www.anthropic.com/news/new-offices-in-paris-and-munich-expand-european-presence",
      "summary": "",
      "image_url": "",
      "published": "2025-11-12T18:10:23.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "e8193ce5e3db5513",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Invests 50 Billion In American Ai Infrastructure",
      "url": "https://www.anthropic.com/news/anthropic-invests-50-billion-in-american-ai-infrastructure",
      "summary": "",
      "image_url": "",
      "published": "2025-11-12T16:56:05.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "15084be5e0440817",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Cognizant Partnership",
      "url": "https://www.anthropic.com/news/cognizant-partnership",
      "summary": "",
      "image_url": "",
      "published": "2025-11-06T20:57:55.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "9e268111a73dd856",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Economic Futures Uk Europe",
      "url": "https://www.anthropic.com/news/economic-futures-uk-europe",
      "summary": "",
      "image_url": "",
      "published": "2025-11-05T07:57:22.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "26eb897cc98a6111",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic And Iceland Announce One Of The World S First National Ai Education Pilots",
      "url": "https://www.anthropic.com/news/anthropic-and-iceland-announce-one-of-the-world-s-first-national-ai-education-pilots",
      "summary": "",
      "image_url": "",
      "published": "2025-11-04T07:36:15.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "da4441c2e4c27664",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Opening Our Tokyo Office",
      "url": "https://www.anthropic.com/news/opening-our-tokyo-office",
      "summary": "",
      "image_url": "",
      "published": "2025-10-29T21:42:18.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "59d3b01ed7721abe",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Advancing Claude For Financial Services",
      "url": "https://www.anthropic.com/news/advancing-claude-for-financial-services",
      "summary": "",
      "image_url": "",
      "published": "2025-10-27T19:47:26.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "fe873d94ab95b6f0",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Seoul Becomes Third Anthropic Office In Asia Pacific",
      "url": "https://www.anthropic.com/news/seoul-becomes-third-anthropic-office-in-asia-pacific",
      "summary": "",
      "image_url": "",
      "published": "2025-10-23T23:11:18.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "c31d7a98e1c0abbc",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Expanding Our Use Of Google Cloud Tpus And Services",
      "url": "https://www.anthropic.com/news/expanding-our-use-of-google-cloud-tpus-and-services",
      "summary": "",
      "image_url": "",
      "published": "2025-10-23T20:34:59.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "eebced64aedd3b8c",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Statement Dario Amodei American Ai Leadership",
      "url": "https://www.anthropic.com/news/statement-dario-amodei-american-ai-leadership",
      "summary": "",
      "image_url": "",
      "published": "2025-10-21T14:30:53.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "b48533d3b12ff543",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude For Life Sciences",
      "url": "https://www.anthropic.com/news/claude-for-life-sciences",
      "summary": "",
      "image_url": "",
      "published": "2025-10-21T02:53:49.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.637
    },
    {
      "id": "9fa6421b6aa531b8",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "monday Service + LangSmith: Building a Code-First Evaluation Strategy from Day 1",
      "url": "https://blog.langchain.com/customers-monday/",
      "summary": "Learn how monday Service developed an eval-driven development framework for their customer-facing service agents.",
      "image_url": "https://blog.langchain.com/content/images/2026/02/Monday-Service-case-study.png",
      "published": "Wed, 18 Feb 2026 08:05:53 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.732,
      "tier1_quick_score": 2.619
    },
    {
      "id": "bef44b20b585b2ba",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.37",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.37",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.44</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.37/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.37/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.37</pre></div>",
      "image_url": "",
      "published": "2026-02-16T21:51:27Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.455,
      "tier1_quick_score": 2.592
    },
    {
      "id": "ea62f2abfbafd592",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "One-Shot Any Web App with Gradio's gr.HTML",
      "url": "https://huggingface.co/blog/gradio-html-one-shot-apps",
      "summary": "",
      "image_url": "",
      "published": "Wed, 18 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.655,
      "tier1_quick_score": 2.592
    },
    {
      "id": "4b13b151ee051ebc",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Nano Banana Pro diff to webcomic",
      "url": "https://simonwillison.net/2026/Feb/17/release-notes-webcomic/#atom-everything",
      "summary": "<p>Given the threat of <a href=\"https://simonwillison.net/tags/cognitive-debt/\">cognitive debt</a> brought on by AI-accelerated software development leading to more projects and less deep understanding of how they work and what they actually do, it's interesting to consider artifacts that might be able to help.</p>\n<p>Nathan Baschez <a href=\"https://twitter.com/nbaschez/status/2023501535343509871\">on Twitter</a>:</p>\n<blockquote>\n<p>my current favorite trick for reducing \"cognitive debt\" (h/t @simonw\n) is to ask the LLM to write two versions of the plan:</p>\n<ol>\n<li>The version for it (highly technical and detailed)</li>\n<li>The version for me (an entertaining essay designed to build my intuition)</li>\n</ol>\n<p>Works great</p>\n</blockquote>\n<p>This inspired me to try something new. I generated <a href=\"https://github.com/simonw/showboat/compare/v0.5.0...v0.6.0.diff\">the diff</a> between v0.5.0 and v0.6.0 of my Showboat project - which introduced <a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing\">the remote publishing feature</a> - and dumped that into Nano Banana Pro with the prompt:</p>\n<blockquote>\n<p>Create a webcomic that explains the new feature as clearly and entertainingly as possible</p>\n</blockquote>\n<p>Here's <a href=\"https://gemini.google.com/share/cce6da8e5083\">what it produced</a>:</p>\n<p><img alt=\"A six-panel comic strip illustrating a tool called &quot;Showboat&quot; for live-streaming document building. Panel 1, titled &quot;THE OLD WAY: Building docs was a lonely voyage. You finished it all before anyone saw it.&quot;, shows a sad bearded man on a wooden boat labeled &quot;THE LOCALHOST&quot; holding papers and saying &quot;Almost done... then I have to export and email the HTML...&quot;. Panel 2, titled &quot;THE UPGRADE: Just set the environment variable!&quot;, shows the same man excitedly plugging in a device with a speech bubble reading &quot;ENV VAR: SHOWBOAT_REMOTE_URL&quot; and the sound effect &quot;*KA-CHUNK!*&quot;. Panel 3, titled &quot;init establishes the uplink and generates a unique UUID beacon.&quot;, shows the man typing at a keyboard with a terminal reading &quot;$ showboat init 'Live Demo'&quot;, a satellite dish transmitting to a floating label &quot;UUID: 550e84...&quot;, and a monitor reading &quot;WAITING FOR STREAM...&quot;. Panel 4, titled &quot;Every note and exec is instantly beamed to the remote viewer!&quot;, shows the man coding with sound effects &quot;*HAMMER!*&quot;, &quot;ZAP!&quot;, &quot;ZAP!&quot;, &quot;BANG!&quot; as red laser beams shoot from a satellite dish to a remote screen displaying &quot;NOTE: Step 1...&quot; and &quot;SUCCESS&quot;. Panel 5, titled &quot;Even image files are teleported in real-time!&quot;, shows a satellite dish firing a cyan beam with the sound effect &quot;*FOOMP!*&quot; toward a monitor displaying a bar chart. Panel 6, titled &quot;You just build. The audience gets the show live.&quot;, shows the man happily working at his boat while a crowd of cheering people watches a projected screen reading &quot;SHOWBOAT LIVE STREAM: Live Demo&quot;, with a label &quot;UUID: 550e84...&quot; and one person in the foreground eating popcorn.\" src=\"https://static.simonwillison.net/static/2026/nano-banana-diff.jpg\" /></p>\n<p>Good enough to publish with the release notes? I don't think so. I'm sharing it here purely to demonstrate the idea. Creating assets like this as a personal tool for thinking about novel ways to explain a feature feels worth exploring further.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/nano-banana\">nano-banana</a>, <a href=\"https://simonwillison.net/tags/gemini\">gemini</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/cognitive-debt\">cognitive-debt</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/text-to-image\">text-to-image</a>, <a href=\"https://simonwillison.net/tags/showboat\">showboat</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/nano-banana-diff.jpg",
      "published": "2026-02-17T04:51:58+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.502,
      "tier1_quick_score": 2.589
    },
    {
      "id": "61d49c9c5c9b00a3",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "NVIDIA Nemotron 2 Nano 9B Japanese: 日本のソブリンAIを支える最先端小規模言語モデル",
      "url": "https://huggingface.co/blog/nvidia/nemotron-nano-9b-v2-japanese-ja",
      "summary": "",
      "image_url": "",
      "published": "Tue, 17 Feb 2026 23:28:52 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.65,
      "tier1_quick_score": 2.587
    },
    {
      "id": "1801a1e24942383b",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Qwen3.5: Towards Native Multimodal Agents",
      "url": "https://simonwillison.net/2026/Feb/17/qwen35/#atom-everything",
      "summary": "<p><strong><a href=\"https://qwen.ai/blog?id=qwen3.5\">Qwen3.5: Towards Native Multimodal Agents</a></strong></p>\nAlibaba's Qwen just released the first two models in the Qwen 3.5 series - one open weights, one proprietary. Both are multi-modal for vision input.</p>\n<p>The open weight one is a Mixture of Experts model called Qwen3.5-397B-A17B. Interesting to see Qwen call out serving efficiency as a benefit of that architecture:</p>\n<blockquote>\n<p>Built on an innovative hybrid architecture that fuses linear attention (via Gated Delta Networks) with a sparse mixture-of-experts, the model attains remarkable inference efficiency: although it comprises 397 billion total parameters, just 17 billion are activated per forward pass, optimizing both speed and cost without sacrificing capability.</p>\n</blockquote>\n<p>It's <a href=\"https://huggingface.co/Qwen/Qwen3.5-397B-A17B\">807GB on Hugging Face</a>, and Unsloth have a <a href=\"https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF\">collection of smaller GGUFs</a> ranging in size from 94.2GB 1-bit to 462GB Q8_K_XL.</p>\n<p>I got this <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle/\">pelican</a> from the <a href=\"https://openrouter.ai/qwen/qwen3.5-397b-a17b\">OpenRouter hosted model</a> (<a href=\"https://gist.github.com/simonw/625546cf6b371f9c0040e64492943b82\">transcript</a>):</p>\n<p><img alt=\"Pelican is quite good although the neck lacks an outline for some reason. Bicycle is very basic with an incomplete frame\" src=\"https://static.simonwillison.net/static/2026/qwen3.5-397b-a17b.png\" /></p>\n<p>The proprietary hosted model is called Qwen3.5 Plus 2026-02-15, and is a little confusing. Qwen researcher <a href=\"https://twitter.com/JustinLin610/status/2023340126479569140\">Junyang Lin  says</a>:</p>\n<blockquote>\n<p>Qwen3-Plus is a hosted API version of 397B. As the model natively supports 256K tokens, Qwen3.5-Plus supports 1M token context length. Additionally it supports search and code interpreter, which you can use on Qwen Chat with Auto mode.</p>\n</blockquote>\n<p>Here's <a href=\"https://gist.github.com/simonw/9507dd47483f78dc1195117735273e20\">its pelican</a>, which is similar in quality to the open weights model:</p>\n<p><img alt=\"Similar quality pelican. The bicycle is taller and has a better frame shape. They are visually quite similar.\" src=\"https://static.simonwillison.net/static/2026/qwen3.5-plus-02-15.png\" />\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/vision-llms\">vision-llms</a>, <a href=\"https://simonwillison.net/tags/qwen\">qwen</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/openrouter\">openrouter</a>, <a href=\"https://simonwillison.net/tags/ai-in-china\">ai-in-china</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/qwen3.5-397b-a17b.png",
      "published": "2026-02-17T04:30:57+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.499,
      "tier1_quick_score": 2.586
    },
    {
      "id": "0b9784e78fa0f8a8",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Two new Showboat tools: Chartroom and datasette-showboat",
      "url": "https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#atom-everything",
      "summary": "<p>I <a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/\">introduced Showboat</a> a week ago - my CLI tool that helps coding agents create Markdown documents that demonstrate the code that they have created. I've been finding new ways to use it on a daily basis, and I've just released two new tools to help get the best out of the Showboat pattern. <a href=\"https://github.com/simonw/chartroom\">Chartroom</a> is a CLI charting tool that works well with Showboat, and <a href=\"https://github.com/simonw/datasette-showboat\">datasette-showboat</a> lets Showboat's new remote publishing feature incrementally push documents to a Datasette instance.</p>\n\n<ul>\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing\">Showboat remote publishing</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#datasette-showboat\">datasette-showboat</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#chartroom\">Chartroom</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#how-i-built-chartroom\">How I built Chartroom</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#the-burgeoning-showboat-ecosystem\">The burgeoning Showboat ecosystem</a></li>\n</ul>\n\n<h4 id=\"showboat-remote-publishing\">Showboat remote publishing</h4>\n<p>I normally use Showboat in Claude Code for web (see <a href=\"https://simonwillison.net/2026/Feb/16/rodney-claude-code/\">note from this morning</a>). I've used it in several different projects in the past few days, each of them with a prompt that looks something like this:</p>\n<blockquote>\n<p><code>Use \"uvx showboat --help\" to perform a very thorough investigation of what happens if you use the Python sqlite-chronicle and sqlite-history-json libraries against the same SQLite database table</code></p>\n</blockquote>\n<p>Here's <a href=\"https://github.com/simonw/research/blob/main/sqlite-chronicle-vs-history-json/demo.md\">the resulting document</a>.</p>\n<p>Just telling Claude Code to run <code>uvx showboat --help</code> is enough for it to learn how to use the tool - the <a href=\"https://github.com/simonw/showboat/blob/main/help.txt\">help text</a> is designed to work as a sort of ad-hoc Skill document.</p>\n<p>The one catch with this approach is that I can't <em>see</em> the new Showboat document until it's finished. I have to wait for Claude to commit the document plus embedded screenshots and push that to a branch in my GitHub repo - then I can view it through the GitHub interface.</p>\n<p>For a while I've been thinking it would be neat to have a remote web server of my own which Claude instances can submit updates to while they are working. Then this morning I realized Showboat might be the ideal mechanism to set that up...</p>\n<p>Showboat <a href=\"https://github.com/simonw/showboat/releases/tag/v0.6.0\">v0.6.0</a> adds a new \"remote\" feature. It's almost invisible to users of the tool itself, instead being configured by an environment variable.</p>\n<p>Set a variable like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> SHOWBOAT_REMOTE_URL=https://www.example.com/submit<span class=\"pl-k\">?</span>token=xyz</pre></div>\n<p>And every time you run a <code>showboat init</code> or <code>showboat note</code> or <code>showboat exec</code> or <code>showboat image</code> command the resulting document fragments will be POSTed to that API endpoint, in addition to the Showboat Markdown file itself being updated.</p>\n<p>There are <a href=\"https://github.com/simonw/showboat/blob/v0.6.0/README.md#remote-document-streaming\">full details in the Showboat README</a> - it's a very simple API format, using regular POST form variables or a multipart form upload for the image attached to <code>showboat image</code>.</p>\n<h4 id=\"datasette-showboat\">datasette-showboat</h4>\n<p>It's simple enough to build a webapp to receive these updates from Showboat, but I needed one that I could easily deploy and would work well with the rest of my personal ecosystem.</p>\n<p>So I had Claude Code write me a Datasette plugin that could act as a Showboat remote endpoint. I actually had this building at the same time as the Showboat remote feature, a neat example of running <a href=\"https://simonwillison.net/2025/Oct/5/parallel-coding-agents/\">parallel agents</a>.</p>\n<p><strong><a href=\"https://github.com/simonw/datasette-showboat\">datasette-showboat</a></strong> is a Datasette plugin that adds a <code>/-/showboat</code> endpoint to Datasette for viewing documents and a <code>/-/showboat/receive</code> endpoint for receiving updates from Showboat.</p>\n<p>Here's a very quick way to try it out:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx --with datasette-showboat --prerelease=allow \\\n  datasette showboat.db --create \\\n  -s plugins.datasette-showboat.database showboat \\\n  -s plugins.datasette-showboat.token secret123 \\\n  --root --secret cookie-secret-123</pre></div>\n<p>Click on the sign in as root link that shows up in the console, then navigate to <a href=\"http://127.0.0.1:8001/-/showboat\">http://127.0.0.1:8001/-/showboat</a> to see the interface.</p>\n<p>Now set your environment variable to point to this instance:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> SHOWBOAT_REMOTE_URL=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>http://127.0.0.1:8001/-/showboat/receive?token=secret123<span class=\"pl-pds\">\"</span></span></pre></div>\n<p>And run Showboat like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx showboat init demo.md <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Showboat Feature Demo<span class=\"pl-pds\">\"</span></span></pre></div>\n<p>Refresh that page and you should see this:</p>\n<p><img alt=\"Title: Showboat. Remote viewer for Showboat documents. Showboat Feature Demo 2026-02-17 00:06 · 6 chunks, UUID. To send showboat output to this server, set the SHOWBOAT_REMOTE_URL environment variable: export SHOWBOAT_REMOTE_URL=&quot;http://127.0.0.1:8001/-/showboat/receive?token=your-token&quot;\" src=\"https://static.simonwillison.net/static/2026/datasette-showboat-documents.jpg\" /></p>\n<p>Click through to the document, then start Claude Code or Codex or your agent of choice and prompt:</p>\n<blockquote>\n<p><code>Run 'uvx showboat --help' and then use showboat to add to the existing demo.md document with notes and exec and image to demonstrate the tool - fetch a placekitten for the image demo.</code></p>\n</blockquote>\n<p>The <code>init</code> command assigns a UUID and title and sends those up to Datasette.</p>\n<p><img alt=\"Animated demo - in the foreground a terminal window runs Claude Code, which executes various Showboat commands. In the background a Firefox window where the Showboat Feature Demo adds notes then some bash commands, then a placekitten image.\" src=\"https://static.simonwillison.net/static/2026/datasette-showboat.gif\" /></p>\n<p>The best part of this is that it works in Claude Code for web. Run the plugin on a server somewhere (an exercise left up to the reader - I use <a href=\"https://fly.io/\">Fly.io</a> to host mine) and set that <code>SHOWBOAT_REMOTE_URL</code> environment variable in your Claude environment, then any time you tell it to use Showboat the document it creates will be transmitted to your server and viewable in real time.</p>\n<p>I built <a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat\">Rodney</a>, a CLI browser automation tool, specifically to work with Showboat. It makes it easy to have a Showboat document load up web pages, interact with them via clicks or injected JavaScript and captures screenshots to embed in the Showboat document and show the effects.</p>\n<p>This is wildly useful for hacking on web interfaces using Claude Code for web, especially when coupled with the new remote publishing feature. I only got this stuff working this morning and I've already had several sessions where Claude Code has published screenshots of its work in progress, which I've then been able to provide feedback on directly in the Claude session while it's still working.</p>\n<h3 id=\"chartroom\">Chartroom</h3>\n<p>A few days ago I had another idea for a way to extend the Showboat ecosystem: what if Showboat documents could easily include charts?</p>\n<p>I sometimes fire up Claude Code for data analysis tasks, often telling it to download a SQLite database and then run queries against it to figure out interesting things from the data.</p>\n<p>With a simple CLI tool that produced PNG images I could have Claude use Showboat to build a document with embedded charts to help illustrate its findings.</p>\n<p><strong><a href=\"https://github.com/simonw/chartroom\">Chartroom</a></strong> is exactly that. It's effectively a thin wrapper around the excellent <a href=\"https://matplotlib.org/\">matplotlib</a> Python library, designed to be used by coding agents to create charts that can be embedded in Showboat documents.</p>\n<p>Here's how to render a simple bar chart:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>name,value</span>\n<span class=\"pl-s\">Alice,42</span>\n<span class=\"pl-s\">Bob,28</span>\n<span class=\"pl-s\">Charlie,35</span>\n<span class=\"pl-s\">Diana,51</span>\n<span class=\"pl-s\">Eve,19<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">|</span> uvx chartroom bar --csv \\\n  --title <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sales by Person<span class=\"pl-pds\">'</span></span> --ylabel <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sales<span class=\"pl-pds\">'</span></span></pre></div>\n<p><a href=\"https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><img alt=\"A chart of those numbers, with a title and y-axis label\" src=\"https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png\" /></a></p>\n<p>It can also do line charts, bar charts, scatter charts, and histograms - as seen in <a href=\"https://github.com/simonw/chartroom/blob/0.2.1/demo/README.md\">this demo document</a> that was built using Showboat.</p>\n<p>Chartroom can also generate alt text. If you add <code>-f alt</code> to the above it will output the alt text for the chart instead of the image:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>name,value</span>\n<span class=\"pl-s\">Alice,42</span>\n<span class=\"pl-s\">Bob,28</span>\n<span class=\"pl-s\">Charlie,35</span>\n<span class=\"pl-s\">Diana,51</span>\n<span class=\"pl-s\">Eve,19<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">|</span> uvx chartroom bar --csv \\\n  --title <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sales by Person<span class=\"pl-pds\">'</span></span> --ylabel <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sales<span class=\"pl-pds\">'</span></span> -f alt</pre></div>\n<p>Outputs:</p>\n<pre><code>Sales by Person. Bar chart of value by name — Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19\n</code></pre>\n<p>Or you can use <code>-f html</code> or <code>-f markdown</code> to get the image tag with alt text directly:</p>\n<div class=\"highlight highlight-text-md\"><pre><span class=\"pl-s\">![</span>Sales by Person. Bar chart of value by name — Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19<span class=\"pl-s\">]</span><span class=\"pl-s\">(</span><span class=\"pl-corl\">/Users/simon/chart-7.png</span><span class=\"pl-s\">)</span></pre></div>\n<p>I added support for Markdown images with alt text to Showboat in <a href=\"https://github.com/simonw/showboat/releases/tag/v0.5.0\">v0.5.0</a>, to complement this feature of Chartroom.</p>\n<p>Finally, Chartroom has support for different <a href=\"https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html\">matplotlib styles</a>. I had Claude build a Showboat document to demonstrate these all in one place - you can see that at <a href=\"https://github.com/simonw/chartroom/blob/main/demo/styles.md\">demo/styles.md</a>.</p>\n<h4 id=\"how-i-built-chartroom\">How I built Chartroom</h4>\n<p>I started the Chartroom repository with my <a href=\"https://github.com/simonw/click-app\">click-app</a> cookiecutter template, then told a fresh Claude Code for web session:</p>\n<blockquote>\n<p>We are building a Python CLI tool which uses matplotlib to generate a PNG image containing a chart. It will have multiple sub commands for different chart types, controlled by command line options. Everything you need to know to use it will be available in the single \"chartroom --help\" output.</p>\n<p>It will accept data from files or standard input as CSV or TSV or JSON, similar to how sqlite-utils accepts data - clone simonw/sqlite-utils to /tmp for reference there. Clone matplotlib/matplotlib for reference as well</p>\n<p>It will also accept data from --sql path/to/sqlite.db \"select ...\" which runs in read-only mode</p>\n<p>Start by asking clarifying questions - do not use the ask user tool though it is broken - and generate a spec for me to approve</p>\n<p>Once approved proceed using red/green TDD running tests with \"uv run pytest\"</p>\n<p>Also while building maintain a demo/README.md document using the \"uvx showboat --help\" tool - each time you get a new chart type working commit the tests, implementation, root level\nREADME update and a new version of that demo/README.md document with an inline image demo of the new chart type (which should be a UUID image filename managed by the showboat image command and should be stored in the demo/ folder</p>\n<p>Make sure \"uv build\" runs cleanly without complaining about extra directories but also ensure dist/ and uv.lock are in gitignore</p>\n</blockquote>\n<p>This got most of the work done. You can see the rest <a href=\"https://github.com/simonw/chartroom/pulls?q=is%3Apr+is%3Aclosed\">in the PRs</a> that followed.</p>\n<h4 id=\"the-burgeoning-showboat-ecosystem\">The burgeoning Showboat ecosystem</h4>\n<p>The Showboat family of tools now consists of <a href=\"https://github.com/simonw/showboat\">Showboat</a> itself, <a href=\"https://github.com/simonw/rodney\">Rodney</a> for browser automation, <a href=\"https://github.com/simonw/chartroom\">Chartroom</a> for charting and <a href=\"https://github.com/simonw/datasette-showboat\">datasette-showboat</a> for streaming remote Showboat documents to Datasette.</p>\n<p>I'm enjoying how these tools can operate together based on a very loose set of conventions. If a tool can output a path to an image Showboat can include that image in a document. Any tool that can output text can be used with Showboat.</p>\n<p>I'll almost certainly be building more tools that fit this pattern. They're very quick to knock out!</p>\n<p>The environment variable mechanism for Showboat's remote streaming is a fun hack too - so far I'm just using it to stream documents somewhere else, but it's effectively a webhook extension mechanism that could likely be used for all sorts of things I haven't thought of yet.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/charting\">charting</a>, <a href=\"https://simonwillison.net/tags/projects\">projects</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/datasette\">datasette</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/showboat\">showboat</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/datasette-showboat-documents.jpg",
      "published": "2026-02-17T00:43:45+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.474,
      "tier1_quick_score": 2.561
    },
    {
      "id": "2cef63a7c25d947d",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Moonshot AI Releases Open-Weight Kimi K2.5 Model with Vision and Agent Swarm Capabilities",
      "url": "https://www.infoq.com/news/2026/02/kimi-k25-swarm/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/kimi-k25-swarm/en/headerimage/generatedHeaderImage-1771079384813.jpg\" /><p>Moonshot AI released Kimi K2.5, their latest open-weight multimodal LLM. K2.5 excels at coding tasks, with benchmark scores comparable to frontier models such as GPT-5 and Gemini. It also features an agent swarm mode, which can direct up to 100 sub-agents for attacking problems with parallel workflow.</p> <i>By Anthony Alford</i>",
      "image_url": "https://res.infoq.com/news/2026/02/kimi-k25-swarm/en/headerimage/generatedHeaderImage-1771079384813.jpg",
      "published": "Tue, 17 Feb 2026 14:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.57,
      "tier1_quick_score": 2.557
    },
    {
      "id": "537f1b86e7eee522",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Leapwork Research Shows Why AI in Testing Still Depends on Reliability, Not Just Innovation",
      "url": "https://www.infoq.com/news/2026/02/leapwork-ai-testing/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/leapwork-ai-testing/en/headerimage/generatedHeaderImage-1771070027501.jpg\" /><p>Leapwork recently released new research showing that while confidence in AI-driven software testing is growing rapidly, accuracy, stability, and ongoing manual effort remain decisive factors in how far teams are willing to trust automation.</p> <i>By Craig Risi</i>",
      "image_url": "https://res.infoq.com/news/2026/02/leapwork-ai-testing/en/headerimage/generatedHeaderImage-1771070027501.jpg",
      "published": "Tue, 17 Feb 2026 12:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.554,
      "tier1_quick_score": 2.541
    },
    {
      "id": "90edba916fb57ab5",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Anthropic's Sonnet 4.6 matches flagship AI performance at one-fifth the cost, accelerating enterprise adoption - VentureBeat",
      "url": "https://news.google.com/rss/articles/CBMirwFBVV95cUxPRjBCTWxTbnowVXMyV25LNWhGZzgyU3ZkX01RTmhaQnhNUXBPb1BWdmNKV3UtNGxfdUw0WXlPVDIwdXpqb2YtT1BWZ1FjMWtHLUd5UnZhSVJZeTZiYkUxWVMzd3BGYU9fYUpDM0c1VmNFSlRHb2lfaWdMS19kTERzQmo3bGNPWkhha2l3bE1TNnUtZWVrV29YNHZqZW9TNnBqREw4S0VIbm5GSzdiM1Nn?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMirwFBVV95cUxPRjBCTWxTbnowVXMyV25LNWhGZzgyU3ZkX01RTmhaQnhNUXBPb1BWdmNKV3UtNGxfdUw0WXlPVDIwdXpqb2YtT1BWZ1FjMWtHLUd5UnZhSVJZeTZiYkUxWVMzd3BGYU9fYUpDM0c1VmNFSlRHb2lfaWdMS19kTERzQmo3bGNPWkhha2l3bE1TNnUtZWVrV29YNHZqZW9TNnBqREw4S0VIbm5GSzdiM1Nn?oc=5\" target=\"_blank\">Anthropic's Sonnet 4.6 matches flagship AI performance at one-fifth the cost, accelerating enterprise adoption</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
      "image_url": "",
      "published": "Tue, 17 Feb 2026 18:01:16 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.602,
      "tier1_quick_score": 2.539
    },
    {
      "id": "db9aaf0e51f7bd55",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Policy Compiler for Secure Agentic Systems",
      "url": "http://arxiv.org/abs/2602.16708v1",
      "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "image_url": "",
      "published": "2026-02-18T18:57:12Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.852,
      "tier1_quick_score": 2.539
    },
    {
      "id": "724eb03b9e6d12e9",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Knowledge-Embedded Latent Projection for Robust Representation Learning",
      "url": "http://arxiv.org/abs/2602.16709v1",
      "summary": "Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.",
      "image_url": "",
      "published": "2026-02-18T18:58:16Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.852,
      "tier1_quick_score": 2.539
    },
    {
      "id": "48be09cc68f55633",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "url": "http://arxiv.org/abs/2602.16703v1",
      "summary": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
      "image_url": "",
      "published": "2026-02-18T18:51:28Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.851,
      "tier1_quick_score": 2.538
    },
    {
      "id": "5b80ee1c6b632255",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "url": "http://arxiv.org/abs/2602.16699v1",
      "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "image_url": "",
      "published": "2026-02-18T18:46:14Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.85,
      "tier1_quick_score": 2.537
    },
    {
      "id": "7168c803e9e213eb",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Causality is Key for Interpretability Claims to Generalise",
      "url": "http://arxiv.org/abs/2602.16698v1",
      "summary": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.",
      "image_url": "",
      "published": "2026-02-18T18:45:04Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.849,
      "tier1_quick_score": 2.536
    },
    {
      "id": "fe784f4880304e45",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Protecting the Undeleted in Machine Unlearning",
      "url": "http://arxiv.org/abs/2602.16697v1",
      "summary": "Machine unlearning aims to remove specific data points from a trained model, often striving to emulate \"perfect retraining\", i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $ω(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.",
      "image_url": "",
      "published": "2026-02-18T18:44:21Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.849,
      "tier1_quick_score": 2.536
    },
    {
      "id": "e670d8ea6dbcb36a",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Parameter-free representations outperform single-cell foundation models on downstream benchmarks",
      "url": "http://arxiv.org/abs/2602.16696v1",
      "summary": "Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.",
      "image_url": "",
      "published": "2026-02-18T18:42:29Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.849,
      "tier1_quick_score": 2.536
    },
    {
      "id": "5983155f3e279e8e",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Qwen3.5-397B-A17B: the smallest Open-Opus class, very efficient model",
      "url": "https://www.latent.space/p/ainews-qwen35-397b-a17b-the-smallest",
      "summary": "Congrats Qwen team!",
      "image_url": "https://substackcdn.com/image/fetch/$s_!1fDP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0472c69a-cd07-4bde-8b10-61bc1d0702a7_2444x1704.png",
      "published": "Tue, 17 Feb 2026 04:22:56 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.498,
      "tier1_quick_score": 2.535
    },
    {
      "id": "ad8b7b8ce1e90895",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Synthetic-Powered Multiple Testing with FDR Control",
      "url": "http://arxiv.org/abs/2602.16690v1",
      "summary": "Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.",
      "image_url": "",
      "published": "2026-02-18T18:36:24Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.848,
      "tier1_quick_score": 2.535
    },
    {
      "id": "0155816bd1e1be33",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Are Object-Centric Representations Better At Compositional Generalization?",
      "url": "http://arxiv.org/abs/2602.16689v1",
      "summary": "Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.",
      "image_url": "",
      "published": "2026-02-18T18:34:07Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.847,
      "tier1_quick_score": 2.534
    },
    {
      "id": "268f0ef94c221223",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "On the Hardness of Approximation of the Fair k-Center Problem",
      "url": "http://arxiv.org/abs/2602.16688v1",
      "summary": "In this work, we study the hardness of approximation of the fair $k$-center problem. Here the data points are partitioned into groups and the task is to choose a prescribed number of data points from each group, called centers, while minimizing the maximum distance from any point to its closest center. Although a polynomial-time $3$-approximation is known for this problem in general metrics, it has remained open whether this approximation guarantee is tight or could be further improved, especially since the unconstrained $k$-center problem admits a polynomial-time factor-$2$ approximation. We resolve this open question by proving that, for every $ε>0$, achieving a $(3-ε)$-approximation is NP-hard, assuming $\\text{P} \\neq \\text{NP}$.\n  Our inapproximability results hold even when only two disjoint groups are present and at least one center must be chosen from each group. Further, it extends to the canonical one-per-group setting with $k$-groups (for arbitrary $k$), where exactly one center must be selected from each group. Consequently, the factor-$3$ barrier for fair $k$-center in general metric spaces is inherent, and existing $3$-approximation algorithms are optimal up to lower-order terms even in these restricted regimes. This result stands in sharp contrast to the $k$-supplier formulation, where both the unconstrained and fair variants admit factor-$3$ approximation in polynomial time.",
      "image_url": "",
      "published": "2026-02-18T18:33:27Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.847,
      "tier1_quick_score": 2.534
    },
    {
      "id": "33f45844a228aa19",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition",
      "url": "http://arxiv.org/abs/2602.16684v1",
      "summary": "Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.",
      "image_url": "",
      "published": "2026-02-18T18:27:21Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.846,
      "tier1_quick_score": 2.533
    },
    {
      "id": "3f5e03016b1baa5d",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Does AI Make the Agile Manifesto Obsolete?",
      "url": "https://www.infoq.com/news/2026/02/ai-agile-manifesto-debate/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/ai-agile-manifesto-debate/en/headerimage/generatedHeaderImage-1770724261142.jpg\" /><p>Capgemini's Steve Jones argues AI agents building apps in hours have killed the Agile Manifesto, as its human-centric principles don't fit agentic SDLCs. While Forrester reports 95% still find Agile relevant, Kent Beck proposes \"augmented coding\" and AWS suggests \"Intent Design\" over sprint planning. The debate: Is Agile dead, or evolving for AI collaboration?</p> <i>By Steef-Jan Wiggers</i>",
      "image_url": "https://res.infoq.com/news/2026/02/ai-agile-manifesto-debate/en/headerimage/generatedHeaderImage-1770724261142.jpg",
      "published": "Tue, 17 Feb 2026 10:34:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.543,
      "tier1_quick_score": 2.53
    },
    {
      "id": "16d0c8738761fc19",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
      "url": "http://arxiv.org/abs/2602.16671v1",
      "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
      "image_url": "",
      "published": "2026-02-18T18:09:03Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.842,
      "tier1_quick_score": 2.529
    },
    {
      "id": "a424c079248a48dd",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Towards a Science of AI Agent Reliability",
      "url": "http://arxiv.org/abs/2602.16666v1",
      "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "image_url": "",
      "published": "2026-02-18T18:05:44Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.842,
      "tier1_quick_score": 2.529
    },
    {
      "id": "76078d7a16d28a21",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Neighborhood Stability as a Measure of Nearest Neighbor Searchability",
      "url": "http://arxiv.org/abs/2602.16673v1",
      "summary": "Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset -- what we call \"searchability.\" To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.",
      "image_url": "",
      "published": "2026-02-18T18:09:47Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.842,
      "tier1_quick_score": 2.529
    },
    {
      "id": "bb9e383be47a373d",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
      "url": "http://arxiv.org/abs/2602.16660v1",
      "summary": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",
      "image_url": "",
      "published": "2026-02-18T18:01:23Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.841,
      "tier1_quick_score": 2.528
    },
    {
      "id": "302f3670a360f324",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "url": "http://arxiv.org/abs/2602.16653v1",
      "summary": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
      "image_url": "",
      "published": "2026-02-18T17:52:17Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.839,
      "tier1_quick_score": 2.526
    },
    {
      "id": "a06b8c001dea73b0",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Investigating Nonlinear Quenching Effects on Polar Field Buildup in the Sun Using Physics-Informed Neural Networks",
      "url": "http://arxiv.org/abs/2602.16656v1",
      "summary": "The solar dynamo relies on the regeneration of the poloidal magnetic field through processes strongly modulated by nonlinear feedbacks such as tilt quenching (TQ) and latitude quenching (LQ). These mechanisms play a decisive role in regulating the buildup of the Sun's polar field and, in turn, the amplitude of future solar cycles. In this work, we employ Physics-Informed Neural Networks (PINN) to solve the surface flux transport (SFT) equation, embedding physical constraints directly into the neural network framework. By systematically varying transport parameters, we isolate the relative contributions of TQ and LQ to polar dipole buildup. We use the residual dipole moment as a diagnostic for cycle-to-cycle amplification and show that TQ suppression strengthens with increasing diffusivity, while LQ dominates in advection-dominated regimes. The ratio $ΔD_{\\mathrm{LQ}}/ΔD_{\\mathrm{TQ}}$ exhibits a smooth inverse-square dependence on the dynamo effectivity range, refining previous empirical fits with improved accuracy and reduced scatter. The results further reveal that the need for a decay term is not essential for PINN set-up due to the training process. Compared with the traditional 1D SFT model, the PINN framework achieves significantly lower error metrics and more robust recovery of nonlinear trends. Our results suggest that the nonlinear interplay between LQ and TQ can naturally produce alternations between weak and strong cycles, providing a physical explanation for the observed even-odd cycle modulation. These findings demonstrate the potential of PINN as an accurate, efficient, and physically consistent tool for solar cycle prediction.",
      "image_url": "",
      "published": "2026-02-18T17:54:59Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.839,
      "tier1_quick_score": 2.526
    },
    {
      "id": "5bbd07b3265d5091",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
      "url": "http://arxiv.org/abs/2602.16650v1",
      "summary": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.",
      "image_url": "",
      "published": "2026-02-18T17:46:09Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.838,
      "tier1_quick_score": 2.525
    },
    {
      "id": "1f7cbea9676948cb",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment",
      "url": "http://arxiv.org/abs/2602.16643v1",
      "summary": "The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.",
      "image_url": "",
      "published": "2026-02-18T17:32:55Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.835,
      "tier1_quick_score": 2.522
    },
    {
      "id": "3ca46a8b1ed8a8a3",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Optimizer choice matters for the emergence of Neural Collapse",
      "url": "http://arxiv.org/abs/2602.16642v1",
      "summary": "Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.",
      "image_url": "",
      "published": "2026-02-18T17:32:43Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.835,
      "tier1_quick_score": 2.522
    },
    {
      "id": "d4fe55778bc87ad5",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
      "url": "http://arxiv.org/abs/2602.16634v1",
      "summary": "The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $Δ$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.",
      "image_url": "",
      "published": "2026-02-18T17:26:15Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.834,
      "tier1_quick_score": 2.521
    },
    {
      "id": "7e078bc27f12cc45",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes",
      "url": "http://arxiv.org/abs/2602.16629v1",
      "summary": "The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.",
      "image_url": "",
      "published": "2026-02-18T17:24:27Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.834,
      "tier1_quick_score": 2.521
    },
    {
      "id": "9c979607448b4873",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
      "url": "http://arxiv.org/abs/2602.16626v1",
      "summary": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.",
      "image_url": "",
      "published": "2026-02-18T17:21:02Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.833,
      "tier1_quick_score": 2.52
    },
    {
      "id": "e48b0840ad24fc8f",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Causal and Compositional Abstraction",
      "url": "http://arxiv.org/abs/2602.16612v1",
      "summary": "Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, and `distributed' causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level' abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI.",
      "image_url": "",
      "published": "2026-02-18T17:06:09Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.83,
      "tier1_quick_score": 2.517
    },
    {
      "id": "e7d163b3bfa38450",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
      "url": "http://arxiv.org/abs/2602.16610v1",
      "summary": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
      "image_url": "",
      "published": "2026-02-18T17:04:02Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.83,
      "tier1_quick_score": 2.517
    },
    {
      "id": "73e2db88bdd47ee7",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models",
      "url": "http://arxiv.org/abs/2602.16608v1",
      "summary": "Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \\textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.",
      "image_url": "",
      "published": "2026-02-18T17:03:10Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.829,
      "tier1_quick_score": 2.516
    },
    {
      "id": "52503e319efa0795",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
      "url": "http://arxiv.org/abs/2602.16603v1",
      "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
      "image_url": "",
      "published": "2026-02-18T16:57:45Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.828,
      "tier1_quick_score": 2.515
    },
    {
      "id": "adf1c09ed3ab0e82",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study",
      "url": "http://arxiv.org/abs/2602.16601v1",
      "summary": "Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, we obtain upper and lower bounds on the accumulated divergence between the generated and target distributions. This allows us to characterize different regimes of drift, depending on the score estimation error and the proportion of fresh data used in each generation. We also provide empirical results on synthetic data and images to illustrate the theory.",
      "image_url": "",
      "published": "2026-02-18T16:56:36Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.828,
      "tier1_quick_score": 2.515
    },
    {
      "id": "561acad2817191ab",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Predicting The Cop Number Using Machine Learning",
      "url": "http://arxiv.org/abs/2602.16600v1",
      "summary": "Cops and Robbers is a pursuit evasion game played on a graph, first introduced independently by Quilliot \\cite{quilliot1978jeux} and Nowakowski and Winkler \\cite{NOWAKOWSKI1983235} over four decades ago. A main interest in recent the literature is identifying the cop number of graph families. The cop number of a graph, $c(G)$, is defined as the minimum number of cops required to guarantee capture of the robber. Determining the cop number is computationally difficult and exact algorithms for this are typically restricted to small graph families. This paper investigates whether classical machine learning methods and graph neural networks can accurately predict a graph's cop number from its structural properties and identify which properties most strongly influence this prediction. Of the classical machine learning models, tree-based models achieve high accuracy in prediction despite class imbalance, whereas graph neural networks achieve comparable results without explicit feature engineering. The interpretability analysis shows that the most predictive features are related to node connectivity, clustering, clique structure, and width parameters, which aligns with known theoretical results. Our findings suggest that machine learning approaches can be used in complement with existing cop number algorithms by offering scalable approximations where computation is infeasible.",
      "image_url": "",
      "published": "2026-02-18T16:52:46Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.827,
      "tier1_quick_score": 2.514
    },
    {
      "id": "fbb0ccd4d5eb8578",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Sequential Membership Inference Attacks",
      "url": "http://arxiv.org/abs/2602.16596v1",
      "summary": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.",
      "image_url": "",
      "published": "2026-02-18T16:51:13Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.827,
      "tier1_quick_score": 2.514
    },
    {
      "id": "5ac927ad86a30a55",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification",
      "url": "http://arxiv.org/abs/2602.16590v1",
      "summary": "Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.",
      "image_url": "",
      "published": "2026-02-18T16:41:32Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.825,
      "tier1_quick_score": 2.512
    },
    {
      "id": "c003729efdcceaa0",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "url": "http://arxiv.org/abs/2602.16585v1",
      "summary": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.",
      "image_url": "",
      "published": "2026-02-18T16:35:47Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.824,
      "tier1_quick_score": 2.511
    },
    {
      "id": "3448c4e5eb803f1d",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Rodney and Claude Code for Desktop",
      "url": "https://simonwillison.net/2026/Feb/16/rodney-claude-code/#atom-everything",
      "summary": "<p>I'm a very heavy user of <a href=\"https://code.claude.com/docs/en/claude-code-on-the-web\">Claude Code on the web</a>, Anthropic's excellent but poorly named cloud version of Claude Code where everything runs in a container environment managed by them, greatly reducing the risk of anything bad happening to a computer I care about.</p>\n<p>I don't use the web interface at all (hence my dislike of the name) - I access it exclusively through their native iPhone and Mac desktop apps.</p>\n<p>Something I particularly appreciate about the desktop app is that it lets you see images that Claude is \"viewing\" via its <code>Read /path/to/image</code> tool. Here's what that looks like:</p>\n<p><img alt=\"Screenshot of a Claude Code session in Claude Desktop. Claude says: The debug page looks good - all items listed with titles and descriptions. Now let me check the nav\nmenu -  Analyzed menu image file - Bash uvx rodney open &quot;http://localhost:8765/&quot; 2&gt;&amp;1 &amp;&amp; uvx rodney click &quot;details.nav-menu summary&quot; 2&gt;&amp;1 &amp;% sleep 0.5 &amp;&amp; uvx rodney screenshot /tmp/menu.png 2&gt;&amp;1 Output reads: Datasette: test, Clicked, /tmp/menu.png - then it says Read /tmp/menu.png and reveals a screenshot of the Datasette interface with the nav menu open, showing only &quot;Debug&quot; and &quot;Log out&quot; options. Claude continues: The menu now has just &quot;Debug&quot; and “Log out&quot; — much cleaner. Both pages look good. Let me clean up the server and run the remaining tests.\" src=\"https://static.simonwillison.net/static/2026/rodney-claude-desktop.jpg\" /></p>\n<p>This means you can get a visual preview of what it's working on while it's working, without waiting for it to push code to GitHub for you to try out yourself later on.</p>\n<p>The prompt I used to trigger the above screenshot was:</p>\n<blockquote>\n<p><code>Run \"uvx rodney --help\" and then use Rodney to manually test the new pages and menu - look at screenshots from it and check you think they look OK</code></p>\n</blockquote>\n<p>I designed <a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat\">Rodney</a> to have <a href=\"https://github.com/simonw/rodney/blob/main/help.txt\">--help output</a> that provides everything a coding agent needs to know in order to use the tool.</p>\n<p>The Claude iPhone app doesn't display opened images yet, so I <a href=\"https://twitter.com/simonw/status/2023432616066879606\">requested it as a feature</a> just now in a thread on Twitter.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/async-coding-agents\">async-coding-agents</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/projects\">projects</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/rodney\">rodney</a></p>",
      "image_url": "",
      "published": "2026-02-16T16:38:57+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.423,
      "tier1_quick_score": 2.51
    },
    {
      "id": "62bdd21f4d2ec76c",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS",
      "url": "http://arxiv.org/abs/2602.16579v1",
      "summary": "Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.",
      "image_url": "",
      "published": "2026-02-18T16:26:36Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.822,
      "tier1_quick_score": 2.509
    },
    {
      "id": "e511296f837cc5d5",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Creating a digital poet",
      "url": "http://arxiv.org/abs/2602.16578v1",
      "summary": "Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.",
      "image_url": "",
      "published": "2026-02-18T16:25:10Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.822,
      "tier1_quick_score": 2.509
    },
    {
      "id": "b624d7c79632a47f",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "“No technology has me dreaming bigger than AI”",
      "url": "https://blog.google/company-news/inside-google/message-ceo/sundar-pichai-ai-impact-summit-2026/",
      "summary": "a stylized design resembling the Ashoka Chakra with colorful network lines and text reading \"भारत 2026 INDIA.\" A vertical line separates it from the Google logo on the right, all set against a light blue gradient background with a faint grid pattern.",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI-Google_2.max-600x600.format-webp.webp",
      "published": "Thu, 19 Feb 2026 04:30:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.972,
      "tier1_quick_score": 2.509
    },
    {
      "id": "4b0d8c688b458fd6",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "AI Impact Summit 2026",
      "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-collection/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Collection_Hero-2.max-600x600.format-webp.webp\" />A look at the partnerships and investments Google announced at the AI Impact Summit 2026.",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Collection_Hero-2.max-600x600.format-webp.webp",
      "published": "Thu, 19 Feb 2026 04:30:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.972,
      "tier1_quick_score": 2.509
    },
    {
      "id": "52eacf36afaf9511",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "MoDE-Boost: Boosting Shared Mobility Demand with Edge-Ready Prediction Models",
      "url": "http://arxiv.org/abs/2602.16573v1",
      "summary": "Urban demand forecasting plays a critical role in optimizing routing, dispatching, and congestion management within Intelligent Transportation Systems. By leveraging data fusion and analytics techniques, traffic demand forecasting serves as a key intermediate measure for identifying emerging spatial and temporal demand patterns. In this paper, we tackle this challenge by proposing two gradient boosting model variations, one for classiffication and one for regression, both capable of generating demand forecasts at various temporal horizons, from 5 minutes up to one hour. Our overall approach effectively integrates temporal and contextual features, enabling accurate predictions that are essential for improving the efficiency of shared (micro-) mobility services. To evaluate its effectiveness, we utilize open shared mobility data derived from e-scooter and e-bike networks in five metropolitan areas. These real-world datasets allow us to compare our approach with state-of-the-art methods as well as a Generative AI-based model, demonstrating its effectiveness in capturing the complexities of modern urban mobility. Ultimately, our methodology offers novel insights on urban micro-mobility management, helping to tackle the challenges arising from rapid urbanization and thus, contributing to more sustainable, efficient, and livable cities.",
      "image_url": "",
      "published": "2026-02-18T16:18:13Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.821,
      "tier1_quick_score": 2.508
    },
    {
      "id": "ef0cbb920f2fedd3",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis",
      "url": "http://arxiv.org/abs/2602.16570v1",
      "summary": "Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.\n  In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\\star}(x) \\propto p(x) \\exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\\top A x + b^\\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).",
      "image_url": "",
      "published": "2026-02-18T16:11:17Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.82,
      "tier1_quick_score": 2.507
    },
    {
      "id": "6a587611956481e6",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Separating Oblivious and Adaptive Models of Variable Selection",
      "url": "http://arxiv.org/abs/2602.16568v1",
      "summary": "Sparse recovery is among the most well-studied problems in learning theory and high-dimensional statistics. In this work, we investigate the statistical and computational landscapes of sparse recovery with $\\ell_\\infty$ error guarantees. This variant of the problem is motivated by \\emph{variable selection} tasks, where the goal is to estimate the support of a $k$-sparse signal in $\\mathbb{R}^d$. Our main contribution is a provable separation between the \\emph{oblivious} (``for each'') and \\emph{adaptive} (``for all'') models of $\\ell_\\infty$ sparse recovery. We show that under an oblivious model, the optimal $\\ell_\\infty$ error is attainable in near-linear time with $\\approx k\\log d$ samples, whereas in an adaptive model, $\\gtrsim k^2$ samples are necessary for any algorithm to achieve this bound. This establishes a surprising contrast with the standard $\\ell_2$ setting, where $\\approx k \\log d$ samples suffice even for adaptive sparse recovery. We conclude with a preliminary examination of a \\emph{partially-adaptive} model, where we show nontrivial variable selection guarantees are possible with $\\approx k\\log d$ measurements.",
      "image_url": "",
      "published": "2026-02-18T16:10:35Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.819,
      "tier1_quick_score": 2.506
    },
    {
      "id": "79bdc4c952b1d60e",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
      "url": "http://arxiv.org/abs/2602.16564v1",
      "summary": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
      "image_url": "",
      "published": "2026-02-18T16:07:01Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.819,
      "tier1_quick_score": 2.506
    },
    {
      "id": "003784bdf397e03e",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Illustration of Barren Plateaus in Quantum Computing",
      "url": "http://arxiv.org/abs/2602.16558v1",
      "summary": "Variational Quantum Circuits (VQCs) have emerged as a promising paradigm for quantum machine learning in the NISQ era. While parameter sharing in VQCs can reduce the parameter space dimensionality and potentially mitigate the barren plateau phenomenon, it introduces a complex trade-off that has been largely overlooked. This paper investigates how parameter sharing, despite creating better global optima with fewer parameters, fundamentally alters the optimization landscape through deceptive gradients -- regions where gradient information exists but systematically misleads optimizers away from global optima. Through systematic experimental analysis, we demonstrate that increasing degrees of parameter sharing generate more complex solution landscapes with heightened gradient magnitudes and measurably higher deceptiveness ratios. Our findings reveal that traditional gradient-based optimizers (Adam, SGD) show progressively degraded convergence as parameter sharing increases, with performance heavily dependent on hyperparameter selection. We introduce a novel gradient deceptiveness detection algorithm and a quantitative framework for measuring optimization difficulty in quantum circuits, establishing that while parameter sharing can improve circuit expressivity by orders of magnitude, this comes at the cost of significantly increased landscape deceptiveness. These insights provide important considerations for quantum circuit design in practical applications, highlighting the fundamental mismatch between classical optimization strategies and quantum parameter landscapes shaped by parameter sharing.",
      "image_url": "",
      "published": "2026-02-18T15:56:54Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.817,
      "tier1_quick_score": 2.504
    },
    {
      "id": "4253601103969d80",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Learning Distributed Equilibria in Linear-Quadratic Stochastic Differential Games: An $α$-Potential Approach",
      "url": "http://arxiv.org/abs/2602.16555v1",
      "summary": "We analyze independent policy-gradient (PG) learning in $N$-player linear-quadratic (LQ) stochastic differential games. Each player employs a distributed policy that depends only on its own state and updates the policy independently using the gradient of its own objective. We establish global linear convergence of these methods to an equilibrium by showing that the LQ game admits an $α$-potential structure, with $α$ determined by the degree of pairwise interaction asymmetry. For pairwise-symmetric interactions, we construct an affine distributed equilibrium by minimizing the potential function and show that independent PG methods converge globally to this equilibrium, with complexity scaling linearly in the population size and logarithmically in the desired accuracy. For asymmetric interactions, we prove that independent projected PG algorithms converge linearly to an approximate equilibrium, with suboptimality proportional to the degree of asymmetry. Numerical experiments confirm the theoretical results across both symmetric and asymmetric interaction networks.",
      "image_url": "",
      "published": "2026-02-18T15:55:13Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.817,
      "tier1_quick_score": 2.504
    },
    {
      "id": "bab33591ecb300d3",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "url": "http://arxiv.org/abs/2602.16554v1",
      "summary": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
      "image_url": "",
      "published": "2026-02-18T15:54:32Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.816,
      "tier1_quick_score": 2.503
    },
    {
      "id": "c7aab029b7b7a60b",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion",
      "url": "http://arxiv.org/abs/2602.16548v1",
      "summary": "The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.",
      "image_url": "",
      "published": "2026-02-18T15:52:26Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.816,
      "tier1_quick_score": 2.503
    },
    {
      "id": "3391b8e52d42b2d5",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding",
      "url": "http://arxiv.org/abs/2602.16545v1",
      "summary": "Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.",
      "image_url": "",
      "published": "2026-02-18T15:46:36Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.815,
      "tier1_quick_score": 2.502
    },
    {
      "id": "2f08fd40d2ec0c88",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning",
      "url": "http://arxiv.org/abs/2602.16543v1",
      "summary": "Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy's internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.",
      "image_url": "",
      "published": "2026-02-18T15:43:36Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.814,
      "tier1_quick_score": 2.501
    },
    {
      "id": "4ae58c17e0b3058f",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Optimal training-conditional regret for online conformal prediction",
      "url": "http://arxiv.org/abs/2602.16537v1",
      "summary": "We study online conformal prediction for non-stationary data streams subject to unknown distribution drift. While most prior work studied this problem under adversarial settings and/or assessed performance in terms of gaps of time-averaged marginal coverage, we instead evaluate performance through training-conditional cumulative regret. We specifically focus on independently generated data with two types of distribution shift: abrupt change points and smooth drift.\n  When non-conformity score functions are pretrained on an independent dataset, we propose a split-conformal style algorithm that leverages drift detection to adaptively update calibration sets, which provably achieves minimax-optimal regret. When non-conformity scores are instead trained online, we develop a full-conformal style algorithm that again incorporates drift detection to handle non-stationarity; this approach relies on stability - rather than permutation symmetry - of the model-fitting algorithm, which is often better suited to online learning under evolving environments. We establish non-asymptotic regret guarantees for our online full conformal algorithm, which match the minimax lower bound under appropriate restrictions on the prediction sets. Numerical experiments corroborate our theoretical findings.",
      "image_url": "",
      "published": "2026-02-18T15:31:15Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.812,
      "tier1_quick_score": 2.499
    },
    {
      "id": "32f2bdb982b2e4cb",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Transfer Learning of Linear Regression with Multiple Pretrained Models: Benefiting from More Pretrained Models via Overparameterization Debiasing",
      "url": "http://arxiv.org/abs/2602.16531v1",
      "summary": "We study transfer learning for a linear regression task using several least-squares pretrained models that can be overparameterized.\n  We formulate the target learning task as optimization that minimizes squared errors on the target dataset with penalty on the distance of the learned model from the pretrained models. We analytically formulate the test error of the learned target model and provide the corresponding empirical evaluations.\n  Our results elucidate when using more pretrained models can improve transfer learning. Specifically, if the pretrained models are overparameterized, using sufficiently many of them is important for beneficial transfer learning. However, the learning may be compromised by overparameterization bias of pretrained models, i.e., the minimum $\\ell_2$-norm solution's restriction to a small subspace spanned by the training examples in the high-dimensional parameter space. We propose a simple debiasing via multiplicative correction factor that can reduce the overparameterization bias and leverage more pretrained models to learn a target predictor.",
      "image_url": "",
      "published": "2026-02-18T15:19:02Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.81,
      "tier1_quick_score": 2.497
    },
    {
      "id": "48665ea95076db1d",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "FEKAN: Feature-Enriched Kolmogorov-Arnold Networks",
      "url": "http://arxiv.org/abs/2602.16530v1",
      "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a compelling alternative to multilayer perceptrons, offering enhanced interpretability via functional decomposition. However, existing KAN architectures, including spline-, wavelet-, radial-basis variants, etc., suffer from high computational cost and slow convergence, limiting scalability and practical applicability. Here, we introduce Feature-Enriched Kolmogorov-Arnold Networks (FEKAN), a simple yet effective extension that preserves all the advantages of KAN while improving computational efficiency and predictive accuracy through feature enrichment, without increasing the number of trainable parameters. By incorporating these additional features, FEKAN accelerates convergence, increases representation capacity, and substantially mitigates the computational overhead characteristic of state-of-the-art KAN architectures. We investigate FEKAN across a comprehensive set of benchmarks, including function-approximation tasks, physics-informed formulations for diverse partial differential equations (PDEs), and neural operator settings that map between input and output function spaces. For function approximation, we systematically compare FEKAN against a broad family of KAN variants, FastKAN, WavKAN, ReLUKAN, HRKAN, ChebyshevKAN, RBFKAN, and the original SplineKAN. Across all tasks, FEKAN demonstrates substantially faster convergence and consistently higher approximation accuracy than the underlying baseline architectures. We also establish the theoretical foundations for FEKAN, showing its superior representation capacity compared to KAN, which contributes to improved accuracy and efficiency.",
      "image_url": "",
      "published": "2026-02-18T15:17:55Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.81,
      "tier1_quick_score": 2.497
    },
    {
      "id": "7b0b34d5369df0ba",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Capacity-constrained demand response in smart grids using deep reinforcement learning",
      "url": "http://arxiv.org/abs/2602.16525v1",
      "summary": "This paper presents a capacity-constrained incentive-based demand response approach for residential smart grids. It aims to maintain electricity grid capacity limits and prevent congestion by financially incentivising end users to reduce or shift their energy consumption. The proposed framework adopts a hierarchical architecture in which a service provider adjusts hourly incentive rates based on wholesale electricity prices and aggregated residential load. The financial interests of both the service provider and end users are explicitly considered. A deep reinforcement learning approach is employed to learn optimal real-time incentive rates under explicit capacity constraints. Heterogeneous user preferences are modelled through appliance-level home energy management systems and dissatisfaction costs. Using real-world residential electricity consumption and price data from three households, simulation results show that the proposed approach effectively reduces peak demand and smooths the aggregated load profile. This leads to an approximately 22.82% reduction in the peak-to-average ratio compared to the no-demand-response case.",
      "image_url": "",
      "published": "2026-02-18T15:13:07Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.809,
      "tier1_quick_score": 2.496
    },
    {
      "id": "e7ccc465039b14c8",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "url": "http://arxiv.org/abs/2602.16520v1",
      "summary": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
      "image_url": "",
      "published": "2026-02-18T15:07:09Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.808,
      "tier1_quick_score": 2.495
    },
    {
      "id": "74d774e666cb24e6",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study",
      "url": "http://arxiv.org/abs/2602.16523v1",
      "summary": "We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \\(R_x\\), \\(R_y\\), and \\(R_z\\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \\(λ\\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \\(5\\times10^{-4}\\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \\(10^{-4}\\)). Both approaches reliably reconstruct computational basis states (between 83\\% and 99\\% success) and Bell states (between 61\\% and 77\\% success). However, scalability saturates for \\(λ\\) of approximately three to four and does not extend to ten-qubit targets even at \\(λ=2\\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.",
      "image_url": "",
      "published": "2026-02-18T15:10:43Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.808,
      "tier1_quick_score": 2.495
    },
    {
      "id": "9bd768317e7870f6",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
      "url": "http://arxiv.org/abs/2602.16512v1",
      "summary": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.",
      "image_url": "",
      "published": "2026-02-18T14:58:25Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.806,
      "tier1_quick_score": 2.493
    },
    {
      "id": "72b5d5a0dcf8227e",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Small molecule retrieval from tandem mass spectrometry: what are we optimizing for?",
      "url": "http://arxiv.org/abs/2602.16507v1",
      "summary": "One of the central challenges in the computational analysis of liquid chromatography-tandem mass spectrometry (LC-MS/MS) data is to identify the compounds underlying the output spectra. In recent years, this problem is increasingly tackled using deep learning methods. A common strategy involves predicting a molecular fingerprint vector from an input mass spectrum, which is then used to search for matches in a chemical compound database. While various loss functions are employed in training these predictive models, their impact on model performance remains poorly understood. In this study, we investigate commonly used loss functions, deriving novel regret bounds that characterize when Bayes-optimal decisions for these objectives must diverge. Our results reveal a fundamental trade-off between the two objectives of (1) fingerprint similarity and (2) molecular retrieval. Optimizing for more accurate fingerprint predictions typically worsens retrieval results, and vice versa. Our theoretical analysis shows this trade-off depends on the similarity structure of candidate sets, providing guidance for loss function and fingerprint selection.",
      "image_url": "",
      "published": "2026-02-18T14:48:08Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.804,
      "tier1_quick_score": 2.491
    },
    {
      "id": "ef7ab438c1334ce1",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Functional Decomposition and Shapley Interactions for Interpreting Survival Models",
      "url": "http://arxiv.org/abs/2602.16505v1",
      "summary": "Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.",
      "image_url": "",
      "published": "2026-02-18T14:47:20Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.804,
      "tier1_quick_score": 2.491
    },
    {
      "id": "f03a2f7d83570693",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
      "url": "http://arxiv.org/abs/2602.16503v1",
      "summary": "Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \\emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms. Overall, CALMs offer a compelling trade-off between predictive accuracy and interpretability.",
      "image_url": "",
      "published": "2026-02-18T14:45:33Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.803,
      "tier1_quick_score": 2.49
    },
    {
      "id": "fba35b30dcf8e42a",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Fast and Scalable Analytical Diffusion",
      "url": "http://arxiv.org/abs/2602.16498v1",
      "summary": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.",
      "image_url": "",
      "published": "2026-02-18T14:41:09Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.803,
      "tier1_quick_score": 2.49
    },
    {
      "id": "6074936de686d757",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Reinforced Fast Weights with Next-Sequence Prediction",
      "url": "http://arxiv.org/abs/2602.16704v1",
      "summary": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
      "image_url": "",
      "published": "2026-02-18T18:53:18Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.851,
      "tier1_quick_score": 2.488
    },
    {
      "id": "2f0e1c45bae4cc29",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
      "url": "http://arxiv.org/abs/2602.16490v1",
      "summary": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.",
      "image_url": "",
      "published": "2026-02-18T14:25:16Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.8,
      "tier1_quick_score": 2.487
    },
    {
      "id": "2bd6afaf9022b4c7",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
      "url": "http://arxiv.org/abs/2602.16488v1",
      "summary": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
      "image_url": "",
      "published": "2026-02-18T14:22:13Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.799,
      "tier1_quick_score": 2.486
    },
    {
      "id": "f86ef9759e281e9d",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
      "url": "http://arxiv.org/abs/2602.16485v1",
      "summary": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.",
      "image_url": "",
      "published": "2026-02-18T14:19:01Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.799,
      "tier1_quick_score": 2.486
    },
    {
      "id": "f7dd894d71965b92",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach",
      "url": "http://arxiv.org/abs/2602.16481v1",
      "summary": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.",
      "image_url": "",
      "published": "2026-02-18T14:15:21Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.798,
      "tier1_quick_score": 2.485
    },
    {
      "id": "73acaed42eaf3eeb",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "url": "http://arxiv.org/abs/2602.16687v1",
      "summary": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
      "image_url": "",
      "published": "2026-02-18T18:32:46Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.847,
      "tier1_quick_score": 2.484
    },
    {
      "id": "c1335c35cb6a0f33",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
      "url": "http://arxiv.org/abs/2602.16467v1",
      "summary": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.",
      "image_url": "",
      "published": "2026-02-18T13:55:57Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.794,
      "tier1_quick_score": 2.481
    },
    {
      "id": "53936e5cf7052cc9",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation",
      "url": "http://arxiv.org/abs/2602.16449v1",
      "summary": "Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.",
      "image_url": "",
      "published": "2026-02-18T13:33:54Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.79,
      "tier1_quick_score": 2.477
    },
    {
      "id": "871bd5ab0404f028",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
      "url": "http://arxiv.org/abs/2602.16444v1",
      "summary": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.",
      "image_url": "",
      "published": "2026-02-18T13:29:43Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.789,
      "tier1_quick_score": 2.476
    },
    {
      "id": "4dd4446517fc4c8c",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
      "url": "http://arxiv.org/abs/2602.16442v1",
      "summary": "As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.",
      "image_url": "",
      "published": "2026-02-18T13:26:22Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.789,
      "tier1_quick_score": 2.476
    },
    {
      "id": "efd9dfa107a9d0f8",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "Improving Deep Agents with harness engineering",
      "url": "https://blog.langchain.com/improving-deep-agents-with-harness-engineering/",
      "summary": "<p>TLDR: Our coding agent went from Top 30 to Top 5 on <a href=\"https://www.tbench.ai/leaderboard/terminal-bench/2.0?ref=blog.langchain.com\">Terminal Bench 2.0</a>. We only changed the harness. Here&#x2019;s our approach to harness engineering (teaser: self-verification &amp; tracing help a lot).</p><h2 id=\"the-goal-of-harness-engineering\">The Goal of Harness Engineering</h2><p>The goal of a harness is to mold the</p>",
      "image_url": "https://blog.langchain.com/content/images/2026/02/Screenshot-2026-02-12-at-12.25.20---PM.png",
      "published": "Tue, 17 Feb 2026 16:15:28 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.588,
      "tier1_quick_score": 2.475
    },
    {
      "id": "a244c8b82e8446b6",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
      "url": "http://arxiv.org/abs/2602.16438v1",
      "summary": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.",
      "image_url": "",
      "published": "2026-02-18T13:19:11Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.788,
      "tier1_quick_score": 2.475
    },
    {
      "id": "c7b4b1246040c587",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
      "url": "http://arxiv.org/abs/2602.16435v1",
      "summary": "Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.",
      "image_url": "",
      "published": "2026-02-18T13:12:11Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.786,
      "tier1_quick_score": 2.473
    },
    {
      "id": "a1878ac3460d1ef7",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems",
      "url": "http://arxiv.org/abs/2602.16430v1",
      "summary": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.",
      "image_url": "",
      "published": "2026-02-18T13:03:05Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.785,
      "tier1_quick_score": 2.472
    },
    {
      "id": "7a9e74929d5ba197",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
      "url": "http://arxiv.org/abs/2602.16640v1",
      "summary": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes \"lexical density\" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.",
      "image_url": "",
      "published": "2026-02-18T17:29:43Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.835,
      "tier1_quick_score": 2.472
    },
    {
      "id": "df4aa1baa6040f92",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models",
      "url": "http://arxiv.org/abs/2602.16639v1",
      "summary": "Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.",
      "image_url": "",
      "published": "2026-02-18T17:28:28Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.834,
      "tier1_quick_score": 2.471
    },
    {
      "id": "0d8d4095995c59c7",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Verifiable Semantics for Agent-to-Agent Communication",
      "url": "http://arxiv.org/abs/2602.16424v1",
      "summary": "Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms (\"core-guarded reasoning\") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.",
      "image_url": "",
      "published": "2026-02-18T12:55:58Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.783,
      "tier1_quick_score": 2.47
    },
    {
      "id": "162303d5c4b42f9c",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model",
      "url": "http://arxiv.org/abs/2602.16422v1",
      "summary": "Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Transformer decoder for report generation. To make WSI processing tractable, we perform multi resolution pyramidal patch selection (downsampling factors 2^3 to 2^6) and remove background and artifacts using Laplacian variance and HSV based criteria. Patch features are extracted with the UNI Vision Transformer and projected to a 6 layer Transformer decoder that generates diagnostic text via cross attention. To better represent biomedical terminology, we tokenize the output using BioGPT. Finally, we add a retrieval based verification step that compares generated reports with a reference corpus using Sentence BERT embeddings; if a high similarity match is found, the generated report is replaced with the retrieved ground truth reference to improve reliability.",
      "image_url": "",
      "published": "2026-02-18T12:55:20Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.783,
      "tier1_quick_score": 2.47
    },
    {
      "id": "5c2ef11a8b96bb5b",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models",
      "url": "http://arxiv.org/abs/2602.16609v1",
      "summary": "Current state-of-the-art multi-vector models are obtained through a small Knowledge Distillation (KD) training step on top of strong single-vector models, leveraging the large-scale pre-training of these models. In this paper, we study the pre-training of multi-vector models and show that large-scale multi-vector pre-training yields much stronger multi-vector models. Notably, a fully ColBERT-pre-trained model, ColBERT-Zero, trained only on public data, outperforms GTE-ModernColBERT as well as its base model, GTE-ModernBERT, which leverages closed and much stronger data, setting new state-of-the-art for model this size. We also find that, although performing only a small KD step is not enough to achieve results close to full pre-training, adding a supervised step beforehand allows to achieve much closer performance while skipping the most costly unsupervised phase. Finally, we find that aligning the fine-tuning and pre-training setups is crucial when repurposing existing models. To enable exploration of our results, we release various checkpoints as well as code used to train them.",
      "image_url": "",
      "published": "2026-02-18T17:03:32Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.83,
      "tier1_quick_score": 2.467
    },
    {
      "id": "4da249d4e5762577",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes",
      "url": "http://arxiv.org/abs/2602.16607v1",
      "summary": "Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.",
      "image_url": "",
      "published": "2026-02-18T17:03:07Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.829,
      "tier1_quick_score": 2.466
    },
    {
      "id": "914636f9e8384ab6",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
      "url": "http://arxiv.org/abs/2602.16571v1",
      "summary": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the \"numeric ambiguity\" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.",
      "image_url": "",
      "published": "2026-02-18T16:12:46Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.82,
      "tier1_quick_score": 2.457
    },
    {
      "id": "f8127c71f0f00b70",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Improved Web Search With Dynamic Filtering",
      "url": "https://claude.com/blog/improved-web-search-with-dynamic-filtering",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.469,
      "tier1_quick_score": 2.456
    },
    {
      "id": "aca7796ceca9e750",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "AI-Driven Structure Refinement of X-ray Diffraction",
      "url": "http://arxiv.org/abs/2602.16372v1",
      "summary": "Artificial intelligence can rapidly propose candidate phases and structures from X-ray diffraction (XRD), but these hypotheses often fail in downstream refinement because peak intensities cannot be stably assigned under severe overlap and diffraction consistency is enforced only weakly. Here we introduce WPEM, a physics-constrained whole-pattern decomposition and refinement workflow that turns Bragg's law into an explicit constraint within a batch expectation--maximization framework. WPEM models the full profile as a probabilistic mixture density and iteratively infers component-resolved intensities while keeping peak centres Bragg-consistent, producing a continuous, physically admissible intensity representation that remains stable in heavily overlapped regions and in the presence of mixed radiation or multiple phases. We benchmark WPEM on standard reference patterns (\\ce{PbSO4} and \\ce{Tb2BaCoO5}), where it yields lower $R_{\\mathrm{p}}$/$R_{\\mathrm{wp}}$ than widely used packages (FullProf and TOPAS) under matched refinement conditions. We further demonstrate generality across realistic experimental scenarios, including phase-resolved decomposition of a multiphase Ti--15Nb thin film, quantitative recovery of \\ce{NaCl}--\\ce{Li2CO3} mixture compositions, separation of crystalline peaks from amorphous halos in semicrystalline polymers, high-throughput operando lattice tracking in layered cathodes, automated refinement of a compositionally disordered Ru--Mn oxide solid solution (CCDC 2530452), and quantitative phase-resolved deciphering of an ancient Egyptian make-up sample from synchrotron powder XRD. By providing Bragg-consistent, uncertainty-aware intensity partitioning as a refinement-ready interface, WPEM closes the gap between AI-generated hypotheses and diffraction-admissible structure refinement on challenging XRD data.",
      "image_url": "",
      "published": "2026-02-18T11:14:35Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.765,
      "tier1_quick_score": 2.452
    },
    {
      "id": "5a18db6634f3b29c",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Articulated 3D Scene Graphs for Open-World Mobile Manipulation",
      "url": "http://arxiv.org/abs/2602.16356v1",
      "summary": "Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and kinematics. In this work, we present MoMa-SG, a novel framework for building semantic-kinematic 3D scene graphs of articulated scenes containing a myriad of interactable objects. Given RGB-D sequences containing multiple object articulations, we temporally segment object interactions and infer object motion using occlusion-robust point tracking. We then lift point trajectories into 3D and estimate articulation models using a novel unified twist estimation formulation that robustly estimates revolute and prismatic joint parameters in a single optimization pass. Next, we associate objects with estimated articulations and detect contained objects by reasoning over parent-child relations at identified opening states. We also introduce the novel Arti4D-Semantic dataset, which uniquely combines hierarchical object semantics including parent-child relation labels with object axis annotations across 62 in-the-wild RGB-D sequences containing 600 object interactions and three distinct observation paradigms. We extensively evaluate the performance of MoMa-SG on two datasets and ablate key design choices of our approach. In addition, real-world experiments on both a quadruped and a mobile manipulator demonstrate that our semantic-kinematic scene graphs enable robust manipulation of articulated objects in everyday home environments. We provide code and data at: https://momasg.cs.uni-freiburg.de.",
      "image_url": "",
      "published": "2026-02-18T10:40:35Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.759,
      "tier1_quick_score": 2.446
    },
    {
      "id": "ae3f099013e4cb23",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification",
      "url": "http://arxiv.org/abs/2602.16516v1",
      "summary": "This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.",
      "image_url": "",
      "published": "2026-02-18T15:04:30Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.807,
      "tier1_quick_score": 2.444
    },
    {
      "id": "40b492b162d1949d",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "HAWX: A Hardware-Aware FrameWork for Fast and Scalable ApproXimation of DNNs",
      "url": "http://arxiv.org/abs/2602.16336v1",
      "summary": "This work presents HAWX, a hardware-aware scalable exploration framework that employs multi-level sensitivity scoring at different DNN abstraction levels (operator, filter, layer, and model) to guide selective integration of heterogeneous AxC blocks. Supported by predictive models for accuracy, power, and area, HAWX accelerates the evaluation of candidate configurations, achieving over 23* speedup in a layer-level search with two candidate approximate blocks and more than (3*106)* speedup at the filter-level search only for LeNet-5, while maintaining accuracy comparable to exhaustive search. Experiments across state-of-the-art DNN benchmarks such as VGG-11, ResNet-18, and EfficientNetLite demonstrate that the efficiency benefits of HAWX scale exponentially with network size. The HAWX hardware-aware search algorithm supports both spatial and temporal accelerator architectures, leveraging either off-the-shelf approximate components or customized designs.",
      "image_url": "",
      "published": "2026-02-18T10:16:57Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.755,
      "tier1_quick_score": 2.442
    },
    {
      "id": "8dbd6cf9c18081db",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Optimizing Soft Prompt Tuning via Structural Evolution",
      "url": "http://arxiv.org/abs/2602.16500v1",
      "summary": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.",
      "image_url": "",
      "published": "2026-02-18T14:43:20Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.803,
      "tier1_quick_score": 2.44
    },
    {
      "id": "093dd73ccf880d9c",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning",
      "url": "http://arxiv.org/abs/2602.16469v1",
      "summary": "Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.",
      "image_url": "",
      "published": "2026-02-18T13:59:08Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.795,
      "tier1_quick_score": 2.432
    },
    {
      "id": "9087ccd14bd2f2ec",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "The AI Vampire",
      "url": "https://simonwillison.net/2026/Feb/15/the-ai-vampire/#atom-everything",
      "summary": "<p><strong><a href=\"https://steve-yegge.medium.com/the-ai-vampire-eda6e4f07163\">The AI Vampire</a></strong></p>\nSteve Yegge's take on agent fatigue, and its relationship to burnout.</p>\n<blockquote>\n<p>Let's pretend you're the only person at your company using AI.</p>\n<p>In Scenario A, you decide you're going to impress your employer, and work for 8 hours a day at 10x productivity. You knock it out of the park and make everyone else look terrible by comparison.</p>\n<p>In that scenario, your employer captures 100% of the value from <em>you</em> adopting AI. You get nothing, or at any rate, it ain't gonna be 9x your salary. And everyone hates you now.</p>\n<p>And you're <em>exhausted.</em> You're tired, Boss. You got nothing for it.</p>\n<p>Congrats, you were just drained by a company. I've been drained to the point of burnout several times in my career, even at Google once or twice. But now with AI, it's oh, so much easier.</p>\n</blockquote>\n<p>Steve reports needing more sleep due to the cognitive burden involved in agentic engineering, and notes that four hours of agent work a day is a more realistic pace:</p>\n<blockquote>\n<p>I’ve argued that AI has turned us all into Jeff Bezos, by automating the easy work, and leaving us with all the difficult decisions, summaries, and problem-solving. I find that I am only really comfortable working at that pace for short bursts of a few hours once or occasionally twice a day, even with lots of practice.</p>\n</blockquote>\n\n    <p><small></small>Via <a href=\"https://cosocial.ca/@timbray/116076167774984883\">Tim Bray</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/steve-yegge\">steve-yegge</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/cognitive-debt\">cognitive-debt</a></p>",
      "image_url": "",
      "published": "2026-02-15T23:59:36+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.336,
      "tier1_quick_score": 2.423
    },
    {
      "id": "264752c3b997bcbb",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
      "url": "http://arxiv.org/abs/2602.16429v1",
      "summary": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.",
      "image_url": "",
      "published": "2026-02-18T13:01:17Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.784,
      "tier1_quick_score": 2.421
    },
    {
      "id": "605f998f6b557ec9",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Em dash",
      "url": "https://simonwillison.net/2026/Feb/15/em-dashes/#atom-everything",
      "summary": "<p>I'm occasionally accused of using LLMs to write the content on my blog. I don't do that, and I don't think my writing has much of an LLM smell to it... with one notable exception:</p>\n<pre>    <span class=\"pl-c\"># Finally, do em dashes</span>\n    <span class=\"pl-s1\">s</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">s</span>.<span class=\"pl-c1\">replace</span>(<span class=\"pl-s\">' - '</span>, <span class=\"pl-s\">u'<span class=\"pl-cce\">\\u2014</span>'</span>)</pre>\n\n<p>That code to add em dashes to my posts dates back to <a href=\"https://github.com/simonw/simonwillisonblog/blob/e6d0327b37debdf820b5cfef4fb7d09a9624cea9/blog/templatetags/entry_tags.py#L145-L146\">at least 2015</a> when I ported my blog from an older version of Django (in a long-lost Mercurial repository) and started afresh on GitHub.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/typography\">typography</a>, <a href=\"https://simonwillison.net/tags/blogging\">blogging</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/python\">python</a></p>",
      "image_url": "",
      "published": "2026-02-15T21:40:46+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.325,
      "tier1_quick_score": 2.412
    },
    {
      "id": "c6e0c1c50f17e587",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Deep Blue",
      "url": "https://simonwillison.net/2026/Feb/15/deep-blue/#atom-everything",
      "summary": "<p>We coined a new term on the <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/\">Oxide and Friends podcast</a> last month (primary credit to Adam Leventhal) covering the sense of psychological ennui leading into existential dread that many software developers are feeling thanks to the encroachment of generative AI into their field of work.</p>\n<p>We're calling it <strong>Deep Blue</strong>.</p>\n<p>You can listen to it being coined in real time <a href=\"https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=2835s\">from 47:15 in the episode</a>. I've included <a href=\"https://simonwillison.net/2026/Feb/15/deep-blue/#transcript\">a transcript below</a>.</p>\n<p>Deep Blue is a very real issue.</p>\n<p>Becoming a professional software engineer is <em>hard</em>. Getting good enough for people to pay you money to write software takes years of dedicated work. The rewards are significant: this is a well compensated career which opens up a lot of great opportunities.</p>\n<p>It's also a career that's mostly free from gatekeepers and expensive prerequisites. You don't need an expensive degree or accreditation. A laptop, an internet connection and a lot of time and curiosity is enough to get you started.</p>\n<p>And it rewards the nerds! Spending your teenage years tinkering with computers turned out to be a very smart investment in your future.</p>\n<p>The idea that this could all be stripped away by a chatbot is <em>deeply</em> upsetting.</p>\n<p>I've seen signs of Deep Blue in most of the online communities I spend time in. I've even faced accusations from my peers that I am actively harming their future careers through my work helping people understand how well AI-assisted programming can work.</p>\n<p>I think this is an issue which is causing genuine mental anguish for a lot of people in our community. Giving it a name makes it easier for us to have conversations about it.</p>\n<h4 id=\"my-experiences-of-deep-blue\">My experiences of Deep Blue</h4>\n<p>I distinctly remember my first experience of Deep Blue. For me it was triggered by ChatGPT Code Interpreter back in early 2023.</p>\n<p>My primary project is <a href=\"https://datasette.io/\">Datasette</a>, an ecosystem of open source tools for telling stories with data. I had dedicated myself to the challenge of helping people (initially focusing on journalists) clean up, analyze and find meaning in data, in all sorts of shapes and sizes.</p>\n<p>I expected I would need to build a lot of software for this! It felt like a challenge that could keep me happily engaged for many years to come.</p>\n<p>Then I tried uploading a CSV file of <a href=\"https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data\">San Francisco Police Department Incident Reports</a> - hundreds of thousands of rows - to ChatGPT Code Interpreter and... it did every piece of data cleanup and analysis I had on my napkin roadmap for the next few years with a couple of prompts.</p>\n<p>It even converted the data into a neatly normalized SQLite database and let me download the result!</p>\n<p>I remember having two competing thoughts in parallel.</p>\n<p>On the one hand, as somebody who wants journalists to be able to do more with data, this felt like a <em>huge</em> breakthrough. Imagine giving every journalist in the world an on-demand analyst who could help them tackle any data question they could think of!</p>\n<p>But on the other hand... <em>what was I even for</em>? My confidence in the value of my own projects took a painful hit. Was the path I'd chosen for myself suddenly a dead end?</p>\n<p>I've had some further pangs of Deep Blue just in the past few weeks, thanks to the Claude Opus 4.5/4.6 and GPT-5.2/5.3 coding agent effect. As many other people are also observing, the latest generation of coding agents, given the right prompts, really can churn away for a few minutes to several hours and produce working, documented and fully tested software that exactly matches the criteria they were given.</p>\n<p>\"The code they write isn't any good\" doesn't really cut it any more.</p>\n<h4 id=\"transcript\">A lightly edited transcript</h4>\n<blockquote>\n<p><strong>Bryan</strong>: I think that we're going to see a real problem with AI induced ennui where software engineers in particular get listless because the AI can do anything. Simon, what do you think about that?</p>\n<p><strong>Simon</strong>: Definitely. Anyone who's paying close attention to coding agents is feeling some of that already. There's an extent where you sort of get over it when you realize that you're still useful, even though your ability to memorize the syntax of program languages is completely irrelevant now.</p>\n<p>Something I see a lot of is people out there who are having existential crises and are very, very unhappy because they're like, \"I dedicated my career to learning this thing and now it just does it. What am I even for?\". I will very happily try and convince those people that they are for a whole bunch of things and that none of that experience they've accumulated has gone to waste, but psychologically it's a difficult time for software engineers.</p>\n<p>[...]</p>\n<p><strong>Bryan</strong>: Okay, so I'm going to predict that we name that. Whatever that is, we have a name for that kind of feeling and that kind of, whether you want to call it a blueness or a loss of purpose, and that we're kind of trying to address it collectively in a directed way.</p>\n<p><strong>Adam</strong>: Okay, this is your big moment. Pick the name. If you call your shot from here, this is you pointing to the stands. You know, I – Like deep blue, you know.</p>\n<p><strong>Bryan</strong>: Yeah, deep blue. I like that. I like deep blue. Deep blue. Oh, did you walk me into that, you bastard? You just blew out the candles on my birthday cake.</p>\n<p>It wasn't my big moment at all. That was your big moment. No, that is, Adam, that is very good. That is deep blue.</p>\n<p><strong>Simon</strong>: All of the chess players and the Go players went through this a decade ago and they have come out stronger.</p>\n</blockquote>\n<p>Turns out it was more than a decade ago: <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov\">Deep Blue defeated Garry Kasparov in 1997</a>.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/oxide\">oxide</a>, <a href=\"https://simonwillison.net/tags/bryan-cantrill\">bryan-cantrill</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a></p>",
      "image_url": "",
      "published": "2026-02-15T21:06:44+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.323,
      "tier1_quick_score": 2.41
    },
    {
      "id": "56a9c4cb4344cd26",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
      "url": "http://arxiv.org/abs/2602.16379v1",
      "summary": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.",
      "image_url": "",
      "published": "2026-02-18T11:38:11Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.769,
      "tier1_quick_score": 2.406
    },
    {
      "id": "6def57c2df42e731",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Variable-Length Semantic IDs for Recommender Systems",
      "url": "http://arxiv.org/abs/2602.16375v1",
      "summary": "Generative models are increasingly used in recommender systems, both for modeling user behavior as event sequences and for integrating large language models into recommendation pipelines. A key challenge in this setting is the extremely large cardinality of item spaces, which makes training generative models difficult and introduces a vocabulary gap between natural language and item identifiers. Semantic identifiers (semantic IDs), which represent items as sequences of low-cardinality tokens, have recently emerged as an effective solution to this problem.\n  However, existing approaches generate semantic identifiers of fixed length, assigning the same description length to all items. This is inefficient, misaligned with natural language, and ignores the highly skewed frequency structure of real-world catalogs, where popular items and rare long-tail items exhibit fundamentally different information requirements. In parallel, the emergent communication literature studies how agents develop discrete communication protocols, often producing variable-length messages in which frequent concepts receive shorter descriptions. Despite the conceptual similarity, these ideas have not been systematically adopted in recommender systems.\n  In this work, we bridge recommender systems and emergent communication by introducing variable-length semantic identifiers for recommendation. We propose a discrete variational autoencoder with Gumbel-Softmax reparameterization that learns item representations of adaptive length under a principled probabilistic framework, avoiding the instability of REINFORCE-based training and the fixed-length constraints of prior semantic ID methods.",
      "image_url": "",
      "published": "2026-02-18T11:29:05Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.768,
      "tier1_quick_score": 2.405
    },
    {
      "id": "afbc3d5ec736210a",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Agoda’s API Agent Converts Any API to MCP with Zero Code and Deployments",
      "url": "https://www.infoq.com/news/2026/02/agoda-api-agent/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/agoda-api-agent/en/headerimage/generatedHeaderImage-1770265458956.jpg\" /><p>Agoda engineers developed API Agent, enabling a single MCP server to access any internal REST or GraphQL API with zero code and zero deployments. The system reduces overhead from multiple APIs, supports AI-assisted queries, and uses in-memory SQL post-processing for safe, scalable data handling across internal services.</p> <i>By Leela Kumili</i>",
      "image_url": "https://res.infoq.com/news/2026/02/agoda-api-agent/en/headerimage/generatedHeaderImage-1770265458956.jpg",
      "published": "Mon, 16 Feb 2026 15:14:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.415,
      "tier1_quick_score": 2.402
    },
    {
      "id": "189952f9f9dc227a",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Gwtar: a static efficient single-file HTML format",
      "url": "https://simonwillison.net/2026/Feb/15/gwtar/#atom-everything",
      "summary": "<p><strong><a href=\"https://gwern.net/gwtar\">Gwtar: a static efficient single-file HTML format</a></strong></p>\nFascinating new project from Gwern Branwen and Said Achmiz that targets the challenge of combining large numbers of assets into a single archived HTML file without that file being inconvenient to view in a browser.</p>\n<p>The key trick it uses is to fire <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/stop\">window.stop()</a> early in the page to prevent the browser from downloading the whole thing, then following that call with inline tar uncompressed content.</p>\n<p>It can then make HTTP range requests to fetch content from that tar data on-demand when it is needed by the page.</p>\n<p>The JavaScript that has already loaded rewrites asset URLs to point to <code>https://localhost/</code> purely so that they will fail to load. Then it uses a <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/PerformanceObserver\">PerformanceObserver</a> to catch those attempted loads:</p>\n<pre><code>let perfObserver = new PerformanceObserver((entryList, observer) =&gt; {\n    resourceURLStringsHandler(entryList.getEntries().map(entry =&gt; entry.name));\n});\nperfObserver.observe({ entryTypes: [ \"resource\" ] });\n</code></pre>\n<p>That <code>resourceURLStringsHandler</code> callback finds the resource if it is already loaded or fetches it with an HTTP range request otherwise and then inserts the resource in the right place using a <code>blob:</code> URL.</p>\n<p>Here's what the <code>window.stop()</code> portion of the document looks like if you view the source:</p>\n<p><img alt=\"Screenshot of a macOS terminal window titled &quot;gw — more big.html — 123×46&quot; showing the source code of a gwtar (self-extracting HTML archive) file. The visible code includes JavaScript with requestIdleCallback(getMainPageHTML);, a noscript block with warnings: a &quot;js-disabled-warning&quot; stating &quot;This HTML page requires JavaScript to be enabled to render, as it is a self-extracting gwtar HTML file,&quot; a description of gwtar as &quot;a portable self-contained standalone HTML file which is designed to nevertheless support efficient lazy loading of all assets such as large media files,&quot; with a link to https://gwern.net/gwtar, a &quot;local-file-warning&quot; with a shell command perl -ne'print $_ if $x; $x=1 if /&lt;!-- GWTAR END/' &amp;lt; foo.gwtar.html | tar --extract, and a &quot;server-fail-warning&quot; about misconfigured servers. Below the HTML closing tags and &lt;!-- GWTAR END comment is binary tar archive data with the filename 2010-02-brianmoriarty-thesecretofpsalm46.html, showing null-padded tar header fields including ustar^@00root and octal size/permission values. At the bottom, a SingleFile metadata comment shows url: https://web.archive.org/web/20230512001411/http://ludix.com/moriarty/psalm46.html and saved date: Sat Jan 17 2026 19:26:49 GMT-0800 (Pacific Standard Time).\" src=\"https://static.simonwillison.net/static/2026/gwtar.jpg\" /></p>\n<p>Amusingly for an archive format it doesn't actually work if you open the file directly on your own computer. Here's what you see if you try to do that:</p>\n<blockquote>\n<p>You are seeing this message, instead of the page you should be seeing, because <code>gwtar</code> files <strong>cannot be opened locally</strong> (due to web browser security restrictions).</p>\n<p>To open this page on your computer, use the following shell command:</p>\n<p><code>perl -ne'print $_ if $x; $x=1 if /&lt;!-- GWTAR END/' &lt; foo.gwtar.html | tar --extract</code></p>\n<p>Then open the file <code>foo.html</code> in any web browser.</p>\n</blockquote>\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=47024506\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/archiving\">archiving</a>, <a href=\"https://simonwillison.net/tags/html\">html</a>, <a href=\"https://simonwillison.net/tags/javascript\">javascript</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/gwtar.jpg",
      "published": "2026-02-15T18:26:08+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.311,
      "tier1_quick_score": 2.398
    },
    {
      "id": "aa5416c7b475a14a",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
      "url": "http://arxiv.org/abs/2602.16346v1",
      "summary": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
      "image_url": "",
      "published": "2026-02-18T10:31:19Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.758,
      "tier1_quick_score": 2.395
    },
    {
      "id": "71f2751bde88bac5",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Three months of OpenClaw",
      "url": "https://simonwillison.net/2026/Feb/15/openclaw/#atom-everything",
      "summary": "<p>It's wild that the first commit to OpenClaw was <a href=\"https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712ccc11\">on November 25th 2025</a>, and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an extremely vague <a href=\"https://www.youtube.com/watch?v=n7I-D4YXbzg\">Super Bowl commercial for AI.com</a>.</p>\n<p>Quoting AI.com founder <a href=\"https://twitter.com/kris/status/2020663711015514399\">Kris Marszalek</a>, purchaser of the <a href=\"https://www.theregister.com/2026/02/09/70m_aicom_domain_sale/\">most expensive domain in history</a> for $70m:</p>\n<blockquote>\n<p>ai.com is the world’s first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.</p>\n</blockquote>\n<p>Looks like vaporware to me - all you can do right now is reserve a handle - but it's still remarkable to see an open source project get to <em>that</em> level of hype in such a short space of time.</p>\n<p><strong>Update</strong>: OpenClaw creator Peter Steinberger <a href=\"https://steipete.me/posts/2026/openclaw\">just announced</a> that he's joining OpenAI and plans to transfer ownership of OpenClaw to a new independent foundation.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/openclaw\">openclaw</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/open-source\">open-source</a>, <a href=\"https://simonwillison.net/tags/domains\">domains</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/peter-steinberger\">peter-steinberger</a></p>",
      "image_url": "",
      "published": "2026-02-15T17:23:28+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.307,
      "tier1_quick_score": 2.394
    },
    {
      "id": "3604c47c77157aa5",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks",
      "url": "http://arxiv.org/abs/2602.16313v1",
      "summary": "Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.",
      "image_url": "",
      "published": "2026-02-18T09:49:14Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.75,
      "tier1_quick_score": 2.387
    },
    {
      "id": "7c0190e57fc5b1a7",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models",
      "url": "http://arxiv.org/abs/2602.16298v1",
      "summary": "Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.",
      "image_url": "",
      "published": "2026-02-18T09:28:53Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.747,
      "tier1_quick_score": 2.384
    },
    {
      "id": "4e9b2f69c4414122",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation",
      "url": "http://arxiv.org/abs/2602.16290v1",
      "summary": "Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.",
      "image_url": "",
      "published": "2026-02-18T09:15:20Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.744,
      "tier1_quick_score": 2.381
    },
    {
      "id": "588621adb31a551e",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Quoting Eric Meyer",
      "url": "https://simonwillison.net/2026/Feb/15/eric-meyer/#atom-everything",
      "summary": "<blockquote cite=\"https://mastodon.social/@Meyerweb/116065151451468199\"><p>I saw yet another “CSS is a massively bloated mess” whine and I’m like.  My dude.  My brother in Chromium.  It is trying as hard as it can to express the totality of visual presentation and layout design and typography and animation and digital interactivity and a few other things in a human-readable text format.  It’s not bloated, it’s fantastically ambitious.  Its reach is greater than most of us can hope to grasp.  Put some <em>respect</em> on its <em>name</em>.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://mastodon.social/@Meyerweb/116065151451468199\">Eric Meyer</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/css\">css</a>, <a href=\"https://simonwillison.net/tags/web-standards\">web-standards</a>, <a href=\"https://simonwillison.net/tags/eric-meyer\">eric-meyer</a></p>",
      "image_url": "",
      "published": "2026-02-15T13:36:20+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.291,
      "tier1_quick_score": 2.378
    },
    {
      "id": "a21120a19de675d2",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Lyapunov Spectral Analysis of Speech Embedding Trajectories in Psychosis",
      "url": "http://arxiv.org/abs/2602.16273v1",
      "summary": "We analyze speech embeddings from structured clinical interviews of psychotic patients and healthy controls by treating language production as a high-dimensional dynamical process. Lyapunov exponent (LE) spectra are computed from word-level and answer-level embeddings generated by two distinct large language models, allowing us to assess the stability of the conclusions with respect to different embedding presentations. Word-level embeddings exhibit uniformly contracting dynamics with no positive LE, while answer-level embeddings, in spite of the overall contraction, display a number of positive LEs and higher-dimensional attractors. The resulting LE spectra robustly separate psychotic from healthy speech, while differentiation within the psychotic group is not statistically significant overall, despite a tendency of the most severe cases to occupy distinct dynamical regimes. These findings indicate that nonlinear dynamical invariants of speech embeddings provide a physics-inspired probe of disordered cognition whose conclusions remain stable across embedding models.",
      "image_url": "",
      "published": "2026-02-18T08:46:46Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.739,
      "tier1_quick_score": 2.376
    },
    {
      "id": "28aeb3b7d9a1e43f",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Article: Architecting Agentic MLOps: A Layered Protocol Strategy with A2A and MCP",
      "url": "https://www.infoq.com/articles/architecting-agentic-mlops-a2a-mcp/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/articles/architecting-agentic-mlops-a2a-mcp/en/headerimage/architecting-agentic-mlops-a2a-mcp-header-1770303550343.jpg\" /><p>In this article, the authors outline protocols for building extensible multi-agent MLOps systems. The core architecture deliberately decouples orchestration from execution, allowing teams to incrementally add capabilities via discovery and evolve operations from static pipelines toward intelligent, adaptive coordination.</p> <i>By Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora</i>",
      "image_url": "https://res.infoq.com/articles/architecting-agentic-mlops-a2a-mcp/en/headerimage/architecting-agentic-mlops-a2a-mcp-header-1770303550343.jpg",
      "published": "Mon, 16 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.381,
      "tier1_quick_score": 2.368
    },
    {
      "id": "97c8646a3c988eac",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Google Explores Scaling Principles for Multi-Agent Coordination",
      "url": "https://www.infoq.com/news/2026/02/google-agent-scaling-principles/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/google-agent-scaling-principles/en/headerimage/google-scaling-agents-principles-1771231654834.jpeg\" /><p>Google Research tried to answer the question of how to design agent systems for optimal performance by running a controlled evaluation of 180 agent configurations. From this, the team derived what they call the \"first quantitative scaling principles for AI agent systems\", showing that multi-agent coordination does not reliably improve results and can even reduce performance.</p> <i>By Sergio De Simone</i>",
      "image_url": "https://res.infoq.com/news/2026/02/google-agent-scaling-principles/en/headerimage/google-scaling-agents-principles-1771231654834.jpeg",
      "published": "Mon, 16 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.381,
      "tier1_quick_score": 2.368
    },
    {
      "id": "393c0547e79081f1",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Are LLMs Ready to Replace Bangla Annotators?",
      "url": "http://arxiv.org/abs/2602.16241v1",
      "summary": "Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.",
      "image_url": "",
      "published": "2026-02-18T07:36:41Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.728,
      "tier1_quick_score": 2.365
    },
    {
      "id": "13ead6ebf4581112",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "A new way to express yourself: Gemini can now create music",
      "url": "https://blog.google/innovation-and-ai/products/gemini-app/lyria-3/",
      "summary": "Image showing sample tracks created with Lyria 3",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/0217_KeywordHeaderFinalc.max-600x600.format-webp.webp",
      "published": "Wed, 18 Feb 2026 16:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.817,
      "tier1_quick_score": 2.354
    },
    {
      "id": "b8ac3f1225cb82c3",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Build unified intelligence with Amazon Bedrock AgentCore",
      "url": "https://aws.amazon.com/blogs/machine-learning/build-unified-intelligence-with-amazon-bedrock-agentcore/",
      "summary": "In this post, we demonstrate how to build unified intelligence systems using Amazon Bedrock AgentCore through our real-world implementation of the Customer Agent and Knowledge Engine (CAKE).",
      "image_url": "",
      "published": "Wed, 18 Feb 2026 23:54:29 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.912,
      "tier1_quick_score": 2.349
    },
    {
      "id": "2e65710c4945577e",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "url": "http://arxiv.org/abs/2602.16201v1",
      "summary": "Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.\n  We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.",
      "image_url": "",
      "published": "2026-02-18T05:49:45Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.71,
      "tier1_quick_score": 2.347
    },
    {
      "id": "0fc45c9f723e9225",
      "source": "arxiv_cs_cl",
      "source_weight": 0.8,
      "title": "The Validity of Coreference-based Evaluations of Natural Language Understanding",
      "url": "http://arxiv.org/abs/2602.16200v1",
      "summary": "In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.",
      "image_url": "",
      "published": "2026-02-18T05:49:28Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 0.837,
      "freshness": 0.71,
      "tier1_quick_score": 2.347
    },
    {
      "id": "dd9ffded5689f601",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt",
      "url": "https://simonwillison.net/2026/Feb/15/cognitive-debt/#atom-everything",
      "summary": "<p><strong><a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt</a></strong></p>\nThis piece by Margaret-Anne Storey is the best explanation of the term <strong>cognitive debt</strong> I've seen so far.</p>\n<blockquote>\n<p><em>Cognitive debt</em>, a term gaining <a href=\"https://www.media.mit.edu/publications/your-brain-on-chatgpt/\">traction</a> recently, instead communicates the notion that the debt compounded from going fast lives in the brains of the developers and affects their lived experiences and abilities to “go fast” or to make changes. Even if AI agents produce code that could be easy to understand, the humans involved may have simply lost the plot and may not understand what the program is supposed to do, how their intentions were implemented, or how to possibly change it.</p>\n</blockquote>\n<p>Margaret-Anne expands on this further with an anecdote about a student team she coached:</p>\n<blockquote>\n<p>But by weeks 7 or 8, one team hit a wall. They could no longer make even simple changes without breaking something unexpected. When I met with them, the team initially blamed technical debt: messy code, poor architecture, hurried implementations. But as we dug deeper, the real problem emerged: no one on the team could explain why certain design decisions had been made or how different parts of the system were supposed to work together. The code might have been messy, but the bigger issue was that the theory of the system, their shared understanding, had fragmented or disappeared entirely. They had accumulated cognitive debt faster than technical debt, and it paralyzed them.</p>\n</blockquote>\n<p>I've experienced this myself on some of my more ambitious vibe-code-adjacent projects. I've been experimenting with prompting entire new features into existence without reviewing their implementations and, while it works surprisingly well, I've found myself getting lost in my own projects.</p>\n<p>I no longer have a firm mental model of what they can do and how they work, which means each additional feature becomes harder to reason about, eventually leading me to lose the ability to make confident decisions about where to go next.\n\n    <p><small></small>Via <a href=\"https://martinfowler.com/fragments/2026-02-13.html\">Martin Fowler</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a>, <a href=\"https://simonwillison.net/tags/cognitive-debt\">cognitive-debt</a></p>",
      "image_url": "",
      "published": "2026-02-15T05:20:11+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.259,
      "tier1_quick_score": 2.346
    },
    {
      "id": "770ccec0a4c97947",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Launching Interop 2026",
      "url": "https://simonwillison.net/2026/Feb/15/interop-2026/#atom-everything",
      "summary": "<p><strong><a href=\"https://hacks.mozilla.org/2026/02/launching-interop-2026/\">Launching Interop 2026</a></strong></p>\nJake Archibald reports on Interop 2026, the initiative between Apple, Google, Igalia, Microsoft, and Mozilla to collaborate on ensuring a targeted set of web platform features reach cross-browser parity over the course of the year.</p>\n<p>I hadn't realized how influential and successful the Interop series has been. It started back in 2021 as <a href=\"https://web.dev/blog/compat2021\">Compat 2021</a> before being rebranded to Interop <a href=\"https://blogs.windows.com/msedgedev/2022/03/03/microsoft-edge-and-interop-2022/\">in 2022</a>.</p>\n<p>The dashboards for each year can be seen here, and they demonstrate how wildly effective the program has been: <a href=\"https://wpt.fyi/interop-2021\">2021</a>, <a href=\"https://wpt.fyi/interop-2022\">2022</a>, <a href=\"https://wpt.fyi/interop-2023\">2023</a>, <a href=\"https://wpt.fyi/interop-2024\">2024</a>, <a href=\"https://wpt.fyi/interop-2025\">2025</a>, <a href=\"https://wpt.fyi/interop-2026\">2026</a>.</p>\n<p>Here's the progress chart for 2025, which shows every browser vendor racing towards a 95%+ score by the end of the year:</p>\n<p><img alt=\"Line chart showing Interop 2025 browser compatibility scores over the year (Jan–Dec) for Chrome, Edge, Firefox, Safari, and Interop. Y-axis ranges from 0% to 100%. Chrome (yellow) and Edge (green) lead, starting around 80% and reaching near 100% by Dec. Firefox (orange) starts around 48% and climbs to ~98%. Safari (blue) starts around 45% and reaches ~96%. The Interop line (dark green/black) starts lowest around 29% and rises to ~95% by Dec. All browsers converge near 95–100% by year's end.\" src=\"https://static.simonwillison.net/static/2026/interop-2025.jpg\" /></p>\n<p>The feature I'm most excited about in 2026 is <a href=\"https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using#basic_mpa_view_transition\">Cross-document View Transitions</a>, building on the successful 2025 target of <a href=\"https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using\">Same-Document View Transitions</a>. This will provide fancy SPA-style transitions between pages on websites with no JavaScript at all.</p>\n<p>As a keen WebAssembly tinkerer I'm also intrigued by this one:</p>\n<blockquote>\n<p><a href=\"https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md\">JavaScript Promise Integration for Wasm</a> allows WebAssembly to asynchronously 'suspend', waiting on the result of an external promise. This simplifies the compilation of languages like C/C++ which expect APIs to run synchronously.</p>\n</blockquote>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/browsers\">browsers</a>, <a href=\"https://simonwillison.net/tags/css\">css</a>, <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/web-standards\">web-standards</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/jake-archibald\">jake-archibald</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/interop-2025.jpg",
      "published": "2026-02-15T04:33:22+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.257,
      "tier1_quick_score": 2.344
    },
    {
      "id": "638f393db5f9b678",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Quoting Boris Cherny",
      "url": "https://simonwillison.net/2026/Feb/14/boris/#atom-everything",
      "summary": "<blockquote cite=\"https://twitter.com/bcherny/status/2022762422302576970\"><p>Someone has to prompt the Claudes, talk to customers, coordinate with other teams, decide what to build next. Engineering is changing and great engineers are more important than ever.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/bcherny/status/2022762422302576970\">Boris Cherny</a>, Claude Code creator, on why Anthropic are still hiring developers</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a></p>",
      "image_url": "",
      "published": "2026-02-14T23:59:09+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.241,
      "tier1_quick_score": 2.328
    },
    {
      "id": "b107c618e81c053a",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.36",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.36",
      "summary": "<h3>New Features</h3>\n<ul>\n<li><strong>Thinking configuration</strong>: Added <code>ThinkingConfig</code> types (<code>ThinkingConfigAdaptive</code>, <code>ThinkingConfigEnabled</code>, <code>ThinkingConfigDisabled</code>) and <code>thinking</code> field to <code>ClaudeAgentOptions</code> for fine-grained control over extended thinking behavior. The new <code>thinking</code> field takes precedence over the now-deprecated <code>max_thinking_tokens</code> field (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-agent-sdk-python/pull/565\">#565</a>)</li>\n<li><strong>Effort option</strong>: Added <code>effort</code> field to <code>ClaudeAgentOptions</code> supporting <code>\"low\"</code>, <code>\"medium\"</code>, <code>\"high\"</code>, and <code>\"max\"</code> values for controlling thinking depth (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-agent-sdk-python/pull/565\">#565</a>)</li>\n</ul>\n<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.42</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.36/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.36/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.36</pre></div>",
      "image_url": "",
      "published": "2026-02-13T20:09:44Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.164,
      "tier1_quick_score": 2.301
    },
    {
      "id": "72ee645e59010c45",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "AI Impact Summit 2026: How we’re partnering to make AI work for everyone",
      "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india/",
      "summary": "four people seated on a conference stage",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI-Summit-Hero.max-600x600.format-webp.webp",
      "published": "Wed, 18 Feb 2026 10:30:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.757,
      "tier1_quick_score": 2.294
    },
    {
      "id": "4b8cd476c6cfcd11",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
      "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon/",
      "summary": "In this post, we present a comprehensive evaluation framework for Amazon agentic AI systems that addresses the complexity of agentic AI applications at Amazon&nbsp;through two core components: a generic evaluation workflow that standardizes assessment procedures across diverse agent implementations, and an agent evaluation library that provides systematic measurements and metrics in Amazon Bedrock AgentCore Evaluations, along with&nbsp;Amazon use case-specific evaluation approaches and metrics.&nbsp;",
      "image_url": "",
      "published": "Wed, 18 Feb 2026 19:21:28 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.856,
      "tier1_quick_score": 2.293
    },
    {
      "id": "1bc6785dbcbd5b33",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Quoting Thoughtworks",
      "url": "https://simonwillison.net/2026/Feb/14/thoughtworks/#atom-everything",
      "summary": "<blockquote cite=\"https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf\"><p>The retreat challenged the narrative that AI eliminates the need for junior developers. Juniors are more profitable than they have ever been. AI tools get them past the awkward initial net-negative phase faster. They serve as a call option on future productivity. And they are better at AI tools than senior engineers, having never developed the habits and assumptions that slow adoption.</p>\n<p>The real concern is mid-level engineers who came up during the decade-long hiring boom and may not have developed the fundamentals needed to thrive in the new environment. This population represents the bulk of the industry by volume, and retraining them is genuinely difficult. The retreat discussed whether apprenticeship models, rotation programs and lifelong learning structures could address this gap, but acknowledged that no organization has solved it yet.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf\">Thoughtworks</a>, findings from a retreat concerning \"the future of software engineering\", conducted under Chatham House rules</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a></p>",
      "image_url": "",
      "published": "2026-02-14T04:54:41+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.185,
      "tier1_quick_score": 2.272
    },
    {
      "id": "0326f1ab2670e0a1",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Anthropic's public benefit mission",
      "url": "https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/#atom-everything",
      "summary": "<p>Someone <a href=\"https://news.ycombinator.com/item?id=47008560#47008978\">asked</a> if there was an Anthropic equivalent to <a href=\"https://simonwillison.net/2026/Feb/13/openai-mission-statement/\">OpenAI's IRS mission statements over time</a>.</p>\n<p>Anthropic are a \"public benefit corporation\" but not a non-profit, so they don't have the same requirements to file public documents with the IRS every year.</p>\n<p>But when I asked Claude it ran a search and dug up this <a href=\"https://drive.google.com/drive/folders/1ImqXYv9_H2FTNAujZfu3EPtYFD4xIlHJ\">Google Drive folder</a> where Zach Stein-Perlman shared Certificate of Incorporation documents he <a href=\"https://ailabwatch.substack.com/p/anthropics-certificate-of-incorporation\">obtained from the State of Delaware</a>!</p>\n<p>Anthropic's are much less interesting that OpenAI's. The earliest document from 2021 states:</p>\n<blockquote>\n<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced Al for the cultural, social and technological improvement of humanity.</p>\n</blockquote>\n<p>Every subsequent document up to 2024 uses an updated version which says:</p>\n<blockquote>\n<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced AI for the long term benefit of humanity.</p>\n</blockquote>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a></p>",
      "image_url": "",
      "published": "2026-02-13T23:59:51+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.173,
      "tier1_quick_score": 2.26
    },
    {
      "id": "10316e2dfaca33ed",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "The evolution of OpenAI's mission statement",
      "url": "https://simonwillison.net/2026/Feb/13/openai-mission-statement/#atom-everything",
      "summary": "<p>As a USA <a href=\"https://en.wikipedia.org/wiki/501(c)(3)_organization\">501(c)(3)</a> the OpenAI non-profit has to file a tax return each year with the IRS. One of the required fields on that tax return is to \"Briefly describe the organization’s mission or most significant activities\" - this has actual legal weight to it as the IRS can use it to evaluate if the organization is sticking to its mission and deserves to maintain its non-profit tax-exempt status.</p>\n<p>You can browse OpenAI's <a href=\"https://projects.propublica.org/nonprofits/organizations/810861541\">tax filings by year</a> on ProPublica's excellent <a href=\"https://projects.propublica.org/nonprofits/\">Nonprofit Explorer</a>.</p>\n<p>I went through and extracted that mission statement for 2016 through 2024, then had Claude Code <a href=\"https://gisthost.github.io/?7a569df89f43f390bccc2c5517718b49/index.html\">help me</a> fake the commit dates to turn it into a git repository and share that as a Gist - which means that Gist's <a href=\"https://gist.github.com/simonw/e36f0e5ef4a86881d145083f759bcf25/revisions\">revisions page</a> shows every edit they've made since they started filing their taxes!</p>\n<p>It's really interesting seeing what they've changed over time.</p>\n<p>The original 2016 mission reads as follows (and yes, the apostrophe in \"OpenAIs\" is missing <a href=\"https://projects.propublica.org/nonprofits/organizations/810861541/201703459349300445/full\">in the original</a>):</p>\n<blockquote>\n<p>OpenAIs goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. We think that artificial intelligence technology will help shape the 21st century, and we want to help the world build safe AI technology and ensure that AI's benefits are as widely and evenly distributed as possible. Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.</p>\n</blockquote>\n<p>In 2018 they dropped the part about \"trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.\"</p>\n<p><img alt=\"Git diff showing the 2018 revision deleting the final two sentences: &quot;Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-3.jpg\" /></p>\n<p>In 2020 they dropped the words \"as a whole\" from \"benefit humanity as a whole\". They're still \"unconstrained by a need to generate financial return\" though.</p>\n<p><img alt=\"Git diff showing the 2020 revision dropping &quot;as a whole&quot; from &quot;benefit humanity as a whole&quot; and changing &quot;We think&quot; to &quot;OpenAI believes&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-5.jpg\" /></p>\n<p>Some interesting changes in 2021. They're still unconstrained by a need to generate financial return, but here we have the first reference to \"general-purpose artificial intelligence\" (replacing \"digital intelligence\"). They're more confident too: it's not \"most likely to benefit humanity\", it's just \"benefits humanity\".</p>\n<p>They previously wanted to \"help the world build safe AI technology\", but now they're going to do that themselves: \"the companys goal is to develop and responsibly deploy safe AI technology\".</p>\n<p><img alt=\"Git diff showing the 2021 revision replacing &quot;goal is to advance digital intelligence&quot; with &quot;mission is to build general-purpose artificial intelligence&quot;, changing &quot;most likely to benefit&quot; to just &quot;benefits&quot;, and replacing &quot;help the world build safe AI technology&quot; with &quot;the companys goal is to develop and responsibly deploy safe AI technology&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-6.jpg\" /></p>\n<p>2022 only changed one significant word: they added \"safely\" to \"build ... (AI) that safely benefits humanity\". They're still unconstrained by those financial returns!</p>\n<p><img alt=\"Git diff showing the 2022 revision adding &quot;(AI)&quot; and the word &quot;safely&quot; so it now reads &quot;that safely benefits humanity&quot;, and changing &quot;the companys&quot; to &quot;our&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-7.jpg\" /></p>\n<p>No changes in 2023... but then in 2024 they deleted almost the entire thing, reducing it to simply:</p>\n<blockquote>\n<p>OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.</p>\n</blockquote>\n<p>They've expanded \"humanity\" to \"all of humanity\", but there's no mention of safety any more and I guess they can finally start focusing on that need to generate financial returns!</p>\n<p><img alt=\"Git diff showing the 2024 revision deleting the entire multi-sentence mission statement and replacing it with just &quot;OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.&quot;\" src=\"https://static.simonwillison.net/static/2026/mission-9.jpg\" /></p>\n\n<p><strong>Update</strong>: I found loosely equivalent but much less interesting documents <a href=\"https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/\">from Anthropic</a>.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/propublica\">propublica</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/mission-3.jpg",
      "published": "2026-02-13T23:38:29+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.172,
      "tier1_quick_score": 2.259
    },
    {
      "id": "bc475ac3fcd28ab6",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Ai Assistance Coding Skills",
      "url": "https://www.anthropic.com/research/AI-assistance-coding-skills",
      "summary": "",
      "image_url": "",
      "published": "2026-02-05T00:34:35.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.009,
      "tier1_quick_score": 2.246
    },
    {
      "id": "a4a5412f81884cb3",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Economic Index Primitives",
      "url": "https://www.anthropic.com/research/economic-index-primitives",
      "summary": "",
      "image_url": "",
      "published": "2026-02-03T15:57:16.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 2.243
    },
    {
      "id": "2a08adab75ed6739",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Disempowerment Patterns",
      "url": "https://www.anthropic.com/research/disempowerment-patterns",
      "summary": "",
      "image_url": "",
      "published": "2026-01-28T22:01:01.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.238
    },
    {
      "id": "122e113ea46c2238",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Assistant Axis",
      "url": "https://www.anthropic.com/research/assistant-axis",
      "summary": "",
      "image_url": "",
      "published": "2026-01-19T20:47:03.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "2c4fcdf9dbaee0a9",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Anthropic Economic Index January 2026 Report",
      "url": "https://www.anthropic.com/research/anthropic-economic-index-january-2026-report",
      "summary": "",
      "image_url": "",
      "published": "2026-01-15T23:15:33.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "13559d5a0b24f9d8",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Anthropic Economic Index September 2025 Report",
      "url": "https://www.anthropic.com/research/anthropic-economic-index-september-2025-report",
      "summary": "",
      "image_url": "",
      "published": "2026-01-15T01:06:36.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "83e037259286a365",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Next Generation Constitutional Classifiers",
      "url": "https://www.anthropic.com/research/next-generation-constitutional-classifiers",
      "summary": "",
      "image_url": "",
      "published": "2026-01-09T21:18:12.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "8786587032c1ecab",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Bloom",
      "url": "https://www.anthropic.com/research/bloom",
      "summary": "",
      "image_url": "",
      "published": "2025-12-20T17:09:41.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "94085dc263d90941",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Project Vend 2",
      "url": "https://www.anthropic.com/research/project-vend-2",
      "summary": "",
      "image_url": "",
      "published": "2025-12-18T16:17:58.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "d3f945bbee7feae5",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Project Vend 1",
      "url": "https://www.anthropic.com/research/project-vend-1",
      "summary": "",
      "image_url": "",
      "published": "2025-12-18T11:02:21.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "13ddab6ab9dcb120",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "How Ai Is Transforming Work At Anthropic",
      "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
      "summary": "",
      "image_url": "",
      "published": "2025-12-09T03:30:54.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "2eb2c33da2cc012d",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Estimating Productivity Gains",
      "url": "https://www.anthropic.com/research/estimating-productivity-gains",
      "summary": "",
      "image_url": "",
      "published": "2025-12-07T14:55:39.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "263ff8b812b8ab5f",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Economic Research",
      "url": "https://www.anthropic.com/research/team/economic-research",
      "summary": "",
      "image_url": "",
      "published": "2025-12-05T22:37:18.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "cfd2d194360d9c43",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Prompt Injection Defenses",
      "url": "https://www.anthropic.com/research/prompt-injection-defenses",
      "summary": "",
      "image_url": "",
      "published": "2025-11-24T18:51:33.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "20eba96c250ff5cb",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Emergent Misalignment Reward Hacking",
      "url": "https://www.anthropic.com/research/emergent-misalignment-reward-hacking",
      "summary": "",
      "image_url": "",
      "published": "2025-11-21T18:24:54.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "3e7b3a8a06f8dcd2",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Societal Impacts",
      "url": "https://www.anthropic.com/research/team/societal-impacts",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T19:03:43.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "0bfa1fec60052996",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Alignment",
      "url": "https://www.anthropic.com/research/team/alignment",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T19:03:42.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "b829cb554218f98f",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Interpretability",
      "url": "https://www.anthropic.com/research/team/interpretability",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T19:03:41.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "a4198c338fe4f0e5",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Project Fetch Robot Dog",
      "url": "https://www.anthropic.com/research/project-fetch-robot-dog",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:17:01.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "ee3faf99469d461d",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Constitutional Classifiers",
      "url": "https://www.anthropic.com/research/constitutional-classifiers",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:16:54.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "0b1f7507e68b6ed7",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Introspection",
      "url": "https://www.anthropic.com/research/introspection",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:16:23.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "1eb8cb42ce3ead18",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Tracing Thoughts Language Model",
      "url": "https://www.anthropic.com/research/tracing-thoughts-language-model",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:16:19.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "c728eca7ebe43f47",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Persona Vectors",
      "url": "https://www.anthropic.com/research/persona-vectors",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:16:15.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "e18c60273b41c0f6",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Toy Models Of Superposition",
      "url": "https://www.anthropic.com/research/toy-models-of-superposition",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:16:12.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "15742a5ecf56dcc3",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Claude Character",
      "url": "https://www.anthropic.com/research/claude-character",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:15:32.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "c434f1a6187d2461",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Alignment Faking",
      "url": "https://www.anthropic.com/research/alignment-faking",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:15:29.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "d96e769992161dc2",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Auditing Hidden Objectives",
      "url": "https://www.anthropic.com/research/auditing-hidden-objectives",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:15:19.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "244c3974d2b52ba5",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Reward Tampering",
      "url": "https://www.anthropic.com/research/reward-tampering",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:15:12.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "526608729c2000bc",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Economic Index Geography",
      "url": "https://www.anthropic.com/research/economic-index-geography",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:14:42.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "f02b59238818e8e4",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Impact Software Development",
      "url": "https://www.anthropic.com/research/impact-software-development",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:14:37.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "d79f9a2f435f0642",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Values Wild",
      "url": "https://www.anthropic.com/research/values-wild",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:14:30.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "7467318a80b609e6",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Collective Constitutional Ai Aligning A Language Model With Public Input",
      "url": "https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:14:28.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "88c0a35869b05c08",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Predictability And Surprise In Large Generative Models",
      "url": "https://www.anthropic.com/research/predictability-and-surprise-in-large-generative-models",
      "summary": "",
      "image_url": "",
      "published": "2025-11-20T16:14:26.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "b49229b7725670dd",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Deprecation Commitments",
      "url": "https://www.anthropic.com/research/deprecation-commitments",
      "summary": "",
      "image_url": "",
      "published": "2025-11-04T16:36:56.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "351da742628d5e98",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Economic Policy Responses",
      "url": "https://www.anthropic.com/research/economic-policy-responses",
      "summary": "",
      "image_url": "",
      "published": "2025-10-14T17:05:02.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "39ced484f83627c0",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Small Samples Poison",
      "url": "https://www.anthropic.com/research/small-samples-poison",
      "summary": "",
      "image_url": "",
      "published": "2025-10-09T16:01:15.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "7b9cd5c2cc7daea5",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Petri Open Source Auditing",
      "url": "https://www.anthropic.com/research/petri-open-source-auditing",
      "summary": "",
      "image_url": "",
      "published": "2025-10-06T17:12:39.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "1447b534eb94d637",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Building Ai Cyber Defenders",
      "url": "https://www.anthropic.com/research/building-ai-cyber-defenders",
      "summary": "",
      "image_url": "",
      "published": "2025-10-03T19:33:59.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "605cb9e6f830d25a",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Clio",
      "url": "https://www.anthropic.com/research/clio",
      "summary": "",
      "image_url": "",
      "published": "2025-08-28T15:58:31.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "b254a528056f8018",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "End Subset Conversations",
      "url": "https://www.anthropic.com/research/end-subset-conversations",
      "summary": "",
      "image_url": "",
      "published": "2025-08-15T19:36:25.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "de57177a638311ae",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Visible Extended Thinking",
      "url": "https://www.anthropic.com/research/visible-extended-thinking",
      "summary": "",
      "image_url": "",
      "published": "2025-07-23T18:02:08.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "7cc31501c76f227f",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Shade Arena Sabotage Monitoring",
      "url": "https://www.anthropic.com/research/shade-arena-sabotage-monitoring",
      "summary": "",
      "image_url": "",
      "published": "2025-06-25T00:45:44.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "bc5cacb6e6a74060",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Agentic Misalignment",
      "url": "https://www.anthropic.com/research/agentic-misalignment",
      "summary": "",
      "image_url": "",
      "published": "2025-06-23T08:51:14.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "cace14b95b5ca5f9",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Confidential Inference Trusted Vms",
      "url": "https://www.anthropic.com/research/confidential-inference-trusted-vms",
      "summary": "",
      "image_url": "",
      "published": "2025-06-18T16:16:11.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "df94d43091648c60",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Open Source Circuit Tracing",
      "url": "https://www.anthropic.com/research/open-source-circuit-tracing",
      "summary": "",
      "image_url": "",
      "published": "2025-05-29T16:14:52.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "55e47dff8b65e922",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Exploring Model Welfare",
      "url": "https://www.anthropic.com/research/exploring-model-welfare",
      "summary": "",
      "image_url": "",
      "published": "2025-04-24T14:38:39.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "dbf5587630180d16",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Reasoning Models Dont Say Think",
      "url": "https://www.anthropic.com/research/reasoning-models-dont-say-think",
      "summary": "",
      "image_url": "",
      "published": "2025-04-04T09:11:36.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "d05ef1a75f3802e3",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Forecasting Rare Behaviors",
      "url": "https://www.anthropic.com/research/forecasting-rare-behaviors",
      "summary": "",
      "image_url": "",
      "published": "2025-02-28T21:57:01.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "d0ef259581ab078a",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Crosscoder Model Diffing",
      "url": "https://www.anthropic.com/research/crosscoder-model-diffing",
      "summary": "",
      "image_url": "",
      "published": "2025-02-20T23:53:28.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "2eea703adcf8adfe",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Evaluating Ai Systems",
      "url": "https://www.anthropic.com/research/evaluating-ai-systems",
      "summary": "",
      "image_url": "",
      "published": "2024-12-19T19:04:12.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "455e5ff358e13eb4",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Training A Helpful And Harmless Assistant With Reinforcement Learning From Human Feedback",
      "url": "https://www.anthropic.com/research/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback",
      "summary": "",
      "image_url": "",
      "published": "2024-12-19T18:59:50.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "e975b9aa59e85668",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Towards Understanding Sycophancy In Language Models",
      "url": "https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models",
      "summary": "",
      "image_url": "",
      "published": "2024-12-19T18:59:42.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "6bfa2364af973e81",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Towards Monosemanticity Decomposing Language Models With Dictionary Learning",
      "url": "https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning",
      "summary": "",
      "image_url": "",
      "published": "2024-12-19T18:59:35.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "c03fe00bf0cf5a95",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Towards Measuring The Representation Of Subjective Global Opinions In Language Models",
      "url": "https://www.anthropic.com/research/towards-measuring-the-representation-of-subjective-global-opinions-in-language-models",
      "summary": "",
      "image_url": "",
      "published": "2024-12-19T18:59:28.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "77cbffe6503e701a",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "The Capacity For Moral Self Correction In Large Language Models",
      "url": "https://www.anthropic.com/research/the-capacity-for-moral-self-correction-in-large-language-models",
      "summary": "",
      "image_url": "",
      "published": "2024-12-19T18:59:21.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "046dd3f3861840cd",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Superposition Memorization And Double Descent",
      "url": "https://www.anthropic.com/research/superposition-memorization-and-double-descent",
      "summary": "",
      "image_url": "",
      "published": "2024-12-19T18:59:07.000Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 2.237
    },
    {
      "id": "d7c065e7fd03c9f2",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Why OpenAI Should Build Slack",
      "url": "https://www.latent.space/p/ainews-why-openai-should-build-slack",
      "summary": "a quiet day lets us answer a Sam Altman question: what should he build next?",
      "image_url": "https://substackcdn.com/image/fetch/$s_!XQAE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ee056a-0ea2-4473-8e1c-9b21f034c717_1474x2116.png",
      "published": "Sat, 14 Feb 2026 07:48:54 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.192,
      "tier1_quick_score": 2.229
    },
    {
      "id": "d31ce3705f466046",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Quoting Anthropic",
      "url": "https://simonwillison.net/2026/Feb/12/anthropic/#atom-everything",
      "summary": "<blockquote cite=\"https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation\"><p>Claude Code was made available to the general public in May 2025. Today, Claude Code’s run-rate revenue has grown to over $2.5 billion; this figure has more than doubled since the beginning of 2026. The number of weekly active Claude Code users has also doubled since January 1 [<em>six weeks ago</em>].</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation\">Anthropic</a>, announcing their $30 billion series G</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
      "image_url": "",
      "published": "2026-02-12T20:22:14+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.118,
      "tier1_quick_score": 2.205
    },
    {
      "id": "edab237724839010",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Covering electricity price increases from our data centers",
      "url": "https://simonwillison.net/2026/Feb/12/covering-electricity-price-increases/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.anthropic.com/news/covering-electricity-price-increases\">Covering electricity price increases from our data centers</a></strong></p>\nOne of the sub-threads of the AI energy usage discourse has been the impact new data centers have on the cost of electricity to nearby residents. Here's <a href=\"https://www.bloomberg.com/graphics/2025-ai-data-centers-electricity-prices/\">detailed analysis from Bloomberg in September</a> reporting \"Wholesale electricity costs as much as 267% more than it did five years ago in areas near data centers\".</p>\n<p>Anthropic appear to be taking on this aspect of the problem directly, promising to cover 100% of necessary grid upgrade costs and also saying:</p>\n<blockquote>\n<p>We will work to bring net-new power generation online to match our data centers’ electricity needs. Where new generation isn’t online, we’ll work with utilities and external experts to estimate and cover demand-driven price effects from our data centers.</p>\n</blockquote>\n<p>I look forward to genuine energy industry experts picking this apart to judge if it will actually have the claimed impact on consumers.</p>\n<p>As always, I remain frustrated at the refusal of the major AI labs to fully quantify their energy usage. The best data we've had on this still comes from Mistral's report <a href=\"https://simonwillison.net/2025/Jul/22/mistral-environmental-standard/\">last July</a> and even that lacked key data such as the breakdown between energy usage for training vs inference.\n\n    <p><small></small>Via <a href=\"https://x.com/anthropicai/status/2021694494215901314\">@anthropicai</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-energy-usage\">ai-energy-usage</a></p>",
      "image_url": "",
      "published": "2026-02-12T20:01:23+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.117,
      "tier1_quick_score": 2.204
    },
    {
      "id": "403beef66c66d60d",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.35",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.35",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.39</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.35/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.35/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.35</pre></div>",
      "image_url": "",
      "published": "2026-02-10T23:21:48Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.063,
      "tier1_quick_score": 2.2
    },
    {
      "id": "3267f7701fc834dc",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Claude Code vs ChatGPT Codex: Which AI coding agent is actually better? - Tom's Guide",
      "url": "https://news.google.com/rss/articles/CBMinwFBVV95cUxOX3k2SkVENHhlc1A3Wkp6OFZHaVRDbHhqN2k5cTZUaV8wamNGOGdfV0ZCMm0xTmlNTFdnRTRPT242LU5YcmhxVkNIeHczOWtQeUdfUVRxSGF2T0JUbi1RaG43a0VmMlJSVXpTQ1VLekxzTUN5UUY1UzE5NVBCVlZFUmJHWW5hdV9OVXFVdGNhLTE0RGhVUHBibGtPTTdHbG8?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMinwFBVV95cUxOX3k2SkVENHhlc1A3Wkp6OFZHaVRDbHhqN2k5cTZUaV8wamNGOGdfV0ZCMm0xTmlNTFdnRTRPT242LU5YcmhxVkNIeHczOWtQeUdfUVRxSGF2T0JUbi1RaG43a0VmMlJSVXpTQ1VLekxzTUN5UUY1UzE5NVBCVlZFUmJHWW5hdV9OVXFVdGNhLTE0RGhVUHBibGtPTTdHbG8?oc=5\" target=\"_blank\">Claude Code vs ChatGPT Codex: Which AI coding agent is actually better?</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Tom's Guide</font>",
      "image_url": "",
      "published": "Sun, 15 Feb 2026 05:33:52 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.26,
      "tier1_quick_score": 2.197
    },
    {
      "id": "b8cc114cbff97dd3",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Sixteen Claude Agents Built a C Compiler without Human Intervention... Almost",
      "url": "https://www.infoq.com/news/2026/02/claude-built-c-compiler/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/claude-built-c-compiler/en/headerimage/claude-built-c-compiler-1771067001094.jpeg\" /><p>In an effort to probe the limits of autonomous software development Anthropic used sixteen Claude Opus 4.6 AI agents to build a Rust-based C compiler from scratch. Working in parallel on a shared repository, the agents coordinated their changes and ultimately produced a compiler capable of building the Linux 6.9 kernel across x86, ARM, and RISC-V, as well as many other open-source projects.</p> <i>By Sergio De Simone</i>",
      "image_url": "https://res.infoq.com/news/2026/02/claude-built-c-compiler/en/headerimage/claude-built-c-compiler-1771067001094.jpeg",
      "published": "Sat, 14 Feb 2026 12:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.204,
      "tier1_quick_score": 2.191
    },
    {
      "id": "5140244ba501d641",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "From Paging to Postmortem: Google Cloud SREs on Using Gemini CLI for Outage Response",
      "url": "https://www.infoq.com/news/2026/02/google-sre-gemini-cli-outage/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/google-sre-gemini-cli-outage/en/headerimage/generatedHeaderImage-1770021438197.jpg\" /><p>A recent article by Google Cloud SREs describes how they use the AI-powered Gemini CLI internally to resolve real-world outages. This approach improves reliability in critical infrastructure operations and reduces incident response time by integrating intelligent reasoning directly into the terminal-based operational tools.</p> <i>By Renato Losio</i>",
      "image_url": "https://res.infoq.com/news/2026/02/google-sre-gemini-cli-outage/en/headerimage/generatedHeaderImage-1770021438197.jpg",
      "published": "Sat, 14 Feb 2026 11:32:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.203,
      "tier1_quick_score": 2.19
    },
    {
      "id": "d5f3273a461090b3",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Firestore Adds Pipeline Operations with over 100 New Query Features",
      "url": "https://www.infoq.com/news/2026/02/firestore-enterprise-pipeline/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/firestore-enterprise-pipeline/en/headerimage/generatedHeaderImage-1770410936080.jpg\" /><p>Google has overhauled Firestore’s query engine, introducing \"Pipeline operations\" that enable complex server-side aggregations and array unnesting. The update shifts Firestore Enterprise toward an optional indexing model, allowing architects to prioritize write speed and lower costs. While it brings parity with MongoDB-style aggregations, the preview currently lacks real-time and emulator support.</p> <i>By Steef-Jan Wiggers</i>",
      "image_url": "https://res.infoq.com/news/2026/02/firestore-enterprise-pipeline/en/headerimage/generatedHeaderImage-1770410936080.jpg",
      "published": "Sat, 14 Feb 2026 10:11:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.199,
      "tier1_quick_score": 2.186
    },
    {
      "id": "aac3a0606c3dd964",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.34",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.34",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.38</li>\n<li>Updated CI workflows to use Claude Opus 4.6 model (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-agent-sdk-python/pull/556\">#556</a>)</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.34/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.34/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.34</pre></div>",
      "image_url": "",
      "published": "2026-02-10T01:04:54Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.046,
      "tier1_quick_score": 2.183
    },
    {
      "id": "5ec4aac6a934d10d",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Our 2026 Responsible AI Progress Report",
      "url": "https://blog.google/innovation-and-ai/products/responsible-ai-2026-report-ongoing-work/",
      "summary": "an illustration of blue and white cubes",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Stocksy_7137793.max-600x600.format-webp.webp",
      "published": "Tue, 17 Feb 2026 22:30:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.641,
      "tier1_quick_score": 2.178
    },
    {
      "id": "8e37b924ee8e53d3",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] new Gemini 3 Deep Think, Anthropic $30B @ $380B, GPT-5.3-Codex Spark, MiniMax M2.5",
      "url": "https://www.latent.space/p/ainews-new-gemini-3-deep-think-anthropic",
      "summary": "There's too much going on!",
      "image_url": "https://substackcdn.com/image/youtube/w_728,c_limit/F_1oDPWxpFQ",
      "published": "Fri, 13 Feb 2026 08:29:19 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.139,
      "tier1_quick_score": 2.176
    },
    {
      "id": "df581b47453ab67e",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.33",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.33",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.37</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.33/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.33/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.33</pre></div>",
      "image_url": "",
      "published": "2026-02-07T19:20:37Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.022,
      "tier1_quick_score": 2.159
    },
    {
      "id": "ab56370f30116c52",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.32",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.32",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.36</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.32/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.32/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.32</pre></div>",
      "image_url": "",
      "published": "2026-02-07T18:12:52Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.022,
      "tier1_quick_score": 2.159
    },
    {
      "id": "5b2af800ae5ab35f",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "Owning the AI Pareto Frontier — Jeff Dean",
      "url": "https://www.latent.space/p/jeffdean",
      "summary": "From rewriting Google&#8217;s search stack in the early 2000s to reviving sparse trillion-parameter models and co-designing TPUs with frontier ML research, Jeff Dean has quietly shaped nearly every layer of the modern AI stack.",
      "image_url": "",
      "published": "Thu, 12 Feb 2026 22:02:35 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.12,
      "tier1_quick_score": 2.157
    },
    {
      "id": "390e63ce19cce33a",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.31",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.31",
      "summary": "<h3>New Features</h3>\n<ul>\n<li><strong>MCP tool annotations support</strong>: Added support for MCP tool annotations via the <code>@tool</code> decorator's new <code>annotations</code> parameter, allowing developers to specify metadata hints like <code>readOnlyHint</code>, <code>destructiveHint</code>, <code>idempotentHint</code>, and <code>openWorldHint</code>. Re-exported <code>ToolAnnotations</code> from <code>claude_agent_sdk</code> for convenience (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-agent-sdk-python/pull/551\">#551</a>)</li>\n</ul>\n<h3>Bug Fixes</h3>\n<ul>\n<li><strong>Large agent definitions</strong>: Fixed an issue where large agent definitions would silently fail to register due to platform-specific CLI argument size limits (ARG_MAX). Agent definitions are now sent via the initialize control request through stdin, matching the TypeScript SDK approach and allowing arbitrarily large agent payloads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-agent-sdk-python/pull/468\">#468</a>)</li>\n</ul>\n<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.33</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.31/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.31/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.31</pre></div>",
      "image_url": "",
      "published": "2026-02-06T02:02:44Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.012,
      "tier1_quick_score": 2.149
    },
    {
      "id": "469945fd476b69a1",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.30",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.30",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.32</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.30/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.30/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.30</pre></div>",
      "image_url": "",
      "published": "2026-02-05T17:59:27Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.011,
      "tier1_quick_score": 2.148
    },
    {
      "id": "25d4dec233ffb03b",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.29",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.29",
      "summary": "<h3>New Features</h3>\n<ul>\n<li>\n<p><strong>New hook events</strong>: Added support for three new hook event types (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-agent-sdk-python/pull/545\">#545</a>):</p>\n<ul>\n<li><code>Notification</code> — for handling notification events with <code>NotificationHookInput</code> and <code>NotificationHookSpecificOutput</code></li>\n<li><code>SubagentStart</code> — for handling subagent startup with <code>SubagentStartHookInput</code> and <code>SubagentStartHookSpecificOutput</code></li>\n<li><code>PermissionRequest</code> — for handling permission requests with <code>PermissionRequestHookInput</code> and <code>PermissionRequestHookSpecificOutput</code></li>\n</ul>\n</li>\n<li>\n<p><strong>Enhanced hook input/output types</strong>: Added missing fields to existing hook types (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-agent-sdk-python/pull/545\">#545</a>):</p>\n<ul>\n<li><code>PreToolUseHookInput</code>: added <code>tool_use_id</code></li>\n<li><code>PostToolUseHookInput</code>: added <code>tool_use_id</code></li>\n<li><code>SubagentStopHookInput</code>: added <code>agent_id</code>, <code>agent_transcript_path</code>, <code>agent_type</code></li>\n<li><code>PreToolUseHookSpecificOutput</code>: added <code>additionalContext</code></li>\n<li><code>PostToolUseHookSpecificOutput</code>: added <code>updatedMCPToolOutput</code></li>\n</ul>\n</li>\n</ul>\n<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.31</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.29/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.29/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.29</pre></div>",
      "image_url": "",
      "published": "2026-02-04T00:54:53Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 2.143
    },
    {
      "id": "fb8310425e15863c",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Presentation: Building Embedding Models for Large-Scale Real-World Applications",
      "url": "https://www.infoq.com/presentations/llm-large-scale-applications/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/presentations/llm-large-scale-applications/en/mediumimage/sahil-dua-medium-1769590214923.jpeg\" /><p>Sahil Dua discusses the critical role of embedding models in powering search and RAG applications at scale. He explains the transformer-based architecture, contrastive learning techniques, and the process of distilling large language models into production-ready student models. He shares insights on optimizing query latency, handling document indexing, and evaluating retrieval quality.</p> <i>By Sahil Dua</i>",
      "image_url": "https://res.infoq.com/presentations/llm-large-scale-applications/en/mediumimage/sahil-dua-medium-1769590214923.jpeg",
      "published": "Fri, 13 Feb 2026 15:50:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.154,
      "tier1_quick_score": 2.141
    },
    {
      "id": "8a5b2209dad8ce1a",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Z.ai GLM-5: New SOTA Open Weights LLM",
      "url": "https://www.latent.space/p/ainews-zai-glm-5-new-sota-open-weights",
      "summary": "We have Opus 4.5 at home",
      "image_url": "https://substackcdn.com/image/fetch/$s_!oOz6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fb42dea-73b0-48fc-84f9-1345076c762e_3600x2072.png",
      "published": "Thu, 12 Feb 2026 07:40:22 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.099,
      "tier1_quick_score": 2.136
    },
    {
      "id": "c79c6992835d65e2",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "🔬Beyond AlphaFold: How Boltz is Open-Sourcing the Future of Drug Discovery",
      "url": "https://www.latent.space/p/boltz",
      "summary": "Inside Boltz, AlphaFold&#8217;s Legacy, and the Tools Powering Next-Gen Molecular Discovery",
      "image_url": "",
      "published": "Thu, 12 Feb 2026 02:12:14 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.091,
      "tier1_quick_score": 2.128
    },
    {
      "id": "8b95dcf6ece12b8d",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "VillageSQL Launches as an Extension-Focused MySQL Fork",
      "url": "https://www.infoq.com/news/2026/02/villagesql-mysql/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/villagesql-mysql/en/headerimage/generatedHeaderImage-1770808435787.jpg\" /><p>A new open-source project, VillageSQL, has been introduced as a tracking fork of MySQL aimed at expanding extensibility and addressing feature gaps increasingly relevant to AI and agent-based workloads.</p> <i>By Robert Krzaczyński</i>",
      "image_url": "https://res.infoq.com/news/2026/02/villagesql-mysql/en/headerimage/generatedHeaderImage-1770808435787.jpg",
      "published": "Fri, 13 Feb 2026 06:52:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.136,
      "tier1_quick_score": 2.123
    },
    {
      "id": "a2f63fd602a7cde7",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Qwen Image 2 and Seedance 2",
      "url": "https://www.latent.space/p/ainews-qwen-image-2-and-seedance",
      "summary": "Strong generative media showings from China",
      "image_url": "https://substackcdn.com/image/fetch/$s_!ih9H!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e22245-fa0e-47cd-8764-60bd2b63168d_2688x1536.png",
      "published": "Wed, 11 Feb 2026 05:19:52 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.068,
      "tier1_quick_score": 2.105
    },
    {
      "id": "9c52d519c1c8d2d4",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "OpenAI Scales Single Primary PostgreSQL Instance to Millions of Queries per Second for ChatGPT",
      "url": "https://www.infoq.com/news/2026/02/openai-runs-chatgpt-postgres/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://www.infoq.com/styles/static/images/logo/logo_bigger.jpg\" /><p>OpenAI described how it scaled PostgreSQL to support ChatGPT and its API platform, handling millions of queries per second for hundreds of millions of users. By running a single-primary PostgreSQL deployment on Azure with nearly 50 read replicas, optimizing query patterns, and offloading write-heavy workloads to sharded systems, OpenAI maintained low-latency reads while managing write pressure.</p> <i>By Leela Kumili</i>",
      "image_url": "https://www.infoq.com/styles/static/images/logo/logo_bigger.jpg",
      "published": "Thu, 12 Feb 2026 15:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.109,
      "tier1_quick_score": 2.096
    },
    {
      "id": "cb9c3e14f4f42ca9",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "The Scientist and the Simulator",
      "url": "https://www.latent.space/p/scientist-simulator",
      "summary": "LLMs (alone) won&#8217;t cure cancer",
      "image_url": "https://substack-post-media.s3.amazonaws.com/public/images/109042a7-6fe5-4155-9c9d-c857490fde4e_1023x569.png",
      "published": "Tue, 10 Feb 2026 15:27:58 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.056,
      "tier1_quick_score": 2.093
    },
    {
      "id": "8e6caf77a88544e7",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] \"Sci-Fi with a touch of Madness\"",
      "url": "https://www.latent.space/p/ainews-sci-fi-with-a-touch-of-madness",
      "summary": "a quiet day lets us reflect on a pithy quote from the ClawFather.",
      "image_url": "https://substackcdn.com/image/fetch/$s_!8fAR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba15034-2dd7-4e35-b07e-4e07638e34ca_1070x628.png",
      "published": "Tue, 10 Feb 2026 04:33:06 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.048,
      "tier1_quick_score": 2.085
    },
    {
      "id": "ba420121d4a3672e",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Conductors to Orchestrators: The Future of Agentic Coding - O'Reilly Media",
      "url": "https://news.google.com/rss/articles/CBMikAFBVV95cUxOYVdlMUlyWnhPTmVpa1pFZzZoazk0VWRSVFdPZTR0R1JwVHNZNWlTTDZ5VDVxZlRIdm0xQ0EwdlllM3ZnMFJ1cUF5LU1jVFBHZUtfZTk3RmNPMzV6YXB4Ry1Ea2tFc0RKLXhHdEhoVVAxMlRJekdDSnh6cDJENWNuTzZXeDAtZnZfOHQ0RkkyeGg?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMikAFBVV95cUxOYVdlMUlyWnhPTmVpa1pFZzZoazk0VWRSVFdPZTR0R1JwVHNZNWlTTDZ5VDVxZlRIdm0xQ0EwdlllM3ZnMFJ1cUF5LU1jVFBHZUtfZTk3RmNPMzV6YXB4Ry1Ea2tFc0RKLXhHdEhoVVAxMlRJekdDSnh6cDJENWNuTzZXeDAtZnZfOHQ0RkkyeGg?oc=5\" target=\"_blank\">Conductors to Orchestrators: The Future of Agentic Coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">O'Reilly Media</font>",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 12:01:49 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.146,
      "tier1_quick_score": 2.083
    },
    {
      "id": "e73b3c4c9fbb1706",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "MiniMax's new open M2.5 and M2.5 Lightning near state-of-the-art while costing 1/20th of Claude Opus 4.6 - VentureBeat",
      "url": "https://news.google.com/rss/articles/CBMipwFBVV95cUxOZW9hYWlJVHFieWxBb1ZJbGRJWk44S3h0dWV5UkhxWDROVm1IT3A4dnQ0eGFpdEFEbzNlYzZIMUJyWUdXMHRFMDdhRGd0dk1wSlRocTVVOF9sSDhNU1dYbVM1ZXF2Wm1HREhkV1dZZF9KZ2pNRmVfTG5JOVBvY3I4ODJwUG41X2lOUVczaXVOb044M2pTb2ljM1BHM1FOMWhLWFVsRmRsSQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMipwFBVV95cUxOZW9hYWlJVHFieWxBb1ZJbGRJWk44S3h0dWV5UkhxWDROVm1IT3A4dnQ0eGFpdEFEbzNlYzZIMUJyWUdXMHRFMDdhRGd0dk1wSlRocTVVOF9sSDhNU1dYbVM1ZXF2Wm1HREhkV1dZZF9KZ2pNRmVfTG5JOVBvY3I4ODJwUG41X2lOUVczaXVOb044M2pTb2ljM1BHM1FOMWhLWFVsRmRsSQ?oc=5\" target=\"_blank\">MiniMax's new open M2.5 and M2.5 Lightning near state-of-the-art while costing 1/20th of Claude Opus 4.6</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 07:51:47 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.138,
      "tier1_quick_score": 2.075
    },
    {
      "id": "a8257b8b88f12ec0",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Custom Kernels for All from Codex and Claude",
      "url": "https://huggingface.co/blog/custom-cuda-kernels-agent-skills",
      "summary": "",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.124,
      "tier1_quick_score": 2.061
    },
    {
      "id": "a9079ecb810f16fd",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "Experts Have World Models. LLMs Have Word Models.",
      "url": "https://www.latent.space/p/adversarial-reasoning",
      "summary": "Most expert work isn&#8217;t &#8220;produce a probable artifact&#8221;; it's \"choose a good move considering other agents, guessing hidden state\". LLMs default to single-shot artifacts and need World Models to progress",
      "image_url": "https://substackcdn.com/image/fetch/$s_!2ITs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6651e22-efa5-4500-ba6e-34fa98c17941_1600x909.png",
      "published": "Sat, 07 Feb 2026 22:11:25 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.023,
      "tier1_quick_score": 2.06
    },
    {
      "id": "9692df1badab5d6d",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] AI vs SaaS: The Unreasonable Effectiveness of Centralizing the AI Heartbeat",
      "url": "https://www.latent.space/p/ainews-ai-vs-saas-the-unreasonable",
      "summary": "A quiet day lets us reflect on a through line from OpenClaw to Frontier to MCP UI to Cursor/Anthropic Teams",
      "image_url": "https://substackcdn.com/image/fetch/$s_!7D7f!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9291b2e-db14-4a90-bd9c-571fb8f130d5_2368x1584.png",
      "published": "Sat, 07 Feb 2026 04:11:08 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.018,
      "tier1_quick_score": 2.055
    },
    {
      "id": "fb08ae7e11df1df0",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "The First Mechanistic Interpretability Frontier Lab — Myra Deng & Mark Bissell of Goodfire AI",
      "url": "https://www.latent.space/p/goodfire",
      "summary": "From Palantir and Two Sigma to building Goodfire into the poster-child for actionable mechanistic interpretability, Mark Bissell (Member of Technical Staff) and Myra Deng (Head of Product) are trying to turn &#8220;peeking inside the model&#8221; into a repeatable production workflow by shipping APIs, landing real enterprise deployments, and now scaling the bet with a recent",
      "image_url": "",
      "published": "Fri, 06 Feb 2026 22:45:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.016,
      "tier1_quick_score": 2.053
    },
    {
      "id": "6a5ffd1a6cafb398",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "OpenAI deploys Cerebras chips for 'near-instant' code generation in first major move beyond Nvidia - VentureBeat",
      "url": "https://news.google.com/rss/articles/CBMirwFBVV95cUxQSTVNUjVwY01qT2t4SlZkOThxdTVkTi13eklyWW1qeUNZVnFMMzcxZm5pbFBhUE90Qy1fUFZra2tEMHRqeXc4bkRXa0dKaEh0dTZqdlA4dnUxeHRUU3VSRHZzVEVnZXh1OGtJdnRzcGFKVXZOU2x4RnJpa2o5WkNhVkhaa0tybi13ekhfN04zM0VrLUtIeHVnUmJPb01xNFQtZ2J6OU5pNnN2UDRiTUlF?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMirwFBVV95cUxQSTVNUjVwY01qT2t4SlZkOThxdTVkTi13eklyWW1qeUNZVnFMMzcxZm5pbFBhUE90Qy1fUFZra2tEMHRqeXc4bkRXa0dKaEh0dTZqdlA4dnUxeHRUU3VSRHZzVEVnZXh1OGtJdnRzcGFKVXZOU2x4RnJpa2o5WkNhVkhaa0tybi13ekhfN04zM0VrLUtIeHVnUmJPb01xNFQtZ2J6OU5pNnN2UDRiTUlF?oc=5\" target=\"_blank\">OpenAI deploys Cerebras chips for 'near-instant' code generation in first major move beyond Nvidia</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
      "image_url": "",
      "published": "Thu, 12 Feb 2026 18:01:17 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.114,
      "tier1_quick_score": 2.051
    },
    {
      "id": "12909e2aa8d44049",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] OpenAI and Anthropic go to war: Claude Opus 4.6 vs GPT 5.3 Codex",
      "url": "https://www.latent.space/p/ainews-openai-and-anthropic-go-to",
      "summary": "The battle of the SOTA Coding Models steps up a notch",
      "image_url": "https://substackcdn.com/image/fetch/$s_!Fplu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788d2586-c80b-4ce6-854f-09a3f725e77a_1512x1516.jpeg",
      "published": "Fri, 06 Feb 2026 04:10:33 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.013,
      "tier1_quick_score": 2.05
    },
    {
      "id": "f521e6e0ad998cab",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] ElevenLabs $500m Series D at $11B, Cerebras $1B Series H at $23B, Vibe Coding -> Agentic Engineering",
      "url": "https://www.latent.space/p/ainews-elevenlabs-500m-series-d-at",
      "summary": "SOTA Audio models, Fast Chips, and Koding Agents are all you need.",
      "image_url": "https://substackcdn.com/image/fetch/$s_!HevS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c3fa7d8-4909-4c52-a4ce-62f0d7b0873f_1192x1078.png",
      "published": "Thu, 05 Feb 2026 08:26:43 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 2.047
    },
    {
      "id": "86fb97fcf6bbdbfa",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Context Graphs and Agent Traces",
      "url": "https://www.latent.space/p/ainews-context-graphs-hype-or-actually",
      "summary": "a quiet day lets us feature a bubbling topic.",
      "image_url": "https://substackcdn.com/image/youtube/w_728,c_limit/zP8P7hJXwE0",
      "published": "Wed, 04 Feb 2026 03:13:58 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 2.043
    },
    {
      "id": "e2376d028ed9bf90",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] OpenAI Codex App: death of the VSCode fork, multitasking worktrees, Skills Automations",
      "url": "https://www.latent.space/p/ainews-openai-codex-app-death-of",
      "summary": "The meta is moving fast.",
      "image_url": "https://substackcdn.com/image/fetch/$s_!PHl8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25b7bb9-a5f6-4ad4-ad5e-d40583f867f5_2088x1850.png",
      "published": "Tue, 03 Feb 2026 07:35:33 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 2.042
    },
    {
      "id": "4788e2c11c6eda6e",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Moltbook — the first Social Network for AI Agents (Clawdbots/OpenClaw bots)",
      "url": "https://www.latent.space/p/ainews-moltbook-the-first-social",
      "summary": "The craziest week in Simulative AI for a while",
      "image_url": "https://substackcdn.com/image/fetch/$s_!m5VV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc03c35e-5e6b-4ff5-bb8f-959b003efd34_725x500.jpeg",
      "published": "Sat, 31 Jan 2026 02:13:41 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.002,
      "tier1_quick_score": 2.039
    },
    {
      "id": "b2c947fb498d45c0",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] SpaceXai Grok Imagine API - the #1 Video Model, Best Pricing and Latency",
      "url": "https://www.latent.space/p/ainews-spacexai-grok-imagine-api",
      "summary": "xAI cements its position as a frontier lab and prepares to merge with SpaceX",
      "image_url": "https://substackcdn.com/image/fetch/$s_!m-eA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29bc2b9a-cc66-409f-bc00-3eb1abffc039_697x317.png",
      "published": "Fri, 30 Jan 2026 06:25:20 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.038
    },
    {
      "id": "8aa0cbdeeb83a33c",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Sam Altman's AI Combinator",
      "url": "https://www.latent.space/p/ainews-sam-altmans-ai-combinator",
      "summary": "a quiet day in the news lets us reflect on Sama's town hall message this week",
      "image_url": "https://substackcdn.com/image/youtube/w_728,c_limit/Wpxv-8nG8ec",
      "published": "Thu, 29 Jan 2026 03:58:09 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 2.038
    },
    {
      "id": "504929088957b0e2",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
      "url": "https://huggingface.co/blog/openenv-turing",
      "summary": "",
      "image_url": "",
      "published": "Thu, 12 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.089,
      "tier1_quick_score": 2.026
    },
    {
      "id": "13f27a4533648d52",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Behind Model Launch What Customers Discovered Testing Claude Opus 4 6 Early",
      "url": "https://claude.com/blog/behind-model-launch-what-customers-discovered-testing-claude-opus-4-6-early",
      "summary": "",
      "image_url": "",
      "published": "2026-02-09T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.033,
      "tier1_quick_score": 2.02
    },
    {
      "id": "9416f6460587ad55",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "On Agent Frameworks and Agent Observability",
      "url": "https://blog.langchain.com/on-agent-frameworks-and-agent-observability/",
      "summary": "<p>Every time LLMs get better, the same question comes back: &quot;Do you still need an agent framework?&quot; It&apos;s a fair question. The best way to build agents changes as the models get more performant and evolve, but fundamentally, the agent is a system <em>around</em> the model,</p>",
      "image_url": "https://blog.langchain.com/content/images/2026/02/On-Agent-Frameworks--1-.png",
      "published": "Fri, 13 Feb 2026 02:23:40 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.128,
      "tier1_quick_score": 2.015
    },
    {
      "id": "df23a683c09c1437",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-sdk==0.3.6",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.6",
      "summary": "<p>Changes since sdk==0.3.5</p>\n<ul>\n<li>release(sdk-py): 0.3.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6805\">#6805</a>)</li>\n<li>chore: update to add prune method (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6804\">#6804</a>)</li>\n<li>chore: Re-organize client files. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6787\">#6787</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-14T19:46:16Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.227,
      "tier1_quick_score": 2.014
    },
    {
      "id": "9ba7c73806fc836c",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "Join us for Interrupt: The Agent Conference",
      "url": "https://blog.langchain.com/join-us-for-interrupt-the-agent-conference/",
      "summary": "<p>Interrupt - The Agent Conference by LangChain - is where builders come to learn what&apos;s actually working in production. This year, we&apos;re bringing together more than 1,000 developers, product leaders, researchers, and founders to share what&apos;s coming next for agents&#x2014;and how</p>",
      "image_url": "https://blog.langchain.com/content/images/2026/02/interrupt-on-sale.png",
      "published": "Thu, 12 Feb 2026 17:42:03 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.113,
      "tier1_quick_score": 2.0
    },
    {
      "id": "238a31e6dec00118",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude Team Updates",
      "url": "https://claude.com/blog/claude-team-updates",
      "summary": "",
      "image_url": "",
      "published": "2026-01-28T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.988
    },
    {
      "id": "985c928b58a8bbca",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Complete Guide To Building Skills For Claude",
      "url": "https://claude.com/blog/complete-guide-to-building-skills-for-claude",
      "summary": "",
      "image_url": "",
      "published": "2026-01-29T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.988
    },
    {
      "id": "b8293190f185749f",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Contribution Metrics",
      "url": "https://claude.com/blog/contribution-metrics",
      "summary": "",
      "image_url": "",
      "published": "2026-01-29T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.988
    },
    {
      "id": "c52bcc5111bddab8",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Cowork Plugins",
      "url": "https://claude.com/blog/cowork-plugins",
      "summary": "",
      "image_url": "",
      "published": "2026-01-30T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.988
    },
    {
      "id": "e595cb99b0c5805a",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "How Leading Retailers Are Turning Ai Pilots Into Enterprise Wide Transformation",
      "url": "https://claude.com/blog/how-leading-retailers-are-turning-ai-pilots-into-enterprise-wide-transformation",
      "summary": "",
      "image_url": "",
      "published": "2026-01-28T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.988
    },
    {
      "id": "50353ccc12986ddb",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "1M Context",
      "url": "https://claude.com/blog/1m-context",
      "summary": "",
      "image_url": "",
      "published": "2025-08-12T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "0c224652445f27b1",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Agent Capabilities Api",
      "url": "https://claude.com/blog/agent-capabilities-api",
      "summary": "",
      "image_url": "",
      "published": "2025-05-22T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "2466d26e6117fbc5",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Amazon Bedrock General Availability",
      "url": "https://claude.com/blog/amazon-bedrock-general-availability",
      "summary": "",
      "image_url": "",
      "published": "2023-09-28T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "7457f4b82e6582a5",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Analysis Tool",
      "url": "https://claude.com/blog/analysis-tool",
      "summary": "",
      "image_url": "",
      "published": "2024-10-24T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "3608228517482926",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Android App",
      "url": "https://claude.com/blog/android-app",
      "summary": "",
      "image_url": "",
      "published": "2024-07-16T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "49fb3f9e04ec20e3",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Artifacts",
      "url": "https://claude.com/blog/artifacts",
      "summary": "",
      "image_url": "",
      "published": "2024-08-27T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "f0cb1d0fa09fdf41",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Automate Security Reviews With Claude Code",
      "url": "https://claude.com/blog/automate-security-reviews-with-claude-code",
      "summary": "",
      "image_url": "",
      "published": "2025-08-06T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "1d64c392769efbe9",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Best Practices For Prompt Engineering",
      "url": "https://claude.com/blog/best-practices-for-prompt-engineering",
      "summary": "",
      "image_url": "",
      "published": "2025-11-10T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "d95ca73f9331aabe",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Beyond Permission Prompts Making Claude Code More Secure And Autonomous",
      "url": "https://claude.com/blog/beyond-permission-prompts-making-claude-code-more-secure-and-autonomous",
      "summary": "",
      "image_url": "",
      "published": "2025-10-08T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "ff1165d20b54aaa9",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Build Responsive Web Layouts",
      "url": "https://claude.com/blog/build-responsive-web-layouts",
      "summary": "",
      "image_url": "",
      "published": "2025-10-10T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "45e7513656cfa4fb",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Building Agents With Skills Equipping Agents For Specialized Work",
      "url": "https://claude.com/blog/building-agents-with-skills-equipping-agents-for-specialized-work",
      "summary": "",
      "image_url": "",
      "published": "2026-01-22T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "ab11b9c5d3086d01",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Building Agents With The Claude Agent Sdk",
      "url": "https://claude.com/blog/building-agents-with-the-claude-agent-sdk",
      "summary": "",
      "image_url": "",
      "published": "2025-09-29T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "c6f0e92f195800a5",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Building Ai Agents For Startups",
      "url": "https://claude.com/blog/building-ai-agents-for-startups",
      "summary": "",
      "image_url": "",
      "published": "2025-11-03T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "df67318843b543a1",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Building Ai Agents In Financial Services",
      "url": "https://claude.com/blog/building-ai-agents-in-financial-services",
      "summary": "",
      "image_url": "",
      "published": "2025-10-30T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "682eafaf5a039705",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Building Ai Agents In Healthcare And Life Sciences",
      "url": "https://claude.com/blog/building-ai-agents-in-healthcare-and-life-sciences",
      "summary": "",
      "image_url": "",
      "published": "2025-10-30T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "b204490409946fa8",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Building Companies With Claude Code",
      "url": "https://claude.com/blog/building-companies-with-claude-code",
      "summary": "",
      "image_url": "",
      "published": "2025-11-17T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "7b520fbdf34d0af6",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Building Multi Agent Systems When And How To Use Them",
      "url": "https://claude.com/blog/building-multi-agent-systems-when-and-how-to-use-them",
      "summary": "",
      "image_url": "",
      "published": "2026-01-23T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "7ce2faa257104578",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude 2 1 Prompting",
      "url": "https://claude.com/blog/claude-2-1-prompting",
      "summary": "",
      "image_url": "",
      "published": "2023-12-06T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "bd8de2fd6410c7b8",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude 2 Amazon Bedrock",
      "url": "https://claude.com/blog/claude-2-amazon-bedrock",
      "summary": "",
      "image_url": "",
      "published": "2023-08-23T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "93e7b80d329d2b73",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude And Slack",
      "url": "https://claude.com/blog/claude-and-slack",
      "summary": "",
      "image_url": "",
      "published": "2025-10-01T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "6053c2685ebfd4aa",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude Code And New Admin Controls For Business Plans",
      "url": "https://claude.com/blog/claude-code-and-new-admin-controls-for-business-plans",
      "summary": "",
      "image_url": "",
      "published": "2025-08-20T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "a80b9ee3b8a9d36e",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude Code And Slack",
      "url": "https://claude.com/blog/claude-code-and-slack",
      "summary": "",
      "image_url": "",
      "published": "2025-12-08T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "48ee3d56f9357da5",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude Code On The Web",
      "url": "https://claude.com/blog/claude-code-on-the-web",
      "summary": "",
      "image_url": "",
      "published": "2025-10-20T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "fc828d6fe0167ba9",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude Code Plugins",
      "url": "https://claude.com/blog/claude-code-plugins",
      "summary": "",
      "image_url": "",
      "published": "2025-10-09T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "c062187d4fcf648c",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude Code Remote Mcp",
      "url": "https://claude.com/blog/claude-code-remote-mcp",
      "summary": "",
      "image_url": "",
      "published": "2025-06-18T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "2733b33e99bd7127",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude For Chrome",
      "url": "https://claude.com/blog/claude-for-chrome",
      "summary": "",
      "image_url": "",
      "published": "2025-08-25T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "d61ef087b60eec99",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude For Enterprise",
      "url": "https://claude.com/blog/claude-for-enterprise",
      "summary": "",
      "image_url": "",
      "published": "2024-09-10T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "464cba96c5f6fca1",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude Now Available In Microsoft 365 Copilot",
      "url": "https://claude.com/blog/claude-now-available-in-microsoft-365-copilot",
      "summary": "",
      "image_url": "",
      "published": "2025-09-24T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "5a11a22373278c2a",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude On Google Cloud Fedramp High",
      "url": "https://claude.com/blog/claude-on-google-cloud-fedramp-high",
      "summary": "",
      "image_url": "",
      "published": "2025-04-02T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "da2226ce7ff4f4b5",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Claude Powered Artifacts",
      "url": "https://claude.com/blog/claude-powered-artifacts",
      "summary": "",
      "image_url": "",
      "published": "2025-07-25T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "1aeb9c1a2cffaa17",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Connectors Directory",
      "url": "https://claude.com/blog/connectors-directory",
      "summary": "",
      "image_url": "",
      "published": "2025-07-14T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "40defcb62bc5ce55",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Context Management",
      "url": "https://claude.com/blog/context-management",
      "summary": "",
      "image_url": "",
      "published": "2025-09-29T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "88b654c642a9edde",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Cowork Research Preview",
      "url": "https://claude.com/blog/cowork-research-preview",
      "summary": "",
      "image_url": "",
      "published": "2026-01-12T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "9b896ce969159446",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Create Files",
      "url": "https://claude.com/blog/create-files",
      "summary": "",
      "image_url": "",
      "published": "2025-09-09T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "e48c7485c496e819",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Driving Ai Transformation With Claude",
      "url": "https://claude.com/blog/driving-ai-transformation-with-claude",
      "summary": "",
      "image_url": "",
      "published": "2025-10-01T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "26b687006df62be0",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Eight Trends Defining How Software Gets Built In 2026",
      "url": "https://claude.com/blog/eight-trends-defining-how-software-gets-built-in-2026",
      "summary": "",
      "image_url": "",
      "published": "2026-01-21T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "6f264d7b14923ac4",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Equipping Agents For The Real World With Agent Skills",
      "url": "https://claude.com/blog/equipping-agents-for-the-real-world-with-agent-skills",
      "summary": "",
      "image_url": "",
      "published": "2025-10-16T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "2da25174c1d1108e",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Evaluate Prompts",
      "url": "https://claude.com/blog/evaluate-prompts",
      "summary": "",
      "image_url": "",
      "published": "2024-07-09T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "4b19076c150fc45a",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Extending Claude Capabilities With Skills Mcp Servers",
      "url": "https://claude.com/blog/extending-claude-capabilities-with-skills-mcp-servers",
      "summary": "",
      "image_url": "",
      "published": "2025-12-19T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "51e2b22eda42a3e5",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Fine Tune Claude 3 Haiku",
      "url": "https://claude.com/blog/fine-tune-claude-3-haiku",
      "summary": "",
      "image_url": "",
      "published": "2024-07-10T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "d3ceb704a602cf60",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Fix Software Bugs Faster With Claude",
      "url": "https://claude.com/blog/fix-software-bugs-faster-with-claude",
      "summary": "",
      "image_url": "",
      "published": "2025-10-28T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "1e0ce0ce1d35aad1",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "How Anthropic Teams Use Claude Code",
      "url": "https://claude.com/blog/how-anthropic-teams-use-claude-code",
      "summary": "",
      "image_url": "",
      "published": "2025-07-24T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "e83b7c3db3142f4f",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "How Anthropic Uses Claude Legal",
      "url": "https://claude.com/blog/how-anthropic-uses-claude-legal",
      "summary": "",
      "image_url": "",
      "published": "2025-12-08T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "4e3ceea9ebdcd548",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "How Anthropic Uses Claude Marketing",
      "url": "https://claude.com/blog/how-anthropic-uses-claude-marketing",
      "summary": "",
      "image_url": "",
      "published": "2026-01-26T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "03fc89dbe7602aeb",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "How Brex Improves Code Quality And Productivity With Claude Code",
      "url": "https://claude.com/blog/how-brex-improves-code-quality-and-productivity-with-claude-code",
      "summary": "",
      "image_url": "",
      "published": "2025-10-30T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "1c939c28f9d91818",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "How Enterprises Are Building Ai Agents In 2026",
      "url": "https://claude.com/blog/how-enterprises-are-building-ai-agents-in-2026",
      "summary": "",
      "image_url": "",
      "published": "2025-12-09T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "ee4686e8ce3f89fc",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "How To Configure Hooks",
      "url": "https://claude.com/blog/how-to-configure-hooks",
      "summary": "",
      "image_url": "",
      "published": "2025-12-11T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "f362d6ef62bcd517",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "How To Create Skills Key Steps Limitations And Examples",
      "url": "https://claude.com/blog/how-to-create-skills-key-steps-limitations-and-examples",
      "summary": "",
      "image_url": "",
      "published": "2025-11-19T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "b3c10c1ee7f8b08a",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Improving Frontend Design Through Skills",
      "url": "https://claude.com/blog/improving-frontend-design-through-skills",
      "summary": "",
      "image_url": "",
      "published": "2025-11-12T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "7df7be315b2dd457",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Integrate Apis Seamlessly",
      "url": "https://claude.com/blog/integrate-apis-seamlessly",
      "summary": "",
      "image_url": "",
      "published": "2025-10-27T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "c6bfabb93460286b",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Integrations",
      "url": "https://claude.com/blog/integrations",
      "summary": "",
      "image_url": "",
      "published": "2025-05-01T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "4da1dec2412bf85a",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Interactive Tools In Claude",
      "url": "https://claude.com/blog/interactive-tools-in-claude",
      "summary": "",
      "image_url": "",
      "published": "2026-01-26T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "7db69784c9b07ea9",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Introducing Citations Api",
      "url": "https://claude.com/blog/introducing-citations-api",
      "summary": "",
      "image_url": "",
      "published": "2025-06-23T00:00:00+00:00",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.987
    },
    {
      "id": "9ea0f90bfed549ed",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Xcode 26.3 Brings Integrated Agentic Coding for Anthropic Claude Agent and OpenAI Codex - infoq.com",
      "url": "https://news.google.com/rss/articles/CBMibkFVX3lxTFBCM3gwVFdyR25XRWthY0pCT2JVaThiQVoyZ0tlQk01UXFrU2NHV2JmLVdMUDN6c05rUTdmVmZRZnY5VXlfZjJ5WnJYQUhpYzMtZG9FUnJMcnZYVVptdm5sM1Bpb0c1dkh1SkxkVzlB?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMibkFVX3lxTFBCM3gwVFdyR25XRWthY0pCT2JVaThiQVoyZ0tlQk01UXFrU2NHV2JmLVdMUDN6c05rUTdmVmZRZnY5VXlfZjJ5WnJYQUhpYzMtZG9FUnJMcnZYVVptdm5sM1Bpb0c1dkh1SkxkVzlB?oc=5\" target=\"_blank\">Xcode 26.3 Brings Integrated Agentic Coding for Anthropic Claude Agent and OpenAI Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">infoq.com</font>",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 11:01:02 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.038,
      "tier1_quick_score": 1.975
    },
    {
      "id": "fb56b97bf4eae007",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Opus 4.6, Codex 5.3, and the post-benchmark era - Interconnects AI",
      "url": "https://news.google.com/rss/articles/CBMiX0FVX3lxTE1jbEZUVklnaEw3ZWZjZFB6WjNwUUFnOHBncjVpX1p4UjNMV3FPazR5c1FPQzBOZzk5bXJOd0xLT1Y5TFhmdHc1OFNVeTU5X2MxNE4yOUxXNlU0NUdPMEpr?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiX0FVX3lxTE1jbEZUVklnaEw3ZWZjZFB6WjNwUUFnOHBncjVpX1p4UjNMV3FPazR5c1FPQzBOZzk5bXJOd0xLT1Y5TFhmdHc1OFNVeTU5X2MxNE4yOUxXNlU0NUdPMEpr?oc=5\" target=\"_blank\">Opus 4.6, Codex 5.3, and the post-benchmark era</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Interconnects AI</font>",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.036,
      "tier1_quick_score": 1.973
    },
    {
      "id": "fccbd276447697a6",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Transformers.js v4 Preview: Now Available on NPM!",
      "url": "https://huggingface.co/blog/transformersjs-v4",
      "summary": "",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.033,
      "tier1_quick_score": 1.97
    },
    {
      "id": "cbe9183ff645d75a",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple lets AI agents write Swift code directly in Xcode - PPC Land",
      "url": "https://news.google.com/rss/articles/CBMifkFVX3lxTE5weEVpWWhuR2JLeUJPS0k1N3B4cnhabjE5Zi1ZV2MyZ0tzUGRwS1V5WkNwbTNWSHdUNkFkYXF4OVhxaDZaRXRMTk9wOGw1bHRhY29DOXo5NEVmSXloekhIMlhrZjJLRWt4d0V4dG54M1U4T2tfbDA0YTFKbzRaZw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMifkFVX3lxTE5weEVpWWhuR2JLeUJPS0k1N3B4cnhabjE5Zi1ZV2MyZ0tzUGRwS1V5WkNwbTNWSHdUNkFkYXF4OVhxaDZaRXRMTk9wOGw1bHRhY29DOXo5NEVmSXloekhIMlhrZjJLRWt4d0V4dG54M1U4T2tfbDA0YTFKbzRaZw?oc=5\" target=\"_blank\">Apple lets AI agents write Swift code directly in Xcode</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">PPC Land</font>",
      "image_url": "",
      "published": "Sun, 08 Feb 2026 18:31:05 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.03,
      "tier1_quick_score": 1.967
    },
    {
      "id": "d3ee385b0647261d",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "I built an iOS app in just two days with just my voice - and it was electrifying - ZDNET",
      "url": "https://news.google.com/rss/articles/CBMidEFVX3lxTE9Mel83bU5vVHZEZWp0S2NmaFA2ZWxwUTdWUDY5Zy00ZksxQU5YOGt6blpqcWc5RHdpby1NNmtZRno1TUNnYmF4TUxDcTdkeDBhOEtDMGI5bGFnMjVvd3NmV0hzT1JJSlR0M1NoMi1jN1pjb1VQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMidEFVX3lxTE9Mel83bU5vVHZEZWp0S2NmaFA2ZWxwUTdWUDY5Zy00ZksxQU5YOGt6blpqcWc5RHdpby1NNmtZRno1TUNnYmF4TUxDcTdkeDBhOEtDMGI5bGFnMjVvd3NmV0hzT1JJSlR0M1NoMi1jN1pjb1VQ?oc=5\" target=\"_blank\">I built an iOS app in just two days with just my voice - and it was electrifying</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ZDNET</font>",
      "image_url": "",
      "published": "Sun, 08 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.026,
      "tier1_quick_score": 1.963
    },
    {
      "id": "a4df9db21adf832a",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "I tried a Claude Code rival that's local, open source, and completely free - how it went - ZDNET",
      "url": "https://news.google.com/rss/articles/CBMiiwFBVV95cUxPa0VEQ1lXZlkzUHkyUG52LW9heG50UmRNRnpqWUdaZGVaMm55SlZSeTM3a3hCWkhicGJ6NjRUWUdJNlFyLWwwWGhTc21HUFFQSkNpc0g2VmxoWXZlTU9ZRGNhY0REYnM0UUl6RTY0RUV6UFhPVFdiS2todGVNMnBaVmE3SWhfdktwRV9r?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiiwFBVV95cUxPa0VEQ1lXZlkzUHkyUG52LW9heG50UmRNRnpqWUdaZGVaMm55SlZSeTM3a3hCWkhicGJ6NjRUWUdJNlFyLWwwWGhTc21HUFFQSkNpc0g2VmxoWXZlTU9ZRGNhY0REYnM0UUl6RTY0RUV6UFhPVFdiS2todGVNMnBaVmE3SWhfdktwRV9r?oc=5\" target=\"_blank\">I tried a Claude Code rival that's local, open source, and completely free - how it went</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ZDNET</font>",
      "image_url": "",
      "published": "Sun, 08 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.026,
      "tier1_quick_score": 1.963
    },
    {
      "id": "42f60e69c8e064a5",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Xcode 26.3 brings OpenAI Codex and Anthropic Claude agent for smarter, agentic coding - MSN",
      "url": "https://news.google.com/rss/articles/CBMi4wJBVV95cUxPdENHNFA4Vk5WUDM3TkxIVVFLVnBvY1pxUTJLcmgxN2x6R1d4RGZnM216U25OcGZucy1TZk9yTV9taERvOGpjY2FpZXBTWVRkT0VZQ0V0dGk1N1oycHk2V2I4ejM2VjBtb0xDMGlaaUREcXN6MmR3cEdGdl9MYUFneE1PQVlWYmVQdENNUnN6WFNQWUxQVnctYjFLYVhDMlV2TTNZcXJwZ2h0RGd6bkUySmR4T0k0VFplZ19WaWprYUdpTDZHN0ZZRjFqN1dRYTRxY0FFaWs5UDhlUHFTY3NEbGRkeDdneHBGRVJRQ0dJb0x0SFF4RU81X09ac3R4VDdUeFhrNkwyZlotaktyczFQOTNsdjlwWUptNzE4Zm94MjlyM0RmeTNLR0xXS0NsTG9xemdOckhxeUY5QzhMOG5HWTBNN1ZScmhZZVVQWm1LNm9JaDBNXzE4Tzl5R0Iza0c1Mmg4?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi4wJBVV95cUxPdENHNFA4Vk5WUDM3TkxIVVFLVnBvY1pxUTJLcmgxN2x6R1d4RGZnM216U25OcGZucy1TZk9yTV9taERvOGpjY2FpZXBTWVRkT0VZQ0V0dGk1N1oycHk2V2I4ejM2VjBtb0xDMGlaaUREcXN6MmR3cEdGdl9MYUFneE1PQVlWYmVQdENNUnN6WFNQWUxQVnctYjFLYVhDMlV2TTNZcXJwZ2h0RGd6bkUySmR4T0k0VFplZ19WaWprYUdpTDZHN0ZZRjFqN1dRYTRxY0FFaWs5UDhlUHFTY3NEbGRkeDdneHBGRVJRQ0dJb0x0SFF4RU81X09ac3R4VDdUeFhrNkwyZlotaktyczFQOTNsdjlwWUptNzE4Zm94MjlyM0RmeTNLR0xXS0NsTG9xemdOckhxeUY5QzhMOG5HWTBNN1ZScmhZZVVQWm1LNm9JaDBNXzE4Tzl5R0Iza0c1Mmg4?oc=5\" target=\"_blank\">Xcode 26.3 brings OpenAI Codex and Anthropic Claude agent for smarter, agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">MSN</font>",
      "image_url": "",
      "published": "Sun, 08 Feb 2026 06:59:04 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.026,
      "tier1_quick_score": 1.963
    },
    {
      "id": "4b4b1fa4025afbc0",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Augment Code makes its semantic coding capability available for any AI agent - SiliconANGLE",
      "url": "https://news.google.com/rss/articles/CBMiowFBVV95cUxNS2FPU0lOTGs0YWMxUzY4NnN5bzRUanktakt4emNGZGx2ZHI0a0lRbG0xSmlDME5CY0VmcHpMUE9LYllycy0yNDlnOFJ3cGJyOEVIejF5Qm1VbzVoNFRPeDlTNXl3cFpxR3dYcUNYWkhfeWp6ODhHR1phY1VORmpINjlua3g1NzhCb1VtSkJ1ZzRTOEczQkJZemNfVDNTUG5HbXlj?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiowFBVV95cUxNS2FPU0lOTGs0YWMxUzY4NnN5bzRUanktakt4emNGZGx2ZHI0a0lRbG0xSmlDME5CY0VmcHpMUE9LYllycy0yNDlnOFJ3cGJyOEVIejF5Qm1VbzVoNFRPeDlTNXl3cFpxR3dYcUNYWkhfeWp6ODhHR1phY1VORmpINjlua3g1NzhCb1VtSkJ1ZzRTOEczQkJZemNfVDNTUG5HbXlj?oc=5\" target=\"_blank\">Augment Code makes its semantic coding capability available for any AI agent</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">SiliconANGLE</font>",
      "image_url": "",
      "published": "Fri, 06 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.013,
      "tier1_quick_score": 1.95
    },
    {
      "id": "d2d3810992df7825",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Introducing SyGra Studio",
      "url": "https://huggingface.co/blog/ServiceNow-AI/sygra-studio",
      "summary": "",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 16:52:28 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.011,
      "tier1_quick_score": 1.948
    },
    {
      "id": "66fc69c3f3074d5c",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Introducing Claude Opus 4.6 - Anthropic",
      "url": "https://news.google.com/rss/articles/CBMiWkFVX3lxTE90b09UTmFMU3laaUE4WGQ0ejltUDEtZHdHMjlSY1pDTUhjX1A3VF9qRU56ejhuRkR3eFFvUFB4UV9ZTjMzaklkTkczSEJZWXRNOE1UX3hDd3dHdw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiWkFVX3lxTE90b09UTmFMU3laaUE4WGQ0ejltUDEtZHdHMjlSY1pDTUhjX1A3VF9qRU56ejhuRkR3eFFvUFB4UV9ZTjMzaklkTkczSEJZWXRNOE1UX3hDd3dHdw?oc=5\" target=\"_blank\">Introducing Claude Opus 4.6</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Anthropic</font>",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 1.947
    },
    {
      "id": "e05b2906fc12f840",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "OpenAI launches new agentic coding model only minutes after Anthropic drops its own - TechCrunch",
      "url": "https://news.google.com/rss/articles/CBMiugFBVV95cUxPVlB0ZWFTR0h6Ql91OVJ6bjJTT05iZUk0akI5SzRTYUdVeHNYQjQ4cVpKdUtBdVpnNk5jNTF4YlROQnFSOVlXU0I0UHRFWjQ5ZjZmUG5vV3ZzYkVYREpCR0Vjbk5UMHlwU2hJY1FrX1JQVGoxZXlaSXBYWFJSaUxIQ0licHhaZzEtVTVucy1MQnJva1JySUgyN1RFVWpFNS1lWkdCVU4tbzQ3YUkwUlAza2pFMWJaVkp4bGc?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiugFBVV95cUxPVlB0ZWFTR0h6Ql91OVJ6bjJTT05iZUk0akI5SzRTYUdVeHNYQjQ4cVpKdUtBdVpnNk5jNTF4YlROQnFSOVlXU0I0UHRFWjQ5ZjZmUG5vV3ZzYkVYREpCR0Vjbk5UMHlwU2hJY1FrX1JQVGoxZXlaSXBYWFJSaUxIQ0licHhaZzEtVTVucy1MQnJva1JySUgyN1RFVWpFNS1lWkdCVU4tbzQ3YUkwUlAza2pFMWJaVkp4bGc?oc=5\" target=\"_blank\">OpenAI launches new agentic coding model only minutes after Anthropic drops its own</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">TechCrunch</font>",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 1.947
    },
    {
      "id": "236f21236bd310b8",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "OpenAI’s GPT-5.3-Codex drops as Anthropic upgrades Claude — AI coding wars heat up ahead of Super Bowl ads - VentureBeat",
      "url": "https://news.google.com/rss/articles/CBMisAFBVV95cUxPX3J2Q0o0c2wybXpmQUVPOVhncWRua1EtT3JmVkFFN0l4bE9RVnJvOVJDejIyVWI4MV91UjhseXYwTkN5WjJfdThobFFZRU1YandSTHRBUTJHbFBNcGc4cW1URjhwX1R4Qi0xLTVWVm9pQmlUM1Y0UTcwU1FGcGJ3Z2k3TEhOcUxueFQ2SFdYVUNCNm5aeWJGMDNFaVhsOWN4SEtvOVJwWTBLQTNLZTZoMg?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMisAFBVV95cUxPX3J2Q0o0c2wybXpmQUVPOVhncWRua1EtT3JmVkFFN0l4bE9RVnJvOVJDejIyVWI4MV91UjhseXYwTkN5WjJfdThobFFZRU1YandSTHRBUTJHbFBNcGc4cW1URjhwX1R4Qi0xLTVWVm9pQmlUM1Y0UTcwU1FGcGJ3Z2k3TEhOcUxueFQ2SFdYVUNCNm5aeWJGMDNFaVhsOWN4SEtvOVJwWTBLQTNLZTZoMg?oc=5\" target=\"_blank\">OpenAI’s GPT-5.3-Codex drops as Anthropic upgrades Claude — AI coding wars heat up ahead of Super Bowl ads</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 1.947
    },
    {
      "id": "97d8f0df880ed3b9",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Introducing GPT-5.3-Codex - OpenAI",
      "url": "https://news.google.com/rss/articles/CBMiYEFVX3lxTE1WSFdyRV9lZDd3TTJWVlhKY1pWMTRaSHJaa0hLU05kTE1QcGVjRVdndHhuNnFhQW5ObmtkcFhpRm1wd21yU1o2dDRYUWE2aDJzX1p4ZERNVlZjSHlaMTl6Qw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiYEFVX3lxTE1WSFdyRV9lZDd3TTJWVlhKY1pWMTRaSHJaa0hLU05kTE1QcGVjRVdndHhuNnFhQW5ObmtkcFhpRm1wd21yU1o2dDRYUWE2aDJzX1p4ZERNVlZjSHlaMTl6Qw?oc=5\" target=\"_blank\">Introducing GPT-5.3-Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">OpenAI</font>",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 1.947
    },
    {
      "id": "ccb6d9b32743e095",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Anthropic's Claude Opus 4.6 brings 1M token context and 'agent teams' to take on OpenAI's Codex - VentureBeat",
      "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxNQnNEQmdUWWJnRV9vaDFWSk5lOHhEeFYtWGdIaGFjendaZTc4ZjRhSllxY1VtOFRaUEJnQ1R5MDlUd1VXRGZkNlhZUEpvemlmaUVEWGFpcGwwc0pjaTgtNmE4dklOOTVBXzR5WHVZV0tLa0hWdGdVWE0zZ3pEMlJpUjBsY1R0eUZXTGlTbnNQOTNoS1d6NENKN3pNU1FwWUlSOFJ0ejVZZ0RIQmloMGc?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxNQnNEQmdUWWJnRV9vaDFWSk5lOHhEeFYtWGdIaGFjendaZTc4ZjRhSllxY1VtOFRaUEJnQ1R5MDlUd1VXRGZkNlhZUEpvemlmaUVEWGFpcGwwc0pjaTgtNmE4dklOOTVBXzR5WHVZV0tLa0hWdGdVWE0zZ3pEMlJpUjBsY1R0eUZXTGlTbnNQOTNoS1d6NENKN3pNU1FwWUlSOFJ0ejVZZ0RIQmloMGc?oc=5\" target=\"_blank\">Anthropic's Claude Opus 4.6 brings 1M token context and 'agent teams' to take on OpenAI's Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 1.947
    },
    {
      "id": "3a266fd468382e9e",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Anthropic’s new Claude Opus 4.6 aims to think through bigger code bases - Fast Company",
      "url": "https://news.google.com/rss/articles/CBMiqgFBVV95cUxNaGh6SGpJVUhQSGJoZ0JyZ21fdHU0VUNpcVFmRDNVREh5NERVU1J0VEFRaFpEcXU2VUZIWWl5ZmFJVGFkRURtbEoweDh3V29QTktWbVhPR2lBdk14WFhPNi1CZURCLU8zN1dZUk9TSXJndXY0TFpyY1ZfX3hGUVpfU25JcldtWElhOWt5Ujh6WHJLVmxjc21SYkFkWDFmeG1QN3ZIZjlTR3Q1QQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiqgFBVV95cUxNaGh6SGpJVUhQSGJoZ0JyZ21fdHU0VUNpcVFmRDNVREh5NERVU1J0VEFRaFpEcXU2VUZIWWl5ZmFJVGFkRURtbEoweDh3V29QTktWbVhPR2lBdk14WFhPNi1CZURCLU8zN1dZUk9TSXJndXY0TFpyY1ZfX3hGUVpfU25JcldtWElhOWt5Ujh6WHJLVmxjc21SYkFkWDFmeG1QN3ZIZjlTR3Q1QQ?oc=5\" target=\"_blank\">Anthropic’s new Claude Opus 4.6 aims to think through bigger code bases</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Fast Company</font>",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 1.947
    },
    {
      "id": "d2ad7905fbc5878e",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple Xcode unleashes AI agents for fast and easy app creation - Cult of Mac",
      "url": "https://news.google.com/rss/articles/CBMiakFVX3lxTE1OT3lFRzRWeDVoNzJnNjJiWWtRU192TkF5c3FqZ3JmUVlKOTFWMTEzMlVvZHFfbWNvZEI3S080cTZTNFBWZWduSWxlSmxZdmtMdnV6c3NlN081VU9Ob19oZnR1dlJMOVpiUVE?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiakFVX3lxTE1OT3lFRzRWeDVoNzJnNjJiWWtRU192TkF5c3FqZ3JmUVlKOTFWMTEzMlVvZHFfbWNvZEI3S080cTZTNFBWZWduSWxlSmxZdmtMdnV6c3NlN081VU9Ob19oZnR1dlJMOVpiUVE?oc=5\" target=\"_blank\">Apple Xcode unleashes AI agents for fast and easy app creation</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Cult of Mac</font>",
      "image_url": "",
      "published": "Wed, 04 Feb 2026 16:07:30 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.008,
      "tier1_quick_score": 1.945
    },
    {
      "id": "0d9b64d5c3c74511",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Want local vibe coding? This AI stack might replace Claude Code and Codex - for free - ZDNET",
      "url": "https://news.google.com/rss/articles/CBMijAFBVV95cUxQMjEzc3Z5aUFlajFuVWRFZ0dFN19pVHpxLVc3NjR4bzgzME1Ec3JUemJLZGliSTZNRHFmQ1J3OHdGTlFCUW9TaXdoaDV2NDNVMzdhZWcyajJvZkhMMmY0OE1NVEY4ZEtxMTF0Mlp1RDF3dVJpUHB0WnJ2djdLaEx1V0pMbEM5bUl1UlVZTg?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMijAFBVV95cUxQMjEzc3Z5aUFlajFuVWRFZ0dFN19pVHpxLVc3NjR4bzgzME1Ec3JUemJLZGliSTZNRHFmQ1J3OHdGTlFCUW9TaXdoaDV2NDNVMzdhZWcyajJvZkhMMmY0OE1NVEY4ZEtxMTF0Mlp1RDF3dVJpUHB0WnJ2djdLaEx1V0pMbEM5bUl1UlVZTg?oc=5\" target=\"_blank\">Want local vibe coding? This AI stack might replace Claude Code and Codex - for free</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ZDNET</font>",
      "image_url": "",
      "published": "Wed, 04 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.007,
      "tier1_quick_score": 1.944
    },
    {
      "id": "8bd01525ea6fcf60",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple’s Xcode 26.3 brings integrated support for agentic coding - InfoWorld",
      "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxOWXhMVVVCZHBuYUlMOGNzRWxnZFNHeG9CcEY0QlpZZUNHZ0hfU3I2VjVUbDNiZDBlUEoxMm9naVlRS3ZPelNkUW9RVzBKQ3NDeUJtZEJoOTlzczJESmUtM2wtazVGTGJFZ3BDeDktY19qOEVFajltRER2RzNaalRhaF9ZdVp0NzNUWng3ajZrUXpGd3c3MTZNWjF1bVhDbktBMnZXNzdKZUtFTU54amc?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxOWXhMVVVCZHBuYUlMOGNzRWxnZFNHeG9CcEY0QlpZZUNHZ0hfU3I2VjVUbDNiZDBlUEoxMm9naVlRS3ZPelNkUW9RVzBKQ3NDeUJtZEJoOTlzczJESmUtM2wtazVGTGJFZ3BDeDktY19qOEVFajltRER2RzNaalRhaF9ZdVp0NzNUWng3ajZrUXpGd3c3MTZNWjF1bVhDbktBMnZXNzdKZUtFTU54amc?oc=5\" target=\"_blank\">Apple’s Xcode 26.3 brings integrated support for agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">InfoWorld</font>",
      "image_url": "",
      "published": "Wed, 04 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.007,
      "tier1_quick_score": 1.944
    },
    {
      "id": "d12dd93609e4f992",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple Brings Agentic AI Coding to Xcode With Claude and Codex - Unite.AI",
      "url": "https://news.google.com/rss/articles/CBMiiwFBVV95cUxPY0ZQN19icnVpWHZZYWJxdk5OOWNYeVlBcXdLVTdNMGVPY010UGU2eGdNYUM4MmtTd3pYRUlRUldaOU1BbmFEWnhvRHpFMWVmeVFUd2NuS2JwbVh2dURhYWtwUFVJV3NzUDJZZnl3TlZrb2Y2YlZSTFY0anJlMUtrdmhBcWZTZmo5LTZV?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiiwFBVV95cUxPY0ZQN19icnVpWHZZYWJxdk5OOWNYeVlBcXdLVTdNMGVPY010UGU2eGdNYUM4MmtTd3pYRUlRUldaOU1BbmFEWnhvRHpFMWVmeVFUd2NuS2JwbVh2dURhYWtwUFVJV3NzUDJZZnl3TlZrb2Y2YlZSTFY0anJlMUtrdmhBcWZTZmo5LTZV?oc=5\" target=\"_blank\">Apple Brings Agentic AI Coding to Xcode With Claude and Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Unite.AI</font>",
      "image_url": "",
      "published": "Wed, 04 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.007,
      "tier1_quick_score": 1.944
    },
    {
      "id": "1b245eb7bddb2baa",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple Brings Agentic Coding to Xcode 26.3 With Claude and Codex - The Mac Observer",
      "url": "https://news.google.com/rss/articles/CBMingFBVV95cUxOb3VtdjVlQlJiSDZTanNhc3VfTVNLTm8xZ0VlMVBoYWlEVUk3Mm14WjRlUTJOT3VOQ1FsSmdqTklnMHFhNUdiUEJMUGFHWWNXcDFMRWpabmFuaFgxZk8wSTdzS3BURjUySDRhZHdxdlJnU2FtM1AzaHBXVHEwa2tvUXBXdVZoQlBfMGlFTU4wUEQwMmRqeGlFdjBEaEJHZw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMingFBVV95cUxOb3VtdjVlQlJiSDZTanNhc3VfTVNLTm8xZ0VlMVBoYWlEVUk3Mm14WjRlUTJOT3VOQ1FsSmdqTklnMHFhNUdiUEJMUGFHWWNXcDFMRWpabmFuaFgxZk8wSTdzS3BURjUySDRhZHdxdlJnU2FtM1AzaHBXVHEwa2tvUXBXdVZoQlBfMGlFTU4wUEQwMmRqeGlFdjBEaEJHZw?oc=5\" target=\"_blank\">Apple Brings Agentic Coding to Xcode 26.3 With Claude and Codex</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The Mac Observer</font>",
      "image_url": "",
      "published": "Wed, 04 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.007,
      "tier1_quick_score": 1.944
    },
    {
      "id": "b61c8e8f19963e91",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "The two patterns by which agents connect sandboxes",
      "url": "https://blog.langchain.com/the-two-patterns-by-which-agents-connect-sandboxes/",
      "summary": "<p><em>Thank you to Nuno Campos from Witan Labs, Tomas Beran and Mikayel Harutyunyan from E2B, Jonathan Wall from Runloop, and Ben Guo from Zo Computer for their review and comments.</em></p><p><strong>TL;DR:</strong></p><ul><li><strong>More and more agents need a workspace: a computer where they can run code, install packages, and access</strong></li></ul>",
      "image_url": "https://blog.langchain.com/content/images/2026/02/Screenshot-2026-02-09-at-9.30.02---PM.png",
      "published": "Tue, 10 Feb 2026 16:32:35 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.057,
      "tier1_quick_score": 1.944
    },
    {
      "id": "fe9da202a4704ea8",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Community Evals: Because we're done trusting black-box leaderboards over the community",
      "url": "https://huggingface.co/blog/community-evals",
      "summary": "",
      "image_url": "",
      "published": "Wed, 04 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 1.943
    },
    {
      "id": "91102c029a7fd92f",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "H Company's new Holo2 model takes the lead in UI Localization",
      "url": "https://huggingface.co/blog/Hcompany/introducing-holo2-235b-a22b",
      "summary": "",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 17:40:14 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 1.943
    },
    {
      "id": "f634b4223a6db742",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "The Future of the Global Open-Source AI Ecosystem: From DeepSeek to AI+",
      "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-3",
      "summary": "",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 15:03:19 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "90c07e0cfebb61a7",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Training Design for Text-to-Image Models: Lessons from Ablations",
      "url": "https://huggingface.co/blog/Photoroom/prx-part2",
      "summary": "",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 11:25:53 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "af39a0ebbc3718e8",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Xcode 26.3 unlocks the power of agentic coding - Apple",
      "url": "https://news.google.com/rss/articles/CBMilwFBVV95cUxNcjcxcmVrb2Z5Ykt2dlMtZVk0dlVtWHltOV82X0xYWTdodUcwUGxVT0hPc1RNamx5TVJUVmw4TnIxRUxxUzBrTGUxS1diSU9SemlhVWtaeGxreGNsWHozclFxODZvYk5ZdXZHV1BMb01KclZjelhzZG5sRFNXYUNxQXBVM2Y5T05obWM0UC1teHVlbmFuOHpv?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMilwFBVV95cUxNcjcxcmVrb2Z5Ykt2dlMtZVk0dlVtWHltOV82X0xYWTdodUcwUGxVT0hPc1RNamx5TVJUVmw4TnIxRUxxUzBrTGUxS1diSU9SemlhVWtaeGxreGNsWHozclFxODZvYk5ZdXZHV1BMb01KclZjelhzZG5sRFNXYUNxQXBVM2Y5T05obWM0UC1teHVlbmFuOHpv?oc=5\" target=\"_blank\">Xcode 26.3 unlocks the power of agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Apple</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "a6ed45c85ec718cf",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple adds agents from Anthropic and OpenAI to its coding tool - CNBC",
      "url": "https://news.google.com/rss/articles/CBMingFBVV95cUxPcDJfMXNqaFZmbjZhUlFBVjFoa0tLSGJLd1o5bGR6Y3d3eFJaY2dYWVo4XzVzRDJMTVMwZWMxd3hxREhwQXZiSW5IZURVcDhhdzFES1l4X0I2N0tIRy1pa2dvYzFLUUVTaVkzSlBZMUZpUW5rQmlieThsTGRKOVBobWtFdV9FeGVRblFrNGVVYkVrODdOTzNUYW5WREN5d9IBowFBVV95cUxOQzR6d25LdmVxQkZoRUVaT2tNdWtfNmZVVVBPR3pKRkNVWFYxY092R0wxUlZaUlF5ME15Q0IxejdfUEEzS0RQX25LQ21YUkhGSWwwaWx3TUpUSlVOTDRlSmxmNXBUM3FKMXhtTlJySjJqVXFRV2F2T0NTR0p4bENVWGZIcXZac3lkU1JjR2hxWjZxY1VIaXdOWk5TT0diX0xuV0hJ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMingFBVV95cUxPcDJfMXNqaFZmbjZhUlFBVjFoa0tLSGJLd1o5bGR6Y3d3eFJaY2dYWVo4XzVzRDJMTVMwZWMxd3hxREhwQXZiSW5IZURVcDhhdzFES1l4X0I2N0tIRy1pa2dvYzFLUUVTaVkzSlBZMUZpUW5rQmlieThsTGRKOVBobWtFdV9FeGVRblFrNGVVYkVrODdOTzNUYW5WREN5d9IBowFBVV95cUxOQzR6d25LdmVxQkZoRUVaT2tNdWtfNmZVVVBPR3pKRkNVWFYxY092R0wxUlZaUlF5ME15Q0IxejdfUEEzS0RQX25LQ21YUkhGSWwwaWx3TUpUSlVOTDRlSmxmNXBUM3FKMXhtTlJySjJqVXFRV2F2T0NTR0p4bENVWGZIcXZac3lkU1JjR2hxWjZxY1VIaXdOWk5TT0diX0xuV0hJ?oc=5\" target=\"_blank\">Apple adds agents from Anthropic and OpenAI to its coding tool</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">CNBC</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "b2932c8a2df7b781",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple announces agentic coding in Xcode with Claude Agent and Codex integration - 9to5Mac",
      "url": "https://news.google.com/rss/articles/CBMisAFBVV95cUxQVEY4dlNGeTZUbElkUmxKVDdRRDlQOU1fWnpFd3pmS2J3c3phMU9rdjBPVktjTjNOYzgtaHpnc01CMklzeGlkcnFveWRrWmtHMW9CekMyMjBFdXRSamUwcjJSUmlFVmkxVnpGai15aURYbEhEMUY2T2UtSi1yVkJMSGdZQ1BFMnJ4bzVtLWdNT2hOMjJyWjVtYzlrZ3Vjc09uSGlDTWJYbU1qVWhMU2F3ZA?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMisAFBVV95cUxQVEY4dlNGeTZUbElkUmxKVDdRRDlQOU1fWnpFd3pmS2J3c3phMU9rdjBPVktjTjNOYzgtaHpnc01CMklzeGlkcnFveWRrWmtHMW9CekMyMjBFdXRSamUwcjJSUmlFVmkxVnpGai15aURYbEhEMUY2T2UtSi1yVkJMSGdZQ1BFMnJ4bzVtLWdNT2hOMjJyWjVtYzlrZ3Vjc09uSGlDTWJYbU1qVWhMU2F3ZA?oc=5\" target=\"_blank\">Apple announces agentic coding in Xcode with Claude Agent and Codex integration</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">9to5Mac</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "2d3a7cc0b219f6b6",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Xcode 26.3 Lets AI Agents From Anthropic and OpenAI Build Apps Autonomously - MacRumors",
      "url": "https://news.google.com/rss/articles/CBMicEFVX3lxTE1aX3dPTnNSU3dzR0ZLM295RDJTLV9aczR0VTh6bWFMUFFEdEkwejQyTXVTeWhOOWJxTUNma0xXQTYzLTdxc0d1WVhIbW1SLXNtS01vOTJ2aWh0N3RsUmdQSlRaWDdnRmthOXF4U21rZXY?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMicEFVX3lxTE1aX3dPTnNSU3dzR0ZLM295RDJTLV9aczR0VTh6bWFMUFFEdEkwejQyTXVTeWhOOWJxTUNma0xXQTYzLTdxc0d1WVhIbW1SLXNtS01vOTJ2aWh0N3RsUmdQSlRaWDdnRmthOXF4U21rZXY?oc=5\" target=\"_blank\">Xcode 26.3 Lets AI Agents From Anthropic and OpenAI Build Apps Autonomously</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">MacRumors</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "eb0b6d0ea7b3493c",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple’s Xcode adds OpenAI and Anthropic’s coding agents - The Verge",
      "url": "https://news.google.com/rss/articles/CBMiiwFBVV95cUxOc2sxTGVTaXpJRjRfNk9XLVR6N2VXcDhrcC1uSUJadmJMR0Y5R05IT3h3ZWhMdWtnd3hkcUxHQ1FmVTBRU2dFeVlUa2JxUVBKWXZuTFprRHl3aUhiNzMySWFBWW16R0ZtRE8zOUhLS05aQTc2dEFCYlVfa182YnhlcTZOcHpFcHZnSHkw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiiwFBVV95cUxOc2sxTGVTaXpJRjRfNk9XLVR6N2VXcDhrcC1uSUJadmJMR0Y5R05IT3h3ZWhMdWtnd3hkcUxHQ1FmVTBRU2dFeVlUa2JxUVBKWXZuTFprRHl3aUhiNzMySWFBWW16R0ZtRE8zOUhLS05aQTc2dEFCYlVfa182YnhlcTZOcHpFcHZnSHkw?oc=5\" target=\"_blank\">Apple’s Xcode adds OpenAI and Anthropic’s coding agents</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The Verge</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "c0dda171108e9966",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Xcode moves into agentic coding with deeper OpenAI and Anthropic integrations - TechCrunch",
      "url": "https://news.google.com/rss/articles/CBMisgFBVV95cUxPc09Ka2NTWlBMZnJ4M2M2ODhSVnlaWC1KSXhnRFJDQml6UjBWUUlYQWtGcnpUTVNrUGZpS3VPQXFVNmlQMUwzaEVuWldOQlZYS2EzenRmeTVzMFhTMGU2bnlxQjN5cE02X1R1LUJhV2pTN3NxNWo4NWFLQkxEV3ppR1hTbkRFeHpDdUY5NGdNOWtvU3lPUkpSdHZlN2szSGVZZlFoek13N2R6SzhzTE16bjhn?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMisgFBVV95cUxPc09Ka2NTWlBMZnJ4M2M2ODhSVnlaWC1KSXhnRFJDQml6UjBWUUlYQWtGcnpUTVNrUGZpS3VPQXFVNmlQMUwzaEVuWldOQlZYS2EzenRmeTVzMFhTMGU2bnlxQjN5cE02X1R1LUJhV2pTN3NxNWo4NWFLQkxEV3ppR1hTbkRFeHpDdUY5NGdNOWtvU3lPUkpSdHZlN2szSGVZZlFoek13N2R6SzhzTE16bjhn?oc=5\" target=\"_blank\">Xcode moves into agentic coding with deeper OpenAI and Anthropic integrations</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">TechCrunch</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "056dacaf831757c5",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple bakes AI agents into Xcode with Anthropic and OpenAI - The Tech Buzz",
      "url": "https://news.google.com/rss/articles/CBMilgFBVV95cUxNMDlDRzA1Y2RTQkcxZnA4Y1R4dmx6RmNmU3pxTjhSNzFrM3JIXzlra3pnZzRHM2dCb2FMejZtQy04NG96NlZNSTcwaE01RktwWWU5ZmcwTGZyaHdvanRjdGMzdlZ1YVF1U0FSWUY1WUVYR0ZLZERQTzlsUjltR3VBejJwUXhHM3V3Y2NpMEV5aUdzNzliT2c?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMilgFBVV95cUxNMDlDRzA1Y2RTQkcxZnA4Y1R4dmx6RmNmU3pxTjhSNzFrM3JIXzlra3pnZzRHM2dCb2FMejZtQy04NG96NlZNSTcwaE01RktwWWU5ZmcwTGZyaHdvanRjdGMzdlZ1YVF1U0FSWUY1WUVYR0ZLZERQTzlsUjltR3VBejJwUXhHM3V3Y2NpMEV5aUdzNzliT2c?oc=5\" target=\"_blank\">Apple bakes AI agents into Xcode with Anthropic and OpenAI</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The Tech Buzz</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "65bc350ab31fa258",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Xcode 26.3 adds built-in support for agentic coding - AppleInsider",
      "url": "https://news.google.com/rss/articles/CBMipwFBVV95cUxQX2lnTi0yUHBaWWhJT0xIS2RTNE9jWVczdFlicHRXVlVHZHh3TmJOV3ZIWm9ER2xXbGJoSnN0NHVIS2tMQ085MThoRWZ5WldwLUVBaHk0RnkwTjNuT05TWFNGQjFxLUVFRnRaWXM5Tlp1MU81a2hXWmRUSk5Sbll3bU1Uc0swMTNjNGpERWVLa1JYYlVaRW5wbFBoTVZkaUo5WmRZbGdhVQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMipwFBVV95cUxQX2lnTi0yUHBaWWhJT0xIS2RTNE9jWVczdFlicHRXVlVHZHh3TmJOV3ZIWm9ER2xXbGJoSnN0NHVIS2tMQ085MThoRWZ5WldwLUVBaHk0RnkwTjNuT05TWFNGQjFxLUVFRnRaWXM5Tlp1MU81a2hXWmRUSk5Sbll3bU1Uc0swMTNjNGpERWVLa1JYYlVaRW5wbFBoTVZkaUo5WmRZbGdhVQ?oc=5\" target=\"_blank\">Xcode 26.3 adds built-in support for agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">AppleInsider</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "7ca011dc7476f464",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple’s Xcode 26.3 Release Candidate Adds Agentic Coding Tools for Developers - MacStories",
      "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxNUkZXa0p3Y0VWTnJ1dHBOR080Qll3NFIxWHhiRVZTZkV1a0NqUEFQY2E5NjZyOFA4am1uQWk4T21MMERHaUlRdTlockR5QkRsZVMyZkNGRkhweUNkc1F1MVhUd2RsOE1BQ2l5VC1VSEtmQ3l6VE9yRk1ObjdvalJqWmJGZUFrU3JkQXA1dmEybmdzeVJIbnNheUNkTTFmS0tuRFhoS2pXR05EbHBjVUE?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxNUkZXa0p3Y0VWTnJ1dHBOR080Qll3NFIxWHhiRVZTZkV1a0NqUEFQY2E5NjZyOFA4am1uQWk4T21MMERHaUlRdTlockR5QkRsZVMyZkNGRkhweUNkc1F1MVhUd2RsOE1BQ2l5VC1VSEtmQ3l6VE9yRk1ObjdvalJqWmJGZUFrU3JkQXA1dmEybmdzeVJIbnNheUNkTTFmS0tuRFhoS2pXR05EbHBjVUE?oc=5\" target=\"_blank\">Apple’s Xcode 26.3 Release Candidate Adds Agentic Coding Tools for Developers</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">MacStories</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "64364edb70353045",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple Unveils Agentic Coding In Xcode 26.3 Update - Evrim Ağacı",
      "url": "https://news.google.com/rss/articles/CBMiigFBVV95cUxQMzRtSzBQMzlkMFBxanNkTXRfSmpHY2VoMkJSc2xvbUlTV2piQnBxOFhwZTN1T28xREJONkdUUFdJRVhPQVkzU1ozbDJESkU4eXV3N2VFZC1PYWxGZEpja0p0ZWUxZU5kam9Wc0I2VTR2T05TVkxZNVFQbzdfWXBPZEhTUmxDZlAtSHc?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiigFBVV95cUxQMzRtSzBQMzlkMFBxanNkTXRfSmpHY2VoMkJSc2xvbUlTV2piQnBxOFhwZTN1T28xREJONkdUUFdJRVhPQVkzU1ozbDJESkU4eXV3N2VFZC1PYWxGZEpja0p0ZWUxZU5kam9Wc0I2VTR2T05TVkxZNVFQbzdfWXBPZEhTUmxDZlAtSHc?oc=5\" target=\"_blank\">Apple Unveils Agentic Coding In Xcode 26.3 Update</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Evrim Ağacı</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "56f26dc85d7888bc",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Xcode 26.3 finally brings agentic coding to Apple's developer tools - ZDNET",
      "url": "https://news.google.com/rss/articles/CBMifkFVX3lxTE9CZ2xRTmpsTGd1b25pa19zN09hdTF3RVJ1UWRzNWRraVlpU3dzajZoQ2hHQXRzSk13a3hvNTYxbGgyZmVCdURvLXBnQVlsX1hIWGNSVzBEdWxjSXFuOWpTNm1Ha24tVHllZ0xkZUxKU0tZRVRaY283Z29iLTNDZw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMifkFVX3lxTE9CZ2xRTmpsTGd1b25pa19zN09hdTF3RVJ1UWRzNWRraVlpU3dzajZoQ2hHQXRzSk13a3hvNTYxbGgyZmVCdURvLXBnQVlsX1hIWGNSVzBEdWxjSXFuOWpTNm1Ha24tVHllZ0xkZUxKU0tZRVRaY283Z29iLTNDZw?oc=5\" target=\"_blank\">Xcode 26.3 finally brings agentic coding to Apple's developer tools</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ZDNET</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "943063ccd3db85da",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "Apple is Bringing Agentic Coding Assistance to Xcode - Thurrott.com",
      "url": "https://news.google.com/rss/articles/CBMikgFBVV95cUxNdDJ2Nk5jOEMtYjN0WThNSkpBWXZGNkdDSFE4akZBbEIydlVEQlZXei1KN0F2YjhXbjVjYjk3bkgyUFBqVloxR2FjQUI1R2g3VHFMYVJyVVRCcURfdWgxcElTcVBUTXhmb0k4d0lrNUw0NkdsdDQ5dFIxLXFvNkxKUFQwejJKUzB4LWNqRFhvXy0wUQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMikgFBVV95cUxNdDJ2Nk5jOEMtYjN0WThNSkpBWXZGNkdDSFE4akZBbEIydlVEQlZXei1KN0F2YjhXbjVjYjk3bkgyUFBqVloxR2FjQUI1R2g3VHFMYVJyVVRCcURfdWgxcElTcVBUTXhmb0k4d0lrNUw0NkdsdDQ5dFIxLXFvNkxKUFQwejJKUzB4LWNqRFhvXy0wUQ?oc=5\" target=\"_blank\">Apple is Bringing Agentic Coding Assistance to Xcode</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Thurrott.com</font>",
      "image_url": "",
      "published": "Tue, 03 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.942
    },
    {
      "id": "23fb82559268d4cb",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "OpenAI launches a Codex desktop app for macOS to run multiple AI coding agents in parallel - VentureBeat",
      "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxPekNKSDB0WXFyYVJwamVZT3IwUmVUdHlnSVN3M3R6SmRJZFJMYzg3ZUN5bUhaX3hSMmJnTm1Xdk1NeDVzYU1FQTQydzR6WlpnWWkyazMzTy1NaEp4M0lBeTEwRzBvT25FbnQ2UEJudjJIYzNxdENJUmVpQVFVcXVlMGZvSlZSRDRiTkh3ekE2YzRodHAyYV92QWJTWDhCTFZmV1pUTWJITll6NFVhamc?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxPekNKSDB0WXFyYVJwamVZT3IwUmVUdHlnSVN3M3R6SmRJZFJMYzg3ZUN5bUhaX3hSMmJnTm1Xdk1NeDVzYU1FQTQydzR6WlpnWWkyazMzTy1NaEp4M0lBeTEwRzBvT25FbnQ2UEJudjJIYzNxdENJUmVpQVFVcXVlMGZvSlZSRDRiTkh3ekE2YzRodHAyYV92QWJTWDhCTFZmV1pUTWJITll6NFVhamc?oc=5\" target=\"_blank\">OpenAI launches a Codex desktop app for macOS to run multiple AI coding agents in parallel</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">VentureBeat</font>",
      "image_url": "",
      "published": "Mon, 02 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.004,
      "tier1_quick_score": 1.941
    },
    {
      "id": "99c7c40767c81aea",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "OpenAI's Codex MacOS App Brings Agentic Coding to Developers - The Tech Buzz",
      "url": "https://news.google.com/rss/articles/CBMimAFBVV95cUxOb3hCZkhNVkJmQmJBbWc1ZjBjUHY3alBXa3dSZm5EaUt1bGU0MTZTUU10VnhsZmZpbFF3bjRvNkJXazdiVTE4dEpINzAxMWpMVmlDTkl6a1RaTDNOOEJPWTI0TVVFRzRqZ245a1ZaS0NZRTlXcUVTQnVVN3RUbFhRV1F4MW9uM296SGRBbDVQX3d4Q3J6U194ZA?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMimAFBVV95cUxOb3hCZkhNVkJmQmJBbWc1ZjBjUHY3alBXa3dSZm5EaUt1bGU0MTZTUU10VnhsZmZpbFF3bjRvNkJXazdiVTE4dEpINzAxMWpMVmlDTkl6a1RaTDNOOEJPWTI0TVVFRzRqZ245a1ZaS0NZRTlXcUVTQnVVN3RUbFhRV1F4MW9uM296SGRBbDVQX3d4Q3J6U194ZA?oc=5\" target=\"_blank\">OpenAI's Codex MacOS App Brings Agentic Coding to Developers</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The Tech Buzz</font>",
      "image_url": "",
      "published": "Mon, 02 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.004,
      "tier1_quick_score": 1.941
    },
    {
      "id": "0e19d40526246e41",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "OpenAI launches new macOS app for agentic coding - TechCrunch",
      "url": "https://news.google.com/rss/articles/CBMiiwFBVV95cUxOTm1SUXVmSkhCcXB3VFQxcjIweHJ4M0ZObnJqSEhVOW9zbnJtdDV5cU1UUzZRM2J1Q2RMWmJYSklDSjZFWW1JT1RMeFF0YlhGSjFaX0FncjU1RDZXZkJfa0FwTjFoS2FmU2FHbk0wN0h1M2FWSlZpaFFMMmtzb2ZEYTJEbHc4T3Y1N01J?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiiwFBVV95cUxOTm1SUXVmSkhCcXB3VFQxcjIweHJ4M0ZObnJqSEhVOW9zbnJtdDV5cU1UUzZRM2J1Q2RMWmJYSklDSjZFWW1JT1RMeFF0YlhGSjFaX0FncjU1RDZXZkJfa0FwTjFoS2FmU2FHbk0wN0h1M2FWSlZpaFFMMmtzb2ZEYTJEbHc4T3Y1N01J?oc=5\" target=\"_blank\">OpenAI launches new macOS app for agentic coding</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">TechCrunch</font>",
      "image_url": "",
      "published": "Mon, 02 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.004,
      "tier1_quick_score": 1.941
    },
    {
      "id": "29d122eeb5bb0bc2",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Introducing Daggr: Chain apps programmatically, inspect visually",
      "url": "https://huggingface.co/blog/daggr",
      "summary": "",
      "image_url": "",
      "published": "Thu, 29 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.938
    },
    {
      "id": "9bd6b937a4204960",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "We Got Claude to Build CUDA Kernels and teach open models!",
      "url": "https://huggingface.co/blog/upskill",
      "summary": "",
      "image_url": "",
      "published": "Wed, 28 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.938
    },
    {
      "id": "ccef47d5befa3bba",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Architectural Choices in China's Open-Source AI Ecosystem: Building Beyond DeepSeek",
      "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2",
      "summary": "",
      "image_url": "",
      "published": "Tue, 27 Jan 2026 15:01:45 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.938
    },
    {
      "id": "bf182a437ebdeb73",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Alyah ⭐️: Toward Robust Evaluation of Emirati Dialect Capabilities in Arabic LLMs",
      "url": "https://huggingface.co/blog/tiiuae/emirati-benchmarks",
      "summary": "",
      "image_url": "",
      "published": "Tue, 27 Jan 2026 10:26:42 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "5c42dd2e17d30e84",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective",
      "url": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
      "summary": "",
      "image_url": "",
      "published": "Tue, 27 Jan 2026 01:53:15 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "7ce7b359b6cedf88",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality",
      "url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face",
      "summary": "",
      "image_url": "",
      "published": "Wed, 21 Jan 2026 06:25:31 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "34a5690dbeed7dab",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "One Year Since the “DeepSeek Moment”",
      "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment",
      "summary": "",
      "image_url": "",
      "published": "Tue, 20 Jan 2026 15:02:10 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "77a6435a098b2d8e",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Differential Transformer V2",
      "url": "https://huggingface.co/blog/microsoft/diff-attn-v2",
      "summary": "",
      "image_url": "",
      "published": "Tue, 20 Jan 2026 03:20:57 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "00a016ffc13d0f85",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Introducing Waypoint-1: Real-time interactive video diffusion from Overworld",
      "url": "https://huggingface.co/blog/waypoint-1",
      "summary": "",
      "image_url": "",
      "published": "Tue, 20 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "941eaaebf3b7a1cf",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Open Responses: What you need to know",
      "url": "https://huggingface.co/blog/open-responses",
      "summary": "",
      "image_url": "",
      "published": "Thu, 15 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "e62eb3fb8a1bc4ed",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI",
      "url": "https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning",
      "summary": "",
      "image_url": "",
      "published": "Mon, 05 Jan 2026 22:56:51 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "52e841e7968c5f43",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture",
      "url": "https://huggingface.co/blog/tiiuae/falcon-h1-arabic",
      "summary": "",
      "image_url": "",
      "published": "Mon, 05 Jan 2026 09:16:51 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "b74e82e624fdd0f0",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "NVIDIA brings agents to life with DGX Spark and Reachy Mini",
      "url": "https://huggingface.co/blog/nvidia-reachy-mini",
      "summary": "",
      "image_url": "",
      "published": "Mon, 05 Jan 2026 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "808f5b4c07f3795e",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
      "url": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
      "summary": "",
      "image_url": "",
      "published": "Tue, 23 Dec 2025 14:07:35 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "b8d1f7c0db043f25",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
      "url": "https://huggingface.co/blog/tokenizers",
      "summary": "",
      "image_url": "",
      "published": "Thu, 18 Dec 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "551dda269267aa50",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator",
      "url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe",
      "summary": "",
      "image_url": "",
      "published": "Wed, 17 Dec 2025 13:22:18 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "f7c6514fd6f76ace",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents",
      "url": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face",
      "summary": "",
      "image_url": "",
      "published": "Mon, 15 Dec 2025 16:01:04 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "6bee81a8f68bf4d2",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "New in llama.cpp: Model Management",
      "url": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp",
      "summary": "",
      "image_url": "",
      "published": "Thu, 11 Dec 2025 15:47:44 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "e8a902cfc34f94d4",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Codex is Open Sourcing AI models",
      "url": "https://huggingface.co/blog/hf-skills-training-codex",
      "summary": "",
      "image_url": "",
      "published": "Thu, 11 Dec 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "518b88720d0a5f21",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face",
      "url": "https://huggingface.co/blog/swift-huggingface",
      "summary": "",
      "image_url": "",
      "published": "Fri, 05 Dec 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "467e7644a71ef33c",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "DeepMath: A lightweight math reasoning Agent with smolagents",
      "url": "https://huggingface.co/blog/intel-deepmath",
      "summary": "",
      "image_url": "",
      "published": "Thu, 04 Dec 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "e96f257938927a3c",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "We Got Claude to Fine-Tune an Open Source LLM",
      "url": "https://huggingface.co/blog/hf-skills-training",
      "summary": "",
      "image_url": "",
      "published": "Thu, 04 Dec 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "ec318a42062d586a",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
      "url": "https://huggingface.co/blog/transformers-v5",
      "summary": "",
      "image_url": "",
      "published": "Mon, 01 Dec 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "daef2cca15dcfd10",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Diffusers welcomes FLUX-2",
      "url": "https://huggingface.co/blog/flux-2",
      "summary": "",
      "image_url": "",
      "published": "Tue, 25 Nov 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "973a146a0013d1b8",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Continuous batching from first principles",
      "url": "https://huggingface.co/blog/continuous_batching",
      "summary": "",
      "image_url": "",
      "published": "Tue, 25 Nov 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "eca5b1aa2a72ec13",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Building Deep Research: How we Achieved State of the Art",
      "url": "https://huggingface.co/blog/Tavily/tavily-deep-research",
      "summary": "",
      "image_url": "",
      "published": "Mon, 24 Nov 2025 17:40:14 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "c10899941a3e4bec",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "OVHcloud on Hugging Face Inference Providers 🔥",
      "url": "https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud",
      "summary": "",
      "image_url": "",
      "published": "Mon, 24 Nov 2025 16:08:47 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "dbae085ad582536d",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "20x Faster TRL Fine-tuning with RapidFire AI",
      "url": "https://huggingface.co/blog/rapidfireai",
      "summary": "",
      "image_url": "",
      "published": "Fri, 21 Nov 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "7573916c05eb2ab7",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks",
      "url": "https://huggingface.co/blog/open-asr-leaderboard",
      "summary": "",
      "image_url": "",
      "published": "Fri, 21 Nov 2025 00:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "bc73a9cb798da45a",
      "source": "search_agent_engineering_news",
      "source_weight": 1.1,
      "title": "OpenAI spills technical details about how its AI coding agent works - Ars Technica",
      "url": "https://news.google.com/rss/articles/CBMipgFBVV95cUxPb1pFNTdBQmNkN1dGUUZkMXBpdkZVQWNXeVZRNU05aDNHT05KaXNPcGs4TXpZN1JrNGJRTFA3cjhVLVRGY2RCb1oyOVBIY2lBeE9GLVI0UlBtXzE1UFdlVzFESVU3SVhnQnhubjhBeVBVQUg3VTd6eV9PUkFDZWk5RzlHY1hYQTBvUW5JeHNEN0xDUEx5SnAwRkdaOUdjWjdFS3kwdmpB?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMipgFBVV95cUxPb1pFNTdBQmNkN1dGUUZkMXBpdkZVQWNXeVZRNU05aDNHT05KaXNPcGs4TXpZN1JrNGJRTFA3cjhVLVRGY2RCb1oyOVBIY2lBeE9GLVI0UlBtXzE1UFdlVzFESVU3SVhnQnhubjhBeVBVQUg3VTd6eV9PUkFDZWk5RzlHY1hYQTBvUW5JeHNEN0xDUEx5SnAwRkdaOUdjWjdFS3kwdmpB?oc=5\" target=\"_blank\">OpenAI spills technical details about how its AI coding agent works</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Ars Technica</font>",
      "image_url": "",
      "published": "Mon, 26 Jan 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.937
    },
    {
      "id": "fb3d4eca8cbe98d4",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "LangSmith is Now Available in Google Cloud Marketplace",
      "url": "https://blog.langchain.com/langsmith-is-now-available-in-google-cloud-marketplace/",
      "summary": "<p>Today, we&apos;re thrilled to announce that LangSmith, the agent engineering platform from LangChain, is available in Google Cloud Marketplace. Google Cloud customers can now procure LangSmith through their existing Google Cloud accounts, enabling seamless billing, simplified procurement, and the ability to draw down on existing Google Cloud commitments.</p>",
      "image_url": "https://blog.langchain.com/content/images/2026/02/header2.png",
      "published": "Tue, 10 Feb 2026 02:47:44 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.047,
      "tier1_quick_score": 1.934
    },
    {
      "id": "087dca4bf820302b",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "January 2026: LangChain Newsletter",
      "url": "https://blog.langchain.com/january-2026-langchain-newsletter/",
      "summary": "Read about the latest product updates, events, and content from the LangChain team",
      "image_url": "https://blog.langchain.com/content/images/2026/01/Jan-Newsletter-2026---Ghost.png",
      "published": "Fri, 30 Jan 2026 02:27:28 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.888
    },
    {
      "id": "4c67073adf58de74",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "Context Management for Deep Agents",
      "url": "https://blog.langchain.com/context-management-for-deepagents/",
      "summary": "<p><em>By Chester Curme and Mason Daugherty</em></p><p>As the addressable task length of AI agents <a href=\"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/?ref=blog.langchain.com\">continues to grow</a>, effective context management becomes critical to prevent <a href=\"https://research.trychroma.com/context-rot?ref=blog.langchain.com\">context rot</a> and to manage LLMs&#x2019; finite memory constraints.</p><p>The <a href=\"https://docs.langchain.com/oss/python/deepagents/overview?ref=blog.langchain.com\">Deep Agents SDK</a> is LangChain&#x2019;s open source, batteries-included <a href=\"https://blog.langchain.com/agent-frameworks-runtimes-and-harnesses-oh-my/\">agent harness</a>. It provides an</p>",
      "image_url": "https://blog.langchain.com/content/images/2026/01/Screenshot-2026-01-23-at-3.03.57---PM-1.png",
      "published": "Wed, 28 Jan 2026 16:11:29 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.888
    },
    {
      "id": "1a66a510c9219569",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "Deploy agents instantly with Agent Builder templates",
      "url": "https://blog.langchain.com/introducing-agent-builder-template-library/",
      "summary": "Introducing the Agent Builder Template Library: a collection of ready-to-deploy agents for common tasks, equipped with the tools you already use.",
      "image_url": "https://blog.langchain.com/content/images/2026/01/LangSmith-Agent-Builder.png",
      "published": "Wed, 21 Jan 2026 17:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.887
    },
    {
      "id": "325506e58b7b9fc8",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "Building Multi-Agent Applications with Deep Agents",
      "url": "https://blog.langchain.com/building-multi-agent-applications-with-deep-agents/",
      "summary": "Breaking down complex tasks across specialized agents is one of the most effective approaches to building capable AI systems. In this post, we'll show you how to build multi-agent systems with Deep Agents.",
      "image_url": "https://blog.langchain.com/content/images/2026/01/Multi-Agent-Apps.png",
      "published": "Wed, 21 Jan 2026 16:30:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.887
    },
    {
      "id": "828000255cc69bd3",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "From Traces to Insights: Understanding Agent Behavior at Scale",
      "url": "https://blog.langchain.com/from-traces-to-insights-understanding-agent-behavior-at-scale/",
      "summary": "<blockquote>Visibility is the easiest piece. The hard part is analyzing and understanding what you&#x2019;re observing. I&#x2019;ve spoken to teams recording 100k+ traces every single day. What are they doing with those traces? Literally nothing. Because it&#x2019;s impossible to read and summarize 100,000 traces</blockquote>",
      "image_url": "https://blog.langchain.com/content/images/2026/01/Screenshot-2026-01-20-at-6.59.15---AM-1.png",
      "published": "Tue, 20 Jan 2026 17:16:56 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.887
    },
    {
      "id": "c7ae15e928121041",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "How Remote uses LangChain and LangGraph to onboard thousands of customers with AI",
      "url": "https://blog.langchain.com/customers-remote/",
      "summary": "<p><em>Guest post written by Jos&#xe9; Mussa (Staff Software Engineer @ Remote)</em></p><p><a href=\"https://remote.com/?ref=blog.langchain.com\"><u>Remote</u></a> is a fast-growing startup helping companies hire, manage, and pay employees globally from a single platform. Remote&#x2019;s customers operate across many countries and regulatory environments, and they trust Remote as the system of record for their</p>",
      "image_url": "https://blog.langchain.com/content/images/2026/01/Remote-case-study.png",
      "published": "Mon, 19 Jan 2026 16:00:07 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.887
    },
    {
      "id": "c69b1a1757242023",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "Choosing the Right Multi-Agent Architecture",
      "url": "https://blog.langchain.com/choosing-the-right-multi-agent-architecture/",
      "summary": "In this post, we’ll explore when multi-agent architectures become necessary, the four main patterns we’ve observed, and how LangChain empowers you to effectively build multi-agent systems.",
      "image_url": "https://blog.langchain.com/content/images/2026/01/Multi-agent-architecture.png",
      "published": "Wed, 14 Jan 2026 18:06:14 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.887
    },
    {
      "id": "b77c1304c6034dc1",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "Now GA: LangSmith Agent Builder",
      "url": "https://blog.langchain.com/langsmith-agent-builder-generally-available/",
      "summary": "LangSmith Agent Builder is now generally available—enabling anyone to build agents for complex daily tasks, without writing code.",
      "image_url": "https://blog.langchain.com/content/images/2026/01/LangSmith-Agent-Builder---GA-Blog.png",
      "published": "Tue, 13 Jan 2026 16:00:38 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.887
    },
    {
      "id": "8bed79565182bbc6",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.14",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.14",
      "summary": "<h1>Release Notes</h1>\n<h2>[2026-02-10]</h2>\n<h3>llama-index-callbacks-wandb [0.4.2]</h3>\n<ul>\n<li>Fix potential crashes and improve security defaults in core components (<a href=\"https://github.com/run-llama/llama_index/pull/20610\">#20610</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.14]</h3>\n<ul>\n<li>fix: catch pydantic ValidationError in VectorStoreQueryOutputParser (<a href=\"https://github.com/run-llama/llama_index/pull/20450\">#20450</a>)</li>\n<li>fix: distinguish empty string from None in MediaResource.hash (<a href=\"https://github.com/run-llama/llama_index/pull/20451\">#20451</a>)</li>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n<li>fix(core): fallback to bundled nltk cache if env var missing (<a href=\"https://github.com/run-llama/llama_index/pull/20528\">#20528</a>)</li>\n<li>feat(callbacks): add TokenBudgetHandler for cost governance (<a href=\"https://github.com/run-llama/llama_index/pull/20546\">#20546</a>)</li>\n<li>fix(core):handled a edge case in truncate_text function (<a href=\"https://github.com/run-llama/llama_index/pull/20551\">#20551</a>)</li>\n<li>fix(core):fix in types Thread passing None when target is None instead of copy_context().run (<a href=\"https://github.com/run-llama/llama_index/pull/20553\">#20553</a>)</li>\n<li>chore: bump llama-index lockfile, and minor test tweaks (<a href=\"https://github.com/run-llama/llama_index/pull/20556\">#20556</a>)</li>\n<li>Compatibility for workflows context changes (<a href=\"https://github.com/run-llama/llama_index/pull/20557\">#20557</a>)</li>\n<li>test(core): fix cache dir path test for Windows compatibility (<a href=\"https://github.com/run-llama/llama_index/pull/20566\">#20566</a>)</li>\n<li>fix(tests): enforce utf-8 encoding in json reader tests for windows compatibility (<a href=\"https://github.com/run-llama/llama_index/pull/20576\">#20576</a>)</li>\n<li>Fix BM25Retriever mapping in upgrade tool / 修复升级工具中的 BM25Retriever 映射 (<a href=\"https://github.com/run-llama/llama_index/pull/20582\">#20582</a>)</li>\n<li>fix(agent): handle empty LLM responses with retry logic and add test cases (<a href=\"https://github.com/run-llama/llama_index/pull/20596\">#20596</a>)</li>\n<li>fix: add show_progress parameter to run_transformations to prevent unexpected keyword argument error (<a href=\"https://github.com/run-llama/llama_index/pull/20608\">#20608</a>)</li>\n<li>Fix potential crashes and improve security defaults in core components (<a href=\"https://github.com/run-llama/llama_index/pull/20610\">#20610</a>)</li>\n<li>Add core 3.14 tests (<a href=\"https://github.com/run-llama/llama_index/pull/20619\">#20619</a>)</li>\n</ul>\n<h3>llama-index-embeddings-cohere [0.7.0]</h3>\n<ul>\n<li>fix(embeddings-cohere): add retry logic with tenacity (<a href=\"https://github.com/run-llama/llama_index/pull/20592\">#20592</a>)</li>\n</ul>\n<h3>llama-index-embeddings-google-genai [0.3.2]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n</ul>\n<h3>llama-index-embeddings-siliconflow [0.3.2]</h3>\n<ul>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n</ul>\n<h3>llama-index-embeddings-upstage [0.5.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-graph-stores-falkordb [0.4.2]</h3>\n<ul>\n<li>fix(falkordb): Fix MENTIONS relationship creation with triplet_source_id (<a href=\"https://github.com/run-llama/llama_index/pull/20650\">#20650</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.8]</h3>\n<ul>\n<li>chore: Update cacheable Anthropic models (<a href=\"https://github.com/run-llama/llama_index/pull/20581\">#20581</a>)</li>\n<li>chore: add support for opus 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20635\">#20635</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.8]</h3>\n<ul>\n<li>fix bedrock converse empty tool config issue (<a href=\"https://github.com/run-llama/llama_index/pull/20571\">#20571</a>)</li>\n<li>fix(llms-bedrock-converse): improve bedrock converse retry handling (<a href=\"https://github.com/run-llama/llama_index/pull/20590\">#20590</a>)</li>\n<li>feat(bedrock-converse): Add support for Claude Opus 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20637\">#20637</a>)</li>\n<li>Add support for adaptive thinking in Bedrock (<a href=\"https://github.com/run-llama/llama_index/pull/20659\">#20659</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-llms-cohere [0.7.1]</h3>\n<ul>\n<li>Feat: add custom base_url support to Cohere LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20534\">#20534</a>)</li>\n<li>fix(llms-cohere): handle additional error types in retry logic (<a href=\"https://github.com/run-llama/llama_index/pull/20591\">#20591</a>)</li>\n</ul>\n<h3>llama-index-llms-dashscope [0.5.2]</h3>\n<ul>\n<li>fix(dashscope): remove empty tool_calls from assistant messages (<a href=\"https://github.com/run-llama/llama_index/pull/20535\">#20535</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.8.7]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n<li>fix(decorator):adds logic to llm_retry_decorator for async methods. (<a href=\"https://github.com/run-llama/llama_index/pull/20588\">#20588</a>)</li>\n<li>Fix/google genai cleanup (<a href=\"https://github.com/run-llama/llama_index/pull/20607\">#20607</a>)</li>\n<li>fix(google-genai): skip model meta fetch when not needed (<a href=\"https://github.com/run-llama/llama_index/pull/20639\">#20639</a>)</li>\n</ul>\n<h3>llama-index-llms-huggingface-api [0.6.2]</h3>\n<ul>\n<li>Update sensible default provider for huggingface inference api (<a href=\"https://github.com/run-llama/llama_index/pull/20589\">#20589</a>)</li>\n</ul>\n<h3>llama-index-llms-langchain [0.7.1]</h3>\n<ul>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.18]</h3>\n<ul>\n<li>OpenAI response fix (<a href=\"https://github.com/run-llama/llama_index/pull/20538\">#20538</a>)</li>\n<li>feat: Add support for gpt-5.2-chat model (<a href=\"https://github.com/run-llama/llama_index/pull/20549\">#20549</a>)</li>\n<li>fix(openai): make image_url detail optional in message dict (<a href=\"https://github.com/run-llama/llama_index/pull/20609\">#20609</a>)</li>\n<li>Add new reasoning types (<a href=\"https://github.com/run-llama/llama_index/pull/20612\">#20612</a>)</li>\n<li>fix(openai): exclude unsupported params for all reasoning models (<a href=\"https://github.com/run-llama/llama_index/pull/20627\">#20627</a>)</li>\n</ul>\n<h3>llama-index-llms-openai-like [0.6.0]</h3>\n<ul>\n<li>make transformers an optional dependency for openai-like (<a href=\"https://github.com/run-llama/llama_index/pull/20580\">#20580</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.4]</h3>\n<ul>\n<li>make transformers an optional dependency for openai-like (<a href=\"https://github.com/run-llama/llama_index/pull/20580\">#20580</a>)</li>\n</ul>\n<h3>llama-index-llms-siliconflow [0.4.3]</h3>\n<ul>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.7.0]</h3>\n<ul>\n<li>add new upstage model(solar-pro3) (<a href=\"https://github.com/run-llama/llama_index/pull/20544\">#20544</a>)</li>\n</ul>\n<h3>llama-index-llms-vllm [0.6.2]</h3>\n<ul>\n<li>feat: add openai-like server mode for VllmServer (<a href=\"https://github.com/run-llama/llama_index/pull/20537\">#20537</a>)</li>\n</ul>\n<h3>llama-index-memory-bedrock-agentcore [0.1.2]</h3>\n<ul>\n<li>Add event and memory record deletion methods in bedrock-agentcorememory (<a href=\"https://github.com/run-llama/llama_index/pull/20428\">#20428</a>)</li>\n<li>chore(deps): update llama-index-core dependency lock to include 0.14.x (<a href=\"https://github.com/run-llama/llama_index/pull/20483\">#20483</a>)</li>\n</ul>\n<h3>llama-index-memory-mem0 [1.0.0]</h3>\n<ul>\n<li>fix: mem0 integration cleanup + refactor (<a href=\"https://github.com/run-llama/llama_index/pull/20532\">#20532</a>)</li>\n</ul>\n<h3>llama-index-node-parser-chonkie [0.1.1]</h3>\n<ul>\n<li>feat: add chonkie integration (<a href=\"https://github.com/run-llama/llama_index/pull/20622\">#20622</a>)</li>\n<li>update readme (<a href=\"https://github.com/run-llama/llama_index/pull/20656\">#20656</a>)</li>\n</ul>\n<h3>llama-index-node-parser-docling [0.4.2]</h3>\n<ul>\n<li>fix: catch pydantic ValidationError in VectorStoreQueryOutputParser (<a href=\"https://github.com/run-llama/llama_index/pull/20450\">#20450</a>)</li>\n</ul>\n<h3>llama-index-packs-code-hierarchy [0.6.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-gmail-openai-agent [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-multidoc-autoretrieval [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-panel-chatbot [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-recursive-retriever [0.7.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-packs-resume-screener [0.9.3]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-retry-engine-weaviate [0.5.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-streamlit-chatbot [0.5.2]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-sub-question-weaviate [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-timescale-vector-autoretrieval [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-cohere-rerank [0.6.0]</h3>\n<ul>\n<li>fix(cohere-rerank): add retry logic and tenacity dependency to cohere rerank (<a href=\"https://github.com/run-llama/llama_index/pull/20593\">#20593</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-nvidia-rerank [0.5.4]</h3>\n<ul>\n<li>fix(nvidia-rerank): fix initialization logic for on-prem auth (<a href=\"https://github.com/run-llama/llama_index/pull/20560\">#20560</a>)</li>\n<li>fix(nvidia-rerank): correct private attribute reference (<a href=\"https://github.com/run-llama/llama_index/pull/20570\">#20570</a>)</li>\n<li>fix(nvidia-rerank): Fix POST request url for locally hosted NIM rerankers (<a href=\"https://github.com/run-llama/llama_index/pull/20579\">#20579</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-tei-rerank [0.4.2]</h3>\n<ul>\n<li>fix(tei-rerank): use index field from API response for correct score … (<a href=\"https://github.com/run-llama/llama_index/pull/20599\">#20599</a>)</li>\n<li>test(tei-rerank): add test coverage for rerank retry coverage (<a href=\"https://github.com/run-llama/llama_index/pull/20600\">#20600</a>)</li>\n</ul>\n<h3>llama-index-protocols-ag-ui [0.2.4]</h3>\n<ul>\n<li>fix: avoid ValueError in ag-ui message conversion for multi-block ChatMessages (<a href=\"https://github.com/run-llama/llama_index/pull/20648\">#20648</a>)</li>\n</ul>\n<h3>llama-index-readers-datasets [0.1.0]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-readers-microsoft-sharepoint [0.7.0]</h3>\n<ul>\n<li>Sharepoint page support events (<a href=\"https://github.com/run-llama/llama_index/pull/20572\">#20572</a>)</li>\n</ul>\n<h3>llama-index-readers-obsidian [0.6.1]</h3>\n<ul>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.6]</h3>\n<ul>\n<li>feat: implement partial_params support to McpToolSpec (<a href=\"https://github.com/run-llama/llama_index/pull/20554\">#20554</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp-discovery [0.1.0]</h3>\n<ul>\n<li>Add llama-index-tools-mcp-discovery integration (<a href=\"https://github.com/run-llama/llama_index/pull/20502\">#20502</a>)</li>\n</ul>\n<h3>llama-index-tools-moss [0.1.0]</h3>\n<ul>\n<li>feat(tools): add Moss search engine integration (<a href=\"https://github.com/run-llama/llama_index/pull/20615\">#20615</a>)</li>\n</ul>\n<h3>llama-index-tools-seltz [0.1.0]</h3>\n<ul>\n<li>feat(tools): add Seltz web knowledge tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20626\">#20626</a>)</li>\n</ul>\n<h3>llama-index-tools-typecast [0.1.0]</h3>\n<ul>\n<li>Migrate Typecast tool to V2 API for voices endpoints (<a href=\"https://github.com/run-llama/llama_index/pull/20548\">#20548</a>)</li>\n</ul>\n<h3>llama-index-tools-wolfram-alpha [0.5.0]</h3>\n<ul>\n<li>feat(wolfram-alpha): switch to LLM API with bearer auth (<a href=\"https://github.com/run-llama/llama_index/pull/20586\">#20586</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-clickhouse [0.6.2]</h3>\n<ul>\n<li>fix(clickhouse): Add drop_existing_table parameter to prevent data loss (<a href=\"https://github.com/run-llama/llama_index/pull/20651\">#20651</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.6]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.1]</h3>\n<ul>\n<li>Update MongoDB vector store tests to use newer model (<a href=\"https://github.com/run-llama/llama_index/pull/20515\">#20515</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-oceanbase [0.4.0]</h3>\n<ul>\n<li>feat(oceanbase): add sparse/fulltext/hybrid search (<a href=\"https://github.com/run-llama/llama_index/pull/20524\">#20524</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [1.0.0]</h3>\n<ul>\n<li>Changed OpenSearch engine default from deprecated <code>nmslib</code> to <code>faiss</code> (<a href=\"https://github.com/run-llama/llama_index/pull/20507\">#20507</a>)</li>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.3]</h3>\n<ul>\n<li>fix(postgres): disable bitmap scan for vector queries (<a href=\"https://github.com/run-llama/llama_index/pull/20514\">#20514</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-yugabytedb [0.5.4]</h3>\n<ul>\n<li>Add YugabyteDB as a Vector Store (<a href=\"https://github.com/run-llama/llama_index/pull/20559\">#20559</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-voice-agents-gemini-live [0.2.2]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-10T23:08:46Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.063,
      "tier1_quick_score": 1.85
    },
    {
      "id": "165fb96a655d42e0",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-sdk==0.3.5",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.5",
      "summary": "<p>Changes since sdk==0.3.4</p>\n<ul>\n<li>chore: server runtime type (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6774\">#6774</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-10T16:56:28Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.058,
      "tier1_quick_score": 1.845
    },
    {
      "id": "9a2a7007f26a8fa7",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph==1.0.8",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/1.0.8",
      "summary": "<p>Changes since 1.0.7</p>\n<ul>\n<li>release(langgraph): 1.0.8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6757\">#6757</a>)</li>\n<li>chore: shallow copy futures (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6755\">#6755</a>)</li>\n<li>fix: pydantic messages double streaming (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6753\">#6753</a>)</li>\n<li>chore(deps-dev): bump ruff from 0.14.7 to 0.14.11 in /libs/sdk-py (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6673\">#6673</a>)</li>\n<li>chore: Omit lock when using connection pool (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6734\">#6734</a>)</li>\n<li>docs: enhance <code>Runtime</code> and <code>ToolRuntime</code> class descriptions for clarity (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6689\">#6689</a>)</li>\n<li>docs: add clarity to use of <code>thread_id</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6515\">#6515</a>)</li>\n<li>docs: add docstrings to <code>add_node</code> overloads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6514\">#6514</a>)</li>\n<li>docs: update notebook links and add archival notices for examples (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6720\">#6720</a>)</li>\n<li>release(cli): 0.4.12 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6716\">#6716</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-06T12:31:26Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.014,
      "tier1_quick_score": 1.801
    },
    {
      "id": "a08265c21cb9aab8",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-sdk==0.3.4",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.4",
      "summary": "<p>Changes since sdk==0.3.3</p>\n<ul>\n<li>chore: release python sdk (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6754\">#6754</a>)</li>\n<li>feat(sdk-py): add update method for crons client (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6742\">#6742</a>)</li>\n<li>feat(sdk-py): add support for enabling/disabling crons (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6740\">#6740</a>)</li>\n<li>chore(deps-dev): bump ruff from 0.14.7 to 0.14.11 in /libs/sdk-py (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6673\">#6673</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>docs: clarify cron job schedule interpretation in UTC (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6692\">#6692</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-06T00:44:26Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.012,
      "tier1_quick_score": 1.799
    },
    {
      "id": "71a7cd16a493bef9",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-checkpoint-postgres==3.0.4",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/checkpointpostgres%3D%3D3.0.4",
      "summary": "<p>Changes since checkpointpostgres==3.0.3</p>\n<ul>\n<li>chore: Omit lock when using connection pool (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6734\">#6734</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-01-31T00:46:04Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.002,
      "tier1_quick_score": 1.789
    },
    {
      "id": "4472a8eadbb3b8d2",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.13",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.13",
      "summary": "<h1>Release Notes</h1>\n<h2>[2026-01-21]</h2>\n<h3>llama-index-core [0.14.13]</h3>\n<ul>\n<li>feat: add early_stopping_method parameter to agent workflows (<a href=\"https://github.com/run-llama/llama_index/pull/20389\">#20389</a>)</li>\n<li>feat: Add token-based code splitting support to CodeSplitter (<a href=\"https://github.com/run-llama/llama_index/pull/20438\">#20438</a>)</li>\n<li>Add RayIngestionPipeline integration for distributed data ingestion (<a href=\"https://github.com/run-llama/llama_index/pull/20443\">#20443</a>)</li>\n<li>Added the multi-modal version of the Condensed Conversation &amp; Context… (<a href=\"https://github.com/run-llama/llama_index/pull/20446\">#20446</a>)</li>\n<li>Replace ChatMemoryBuffer with Memory (<a href=\"https://github.com/run-llama/llama_index/pull/20458\">#20458</a>)</li>\n<li>fix(bug):Raise value error on when input is empty list in mean_agg instead of returning float (<a href=\"https://github.com/run-llama/llama_index/pull/20466\">#20466</a>)</li>\n<li>fix: The classmethod of ReActChatFormatter should use cls instead of the class name (<a href=\"https://github.com/run-llama/llama_index/pull/20475\">#20475</a>)</li>\n<li>feat: add configurable empty response message to synthesizers (<a href=\"https://github.com/run-llama/llama_index/pull/20503\">#20503</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.3]</h3>\n<ul>\n<li>Enable use of ARNs for Bedrock Embedding Models (<a href=\"https://github.com/run-llama/llama_index/pull/20435\">#20435</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.6]</h3>\n<ul>\n<li>Improved Ollama batch embedding (<a href=\"https://github.com/run-llama/llama_index/pull/20447\">#20447</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.3]</h3>\n<ul>\n<li>Adding voyage-4 models (<a href=\"https://github.com/run-llama/llama_index/pull/20497\">#20497</a>)</li>\n</ul>\n<h3>llama-index-ingestion-ray [0.1.0]</h3>\n<ul>\n<li>Add RayIngestionPipeline integration for distributed data ingestion (<a href=\"https://github.com/run-llama/llama_index/pull/20443\">#20443</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.6]</h3>\n<ul>\n<li>feat: enhance structured predict methods for anthropic (<a href=\"https://github.com/run-llama/llama_index/pull/20440\">#20440</a>)</li>\n<li>fix: preserve input_tokens in Anthropic stream_chat responses (<a href=\"https://github.com/run-llama/llama_index/pull/20512\">#20512</a>)</li>\n</ul>\n<h3>llama-index-llms-apertis [0.1.0]</h3>\n<ul>\n<li>Add Apertis LLM integration with example notebook (<a href=\"https://github.com/run-llama/llama_index/pull/20436\">#20436</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.4]</h3>\n<ul>\n<li>chore(bedrock-converse): Remove extraneous thinking_delta kwarg from ChatMessage (<a href=\"https://github.com/run-llama/llama_index/pull/20455\">#20455</a>)</li>\n</ul>\n<h3>llama-index-llms-gemini [0.6.2]</h3>\n<ul>\n<li>chore: deprecate llama-index-llms-gemini (<a href=\"https://github.com/run-llama/llama_index/pull/20511\">#20511</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.13]</h3>\n<ul>\n<li>Sanitize OpenAI structured output JSON schema name for generic Pydantic models (<a href=\"https://github.com/run-llama/llama_index/pull/20452\">#20452</a>)</li>\n<li>chore: vbump openai (<a href=\"https://github.com/run-llama/llama_index/pull/20482\">#20482</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.3]</h3>\n<ul>\n<li>Feature/openrouter provider routing support (<a href=\"https://github.com/run-llama/llama_index/pull/20431\">#20431</a>)</li>\n</ul>\n<h3>llama-index-packs-recursive-retriever [0.7.1]</h3>\n<ul>\n<li>security: remove exposed OpenAI API keys from notebook outputs (<a href=\"https://github.com/run-llama/llama_index/pull/20474\">#20474</a>)</li>\n</ul>\n<h3>llama-index-packs-sentence-window-retriever [0.5.1]</h3>\n<ul>\n<li>security: remove exposed OpenAI API keys from notebook outputs (<a href=\"https://github.com/run-llama/llama_index/pull/20474\">#20474</a>)</li>\n</ul>\n<h3>llama-index-readers-datasets [0.1.0]</h3>\n<ul>\n<li>Add HuggingFace datasets reader integration (<a href=\"https://github.com/run-llama/llama_index/pull/20468\">#20468</a>)</li>\n</ul>\n<h3>llama-index-readers-patentsview [1.0.0]</h3>\n<ul>\n<li>Patentsview reader api changes (<a href=\"https://github.com/run-llama/llama_index/pull/20481\">#20481</a>)</li>\n</ul>\n<h3>llama-index-retrievers-you [1.0.0]</h3>\n<ul>\n<li>Revamp YouRetriever integration (<a href=\"https://github.com/run-llama/llama_index/pull/20493\">#20493</a>)</li>\n</ul>\n<h3>llama-index-tools-parallel-web-systems [0.1.0]</h3>\n<ul>\n<li>feat: added Parallel Web System tools (<a href=\"https://github.com/run-llama/llama_index/pull/20442\">#20442</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-alibabacloud-mysql [0.1.0]</h3>\n<ul>\n<li>Feature/alibaba mysql vector integration (<a href=\"https://github.com/run-llama/llama_index/pull/20396\">#20396</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.6]</h3>\n<ul>\n<li>Feat milvus partition names (<a href=\"https://github.com/run-llama/llama_index/pull/20445\">#20445</a>)</li>\n<li>improve(llama-index-vector-stores-milvus): Changed the partition parameter to <code>milvus_partition_name</code> in add/delete. (<a href=\"https://github.com/run-llama/llama_index/pull/20460\">#20460</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.1]</h3>\n<ul>\n<li>INTPYTHON-863 Fix mongodb async integration (<a href=\"https://github.com/run-llama/llama_index/pull/20444\">#20444</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-neo4jvector [0.5.2]</h3>\n<ul>\n<li>Handle missing metadata for neo4j vector store (<a href=\"https://github.com/run-llama/llama_index/pull/20491\">#20491</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [0.6.3]</h3>\n<ul>\n<li>fix (opensearch): add close and aclose methods to vector client (<a href=\"https://github.com/run-llama/llama_index/pull/20463\">#20463</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.9.1]</h3>\n<ul>\n<li>Qdrant search params (<a href=\"https://github.com/run-llama/llama_index/pull/20476\">#20476</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.4]</h3>\n<ul>\n<li>feat(vertexaivectorsearch): add hybrid search support (<a href=\"https://github.com/run-llama/llama_index/pull/20487\">#20487</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-volcenginemysql [0.2.0]</h3>\n<ul>\n<li>feat: Volcengine MySQL vector store integration (<a href=\"https://github.com/run-llama/llama_index/pull/20404\">#20404</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-01-21T20:44:52Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "1b059a88e8ce2e31",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.12",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.12",
      "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-30]</h2>\n<h3>llama-index-callbacks-agentops [0.4.1]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.12]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n<li>Improve <code>MockFunctionCallingLLM</code> (<a href=\"https://github.com/run-llama/llama_index/pull/20356\">#20356</a>)</li>\n<li>fix(openai): sanitize generic Pydantic model schema names (<a href=\"https://github.com/run-llama/llama_index/pull/20371\">#20371</a>)</li>\n<li>Element node parser (<a href=\"https://github.com/run-llama/llama_index/pull/20399\">#20399</a>)</li>\n<li>improve llama dev logging (<a href=\"https://github.com/run-llama/llama_index/pull/20411\">#20411</a>)</li>\n<li>test(node_parser): add unit tests for Java CodeSplitter (<a href=\"https://github.com/run-llama/llama_index/pull/20423\">#20423</a>)</li>\n<li>fix: crash in log_vector_store_query_result when result.ids is None (<a href=\"https://github.com/run-llama/llama_index/pull/20427\">#20427</a>)</li>\n</ul>\n<h3>llama-index-embeddings-litellm [0.4.1]</h3>\n<ul>\n<li>Add docstring to LiteLLM embedding class (<a href=\"https://github.com/run-llama/llama_index/pull/20336\">#20336</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.5]</h3>\n<ul>\n<li>feat(llama-index-embeddings-ollama): Add keep_alive parameter (<a href=\"https://github.com/run-llama/llama_index/pull/20395\">#20395</a>)</li>\n<li>docs: improve Ollama embeddings README with comprehensive documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20414\">#20414</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.2]</h3>\n<ul>\n<li>Voyage multimodal 35 (<a href=\"https://github.com/run-llama/llama_index/pull/20398\">#20398</a>)</li>\n</ul>\n<h3>llama-index-graph-stores-nebula [0.5.1]</h3>\n<ul>\n<li>feat(nebula): add MENTIONS edge to property graph store (<a href=\"https://github.com/run-llama/llama_index/pull/20401\">#20401</a>)</li>\n</ul>\n<h3>llama-index-llms-aibadgr [0.1.0]</h3>\n<ul>\n<li>feat(llama-index-llms-aibadgr): Add AI Badgr OpenAI‑compatible LLM integration (<a href=\"https://github.com/run-llama/llama_index/pull/20365\">#20365</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.4]</h3>\n<ul>\n<li>add back haiku-3 support (<a href=\"https://github.com/run-llama/llama_index/pull/20408\">#20408</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.3]</h3>\n<ul>\n<li>fix: bedrock converse thinking block issue (<a href=\"https://github.com/run-llama/llama_index/pull/20355\">#20355</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.8.3]</h3>\n<ul>\n<li>Switch use_file_api to Flexible file_mode; Improve File Upload Handling &amp; Bump google-genai to v1.52.0 (<a href=\"https://github.com/run-llama/llama_index/pull/20347\">#20347</a>)</li>\n<li>Fix missing role from Google-GenAI (<a href=\"https://github.com/run-llama/llama_index/pull/20357\">#20357</a>)</li>\n<li>Add signature index fix (<a href=\"https://github.com/run-llama/llama_index/pull/20362\">#20362</a>)</li>\n<li>Add positional thought signature for thoughts (<a href=\"https://github.com/run-llama/llama_index/pull/20418\">#20418</a>)</li>\n</ul>\n<h3>llama-index-llms-ollama [0.9.1]</h3>\n<ul>\n<li>feature: pydantic no longer complains if you pass 'low', 'medium', 'h… (<a href=\"https://github.com/run-llama/llama_index/pull/20394\">#20394</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.12]</h3>\n<ul>\n<li>fix: Handle tools=None in OpenAIResponses._get_model_kwargs (<a href=\"https://github.com/run-llama/llama_index/pull/20358\">#20358</a>)</li>\n<li>feat: add support for gpt-5.2 and 5.2 pro (<a href=\"https://github.com/run-llama/llama_index/pull/20361\">#20361</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.6.1]</h3>\n<ul>\n<li>fix(confluence): support Python 3.14 (<a href=\"https://github.com/run-llama/llama_index/pull/20370\">#20370</a>)</li>\n</ul>\n<h3>llama-index-readers-file [0.5.6]</h3>\n<ul>\n<li>Loosen constraint on <code>pandas</code> version (<a href=\"https://github.com/run-llama/llama_index/pull/20387\">#20387</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>chore(deps): bump urllib3 from 2.5.0 to 2.6.0 in /llama-index-integrations/readers/llama-index-readers-service-now in the pip group across 1 directory (<a href=\"https://github.com/run-llama/llama_index/pull/20341\">#20341</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.5]</h3>\n<ul>\n<li>fix: pass timeout parameters to transport clients in BasicMCPClient (<a href=\"https://github.com/run-llama/llama_index/pull/20340\">#20340</a>)</li>\n<li>feature: Permit to pass a custom httpx.AsyncClient when creating a BasicMcpClient (<a href=\"https://github.com/run-llama/llama_index/pull/20368\">#20368</a>)</li>\n</ul>\n<h3>llama-index-tools-typecast [0.1.0]</h3>\n<ul>\n<li>feat: add Typecast tool integration with text to speech features (<a href=\"https://github.com/run-llama/llama_index/pull/20343\">#20343</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azurepostgresql [0.2.0]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-chroma [0.5.5]</h3>\n<ul>\n<li>Fix chroma nested metadata filters (<a href=\"https://github.com/run-llama/llama_index/pull/20424\">#20424</a>)</li>\n<li>fix(chroma): support multimodal results (<a href=\"https://github.com/run-llama/llama_index/pull/20426\">#20426</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-couchbase [0.6.0]</h3>\n<ul>\n<li>Update FTS &amp; GSI reference docs for Couchbase vector-store (<a href=\"https://github.com/run-llama/llama_index/pull/20346\">#20346</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-faiss [0.5.2]</h3>\n<ul>\n<li>fix(faiss): pass numpy array instead of int to add_with_ids (<a href=\"https://github.com/run-llama/llama_index/pull/20384\">#20384</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-lancedb [0.4.4]</h3>\n<ul>\n<li>Feat/async tool spec support (<a href=\"https://github.com/run-llama/llama_index/pull/20338\">#20338</a>)</li>\n<li>fix(vector_stores/lancedb): add missing '&lt;' filter operator (<a href=\"https://github.com/run-llama/llama_index/pull/20364\">#20364</a>)</li>\n<li>fix(lancedb): fix metadata filtering logic and list value SQL generation (<a href=\"https://github.com/run-llama/llama_index/pull/20374\">#20374</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.0]</h3>\n<ul>\n<li>Update mongo vector store to initialize without list permissions (<a href=\"https://github.com/run-llama/llama_index/pull/20354\">#20354</a>)</li>\n<li>add mongodb delete index (<a href=\"https://github.com/run-llama/llama_index/pull/20429\">#20429</a>)</li>\n<li>async mongodb atlas support (<a href=\"https://github.com/run-llama/llama_index/pull/20430\">#20430</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-redis [0.6.2]</h3>\n<ul>\n<li>Redis metadata filter fix (<a href=\"https://github.com/run-llama/llama_index/pull/20359\">#20359</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.3]</h3>\n<ul>\n<li>feat(vertex-vector-search): Add Google Vertex AI Vector Search v2.0 support (<a href=\"https://github.com/run-llama/llama_index/pull/20351\">#20351</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2025-12-30T01:07:03Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "12a10411c82007d3",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.10",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.10",
      "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-04]</h2>\n<h3>llama-index-core [0.14.10]</h3>\n<ul>\n<li>feat: add mock function calling llm (<a href=\"https://github.com/run-llama/llama_index/pull/20331\">#20331</a>)</li>\n</ul>\n<h3>llama-index-llms-qianfan [0.4.1]</h3>\n<ul>\n<li>test: fix typo 'reponse' to 'response' in variable names (<a href=\"https://github.com/run-llama/llama_index/pull/20329\">#20329</a>)</li>\n</ul>\n<h3>llama-index-tools-airweave [0.1.0]</h3>\n<ul>\n<li>feat: add Airweave tool integration with advanced search features (<a href=\"https://github.com/run-llama/llama_index/pull/20111\">#20111</a>)</li>\n</ul>\n<h3>llama-index-utils-qianfan [0.4.1]</h3>\n<ul>\n<li>test: fix typo 'reponse' to 'response' in variable names (<a href=\"https://github.com/run-llama/llama_index/pull/20329\">#20329</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2025-12-04T19:46:03Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "76e7d59ba15bdef3",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.9",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.9",
      "summary": "<h1>Release Notes</h1>\n<h2>[2025-12-02]</h2>\n<h3>llama-index-agent-azure [0.2.1]</h3>\n<ul>\n<li>fix: Pin azure-ai-projects version to prevent breaking changes (<a href=\"https://github.com/run-llama/llama_index/pull/20255\">#20255</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.9]</h3>\n<ul>\n<li>MultiModalVectorStoreIndex now returns a multi-modal ContextChatEngine. (<a href=\"https://github.com/run-llama/llama_index/pull/20265\">#20265</a>)</li>\n<li>Ingestion to vector store now ensures that _node-content is readable (<a href=\"https://github.com/run-llama/llama_index/pull/20266\">#20266</a>)</li>\n<li>fix: ensure context is copied with async utils run_async (<a href=\"https://github.com/run-llama/llama_index/pull/20286\">#20286</a>)</li>\n<li>fix(memory): ensure first message in queue is always a user message after flush (<a href=\"https://github.com/run-llama/llama_index/pull/20310\">#20310</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.2]</h3>\n<ul>\n<li>feat(embeddings-bedrock): Add support for Amazon Bedrock Application Inference Profiles (<a href=\"https://github.com/run-llama/llama_index/pull/20267\">#20267</a>)</li>\n<li>fix:(embeddings-bedrock) correct extraction of provider from model_name (<a href=\"https://github.com/run-llama/llama_index/pull/20295\">#20295</a>)</li>\n<li>Bump version of bedrock-embedding (<a href=\"https://github.com/run-llama/llama_index/pull/20304\">#20304</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.1]</h3>\n<ul>\n<li>VoyageAI correction and documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20251\">#20251</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.3]</h3>\n<ul>\n<li>feat: add anthropic opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20306\">#20306</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.2]</h3>\n<ul>\n<li>fix(bedrock-converse): Only use guardrail_stream_processing_mode in streaming functions (<a href=\"https://github.com/run-llama/llama_index/pull/20289\">#20289</a>)</li>\n<li>feat: add anthropic opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20306\">#20306</a>)</li>\n<li>feat(bedrock-converse): Additional support for Claude Opus 4.5 (<a href=\"https://github.com/run-llama/llama_index/pull/20317\">#20317</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.7.4]</h3>\n<ul>\n<li>Fix gemini-3 support and gemini function call support (<a href=\"https://github.com/run-llama/llama_index/pull/20315\">#20315</a>)</li>\n</ul>\n<h3>llama-index-llms-helicone [0.1.1]</h3>\n<ul>\n<li>update helicone docs + examples (<a href=\"https://github.com/run-llama/llama_index/pull/20208\">#20208</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.10]</h3>\n<ul>\n<li>Smallest Nit (<a href=\"https://github.com/run-llama/llama_index/pull/20252\">#20252</a>)</li>\n<li>Feat: Add gpt-5.1-chat model support (<a href=\"https://github.com/run-llama/llama_index/pull/20311\">#20311</a>)</li>\n</ul>\n<h3>llama-index-llms-ovhcloud [0.1.0]</h3>\n<ul>\n<li>Add OVHcloud AI Endpoints provider (<a href=\"https://github.com/run-llama/llama_index/pull/20288\">#20288</a>)</li>\n</ul>\n<h3>llama-index-llms-siliconflow [0.4.2]</h3>\n<ul>\n<li>[Bugfix] None check on content in delta in siliconflow LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20327\">#20327</a>)</li>\n</ul>\n<h3>llama-index-node-parser-docling [0.4.2]</h3>\n<ul>\n<li>Relax docling Python constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20322\">#20322</a>)</li>\n</ul>\n<h3>llama-index-packs-resume-screener [0.9.3]</h3>\n<ul>\n<li>feat: Update pypdf to latest version (<a href=\"https://github.com/run-llama/llama_index/pull/20285\">#20285</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-voyageai-rerank [0.4.1]</h3>\n<ul>\n<li>VoyageAI correction and documentation (<a href=\"https://github.com/run-llama/llama_index/pull/20251\">#20251</a>)</li>\n</ul>\n<h3>llama-index-protocols-ag-ui [0.2.3]</h3>\n<ul>\n<li>fix: correct order of ag-ui events to avoid event conflicts (<a href=\"https://github.com/run-llama/llama_index/pull/20296\">#20296</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.6.0]</h3>\n<ul>\n<li>Refactor Confluence integration: Update license to MIT, remove requirements.txt, and implement HtmlTextParser for HTML to Markdown conversion. Update dependencies and tests accordingly. (<a href=\"https://github.com/run-llama/llama_index/pull/20262\">#20262</a>)</li>\n</ul>\n<h3>llama-index-readers-docling [0.4.2]</h3>\n<ul>\n<li>Relax docling Python constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20322\">#20322</a>)</li>\n</ul>\n<h3>llama-index-readers-file [0.5.5]</h3>\n<ul>\n<li>feat: Update pypdf to latest version (<a href=\"https://github.com/run-llama/llama_index/pull/20285\">#20285</a>)</li>\n</ul>\n<h3>llama-index-readers-reddit [0.4.1]</h3>\n<ul>\n<li>Fix typo in README.md for Reddit integration (<a href=\"https://github.com/run-llama/llama_index/pull/20283\">#20283</a>)</li>\n</ul>\n<h3>llama-index-storage-chat-store-postgres [0.3.2]</h3>\n<ul>\n<li>[FIX] Postgres ChatStore automatically prefix table name with \"data_\" (<a href=\"https://github.com/run-llama/llama_index/pull/20241\">#20241</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azureaisearch [0.4.4]</h3>\n<ul>\n<li><code>vector-azureaisearch</code>: check if user agent already in policy before add it to azure client (<a href=\"https://github.com/run-llama/llama_index/pull/20243\">#20243</a>)</li>\n<li>fix(azureaisearch): Add close/aclose methods to fix unclosed client session warnings (<a href=\"https://github.com/run-llama/llama_index/pull/20309\">#20309</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.4]</h3>\n<ul>\n<li>Fix/consistency level param for milvus (<a href=\"https://github.com/run-llama/llama_index/pull/20268\">#20268</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.2]</h3>\n<ul>\n<li>Fix postgresql dispose (<a href=\"https://github.com/run-llama/llama_index/pull/20312\">#20312</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.9.0]</h3>\n<ul>\n<li>fix: Update qdrant-client version constraints (<a href=\"https://github.com/run-llama/llama_index/pull/20280\">#20280</a>)</li>\n<li>Feat: update Qdrant client to 1.16.0 (<a href=\"https://github.com/run-llama/llama_index/pull/20287\">#20287</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-vertexaivectorsearch [0.3.2]</h3>\n<ul>\n<li>fix: update blob path in batch_update_index (<a href=\"https://github.com/run-llama/llama_index/pull/20281\">#20281</a>)</li>\n</ul>\n<h3>llama-index-voice-agents-openai [0.2.2]</h3>\n<ul>\n<li>Smallest Nit (<a href=\"https://github.com/run-llama/llama_index/pull/20252\">#20252</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2025-12-02T21:31:18Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "1b2cfee52de7ca3d",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.8",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.8",
      "summary": "<h1>Release Notes</h1>\n<h2>[2025-11-10]</h2>\n<h3>llama-index-core [0.14.8]</h3>\n<ul>\n<li>Fix ReActOutputParser getting stuck when \"Answer:\" contains \"Action:\" (<a href=\"https://github.com/run-llama/llama_index/pull/20098\">#20098</a>)</li>\n<li>Add buffer to image, audio, video and document blocks (<a href=\"https://github.com/run-llama/llama_index/pull/20153\">#20153</a>)</li>\n<li>fix(agent): Handle multi-block ChatMessage in ReActAgent (<a href=\"https://github.com/run-llama/llama_index/pull/20196\">#20196</a>)</li>\n<li>Fix/20209 (<a href=\"https://github.com/run-llama/llama_index/pull/20214\">#20214</a>)</li>\n<li>Preserve Exception in ToolOutput (<a href=\"https://github.com/run-llama/llama_index/pull/20231\">#20231</a>)</li>\n<li>fix weird pydantic warning (<a href=\"https://github.com/run-llama/llama_index/pull/20235\">#20235</a>)</li>\n</ul>\n<h3>llama-index-embeddings-nvidia [0.4.2]</h3>\n<ul>\n<li>docs: Edit pass and update example model (<a href=\"https://github.com/run-llama/llama_index/pull/20198\">#20198</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ollama [0.8.4]</h3>\n<ul>\n<li>Added a test case (no code) to check the embedding through an actual connection to a Ollama server (after checking that the ollama server exists) (<a href=\"https://github.com/run-llama/llama_index/pull/20230\">#20230</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.2]</h3>\n<ul>\n<li>feat(llms/anthropic): Add support for RawMessageDeltaEvent in streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20206\">#20206</a>)</li>\n<li>chore: remove unsupported models (<a href=\"https://github.com/run-llama/llama_index/pull/20211\">#20211</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.11.1]</h3>\n<ul>\n<li>feat: integrate bedrock converse with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20099\">#20099</a>)</li>\n<li>feat: Update model name extraction to include 'jp' region prefix and … (<a href=\"https://github.com/run-llama/llama_index/pull/20233\">#20233</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.7.3]</h3>\n<ul>\n<li>feat: google genai integration with tool block (<a href=\"https://github.com/run-llama/llama_index/pull/20096\">#20096</a>)</li>\n<li>fix: non-streaming gemini tool calling (<a href=\"https://github.com/run-llama/llama_index/pull/20207\">#20207</a>)</li>\n<li>Add token usage information in GoogleGenAI chat additional_kwargs (<a href=\"https://github.com/run-llama/llama_index/pull/20219\">#20219</a>)</li>\n<li>bug fix google genai stream_complete (<a href=\"https://github.com/run-llama/llama_index/pull/20220\">#20220</a>)</li>\n</ul>\n<h3>llama-index-llms-nvidia [0.4.4]</h3>\n<ul>\n<li>docs: Edit pass and code example updates (<a href=\"https://github.com/run-llama/llama_index/pull/20200\">#20200</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.8]</h3>\n<ul>\n<li>FixV2: Correct DocumentBlock type for OpenAI from 'input_file' to 'file' (<a href=\"https://github.com/run-llama/llama_index/pull/20203\">#20203</a>)</li>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.6.5]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-packs-streamlit-chatbot [0.5.2]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-packs-voyage-query-engine [0.5.2]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-nvidia-rerank [0.5.1]</h3>\n<ul>\n<li>docs: Edit pass (<a href=\"https://github.com/run-llama/llama_index/pull/20199\">#20199</a>)</li>\n</ul>\n<h3>llama-index-readers-web [0.5.6]</h3>\n<ul>\n<li>feat: Add ScrapyWebReader Integration (<a href=\"https://github.com/run-llama/llama_index/pull/20212\">#20212</a>)</li>\n<li>Update Scrapy dependency to 2.13.3 (<a href=\"https://github.com/run-llama/llama_index/pull/20228\">#20228</a>)</li>\n</ul>\n<h3>llama-index-readers-whisper [0.3.0]</h3>\n<ul>\n<li>OpenAI v2 sdk support (<a href=\"https://github.com/run-llama/llama_index/pull/20234\">#20234</a>)</li>\n</ul>\n<h3>llama-index-storage-kvstore-postgres [0.4.3]</h3>\n<ul>\n<li>fix: Ensure schema creation only occurs if it doesn't already exist (<a href=\"https://github.com/run-llama/llama_index/pull/20225\">#20225</a>)</li>\n</ul>\n<h3>llama-index-tools-brightdata [0.2.1]</h3>\n<ul>\n<li>docs: add api key claim instructions (<a href=\"https://github.com/run-llama/llama_index/pull/20204\">#20204</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.3]</h3>\n<ul>\n<li>Added test case for issue 19211. No code change (<a href=\"https://github.com/run-llama/llama_index/pull/20201\">#20201</a>)</li>\n</ul>\n<h3>llama-index-utils-oracleai [0.3.1]</h3>\n<ul>\n<li>Update llama-index-core dependency to 0.12.45 (<a href=\"https://github.com/run-llama/llama_index/pull/20227\">#20227</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-lancedb [0.4.2]</h3>\n<ul>\n<li>fix: FTS index recreation bug on every LanceDB query (<a href=\"https://github.com/run-llama/llama_index/pull/20213\">#20213</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2025-11-10T22:18:42Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "673d75a2997ef7ce",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.7",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.7",
      "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-30]</h2>\n<h3>llama-index-core [0.14.7]</h3>\n<ul>\n<li>Feat/serpex tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20141\">#20141</a>)</li>\n<li>Fix outdated error message about setting LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20157\">#20157</a>)</li>\n<li>Fixing some recently failing tests (<a href=\"https://github.com/run-llama/llama_index/pull/20165\">#20165</a>)</li>\n<li>Fix: update lock to latest workflow and fix issues (<a href=\"https://github.com/run-llama/llama_index/pull/20173\">#20173</a>)</li>\n<li>fix: ensure full docstring is used in FunctionTool (<a href=\"https://github.com/run-llama/llama_index/pull/20175\">#20175</a>)</li>\n<li>fix api docs build (<a href=\"https://github.com/run-llama/llama_index/pull/20180\">#20180</a>)</li>\n</ul>\n<h3>llama-index-embeddings-voyageai [0.5.0]</h3>\n<ul>\n<li>Updating the VoyageAI integration (<a href=\"https://github.com/run-llama/llama_index/pull/20073\">#20073</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.0]</h3>\n<ul>\n<li>feat: integrate anthropic with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20100\">#20100</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.10.7]</h3>\n<ul>\n<li>feat: Add support for Bedrock Guardrails streamProcessingMode (<a href=\"https://github.com/run-llama/llama_index/pull/20150\">#20150</a>)</li>\n<li>bedrock structured output optional force (<a href=\"https://github.com/run-llama/llama_index/pull/20158\">#20158</a>)</li>\n</ul>\n<h3>llama-index-llms-fireworks [0.4.5]</h3>\n<ul>\n<li>Update FireworksAI models (<a href=\"https://github.com/run-llama/llama_index/pull/20169\">#20169</a>)</li>\n</ul>\n<h3>llama-index-llms-mistralai [0.9.0]</h3>\n<ul>\n<li>feat: mistralai integration with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20103\">#20103</a>)</li>\n</ul>\n<h3>llama-index-llms-ollama [0.9.0]</h3>\n<ul>\n<li>feat: integrate ollama with tool call block (<a href=\"https://github.com/run-llama/llama_index/pull/20097\">#20097</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.6]</h3>\n<ul>\n<li>Allow setting temp of gpt-5-chat (<a href=\"https://github.com/run-llama/llama_index/pull/20156\">#20156</a>)</li>\n</ul>\n<h3>llama-index-readers-confluence [0.5.0]</h3>\n<ul>\n<li>feat(confluence): make SVG processing optional to fix pycairo install… (<a href=\"https://github.com/run-llama/llama_index/pull/20115\">#20115</a>)</li>\n</ul>\n<h3>llama-index-readers-github [0.9.0]</h3>\n<ul>\n<li>Add GitHub App authentication support (<a href=\"https://github.com/run-llama/llama_index/pull/20106\">#20106</a>)</li>\n</ul>\n<h3>llama-index-retrievers-bedrock [0.5.1]</h3>\n<ul>\n<li>Fixing some recently failing tests (<a href=\"https://github.com/run-llama/llama_index/pull/20165\">#20165</a>)</li>\n</ul>\n<h3>llama-index-tools-serpex [0.1.0]</h3>\n<ul>\n<li>Feat/serpex tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20141\">#20141</a>)</li>\n<li>add missing toml info (<a href=\"https://github.com/run-llama/llama_index/pull/20186\">#20186</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-couchbase [0.6.0]</h3>\n<ul>\n<li>Add Hyperscale and Composite Vector Indexes support for Couchbase vector-store (<a href=\"https://github.com/run-llama/llama_index/pull/20170\">#20170</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2025-10-30T23:58:43Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "a4240f028d473960",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.6",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.6",
      "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-26]</h2>\n<h3>llama-index-core [0.14.6]</h3>\n<ul>\n<li>Add allow_parallel_tool_calls for non-streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20117\">#20117</a>)</li>\n<li>Fix invalid use of field-specific metadata (<a href=\"https://github.com/run-llama/llama_index/pull/20122\">#20122</a>)</li>\n<li>update doc for SemanticSplitterNodeParser (<a href=\"https://github.com/run-llama/llama_index/pull/20125\">#20125</a>)</li>\n<li>fix rare cases when sentence splits are larger than chunk size (<a href=\"https://github.com/run-llama/llama_index/pull/20147\">#20147</a>)</li>\n</ul>\n<h3>llama-index-embeddings-bedrock [0.7.0]</h3>\n<ul>\n<li>Fix BedrockEmbedding to support Cohere v4 response format (<a href=\"https://github.com/run-llama/llama_index/pull/20094\">#20094</a>)</li>\n</ul>\n<h3>llama-index-embeddings-isaacus [0.1.0]</h3>\n<ul>\n<li>feat: Isaacus embeddings integration (<a href=\"https://github.com/run-llama/llama_index/pull/20124\">#20124</a>)</li>\n</ul>\n<h3>llama-index-embeddings-oci-genai [0.4.2]</h3>\n<ul>\n<li>Update OCI GenAI cohere models (<a href=\"https://github.com/run-llama/llama_index/pull/20146\">#20146</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.9.7]</h3>\n<ul>\n<li>Fix double token stream in anthropic llm (<a href=\"https://github.com/run-llama/llama_index/pull/20108\">#20108</a>)</li>\n<li>Ensure anthropic content delta only has user facing response (<a href=\"https://github.com/run-llama/llama_index/pull/20113\">#20113</a>)</li>\n</ul>\n<h3>llama-index-llms-baseten [0.1.7]</h3>\n<ul>\n<li>add GLM (<a href=\"https://github.com/run-llama/llama_index/pull/20121\">#20121</a>)</li>\n</ul>\n<h3>llama-index-llms-helicone [0.1.0]</h3>\n<ul>\n<li>integrate helicone to llama-index (<a href=\"https://github.com/run-llama/llama_index/pull/20131\">#20131</a>)</li>\n</ul>\n<h3>llama-index-llms-oci-genai [0.6.4]</h3>\n<ul>\n<li>Update OCI GenAI cohere models (<a href=\"https://github.com/run-llama/llama_index/pull/20146\">#20146</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.5]</h3>\n<ul>\n<li>chore: openai vbump (<a href=\"https://github.com/run-llama/llama_index/pull/20095\">#20095</a>)</li>\n</ul>\n<h3>llama-index-readers-imdb-review [0.4.2]</h3>\n<ul>\n<li>chore: Update selenium dependency in imdb-review reader (<a href=\"https://github.com/run-llama/llama_index/pull/20105\">#20105</a>)</li>\n</ul>\n<h3>llama-index-retrievers-bedrock [0.5.0]</h3>\n<ul>\n<li>feat(bedrock): add async support for AmazonKnowledgeBasesRetriever (<a href=\"https://github.com/run-llama/llama_index/pull/20114\">#20114</a>)</li>\n</ul>\n<h3>llama-index-retrievers-superlinked [0.1.3]</h3>\n<ul>\n<li>Update README.md (<a href=\"https://github.com/run-llama/llama_index/pull/19829\">#19829</a>)</li>\n</ul>\n<h3>llama-index-storage-kvstore-postgres [0.4.2]</h3>\n<ul>\n<li>fix: Replace raw SQL string interpolation with proper SQLAlchemy parameterized APIs in PostgresKVStore (<a href=\"https://github.com/run-llama/llama_index/pull/20104\">#20104</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.3]</h3>\n<ul>\n<li>Fix BasicMCPClient resource signatures (<a href=\"https://github.com/run-llama/llama_index/pull/20118\">#20118</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.1]</h3>\n<ul>\n<li>Add GIN index support for text array metadata in PostgreSQL vector store (<a href=\"https://github.com/run-llama/llama_index/pull/20130\">#20130</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2025-10-26T03:01:31Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "6b02016212dad53a",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.5",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.5",
      "summary": "<h1>Release Notes</h1>\n<h2>[2025-10-15]</h2>\n<h3>llama-index-core [0.14.5]</h3>\n<ul>\n<li>Remove debug print (<a href=\"https://github.com/run-llama/llama_index/pull/20000\">#20000</a>)</li>\n<li>safely initialize RefDocInfo in Docstore (<a href=\"https://github.com/run-llama/llama_index/pull/20031\">#20031</a>)</li>\n<li>Add progress bar for multiprocess loading (<a href=\"https://github.com/run-llama/llama_index/pull/20048\">#20048</a>)</li>\n<li>Fix duplicate node positions when identical text appears multiple times in document (<a href=\"https://github.com/run-llama/llama_index/pull/20050\">#20050</a>)</li>\n<li>chore: tool call block - part 1 (<a href=\"https://github.com/run-llama/llama_index/pull/20074\">#20074</a>)</li>\n</ul>\n<h3>llama-index-instrumentation [0.4.2]</h3>\n<ul>\n<li>update instrumentation package metadata (<a href=\"https://github.com/run-llama/llama_index/pull/20079\">#20079</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.9.5]</h3>\n<ul>\n<li>✨ feat(anthropic): add prompt caching model validation utilities (<a href=\"https://github.com/run-llama/llama_index/pull/20069\">#20069</a>)</li>\n<li>fix streaming thinking/tool calling with anthropic (<a href=\"https://github.com/run-llama/llama_index/pull/20077\">#20077</a>)</li>\n<li>Add haiku 4.5 support (<a href=\"https://github.com/run-llama/llama_index/pull/20092\">#20092</a>)</li>\n</ul>\n<h3>llama-index-llms-baseten [0.1.6]</h3>\n<ul>\n<li>Baseten provider Kimi K2 0711, Llama 4 Maverick and Llama 4 Scout Model APIs deprecation (<a href=\"https://github.com/run-llama/llama_index/pull/20042\">#20042</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.10.5]</h3>\n<ul>\n<li>feat: List Claude Sonnet 4.5 as a reasoning model (<a href=\"https://github.com/run-llama/llama_index/pull/20022\">#20022</a>)</li>\n<li>feat: Support global cross-region inference profile prefix (<a href=\"https://github.com/run-llama/llama_index/pull/20064\">#20064</a>)</li>\n<li>Update utils.py for opus 4.1 (<a href=\"https://github.com/run-llama/llama_index/pull/20076\">#20076</a>)</li>\n<li>4.1 opus bedrockconverse missing in funcitoncalling models (<a href=\"https://github.com/run-llama/llama_index/pull/20084\">#20084</a>)</li>\n<li>Add haiku 4.5 support (<a href=\"https://github.com/run-llama/llama_index/pull/20092\">#20092</a>)</li>\n</ul>\n<h3>llama-index-llms-fireworks [0.4.4]</h3>\n<ul>\n<li>Add Support for Custom Models in Fireworks LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20023\">#20023</a>)</li>\n<li>fix(llms/fireworks): Cannot use Fireworks Deepseek V3.1-20006 issue (<a href=\"https://github.com/run-llama/llama_index/pull/20028\">#20028</a>)</li>\n</ul>\n<h3>llama-index-llms-oci-genai [0.6.3]</h3>\n<ul>\n<li>Add support for xAI models in OCI GenAI (<a href=\"https://github.com/run-llama/llama_index/pull/20089\">#20089</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.4]</h3>\n<ul>\n<li>Gpt 5 pro addition (<a href=\"https://github.com/run-llama/llama_index/pull/20029\">#20029</a>)</li>\n<li>fix collecting final response with openai responses streaming (<a href=\"https://github.com/run-llama/llama_index/pull/20037\">#20037</a>)</li>\n<li>Add support for GPT-5 models in utils.py (JSON_SCHEMA_MODELS) (<a href=\"https://github.com/run-llama/llama_index/pull/20045\">#20045</a>)</li>\n<li>chore: tool call block - part 1 (<a href=\"https://github.com/run-llama/llama_index/pull/20074\">#20074</a>)</li>\n</ul>\n<h3>llama-index-llms-sglang [0.1.0]</h3>\n<ul>\n<li>Added Sglang llm integration (<a href=\"https://github.com/run-llama/llama_index/pull/20020\">#20020</a>)</li>\n</ul>\n<h3>llama-index-readers-gitlab [0.5.1]</h3>\n<ul>\n<li>feat(gitlab): add pagination params for repository tree and issues (<a href=\"https://github.com/run-llama/llama_index/pull/20052\">#20052</a>)</li>\n</ul>\n<h3>llama-index-readers-json [0.4.2]</h3>\n<ul>\n<li>vbump the JSON reader (<a href=\"https://github.com/run-llama/llama_index/pull/20039\">#20039</a>)</li>\n</ul>\n<h3>llama-index-readers-web [0.5.5]</h3>\n<ul>\n<li>fix: ScrapflyReader Pydantic validation error (<a href=\"https://github.com/run-llama/llama_index/pull/19999\">#19999</a>)</li>\n</ul>\n<h3>llama-index-storage-chat-store-dynamodb [0.4.2]</h3>\n<ul>\n<li>bump dynamodb chat store deps (<a href=\"https://github.com/run-llama/llama_index/pull/20078\">#20078</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.2]</h3>\n<ul>\n<li>🐛 fix(tools/mcp): Fix dict type handling and reference resolution in … (<a href=\"https://github.com/run-llama/llama_index/pull/20082\">#20082</a>)</li>\n</ul>\n<h3>llama-index-tools-signnow [0.1.0]</h3>\n<ul>\n<li>feat(signnow): SignNow mcp tools integration (<a href=\"https://github.com/run-llama/llama_index/pull/20057\">#20057</a>)</li>\n</ul>\n<h3>llama-index-tools-tavily-research [0.4.2]</h3>\n<ul>\n<li>feat: Add Tavily extract function for URL content extraction (<a href=\"https://github.com/run-llama/llama_index/pull/20038\">#20038</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-azurepostgresql [0.2.0]</h3>\n<ul>\n<li>Add hybrid search to Azure PostgreSQL integration (<a href=\"https://github.com/run-llama/llama_index/pull/20027\">#20027</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.3]</h3>\n<ul>\n<li>fix: Milvus get_field_kwargs() (<a href=\"https://github.com/run-llama/llama_index/pull/20086\">#20086</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [0.6.2]</h3>\n<ul>\n<li>fix(opensearch): Correct version check for efficient filtering (<a href=\"https://github.com/run-llama/llama_index/pull/20067\">#20067</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-qdrant [0.8.6]</h3>\n<ul>\n<li>fix(qdrant): Allow async-only initialization with hybrid search (<a href=\"https://github.com/run-llama/llama_index/pull/20005\">#20005</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2025-10-15T19:10:57Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "b2b8ee2468790fbd",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-cli==0.4.12",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/cli%3D%3D0.4.12",
      "summary": "<p>Changes since cli==0.4.11</p>\n<ul>\n<li>release(cli): 0.4.12 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6716\">#6716</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-01-23T13:34:28Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "a509bbcb9e389f17",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph==1.0.7",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/1.0.7",
      "summary": "<p>Changes since 1.0.6</p>\n<ul>\n<li>release: langgraph and prebuilt 1.0.7 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6712\">#6712</a>)</li>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-01-22T16:57:59Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "55029e01959ae6f4",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-prebuilt==1.0.7",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/prebuilt%3D%3D1.0.7",
      "summary": "<p>Changes since prebuilt==1.0.6</p>\n<ul>\n<li>release: langgraph and prebuilt 1.0.7 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6712\">#6712</a>)</li>\n<li>feat: support dynamic tool calling via <code>tool</code> override in <code>wrap_model_call</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6711\">#6711</a>)</li>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-01-22T16:45:37Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "fc8438b25945c3c3",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-checkpoint-sqlite==3.0.3",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/checkpointsqlite%3D%3D3.0.3",
      "summary": "<p>Changes since checkpointsqlite==3.0.2</p>\n<ul>\n<li>fix: aiosqlite's breaking change (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6699\">#6699</a>)</li>\n<li>chore(deps): upgrade dependencies with <code>uv lock --upgrade</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6671\">#6671</a>)</li>\n<li>chore: update twitter URLs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6683\">#6683</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-01-19T00:38:58Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.787
    },
    {
      "id": "a948cd93e3704656",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "What is LLM Inference? - IBM",
      "url": "https://news.google.com/rss/articles/CBMiWkFVX3lxTE5iMnhObndxOVBoZFg2b1lncVhGeVVpcXZoRlJmckQyLTVycU56bE55ZzdjRXZsNXg0T2FmdFhlTlhrTVpYa3F0em51NEFNdS1RR0dEZ3kzem41QQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiWkFVX3lxTE5iMnhObndxOVBoZFg2b1lncVhGeVVpcXZoRlJmckQyLTVycU56bE55ZzdjRXZsNXg0T2FmdFhlTlhrTVpYa3F0em51NEFNdS1RR0dEZ3kzem41QQ?oc=5\" target=\"_blank\">What is LLM Inference?</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">IBM</font>",
      "image_url": "",
      "published": "Sat, 07 Feb 2026 07:11:40 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.018,
      "tier1_quick_score": 1.659
    },
    {
      "id": "2dba7ccd26b39904",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Gemini 3 Deep Think: Advancing science, research and engineering",
      "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/",
      "summary": "Gemini 3 Deep Think logo",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3_deep-think_keyword_hea.max-600x600.format-webp.webp",
      "published": "Thu, 12 Feb 2026 16:13:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.111,
      "tier1_quick_score": 1.648
    },
    {
      "id": "c4198283e2e31429",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "India Fuels Its AI Mission With NVIDIA",
      "url": "https://blogs.nvidia.com/blog/india-ai-mission-infrastructure-models/",
      "summary": "From AI infrastructure leaders to frontier model developers, India is teaming with NVIDIA to drive AI transformation across the nation.",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/ai-infra-corp-blog-india-impact-summit-2026-1280x680-1.png",
      "published": "Wed, 18 Feb 2026 00:30:49 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.659,
      "tier1_quick_score": 1.646
    },
    {
      "id": "68b4572608ae1a3f",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "India’s Global Systems Integrators Build Next Wave of Enterprise Agents With NVIDIA AI, Transforming Back Office and Customer Support",
      "url": "https://blogs.nvidia.com/blog/india-enterprise-ai-agents/",
      "summary": "Agentic AI is reshaping India’s tech industry, delivering leaps in services worldwide. Tapping into NVIDIA AI Enterprise software and NVIDIA Nemotron models, India’s technology leaders are accelerating productivity and efficiency across industries — from call centers to telecommunications and healthcare. Infosys, Persistent, Tech Mahindra and Wipro are leading the way for business transformation, improving back-office\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/india-enterprise-ai-agents/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/ai-impact-summit-in26-visual-gsi-4871450-concept1-r2-scaled.png",
      "published": "Wed, 18 Feb 2026 00:30:41 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.659,
      "tier1_quick_score": 1.646
    },
    {
      "id": "4d60c718eedace1c",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "NVIDIA and Global Industrial Software Leaders Partner With India’s Largest Manufacturers to Drive AI Boom",
      "url": "https://blogs.nvidia.com/blog/india-global-industrial-software-leaders-manufacturers-ai/",
      "summary": "India is entering a new age of industrialization, as AI transforms how the world designs, builds and runs physical products and systems. The country is investing $134 billion dollars in new manufacturing capacity across construction, automotive, renewable energy and robotics, creating both a massive challenge and opportunity to build software-defined factories from day one. At\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/india-global-industrial-software-leaders-manufacturers-ai/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/NVIDIA-India-AI-Impact-Summit-Indias-Manufacturers-Drive-AI-Boom-scaled.jpg",
      "published": "Wed, 18 Feb 2026 00:30:32 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.659,
      "tier1_quick_score": 1.646
    },
    {
      "id": "58d4f300cca7a10b",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Optimize LLM response costs and latency with effective caching - Amazon Web Services (AWS)",
      "url": "https://news.google.com/rss/articles/CBMiowFBVV95cUxPbEZ6a21GcHRsclFfd19XSkdZT3VVV0M4MjR0VmFDX2o2WVYwT3Z6MXFMcENfa3BQTEU0RXVYeW5sZkp6Q1czaEdxODVmaWVVVmFaLTEwOXZ3WHlSOVlVSjJFS2dJejl4OVBwUi1rS0VCSDRaYVBmTUdJWVJFWDZLa2EydEdrTEVfbXFHLXNXclVUWDRVU0pLbUZGUzFNRVA2ME44?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiowFBVV95cUxPbEZ6a21GcHRsclFfd19XSkdZT3VVV0M4MjR0VmFDX2o2WVYwT3Z6MXFMcENfa3BQTEU0RXVYeW5sZkp6Q1czaEdxODVmaWVVVmFaLTEwOXZ3WHlSOVlVSjJFS2dJejl4OVBwUi1rS0VCSDRaYVBmTUdJWVJFWDZLa2EydEdrTEVfbXFHLXNXclVUWDRVU0pLbUZGUzFNRVA2ME44?oc=5\" target=\"_blank\">Optimize LLM response costs and latency with effective caching</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services (AWS)</font>",
      "image_url": "",
      "published": "Mon, 02 Feb 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.004,
      "tier1_quick_score": 1.645
    },
    {
      "id": "556d2e5e8f51fcec",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Reducing Cold Start Latency for LLM Inference with NVIDIA Run:ai Model Streamer | NVIDIA Technical Blog - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMiswFBVV95cUxPUVFITlZ2N1JGelJlQ2htQ2RneXdRRnZUM0RTYnFlNFBCR2JtZjE4ZjFpdTRxWm5sYV9oME4yZ0RxLXVHQTQ0eUQyLTlxVW9lbTBpaG5GQlVkaUJUckVaVzVwOWNtUDR5Snp0UDVzbnNMQmltemp3VG4tcG93b1V2eTFObFpHdm81NzlGSm91T0htYkpGRk1OUy1yRkxLcEJBUjJFS0ZqTUlRUWx2N2NwVzRVUQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiswFBVV95cUxPUVFITlZ2N1JGelJlQ2htQ2RneXdRRnZUM0RTYnFlNFBCR2JtZjE4ZjFpdTRxWm5sYV9oME4yZ0RxLXVHQTQ0eUQyLTlxVW9lbTBpaG5GQlVkaUJUckVaVzVwOWNtUDR5Snp0UDVzbnNMQmltemp3VG4tcG93b1V2eTFObFpHdm81NzlGSm91T0htYkpGRk1OUy1yRkxLcEJBUjJFS0ZqTUlRUWx2N2NwVzRVUQ?oc=5\" target=\"_blank\">Reducing Cold Start Latency for LLM Inference with NVIDIA Run:ai Model Streamer | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Tue, 16 Sep 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "14123a23d6cb92aa",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Implementing High-Performance LLM Serving on GKE: An Inference Gateway Walkthrough - Google Cloud",
      "url": "https://news.google.com/rss/articles/CBMi2wFBVV95cUxQSHNLa053dm01Nk9xUjE3NGJzM0NOekNBbF9xa1U5T25RVm1hVG93NFd1TnJPN0lpTmg0SE1yenAyQjZxS0FqZERldDBBeXNhaW1nQjVDR0hzdEJ0dDh3NDVLQVY4X3IydENfZ0VaX0NTLTk3OFNrZi1RbTQwNjJoR0VRdUJvUHRkSWhyaHZPVF90ZUhBeDQxeVNLR3g3TUpnTWNIUjBhSmY4YXFkUFIybG5mczMxd0VuZHIyU24xVDBvcFdDZFp3aWdRY1IzWDdsSWpPVGdONTR0WkE?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi2wFBVV95cUxQSHNLa053dm01Nk9xUjE3NGJzM0NOekNBbF9xa1U5T25RVm1hVG93NFd1TnJPN0lpTmg0SE1yenAyQjZxS0FqZERldDBBeXNhaW1nQjVDR0hzdEJ0dDh3NDVLQVY4X3IydENfZ0VaX0NTLTk3OFNrZi1RbTQwNjJoR0VRdUJvUHRkSWhyaHZPVF90ZUhBeDQxeVNLR3g3TUpnTWNIUjBhSmY4YXFkUFIybG5mczMxd0VuZHIyU24xVDBvcFdDZFp3aWdRY1IzWDdsSWpPVGdONTR0WkE?oc=5\" target=\"_blank\">Implementing High-Performance LLM Serving on GKE: An Inference Gateway Walkthrough</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Google Cloud</font>",
      "image_url": "",
      "published": "Wed, 16 Jul 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "0a211746be25529d",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "How we built the most efficient inference engine for Cloudflare’s network - The Cloudflare Blog",
      "url": "https://news.google.com/rss/articles/CBMigAFBVV95cUxPUXVHMGtiLVNRQmFKXzFEd2tSY25fTC1UN0Mzc3ExOEM2NUlwd3pCeUVCLXlHQkdpTm52eUZUcUk3VlBOUkpOcFNBWnJfak9fS0JjY0FvVE1tZW1yMmZiSVRYX2t6dEdHRjVmNldVQVJubEw2OWRSN2xBQVhmME1FSQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMigAFBVV95cUxPUXVHMGtiLVNRQmFKXzFEd2tSY25fTC1UN0Mzc3ExOEM2NUlwd3pCeUVCLXlHQkdpTm52eUZUcUk3VlBOUkpOcFNBWnJfak9fS0JjY0FvVE1tZW1yMmZiSVRYX2t6dEdHRjVmNldVQVJubEw2OWRSN2xBQVhmME1FSQ?oc=5\" target=\"_blank\">How we built the most efficient inference engine for Cloudflare’s network</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The Cloudflare Blog</font>",
      "image_url": "",
      "published": "Wed, 27 Aug 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "4ad7b3570799cee1",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Inference Optimization for LLMs with vLLM - O'Reilly Media",
      "url": "https://news.google.com/rss/articles/CBMilwFBVV95cUxQdGNIVThYZDZXUGh5YzVyMGl2dFNvXzJuUlJlSDNYZkxiZEdMZGhESGw0U2FtTzN0UmF4VG5HWEplajJKR0JKRzlnWHdMeHhYYWR4X3BWUVJWWWREVUpGNjBGR3BZTVFTLUhjV2w1cXByQUY2WERhcGU4S1JDdjdyVTlsWGkwUmpmd2ctdWlyVExFcXdGeThn?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMilwFBVV95cUxQdGNIVThYZDZXUGh5YzVyMGl2dFNvXzJuUlJlSDNYZkxiZEdMZGhESGw0U2FtTzN0UmF4VG5HWEplajJKR0JKRzlnWHdMeHhYYWR4X3BWUVJWWWREVUpGNjBGR3BZTVFTLUhjV2w1cXByQUY2WERhcGU4S1JDdjdyVTlsWGkwUmpmd2ctdWlyVExFcXdGeThn?oc=5\" target=\"_blank\">Inference Optimization for LLMs with vLLM</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">O'Reilly Media</font>",
      "image_url": "",
      "published": "Wed, 25 Jun 2025 05:19:07 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "25cd9c21066b53df",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Effectively benchmarking OCI Compute Shapes for LLM inference serving - Oracle Blogs",
      "url": "https://news.google.com/rss/articles/CBMikwFBVV95cUxPaUF1dmdJT2EwR1NpQ01hcTYtTlQ4bG15RDFSekR0T1g4SWpGSnBWMjk2VUxZZ3RZdDRleGtJdjVmdFA1WUFEVUhsVy1WWTBLRE9YdURWekpFZUhFbDJldmJ2SHhOVDJNYmhGVUZxclQ3STQzVHQ5S1E1a2hoWFhGaEk1cnhiZzFfQUxFWm9rdVFwaGs?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMikwFBVV95cUxPaUF1dmdJT2EwR1NpQ01hcTYtTlQ4bG15RDFSekR0T1g4SWpGSnBWMjk2VUxZZ3RZdDRleGtJdjVmdFA1WUFEVUhsVy1WWTBLRE9YdURWekpFZUhFbDJldmJ2SHhOVDJNYmhGVUZxclQ3STQzVHQ5S1E1a2hoWFhGaEk1cnhiZzFfQUxFWm9rdVFwaGs?oc=5\" target=\"_blank\">Effectively benchmarking OCI Compute Shapes for LLM inference serving</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Oracle Blogs</font>",
      "image_url": "",
      "published": "Thu, 22 May 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "72d411b20e757bb4",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Allen School researchers receive Best Paper Award for speeding up LLM performance with FlashInfer - Allen School News",
      "url": "https://news.google.com/rss/articles/CBMi1wFBVV95cUxPaS0tT21tX0JsZ3dpYVJLSGRuTDNRSWlxdFpVMnVwZmNmcm1UNlgzcExGamp5X181ekNOMHo3WE5uTFpIMmpnLVVHOW05ZDctZjZWTTFOelRKalgzLWRyQjFUWkRDdGp0cHdXdzhqSjB4aEh2LUtUMXNsUjY4S0hZX3J3dXQ2U1Q1UEpLc19ybEJuQlo4Q29xcmxyOG1ITy03d09HYjRuN2lVZ0VGanFrUXRmbW1MTElSMU5aWTVrMVJKSzI1THI0a0VDMERTTlB6WlQ5Wm1NQQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi1wFBVV95cUxPaS0tT21tX0JsZ3dpYVJLSGRuTDNRSWlxdFpVMnVwZmNmcm1UNlgzcExGamp5X181ekNOMHo3WE5uTFpIMmpnLVVHOW05ZDctZjZWTTFOelRKalgzLWRyQjFUWkRDdGp0cHdXdzhqSjB4aEh2LUtUMXNsUjY4S0hZX3J3dXQ2U1Q1UEpLc19ybEJuQlo4Q29xcmxyOG1ITy03d09HYjRuN2lVZ0VGanFrUXRmbW1MTElSMU5aWTVrMVJKSzI1THI0a0VDMERTTlB6WlQ5Wm1NQQ?oc=5\" target=\"_blank\">Allen School researchers receive Best Paper Award for speeding up LLM performance with FlashInfer</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Allen School News</font>",
      "image_url": "",
      "published": "Tue, 01 Jul 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "fc015eaafb3d2ddb",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "A Gentle Introduction to vLLM for Serving - KDnuggets",
      "url": "https://news.google.com/rss/articles/CBMidkFVX3lxTE1mZTVLOWM2RUZjYXRrQnFKVExBdnY4OVNpTTc3OWs1WEs3Ui1CWnNUdmZUQ1p6eDd4Tkk1SGdGYmFPNjU3T2tNNUE1N0NqSWZROENaY3FkTVhmUlBtMzlpeUZpM0h5WG9rOVdBME1EQXNIVE5YMFE?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMidkFVX3lxTE1mZTVLOWM2RUZjYXRrQnFKVExBdnY4OVNpTTc3OWs1WEs3Ui1CWnNUdmZUQ1p6eDd4Tkk1SGdGYmFPNjU3T2tNNUE1N0NqSWZROENaY3FkTVhmUlBtMzlpeUZpM0h5WG9rOVdBME1EQXNIVE5YMFE?oc=5\" target=\"_blank\">A Gentle Introduction to vLLM for Serving</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">KDnuggets</font>",
      "image_url": "",
      "published": "Thu, 18 Sep 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "32369f4adfab2868",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Rapid-serve Achieves 4.1x LLM Inference Speedup With Intra-GPU Disaggregation - Quantum Zeitgeist",
      "url": "https://news.google.com/rss/articles/CBMie0FVX3lxTFBGVFdLZzRZQk1BM0EySjFZOWJ0N1FkVmJaNXZhR3dfbG9KU3hrWmwtODRGWmZ3QVJMMDFDWEpHNzdXdVNDczJhejljaUN5aE1CTWhtaUhoNUFMME0wenpUWWU3aF90MzRmeHpsQkYzZkZkZDcwSE45eDZaVQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMie0FVX3lxTFBGVFdLZzRZQk1BM0EySjFZOWJ0N1FkVmJaNXZhR3dfbG9KU3hrWmwtODRGWmZ3QVJMMDFDWEpHNzdXdVNDczJhejljaUN5aE1CTWhtaUhoNUFMME0wenpUWWU3aF90MzRmeHpsQkYzZkZkZDcwSE45eDZaVQ?oc=5\" target=\"_blank\">Rapid-serve Achieves 4.1x LLM Inference Speedup With Intra-GPU Disaggregation</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Quantum Zeitgeist</font>",
      "image_url": "",
      "published": "Thu, 22 Jan 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "7f9f3ce966f184cf",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Optimizing LLM inference on Amazon SageMaker AI with BentoML’s LLM- Optimizer - Amazon Web Services (AWS)",
      "url": "https://news.google.com/rss/articles/CBMivwFBVV95cUxNTGwxRkQ2bENtcHUtUTRweHotWW5XS1hRa25GM3JjUUVCTC11X2syU3V2ZHFJYUZORnhHRXJsQUtoaEswVm40Z1htMXVCaGdJeUJvOWFoUWREclpvV2JsRVAzWUc5YUpncnRqS2F0T21nRTFMT3RtRnk0MXZ4aEIwRkNBUmg3WXFESU1oaVFvR0ktT1lZaVVvZUh1NFJaSWZRd1ZEUlJSS1RocllZd2dWS1lMcUk2dU4xVEVESlJtbw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMivwFBVV95cUxNTGwxRkQ2bENtcHUtUTRweHotWW5XS1hRa25GM3JjUUVCTC11X2syU3V2ZHFJYUZORnhHRXJsQUtoaEswVm40Z1htMXVCaGdJeUJvOWFoUWREclpvV2JsRVAzWUc5YUpncnRqS2F0T21nRTFMT3RtRnk0MXZ4aEIwRkNBUmg3WXFESU1oaVFvR0ktT1lZaVVvZUh1NFJaSWZRd1ZEUlJSS1RocllZd2dWS1lMcUk2dU4xVEVESlJtbw?oc=5\" target=\"_blank\">Optimizing LLM inference on Amazon SageMaker AI with BentoML’s LLM- Optimizer</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services (AWS)</font>",
      "image_url": "",
      "published": "Wed, 24 Dec 2025 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "7209b7f1edf61119",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "An Introduction to Speculative Decoding for Reducing Latency in AI Inference - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMisAFBVV95cUxPbEJCVnJUY1BONmF4bWdFWjlnc0hLTlBWNkxkX3BkMHo1eFE0Tk03Q3Q0Z2ZqN0UwNDc4bENLT1hHZ1dfaHdhR0xoSXhsLVNRbzV4WXhSaFlOLUtnRjk5RGc1SFpmMHhkdmpZWFJ2TVR2OVRNWGN6djlhZWpqV3JkcDZidlF2NjhyQUVlR1FVZWlTZ2ZEbEdId25FQVdwaXVSY1ZDMkdEWklSSXViR0hySg?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMisAFBVV95cUxPbEJCVnJUY1BONmF4bWdFWjlnc0hLTlBWNkxkX3BkMHo1eFE0Tk03Q3Q0Z2ZqN0UwNDc4bENLT1hHZ1dfaHdhR0xoSXhsLVNRbzV4WXhSaFlOLUtnRjk5RGc1SFpmMHhkdmpZWFJ2TVR2OVRNWGN6djlhZWpqV3JkcDZidlF2NjhyQUVlR1FVZWlTZ2ZEbEdId25FQVdwaXVSY1ZDMkdEWklSSXViR0hySg?oc=5\" target=\"_blank\">An Introduction to Speculative Decoding for Reducing Latency in AI Inference</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Wed, 17 Sep 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "065412c4f58b3353",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Smart Multi-Node Scheduling for Fast and Efficient LLM Inference with NVIDIA Run:ai and NVIDIA Dynamo - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMi0AFBVV95cUxPeVNxVzZ1M0VET21hWnh4aVJqWC1uZHJlUHBUbXl4MTlicExQSWFTbzg2NHdxbzdaVUU0NmNkallabHdLUWE4ZXdZUHRXUll6emRRLVktb2JoeVRteUw4RzA4WDl3LTIzZXBHazNEeU0wVXNKbUs0RDIwWFVQTHgtRlpPbVZGYkZpeV9GZVB3RlJ0RG5OV1g1bmFlOHk0akpRYkpYLVA3YXJBbkhSTktqa0t5cVdBdEhjOTZHNk5PMXNUaXNnZE1rSjVoczA1N0pX?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi0AFBVV95cUxPeVNxVzZ1M0VET21hWnh4aVJqWC1uZHJlUHBUbXl4MTlicExQSWFTbzg2NHdxbzdaVUU0NmNkallabHdLUWE4ZXdZUHRXUll6emRRLVktb2JoeVRteUw4RzA4WDl3LTIzZXBHazNEeU0wVXNKbUs0RDIwWFVQTHgtRlpPbVZGYkZpeV9GZVB3RlJ0RG5OV1g1bmFlOHk0akpRYkpYLVA3YXJBbkhSTktqa0t5cVdBdEhjOTZHNk5PMXNUaXNnZE1rSjVoczA1N0pX?oc=5\" target=\"_blank\">Smart Multi-Node Scheduling for Fast and Efficient LLM Inference with NVIDIA Run:ai and NVIDIA Dynamo</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Mon, 29 Sep 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "d48a471f79f4de03",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers - Amazon Web Services (AWS)",
      "url": "https://news.google.com/rss/articles/CBMipwFBVV95cUxOTVJwY0MzSzFxTkgwMHNiTEhWTXhmU2hBd3gzOXBQR24tM2dQNVpyN2ZEOXAyZmJRUEtuQWlhYkgyMDFVcEhxWF9pNngzRkFxR0F0UTJmQnltNnR3bnMxU3dCejMzOElEcENGVGFndDV3UWo4aGcyc3UydVh4QURPd1VEY1o5M01pdGEtdXNSdk9lVnFiWGYyQl8zZFd0X0V3V1F6V1FvRQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMipwFBVV95cUxOTVJwY0MzSzFxTkgwMHNiTEhWTXhmU2hBd3gzOXBQR24tM2dQNVpyN2ZEOXAyZmJRUEtuQWlhYkgyMDFVcEhxWF9pNngzRkFxR0F0UTJmQnltNnR3bnMxU3dCejMzOElEcENGVGFndDV3UWo4aGcyc3UydVh4QURPd1VEY1o5M01pdGEtdXNSdk9lVnFiWGYyQl8zZFd0X0V3V1F6V1FvRQ?oc=5\" target=\"_blank\">Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services (AWS)</font>",
      "image_url": "",
      "published": "Thu, 14 Aug 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "01eefaab4a3fa59a",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Accelerate Deep Learning and LLM Inference with Apache Spark in the Cloud | NVIDIA Technical Blog - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMirAFBVV95cUxNcU5wMzU3RG5nLU9RTV9KeUpVR1ZvdzdUWWFMQjIyUzlFQ2ctVEFOVWJGeXVnNmx3cktYOUs3dXRSel8yTnMxeHpJZDFydWU2R0xBWWVjeHV4TWhhYlEyMlE2bi1QTnNVUEFJZUMxb3BlNXowS3VRRGIwV3VBRWRKYlZmYW0yYmJ1N1ZnWHVyaThFMTVrNXZPZGdtdmNqcjhYUVlrcjJWczg4aUFI?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMirAFBVV95cUxNcU5wMzU3RG5nLU9RTV9KeUpVR1ZvdzdUWWFMQjIyUzlFQ2ctVEFOVWJGeXVnNmx3cktYOUs3dXRSel8yTnMxeHpJZDFydWU2R0xBWWVjeHV4TWhhYlEyMlE2bi1QTnNVUEFJZUMxb3BlNXowS3VRRGIwV3VBRWRKYlZmYW0yYmJ1N1ZnWHVyaThFMTVrNXZPZGdtdmNqcjhYUVlrcjJWczg4aUFI?oc=5\" target=\"_blank\">Accelerate Deep Learning and LLM Inference with Apache Spark in the Cloud | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Thu, 08 May 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "d6e346d738f6b236",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Run High-Performance LLM Inference Kernels from NVIDIA Using FlashInfer​​ - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMiqgFBVV95cUxQRXVBTGczenB4dEZnQlhHbVhfN19Eb2RXWmJwUnhLZlcwY1dBYzN6SUJRcDhsMlNLUnpUR2dXUnVqODlmaVluVW52Z2cyWl95ODJLQzNaRG1aRXE1ZW52dGk2N0E5bnhBM3NrOUoxSzc1OXM5NVdUSUNwQzRhOFRWMnh1XzdmamM4WHVrcWFwYUJ6Zl9zSkJRMVlTZEpWalY3ZWpRQ1VsdjZWUQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiqgFBVV95cUxQRXVBTGczenB4dEZnQlhHbVhfN19Eb2RXWmJwUnhLZlcwY1dBYzN6SUJRcDhsMlNLUnpUR2dXUnVqODlmaVluVW52Z2cyWl95ODJLQzNaRG1aRXE1ZW52dGk2N0E5bnhBM3NrOUoxSzc1OXM5NVdUSUNwQzRhOFRWMnh1XzdmamM4WHVrcWFwYUJ6Zl9zSkJRMVlTZEpWalY3ZWpRQ1VsdjZWUQ?oc=5\" target=\"_blank\">Run High-Performance LLM Inference Kernels from NVIDIA Using FlashInfer​​</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Fri, 13 Jun 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "54db6c3a9fc6fea9",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "LLM Inference Benchmarking: Performance Tuning with TensorRT-LLM | NVIDIA Technical Blog - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMinwFBVV95cUxOeXd1SF9ZTnAwQlJ3S1pYMzh2YTNHU09HZHZGNjA2a1JsV0RRT0ZpbC1uemZMS1NkT25xSnJkNDA1RTVCWkJPSUhEYndFYzE3MnVZMDJ2T2FxdjNRaV9GX2JITWVHbTJicVNWeDFXaW9KaHF1TngtanF6eVRtU3N2MVdjNUNnbnVPNGNLZlowUlRCTHpUdEdIY2pGdzRpaVU?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMinwFBVV95cUxOeXd1SF9ZTnAwQlJ3S1pYMzh2YTNHU09HZHZGNjA2a1JsV0RRT0ZpbC1uemZMS1NkT25xSnJkNDA1RTVCWkJPSUhEYndFYzE3MnVZMDJ2T2FxdjNRaV9GX2JITWVHbTJicVNWeDFXaW9KaHF1TngtanF6eVRtU3N2MVdjNUNnbnVPNGNLZlowUlRCTHpUdEdIY2pGdzRpaVU?oc=5\" target=\"_blank\">LLM Inference Benchmarking: Performance Tuning with TensorRT-LLM | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Mon, 07 Jul 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "a3e69aaecac47bd9",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "LLM Inference Benchmarking: Fundamental Concepts | NVIDIA Technical Blog - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMifEFVX3lxTE5Rbk84V0tQWlRrNTA4aXJIUm9scS1XZmdvTm9SUUV3Uk9tQ2RGZEFrUkpUdGphZ0dvUkFMVmJpNFVieVFaOHc2NmdDNG04N014bU96d3JpcmZjYnlrRl9aYk10Y0hIdFNvYlJRenFyd1dNUkYzd3VuTldhV3Y?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMifEFVX3lxTE5Rbk84V0tQWlRrNTA4aXJIUm9scS1XZmdvTm9SUUV3Uk9tQ2RGZEFrUkpUdGphZ0dvUkFMVmJpNFVieVFaOHc2NmdDNG04N014bU96d3JpcmZjYnlrRl9aYk10Y0hIdFNvYlJRenFyd1dNUkYzd3VuTldhV3Y?oc=5\" target=\"_blank\">LLM Inference Benchmarking: Fundamental Concepts | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Wed, 02 Apr 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "14715f1b228fba8d",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "LLM Inference Benchmarking: How Much Does Your LLM Inference Cost? | NVIDIA Technical Blog - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMioAFBVV95cUxQeG1jWk0zR3RobjF4ZEtFdjhPdmFkakJyaXdFcmhiTG8zdHJ4dGpNWktSaklKaGVyeFlwUzBwM3Y1cVNMa0l0ci16d3NwdHQydURLRXdFRmxyM3RJZGEyOWw0Q2picEZBZ1VaLVloV1dCOVNYT21hZGR4ZVNYdU9Zb0xKcW1TQjVFaUJiaG0wdTZBTWVZS0FiUnFZV3o4YU1W?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMioAFBVV95cUxQeG1jWk0zR3RobjF4ZEtFdjhPdmFkakJyaXdFcmhiTG8zdHJ4dGpNWktSaklKaGVyeFlwUzBwM3Y1cVNMa0l0ci16d3NwdHQydURLRXdFRmxyM3RJZGEyOWw0Q2picEZBZ1VaLVloV1dCOVNYT21hZGR4ZVNYdU9Zb0xKcW1TQjVFaUJiaG0wdTZBTWVZS0FiUnFZV3o4YU1W?oc=5\" target=\"_blank\">LLM Inference Benchmarking: How Much Does Your LLM Inference Cost? | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Wed, 18 Jun 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "fb17a8e8e975e486",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Six Frameworks for Efficient LLM Inferencing - The New Stack",
      "url": "https://news.google.com/rss/articles/CBMid0FVX3lxTE9keXE5b3BtMG9RUnNGdVBmanA3LVdVVi11QUtfOGhZTy13SEhER0hybFV3S0xScHFESy1ySTNoVHU5Nlp0ZUNaUWdYbjRBQ3oyRmVVM283b2hTamZJOEEtcEZRQUs1Uk10SUh3dGV0SjN1T1UyOVlz?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMid0FVX3lxTE9keXE5b3BtMG9RUnNGdVBmanA3LVdVVi11QUtfOGhZTy13SEhER0hybFV3S0xScHFESy1ySTNoVHU5Nlp0ZUNaUWdYbjRBQ3oyRmVVM283b2hTamZJOEEtcEZRQUs1Uk10SUh3dGV0SjN1T1UyOVlz?oc=5\" target=\"_blank\">Six Frameworks for Efficient LLM Inferencing</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The New Stack</font>",
      "image_url": "",
      "published": "Fri, 19 Sep 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "2eae253da55b55d9",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "NVIDIA Dynamo, A Low-Latency Distributed Inference Framework for Scaling Reasoning AI Models - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMi1AFBVV95cUxOZFlYTlRURWtqOFZvXzJLZTdqTVZrNmgwdGFVT1hvYWtkQjE1QzBNb1hmNWhSY3h0eTZxc016QlZqeXAtd3VUMFBwSGVqSXBlcDBzQXZ3VndTTVdPVm9PUGpMOVNkN0FjX3hkdXhSSFdRV2hTSU1Kb1c3M0VJTXpqbjhSWVdIMnhqNDctR0VzTXRIOFNQVndSamlqZU4wcDJhYlpqSDRiTDlxazRpaVV4WnI4bnB0dVgzWEFCd2J5WkFBWGRnUFNrZjFxME5aSFJWek91RA?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi1AFBVV95cUxOZFlYTlRURWtqOFZvXzJLZTdqTVZrNmgwdGFVT1hvYWtkQjE1QzBNb1hmNWhSY3h0eTZxc016QlZqeXAtd3VUMFBwSGVqSXBlcDBzQXZ3VndTTVdPVm9PUGpMOVNkN0FjX3hkdXhSSFdRV2hTSU1Kb1c3M0VJTXpqbjhSWVdIMnhqNDctR0VzTXRIOFNQVndSamlqZU4wcDJhYlpqSDRiTDlxazRpaVV4WnI4bnB0dVgzWEFCd2J5WkFBWGRnUFNrZjFxME5aSFJWek91RA?oc=5\" target=\"_blank\">NVIDIA Dynamo, A Low-Latency Distributed Inference Framework for Scaling Reasoning AI Models</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Tue, 18 Mar 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "002d65ac4348050f",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Ulysses: Unlocking Low-Latency, High-Throughput Inference for Long-Context LLMs - Snowflake",
      "url": "https://news.google.com/rss/articles/CBMihwFBVV95cUxNbGZxU1RfUUYybFFvVlZsSFJoTVN2M2NFZFlwazlvT3kxTFZRWlFQMXdjcWl1NWZ3OGZCOEtHSlhGOGw1S2J4QWdwVDQzX21tald5dTNUTmdodnhwNkJiTWhBbDhDeUFiaE44QzVnVzRtY2d0eVFsWU9DV3NVUDJoX3JhbFBPVTA?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMihwFBVV95cUxNbGZxU1RfUUYybFFvVlZsSFJoTVN2M2NFZFlwazlvT3kxTFZRWlFQMXdjcWl1NWZ3OGZCOEtHSlhGOGw1S2J4QWdwVDQzX21tald5dTNUTmdodnhwNkJiTWhBbDhDeUFiaE44QzVnVzRtY2d0eVFsWU9DV3NVUDJoX3JhbFBPVTA?oc=5\" target=\"_blank\">Ulysses: Unlocking Low-Latency, High-Throughput Inference for Long-Context LLMs</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Snowflake</font>",
      "image_url": "",
      "published": "Thu, 03 Apr 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "eb7262cb35d001a6",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "NVIDIA Dynamo Accelerates llm-d Community Initiatives for Advancing Large-Scale Distributed Inference - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMi0gFBVV95cUxORXRTYnJ2cFVSTVBISmpvSl9jWnVKUTJfOEUtS2Z5MVNjOUlHUXdDRWJ5YWlySTNUdzBmQXBlZV80MWRTNFQzbmRBY2NEQkhBSmluVDJsVWw4VWZDVEJzeEVlYnVnUzJIa3c2YjhZRFlfUWpDWTBFRGJlUzdrTF9Za0d1R25kMjZ1bGNXT3ZiMlZ4ZHZocDlheDhKWnYtUHYySENxYnhxOGhDSjJ3bHByZGpYY2NfMXoxNlcxVU1qdWU4Skl1ZTJoN2trUzRubTJ6Y2c?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi0gFBVV95cUxORXRTYnJ2cFVSTVBISmpvSl9jWnVKUTJfOEUtS2Z5MVNjOUlHUXdDRWJ5YWlySTNUdzBmQXBlZV80MWRTNFQzbmRBY2NEQkhBSmluVDJsVWw4VWZDVEJzeEVlYnVnUzJIa3c2YjhZRFlfUWpDWTBFRGJlUzdrTF9Za0d1R25kMjZ1bGNXT3ZiMlZ4ZHZocDlheDhKWnYtUHYySENxYnhxOGhDSjJ3bHByZGpYY2NfMXoxNlcxVU1qdWU4Skl1ZTJoN2trUzRubTJ6Y2c?oc=5\" target=\"_blank\">NVIDIA Dynamo Accelerates llm-d Community Initiatives for Advancing Large-Scale Distributed Inference</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Wed, 21 May 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "4ce20914418f360c",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "vLLM: Advancing open-source LLM inference - Nebius",
      "url": "https://news.google.com/rss/articles/CBMiUkFVX3lxTFBTb0s2a3JRYzd5dzdPckJVeFVfeUJtT2pXdy1yMGRCeERPMUxPZkkzbzE5Tm81dVVPc2xjNFM3cVc4bnJOYlJaZ2daYkU2em51Qnc?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiUkFVX3lxTFBTb0s2a3JRYzd5dzdPckJVeFVfeUJtT2pXdy1yMGRCeERPMUxPZkkzbzE5Tm81dVVPc2xjNFM3cVc4bnJOYlJaZ2daYkU2em51Qnc?oc=5\" target=\"_blank\">vLLM: Advancing open-source LLM inference</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Nebius</font>",
      "image_url": "",
      "published": "Thu, 17 Apr 2025 01:12:09 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "d546e4dd83cef371",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Comparing the Top 6 Inference Runtimes for LLM Serving in 2025 - MarkTechPost",
      "url": "https://news.google.com/rss/articles/CBMipgFBVV95cUxPZmQ3aFl2Y2IxTDZmV202WjZScHVleldrME03V1FfdGFURWR6Q3BQMUxmREs0YnlXSlNtNWlWdloxaUhwUHNKLXBvdUtyMlJtTDBnZVNxVnVMRjhrcUw3VlBmZ0hzelEyamxUSDlzLWx3aGNONXRHaWRMNS1TOXB3T19nSHA4MDhiYmowQWtzTDRzMTRiNzc2QmxXaGk2MThaSHZxVlBR0gGrAUFVX3lxTE82eFJXRlJBei1NTThMbkxiZlkxSmlfZWozY0pnSHB3OFFsVlJRSTNjZmNGR0pXRy16ekFFb2l0STFMbUxWM08zR3FyQUFicTB4N0o1UzVZbGRUV2F2OG53UUFvMmk2dFJteVFyanFVUUIxaDVnU2hXaFdjMTI5dFZVRFJRbGY2X0ktajNIM0kwaDE3RzhzdXZQNGFhejZLVVBYYTF3NnlaSW1XYw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMipgFBVV95cUxPZmQ3aFl2Y2IxTDZmV202WjZScHVleldrME03V1FfdGFURWR6Q3BQMUxmREs0YnlXSlNtNWlWdloxaUhwUHNKLXBvdUtyMlJtTDBnZVNxVnVMRjhrcUw3VlBmZ0hzelEyamxUSDlzLWx3aGNONXRHaWRMNS1TOXB3T19nSHA4MDhiYmowQWtzTDRzMTRiNzc2QmxXaGk2MThaSHZxVlBR0gGrAUFVX3lxTE82eFJXRlJBei1NTThMbkxiZlkxSmlfZWozY0pnSHB3OFFsVlJRSTNjZmNGR0pXRy16ekFFb2l0STFMbUxWM08zR3FyQUFicTB4N0o1UzVZbGRUV2F2OG53UUFvMmk2dFJteVFyanFVUUIxaDVnU2hXaFdjMTI5dFZVRFJRbGY2X0ktajNIM0kwaDE3RzhzdXZQNGFhejZLVVBYYTF3NnlaSW1XYw?oc=5\" target=\"_blank\">Comparing the Top 6 Inference Runtimes for LLM Serving in 2025</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">MarkTechPost</font>",
      "image_url": "",
      "published": "Fri, 07 Nov 2025 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "c157ab42400edbac",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Supercharge your LLM performance with Amazon SageMaker Large Model Inference container v15 - Amazon Web Services (AWS)",
      "url": "https://news.google.com/rss/articles/CBMi0wFBVV95cUxNYVFBSUd2RDJ0U3dvUlBaVzFhTGVRamFIb04zeHBCTFNkamJNTHJoclU5SDNBeGZNb29rUF9WT0hMRjEyVW54QnNTYXBEdGp3T3ZDSFN3RjBsVUlrMXpFSC1lLWRUWkM2ZWI5dER5R1R0a2gzZ1A2TE9keU5QZGpDWDl3UWpDVzctM3JwR2pkNFNqdndyQzlWTlFHaVp6NVpsaC1VdnM1NmZJRUxIVER4X292X3Z6MUdDZXNwWXIwT25xREVuLXBmbFI4aVFYQWI5dkxV?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi0wFBVV95cUxNYVFBSUd2RDJ0U3dvUlBaVzFhTGVRamFIb04zeHBCTFNkamJNTHJoclU5SDNBeGZNb29rUF9WT0hMRjEyVW54QnNTYXBEdGp3T3ZDSFN3RjBsVUlrMXpFSC1lLWRUWkM2ZWI5dER5R1R0a2gzZ1A2TE9keU5QZGpDWDl3UWpDVzctM3JwR2pkNFNqdndyQzlWTlFHaVp6NVpsaC1VdnM1NmZJRUxIVER4X292X3Z6MUdDZXNwWXIwT25xREVuLXBmbFI4aVFYQWI5dkxV?oc=5\" target=\"_blank\">Supercharge your LLM performance with Amazon SageMaker Large Model Inference container v15</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services (AWS)</font>",
      "image_url": "",
      "published": "Tue, 22 Apr 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "75cd9d3f6aa0945d",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "AMD Making It Easier To Install vLLM For ROCm - Phoronix",
      "url": "https://news.google.com/rss/articles/CBMiXkFVX3lxTE42M1p2dkJ0bC1vbk1ZN0VCbHh1dElJdG0tV2VzYlNmYmVQQW5MMzRQWjgzVmE4eTNKN2NEWlluZ0swQzRGRE44NTh0OEVtNzVyakYxd2lINFVXYWNHMXc?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiXkFVX3lxTE42M1p2dkJ0bC1vbk1ZN0VCbHh1dElJdG0tV2VzYlNmYmVQQW5MMzRQWjgzVmE4eTNKN2NEWlluZ0swQzRGRE44NTh0OEVtNzVyakYxd2lINFVXYWNHMXc?oc=5\" target=\"_blank\">AMD Making It Easier To Install vLLM For ROCm</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Phoronix</font>",
      "image_url": "",
      "published": "Tue, 20 Jan 2026 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "895e36c479cade37",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "From LLMs to image generation: Accelerate inference workloads with AI Hypercomputer - Google Cloud",
      "url": "https://news.google.com/rss/articles/CBMirwFBVV95cUxNWkJMWmg1a1N4Z0FSY1JzQ1ZhQVhXLWdOQWRkVE9wYUJyU0dmWXIxMXhoN1N5Y0dUcmd1dXFrRFRMRmV4Wi1sS042WXZ5R3QwNXhVVUdDNDdDbjVVeVhfaHY5WG42WXcyb1B6NjdNcWY5c3pvSzFCMlRZVE1MckFJQUw0Qmt1ODlDUG1KRWd5YkxMZUJibXVSU3luMC1FdzVlaHkyQjBRd1BHTjgtelpF?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMirwFBVV95cUxNWkJMWmg1a1N4Z0FSY1JzQ1ZhQVhXLWdOQWRkVE9wYUJyU0dmWXIxMXhoN1N5Y0dUcmd1dXFrRFRMRmV4Wi1sS042WXZ5R3QwNXhVVUdDNDdDbjVVeVhfaHY5WG42WXcyb1B6NjdNcWY5c3pvSzFCMlRZVE1MckFJQUw0Qmt1ODlDUG1KRWd5YkxMZUJibXVSU3luMC1FdzVlaHkyQjBRd1BHTjgtelpF?oc=5\" target=\"_blank\">From LLMs to image generation: Accelerate inference workloads with AI Hypercomputer</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Google Cloud</font>",
      "image_url": "",
      "published": "Fri, 09 May 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "b520981905a392fd",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "LLM Inference Benchmarking Guide: NVIDIA GenAI-Perf and NIM - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMisgFBVV95cUxQd2VsOWdrWS1haUdrZEJHUkhZUGdCMVpsUG1YVHJHSTRLUnh0OGxXd2xvWWRDVHVSUzFqbXgzWUhnaTdZTERvLVZNbzJ1aDd0T1JhOWRnUUNHR2liT05pWUlQSzQzVGUwQ0pqR0VxbmdTNEdnR2xSdnNNY0tPNXRXem5uMzdLNk5ycWp2UUpxbU12VHJLd0hOeVFST0RsRDJWSTRlTUhtdWpKRVZ1LWRMZDR3?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMisgFBVV95cUxQd2VsOWdrWS1haUdrZEJHUkhZUGdCMVpsUG1YVHJHSTRLUnh0OGxXd2xvWWRDVHVSUzFqbXgzWUhnaTdZTERvLVZNbzJ1aDd0T1JhOWRnUUNHR2liT05pWUlQSzQzVGUwQ0pqR0VxbmdTNEdnR2xSdnNNY0tPNXRXem5uMzdLNk5ycWp2UUpxbU12VHJLd0hOeVFST0RsRDJWSTRlTUhtdWpKRVZ1LWRMZDR3?oc=5\" target=\"_blank\">LLM Inference Benchmarking Guide: NVIDIA GenAI-Perf and NIM</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Tue, 06 May 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "a95005c31f2bfb4c",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "NVIDIA Accelerates OpenAI gpt-oss Models Delivering 1.5 M TPS Inference on NVIDIA GB200 NVL72 - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMi4AFBVV95cUxNblpGbFE5Y3JfZ0JuMTFHM0xDTml1R2tLU09Fc1VCYVFOeUs3YWdwYi05V0ptMEY0ZThyRGpocVNXS2pUQW9jUkJNNVQzNk9nOTJsdnNMam1PM2YwbzUtQmFMbTV0cHQ2cUw1eXQ3dzBtY0hmX1ltZ1VqbjlMRWplWXlweDJLVEppNHRzWTNzLUQycWQyTzh5Tm5hdFZEOWJQeVdDNEwxS1lsRm9Zc29LU0xUOC05T1YtdnBvbmdIeldKdVV3cHZwaE0zOXZPWm9EcUoyQTFuOHV6YkRfVm5DVA?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi4AFBVV95cUxNblpGbFE5Y3JfZ0JuMTFHM0xDTml1R2tLU09Fc1VCYVFOeUs3YWdwYi05V0ptMEY0ZThyRGpocVNXS2pUQW9jUkJNNVQzNk9nOTJsdnNMam1PM2YwbzUtQmFMbTV0cHQ2cUw1eXQ3dzBtY0hmX1ltZ1VqbjlMRWplWXlweDJLVEppNHRzWTNzLUQycWQyTzh5Tm5hdFZEOWJQeVdDNEwxS1lsRm9Zc29LU0xUOC05T1YtdnBvbmdIeldKdVV3cHZwaE0zOXZPWm9EcUoyQTFuOHV6YkRfVm5DVA?oc=5\" target=\"_blank\">NVIDIA Accelerates OpenAI gpt-oss Models Delivering 1.5 M TPS Inference on NVIDIA GB200 NVL72</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Tue, 05 Aug 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "f36b56b3f58119a0",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Fastest Speculative Decoding in vLLM with Arctic Inference and Arctic Training - Snowflake",
      "url": "https://news.google.com/rss/articles/CBMijAFBVV95cUxQa0h6M1hwQVdhMVpSc2NuQ0l3a0I5NUNJX3ViMzBnelcwNUVpVWpQSVdZZldOcGJRMURoYU56RjJwY0poVjZBU2JTZk04cV9zTzBBTG5UaXc4elliaEd3b25CYzl0ZlFUbEdzN2JlRnFWNUVGWVZLVVh3dy12b2V3OGlsUWZuQVRSVlRRNQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMijAFBVV95cUxQa0h6M1hwQVdhMVpSc2NuQ0l3a0I5NUNJX3ViMzBnelcwNUVpVWpQSVdZZldOcGJRMURoYU56RjJwY0poVjZBU2JTZk04cV9zTzBBTG5UaXc4elliaEd3b25CYzl0ZlFUbEdzN2JlRnFWNUVGWVZLVVh3dy12b2V3OGlsUWZuQVRSVlRRNQ?oc=5\" target=\"_blank\">Fastest Speculative Decoding in vLLM with Arctic Inference and Arctic Training</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Snowflake</font>",
      "image_url": "",
      "published": "Thu, 01 May 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "23b6a843fe758033",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Optimizing LLM Performance with LM Cache: Architectures, Strategies, and Real-World Applications - HackerNoon",
      "url": "https://news.google.com/rss/articles/CBMitwFBVV95cUxNNWx4OWN2Yms5SHZHd2lFSk1rRXpCdUxmRnpydzR6U3c5eWIxbEJDZ2JEMTB6WmdQNkFfdEtnOGRVdE00RGkxWG1lQVpQZlFZcllRTUFwbHdkZENBLVVRODkyR0wydzBpcWZlZWdZVzB4WUFwQXFUSHJlc3AyTnhBNjNQRHpJcllieXp4NWJoNUpQTzUyMW51M2kwaDd5ZnhjT0ROR2ZBX2pFR1EwMmxIdkNEcF9PNFU?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMitwFBVV95cUxNNWx4OWN2Yms5SHZHd2lFSk1rRXpCdUxmRnpydzR6U3c5eWIxbEJDZ2JEMTB6WmdQNkFfdEtnOGRVdE00RGkxWG1lQVpQZlFZcllRTUFwbHdkZENBLVVRODkyR0wydzBpcWZlZWdZVzB4WUFwQXFUSHJlc3AyTnhBNjNQRHpJcllieXp4NWJoNUpQTzUyMW51M2kwaDd5ZnhjT0ROR2ZBX2pFR1EwMmxIdkNEcF9PNFU?oc=5\" target=\"_blank\">Optimizing LLM Performance with LM Cache: Architectures, Strategies, and Real-World Applications</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">HackerNoon</font>",
      "image_url": "",
      "published": "Sun, 10 Aug 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "3a438f5575a98625",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "NVIDIA Dynamo Adds Support for AWS Services to Deliver Cost-Efficient Inference at Scale | NVIDIA Technical Blog - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMiwAFBVV95cUxQSGJwVkxfLXhuaGJSMG1kX2VkYnNlS2NQSm42dWJSdTJsWnljaEdZR0dGdkFhQ1hQOWpweF9KTDFtcEdZUlFSOFNoNVNDeEFpNzF3cHVLWEsxbkFscWlLM2tINGk1aWx0VmZOTEJnMFVVbExZRXdFTnAyT1R4UmFXZ3NjakRsYnpFQUItYUVTLTNvS2VhektnMWFBZGFMM2dTR3JkZnI5YXphTWVUZFFwYTVHRlZURmtxcTdwNDc1QUM?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiwAFBVV95cUxQSGJwVkxfLXhuaGJSMG1kX2VkYnNlS2NQSm42dWJSdTJsWnljaEdZR0dGdkFhQ1hQOWpweF9KTDFtcEdZUlFSOFNoNVNDeEFpNzF3cHVLWEsxbkFscWlLM2tINGk1aWx0VmZOTEJnMFVVbExZRXdFTnAyT1R4UmFXZ3NjakRsYnpFQUItYUVTLTNvS2VhektnMWFBZGFMM2dTR3JkZnI5YXphTWVUZFFwYTVHRlZURmtxcTdwNDc1QUM?oc=5\" target=\"_blank\">NVIDIA Dynamo Adds Support for AWS Services to Deliver Cost-Efficient Inference at Scale | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Tue, 15 Jul 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "5c8c94974f7012f8",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "A Look at AIBrix, an Open Source LLM Inference Platform - The New Stack",
      "url": "https://news.google.com/rss/articles/CBMihAFBVV95cUxQeFc5UlB6eld6QWoxXzlfY0pSYUNxZ1A0b0psNDFLM3pZLWxmNDhrNTJPZWVRNlROb3FqQjFKWV9HSmlwbjdicmNTb1NaQUhmVnRFQmlVd290Y1lzRDZ2aldzQk4xQ3RVZUNuWkJGTFRwS21mYm5YMkdlWUVuX2t2VjZZcXQ?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMihAFBVV95cUxQeFc5UlB6eld6QWoxXzlfY0pSYUNxZ1A0b0psNDFLM3pZLWxmNDhrNTJPZWVRNlROb3FqQjFKWV9HSmlwbjdicmNTb1NaQUhmVnRFQmlVd290Y1lzRDZ2aldzQk4xQ3RVZUNuWkJGTFRwS21mYm5YMkdlWUVuX2t2VjZZcXQ?oc=5\" target=\"_blank\">A Look at AIBrix, an Open Source LLM Inference Platform</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The New Stack</font>",
      "image_url": "",
      "published": "Fri, 04 Apr 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "29d06fba37962877",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "vLLM vs TensorRT-LLM vs HF TGI vs LMDeploy, A Deep Technical Comparison for Production LLM Inference - MarkTechPost",
      "url": "https://news.google.com/rss/articles/CBMi1wFBVV95cUxOR3AwMF82VkxhakZ6d2dzSHBhdHhHejZKZFIwX2tEaDloWWJJdnY4bmp6ckwwcnFJQkRQWGk1RUR5Q1p2N3lDUjdoalhXSDJ2SG1nYTJGZGUzMlB1X2Z6WDJ1LW5xZUNhazI2b25Gak1nWmpxRXBhajRFN2d2RUh5ck9LLVREUGtzckRtZlZFanFGbTlwaU9OSTNJZlpWSVNNX0tPTW95bklpM2czYmlPM1dHbU9Ua0hXLUkxb1RhWWhYZWI4T0ZYNWllbVltQjFtMGVpZC03NNIB3AFBVV95cUxPVkhUbGtxUEE0eXRyQWxGak4wZkdHQkFTd3N0LUFsQTJzNnRfcG90MnFBcWlPSjJ4QTNKZ1NGU1FWeWxYYXNOcnZBQVlIX1gzUzFYb3FOYnk3MnJiZ2FUMUpsWS1XbjJCT01VQy1rb3UtbkhvVmd6a3d5NngwMnVOX1dkUFpIMEV4UEZMVkVzUjB4TnYzZEJhd3NwVnNiZF92WEFXbkc1MkFlNnhwNi16TVN1OER3UG16d3lZaUdlS1REQ2RUXzFsZDFsQVl0T1RLQmM1NXZ2MWMyM3pm?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi1wFBVV95cUxOR3AwMF82VkxhakZ6d2dzSHBhdHhHejZKZFIwX2tEaDloWWJJdnY4bmp6ckwwcnFJQkRQWGk1RUR5Q1p2N3lDUjdoalhXSDJ2SG1nYTJGZGUzMlB1X2Z6WDJ1LW5xZUNhazI2b25Gak1nWmpxRXBhajRFN2d2RUh5ck9LLVREUGtzckRtZlZFanFGbTlwaU9OSTNJZlpWSVNNX0tPTW95bklpM2czYmlPM1dHbU9Ua0hXLUkxb1RhWWhYZWI4T0ZYNWllbVltQjFtMGVpZC03NNIB3AFBVV95cUxPVkhUbGtxUEE0eXRyQWxGak4wZkdHQkFTd3N0LUFsQTJzNnRfcG90MnFBcWlPSjJ4QTNKZ1NGU1FWeWxYYXNOcnZBQVlIX1gzUzFYb3FOYnk3MnJiZ2FUMUpsWS1XbjJCT01VQy1rb3UtbkhvVmd6a3d5NngwMnVOX1dkUFpIMEV4UEZMVkVzUjB4TnYzZEJhd3NwVnNiZF92WEFXbkc1MkFlNnhwNi16TVN1OER3UG16d3lZaUdlS1REQ2RUXzFsZDFsQVl0T1RLQmM1NXZ2MWMyM3pm?oc=5\" target=\"_blank\">vLLM vs TensorRT-LLM vs HF TGI vs LMDeploy, A Deep Technical Comparison for Production LLM Inference</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">MarkTechPost</font>",
      "image_url": "",
      "published": "Wed, 19 Nov 2025 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "80df6e9abb99fa53",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Pliops XDP LightningAI Supercharges KV Cache to Optimize LLM Inference with NVIDIA Dynamo - StorageReview.com",
      "url": "https://news.google.com/rss/articles/CBMixAFBVV95cUxOYVo5T3R2dkFUWUxtdFhpNWZ0bjFiMElYbUJoalJLUmpEd3J1OHN0dVZjQVl0SXRseU1qdEowMGdMbGJYNURycUtkaVNRNTE3R2lPNnhUamYzT2lNWEhWckJ4Q05lR2lQelhpOGZZVHVsSHl5VWdUOWdqbS1HbXN0eUJ6dS1MY2VNXzBMUF9SYUtoQ01vU1ljSTlqS2dRNWtOU19QTUhLOWcwZ2pRLXFVMFZUbk5ISlp2R0UtODlWMnhyZGl30gHKAUFVX3lxTE5ZckM5Y0VSb3JxeGVhbF9EemxlRHRUalU2c0gtSk5DTjNtdWYyZ0dvblg5cWNsbmplYm5OcmI2ZDZfMVBiVGZ2RGpxMTdpT0RPZEpzVVdJOWwyMkJzeWZKczZZTGJfbEtRSFZPQktqY3BOWFJDVWNpb09QMWdLa21oRndrWUV6cE5zcWNoRXdBb1J3R09yU3RJT3plQVRmb2NMelh6ZGU5MXZUeG9ldmJfZVVqbko1dmVmZzhEd003MzRoOHJVRDR0MlE?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMixAFBVV95cUxOYVo5T3R2dkFUWUxtdFhpNWZ0bjFiMElYbUJoalJLUmpEd3J1OHN0dVZjQVl0SXRseU1qdEowMGdMbGJYNURycUtkaVNRNTE3R2lPNnhUamYzT2lNWEhWckJ4Q05lR2lQelhpOGZZVHVsSHl5VWdUOWdqbS1HbXN0eUJ6dS1MY2VNXzBMUF9SYUtoQ01vU1ljSTlqS2dRNWtOU19QTUhLOWcwZ2pRLXFVMFZUbk5ISlp2R0UtODlWMnhyZGl30gHKAUFVX3lxTE5ZckM5Y0VSb3JxeGVhbF9EemxlRHRUalU2c0gtSk5DTjNtdWYyZ0dvblg5cWNsbmplYm5OcmI2ZDZfMVBiVGZ2RGpxMTdpT0RPZEpzVVdJOWwyMkJzeWZKczZZTGJfbEtRSFZPQktqY3BOWFJDVWNpb09QMWdLa21oRndrWUV6cE5zcWNoRXdBb1J3R09yU3RJT3plQVRmb2NMelh6ZGU5MXZUeG9ldmJfZVVqbko1dmVmZzhEd003MzRoOHJVRDR0MlE?oc=5\" target=\"_blank\">Pliops XDP LightningAI Supercharges KV Cache to Optimize LLM Inference with NVIDIA Dynamo</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">StorageReview.com</font>",
      "image_url": "",
      "published": "Tue, 20 May 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "8b82e5565cf054fd",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Integrate and Deploy Tongyi Qwen3 Models into Production Applications with NVIDIA - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMitwFBVV95cUxQM0lnMkloWnRaZ0N2ZHhMLVZJZ0NxT2luSzM2cTlIV1Q5V2ctektxLVZPUkNlVFdFNk9wUUZIRS13bkZtaXNDdlBycGhYWkE4aFFRYS10VDhVcC1ZOWxwYUl5S21OVXFnOXF3QnFLQTYySjJDR0xKbjZseDBHcW83UkNpTWRMcGdmbVZjTENUc3RWamVhVnJZR1BEQW9WaWJlZEZWT0VwREdmS1V5SHI3Q0I3eW9LcGs?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMitwFBVV95cUxQM0lnMkloWnRaZ0N2ZHhMLVZJZ0NxT2luSzM2cTlIV1Q5V2ctektxLVZPUkNlVFdFNk9wUUZIRS13bkZtaXNDdlBycGhYWkE4aFFRYS10VDhVcC1ZOWxwYUl5S21OVXFnOXF3QnFLQTYySjJDR0xKbjZseDBHcW83UkNpTWRMcGdmbVZjTENUc3RWamVhVnJZR1BEQW9WaWJlZEZWT0VwREdmS1V5SHI3Q0I3eW9LcGs?oc=5\" target=\"_blank\">Integrate and Deploy Tongyi Qwen3 Models into Production Applications with NVIDIA</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Fri, 02 May 2025 07:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "c33276e070120850",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Spotlight: NAVER Place Optimizes SLM-Based Vertical Services with NVIDIA TensorRT-LLM | NVIDIA Technical Blog - NVIDIA Developer",
      "url": "https://news.google.com/rss/articles/CBMiuwFBVV95cUxNT0VMakpFbDNPRExCb1c1SFNjWnhWUW9BcnA1YWVXLUU0MXJVdVpUMk9TSzRoN2x5cXlwbVVmTi1jNjh6QlBMV1V2bV9kdmVkaFB0OWpVSGtHRFBEVTUxWVFGQmpvSEU4d2RRbWNtWVlpblVFLW5Mb01BLVBjT2Zsd3lidzJYVDRPMV9WczlQWW0xUHdXWjRxcEtuTTBBMkdEME9SaG9haU9Kc1lncWtQUXhzN01pMU9jNVgw?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMiuwFBVV95cUxNT0VMakpFbDNPRExCb1c1SFNjWnhWUW9BcnA1YWVXLUU0MXJVdVpUMk9TSzRoN2x5cXlwbVVmTi1jNjh6QlBMV1V2bV9kdmVkaFB0OWpVSGtHRFBEVTUxWVFGQmpvSEU4d2RRbWNtWVlpblVFLW5Mb01BLVBjT2Zsd3lidzJYVDRPMV9WczlQWW0xUHdXWjRxcEtuTTBBMkdEME9SaG9haU9Kc1lncWtQUXhzN01pMU9jNVgw?oc=5\" target=\"_blank\">Spotlight: NAVER Place Optimizes SLM-Based Vertical Services with NVIDIA TensorRT-LLM | NVIDIA Technical Blog</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">NVIDIA Developer</font>",
      "image_url": "",
      "published": "Fri, 28 Feb 2025 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "40305dc0dcb0e422",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Optimizing AI responsiveness: A practical guide to Amazon Bedrock latency-optimized inference - Amazon Web Services (AWS)",
      "url": "https://news.google.com/rss/articles/CBMi1gFBVV95cUxNaHR1X2dOZzJubWI4ME5pNjJpVXYyeTlUb2t1RGVaMEtrdFd6Q2NkY2ppZmEtUHI4YlV0WVBFMHFTcXk4RFdjNmtxelBmNF9pYzN2UXFuRzM3WXpDSFBzSWZkQWVMX0RRTFh1ZjdfekxNZXhRTTgwdHZ2RTZpeGhTTk1wTVgtSTVFcHg5QXg1a1RwWjVva19ReFJ1OXZxYVo1RXVEckRaMExmeEhnMkZ3R1I1MHd0ODczV0lLemVQNDVUT3BJYUotMGpDMVZwbllERV9vQjV3?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi1gFBVV95cUxNaHR1X2dOZzJubWI4ME5pNjJpVXYyeTlUb2t1RGVaMEtrdFd6Q2NkY2ppZmEtUHI4YlV0WVBFMHFTcXk4RFdjNmtxelBmNF9pYzN2UXFuRzM3WXpDSFBzSWZkQWVMX0RRTFh1ZjdfekxNZXhRTTgwdHZ2RTZpeGhTTk1wTVgtSTVFcHg5QXg1a1RwWjVva19ReFJ1OXZxYVo1RXVEckRaMExmeEhnMkZ3R1I1MHd0ODczV0lLemVQNDVUT3BJYUotMGpDMVZwbllERV9vQjV3?oc=5\" target=\"_blank\">Optimizing AI responsiveness: A practical guide to Amazon Bedrock latency-optimized inference</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services (AWS)</font>",
      "image_url": "",
      "published": "Tue, 28 Jan 2025 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "806166f674564638",
      "source": "search_llm_ops_news",
      "source_weight": 0.8,
      "title": "Scaling your LLM inference workloads: multi-node deployment with TensorRT-LLM and Triton on Amazon EKS - Amazon Web Services (AWS)",
      "url": "https://news.google.com/rss/articles/CBMi0AFBVV95cUxOQVg3RDdidllJYzVPM3RWMHBFbG1nVmxrbW1hMlhhUkNLcVEtck44R1RURU5Bclg4UGt3WXJlTnh0UXlnRW9RRVNXMkRneUJCc1JCZ0s2VmdydHp4cEZVSUx2d3ZERU5VeWFOeS05NlhSenljMG1IR1Juc250azFGZ25ST1NBMGRFMURvWXNTaW5XNFVrY3pBZ2JCdS1rSUgyN2xMblY4T1h2cE9NakoyRGFjc1VYaE1XWEl6UlBkenQtSk1GSHAtQko3eV9fU1Jh?oc=5",
      "summary": "<a href=\"https://news.google.com/rss/articles/CBMi0AFBVV95cUxOQVg3RDdidllJYzVPM3RWMHBFbG1nVmxrbW1hMlhhUkNLcVEtck44R1RURU5Bclg4UGt3WXJlTnh0UXlnRW9RRVNXMkRneUJCc1JCZ0s2VmdydHp4cEZVSUx2d3ZERU5VeWFOeS05NlhSenljMG1IR1Juc250azFGZ25ST1NBMGRFMURvWXNTaW5XNFVrY3pBZ2JCdS1rSUgyN2xMblY4T1h2cE9NakoyRGFjc1VYaE1XWEl6UlBkenQtSk1GSHAtQko3eV9fU1Jh?oc=5\" target=\"_blank\">Scaling your LLM inference workloads: multi-node deployment with TensorRT-LLM and Triton on Amazon EKS</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Amazon Web Services (AWS)</font>",
      "image_url": "",
      "published": "Mon, 02 Dec 2024 08:00:00 GMT",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.841,
      "freshness": 0.0,
      "tier1_quick_score": 1.641
    },
    {
      "id": "18ed65b18f8661b8",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Customize AI agent browsing with proxies, profiles, and extensions in Amazon Bedrock AgentCore Browser",
      "url": "https://aws.amazon.com/blogs/machine-learning/customize-ai-agent-browsing-with-proxies-profiles-and-extensions-in-amazon-bedrock-agentcore-browser/",
      "summary": "Today, we are announcing three new capabilities that address these requirements: proxy configuration, browser profiles, and browser extensions. Together, these features give you fine-grained control over how your AI agents interact with the web. This post will walk through each capability with configuration examples and practical use cases to help you get started.",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 22:57:34 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.17,
      "tier1_quick_score": 1.607
    },
    {
      "id": "3bf9086fd27aeeb5",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "9 fun questions to try asking Google Photos",
      "url": "https://blog.google/products-and-platforms/products/photos/ask-button-ask-photos-tips/",
      "summary": "A collage of outdoor images, a blue icon that say \"Ask Photos,\" and examples of Ask Photos prompts.",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2025-Travel-Trends_hero.max-600x600.format-webp.webp",
      "published": "Tue, 10 Feb 2026 17:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.058,
      "tier1_quick_score": 1.595
    },
    {
      "id": "65afd187aa2ff455",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Helping kids and teens learn and grow online on Safer Internet Day",
      "url": "https://blog.google/innovation-and-ai/technology/safety-security/safer-internet-day-2026-kids-teens/",
      "summary": "User profile on smartphone connected to security, media, and settings icons.",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Helpingkidsteens_Hero.max-600x600.format-webp.webp",
      "published": "Tue, 10 Feb 2026 02:30:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.047,
      "tier1_quick_score": 1.584
    },
    {
      "id": "caf0f5862fc0931a",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "AI meets HR: Transforming talent acquisition with Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/ai-meets-hr-transforming-talent-acquisition-with-amazon-bedrock/",
      "summary": "In this post, we show how to create an AI-powered recruitment system using Amazon Bedrock, Amazon Bedrock Knowledge Bases, AWS Lambda, and other AWS services to enhance job description creation, candidate communication, and interview preparation while maintaining human oversight.",
      "image_url": "",
      "published": "Thu, 12 Feb 2026 20:18:58 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.117,
      "tier1_quick_score": 1.554
    },
    {
      "id": "ce4da08a3b944062",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Build long-running MCP servers on Amazon Bedrock AgentCore with Strands Agents integration",
      "url": "https://aws.amazon.com/blogs/machine-learning/build-long-running-mcp-servers-on-amazon-bedrock-agentcore-with-strands-agents-integration/",
      "summary": "In this post, we provide you with a comprehensive approach to achieve this. First, we introduce a context message strategy that maintains continuous communication between servers and clients during extended operations. Next, we develop an asynchronous task management framework that allows your AI agents to initiate long-running processes without blocking other operations. Finally, we demonstrate how to bring these strategies together with Amazon Bedrock AgentCore and Strands Agents to build production-ready AI agents that can handle complex, time-intensive operations reliably.",
      "image_url": "",
      "published": "Thu, 12 Feb 2026 20:16:20 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.117,
      "tier1_quick_score": 1.554
    },
    {
      "id": "e5dc30bcbd963b92",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Natively Adaptive Interfaces: A new framework for AI accessibility",
      "url": "https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/",
      "summary": "A collage of four images, the first of a woman with curly hair in front of a silver laptop, the second of the same woman and a man with short black hair speaking on a stairwell, the third of a the same man with glasses, and an aerial image of NTID",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Natively_Adaptive_Interfaces_He.max-600x600.format-webp.webp",
      "published": "Thu, 05 Feb 2026 17:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.011,
      "tier1_quick_score": 1.548
    },
    {
      "id": "0b7dc60d8af90d37",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "How Google Cloud is helping Team USA elevate their tricks with AI",
      "url": "https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/",
      "summary": "A woman outdoors in the snow looks at a tablet. A half pipe is behind her.",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/helping_Team_USA_Hero.max-600x600.format-webp.webp",
      "published": "Thu, 05 Feb 2026 16:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.011,
      "tier1_quick_score": 1.548
    },
    {
      "id": "26a63f74613e6671",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Watch our new Gemini ad ahead of football’s biggest weekend",
      "url": "https://blog.google/company-news/inside-google/company-announcements/gemini-ad-new-home/",
      "summary": "A toddler in a blue and yellow striped shirt sits on a kitchen counter eating a red apple. Text in the corner reads: 'New Home, Google Gemini SB Commercial’",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/SB_2026_New_Home_16_9_Thumbnail.max-600x600.format-webp.webp",
      "published": "Thu, 05 Feb 2026 14:30:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.011,
      "tier1_quick_score": 1.548
    },
    {
      "id": "2dede2dbdc9c1702",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "The latest AI news we announced in January",
      "url": "https://blog.google/innovation-and-ai/products/google-ai-updates-january-2026/",
      "summary": "mp4 showing a carousel of images including a card reading \"Help that's made for you\"",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/LatestAI_v5.max-600x600.format-webp.webp",
      "published": "Wed, 04 Feb 2026 16:55:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.008,
      "tier1_quick_score": 1.545
    },
    {
      "id": "0fc6449cf17e4c48",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "How we’re helping preserve the genetic information of endangered species with AI",
      "url": "https://blog.google/innovation-and-ai/technology/ai/ai-to-preserve-endangered-species/",
      "summary": "A four-part vertical collage showing a cotton-top tamarin, an ibex, a golden lion tamarin, and a penguin.",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Terria-Clay_Collage_hero.max-600x600.format-webp.webp",
      "published": "Mon, 02 Feb 2026 18:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.004,
      "tier1_quick_score": 1.541
    },
    {
      "id": "9233762a8b5309dd",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Advancing AI benchmarking with Game Arena",
      "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
      "summary": "An illustration of a King and Ace playing card, a wolf's head, two chess pieces, a poker chip, and other abstract shapes on a white background.1",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/kaggle_Gsmes_Hero.png",
      "published": "Mon, 02 Feb 2026 17:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.004,
      "tier1_quick_score": 1.541
    },
    {
      "id": "6f71c08f77142fcb",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Project Genie: Experimenting with infinite, interactive worlds",
      "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/",
      "summary": "Text reads Introducing Project Genie",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/genie-3__project-genie__hero-fi.max-600x600.format-webp_d7CisM6.webp",
      "published": "Thu, 29 Jan 2026 17:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.538
    },
    {
      "id": "29936bb569042f97",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Hear more about interactive world models in our latest podcast.",
      "url": "https://blog.google/innovation-and-ai/technology/ai/release-notes-podcast-project-genie/",
      "summary": "Project Genie: Create and explore worlds",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/RN25_Episode_Thumbnail.max-600x600.format-webp.webp",
      "published": "Thu, 29 Jan 2026 15:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.538
    },
    {
      "id": "9b9db1424f88a8b6",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Google AI Plus is now available everywhere our AI plans are available, including the U.S.",
      "url": "https://blog.google/products-and-platforms/products/google-one/google-ai-plus-availability/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Plus_Hero_Visual_2096.max-600x600.format-webp_4ffr2GI.webp\" />We’re launching Google AI Plus in 35 new countries and territories including the US, making it available everywhere Google AI plans are available.",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Plus_Hero_Visual_2096.max-600x600.format-webp_4ffr2GI.webp",
      "published": "Tue, 27 Jan 2026 18:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.538
    },
    {
      "id": "a4e42dbf35ad9723",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "Just ask anything: a seamless new Search experience",
      "url": "https://blog.google/products-and-platforms/products/search/ai-mode-ai-overviews-updates/",
      "summary": "A centered, elongated oval shape resembling a search bar with the text \"Ask anything\" inside it.",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Seamless_Search_-_Blog_header.max-600x600.format-webp.webp",
      "published": "Tue, 27 Jan 2026 17:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.538
    },
    {
      "id": "f6ce5d131c6814c1",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "In our latest podcast, hear how the “Smoke Jumpers” team brings Gemini to billions of people.",
      "url": "https://blog.google/products-and-platforms/products/gemini/release-notes-podcast-smokejumpers/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/thumbnails_EP24_002_ccRelease_N.max-600x600.format-webp.webp\" />Bringing Gemini to billions of users requires a massive, coordinated infrastructure effort. In the latest episode of the Google AI: Release Notes podcast, host Logan Kil…",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/thumbnails_EP24_002_ccRelease_N.max-600x600.format-webp.webp",
      "published": "Tue, 27 Jan 2026 10:28:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.537
    },
    {
      "id": "a5a028261628d3f1",
      "source": "google_ai_blog",
      "source_weight": 0.7,
      "title": "How animators and AI researchers made ‘Dear Upstairs Neighbors’",
      "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/dear-upstairs-neighbors/",
      "summary": "A movie poster for a film titled “Dear Upstairs Neighbors”, hand-painted in a vivid expressionist style, featuring an exasperated cartoon woman clutching her ears, surrounded by neon-colored images of noisy things like howling dogs and stomping shoes",
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DUN_poster_16x9_v02.max-600x600.format-webp.webp",
      "published": "Mon, 26 Jan 2026 18:00:00 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.537
    },
    {
      "id": "31398854086f7b11",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "NVIDIA Nemotron 3 Nano 30B MoE model is now available in Amazon SageMaker JumpStart",
      "url": "https://aws.amazon.com/blogs/machine-learning/nvidia-nemotron-3-nano-30b-is-now-available-in-amazon-sagemaker-jumpstart/",
      "summary": "Today we’re excited to announce that the NVIDIA Nemotron 3 Nano 30B model with &nbsp;3B active parameters is now generally available in the Amazon SageMaker JumpStart model catalog. You can accelerate innovation and deliver tangible business value with Nemotron 3 Nano on Amazon Web Services (AWS) without having to manage model deployment complexities. You can power your generative AI applications with Nemotron capabilities using the managed deployment capabilities offered by SageMaker JumpStart.",
      "image_url": "",
      "published": "Wed, 11 Feb 2026 19:38:47 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.083,
      "tier1_quick_score": 1.52
    },
    {
      "id": "f7ecc60851a906e9",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Swann provides Generative AI to millions of IoT Devices using Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/swann-provides-generative-ai-to-millions-of-iot-devices-using-amazon-bedrock/",
      "summary": "This post shows you how to implement intelligent notification filtering using Amazon Bedrock and its gen-AI capabilities. You'll learn model selection strategies, cost optimization techniques, and architectural patterns for deploying gen-AI at IoT scale, based on Swann Communications deployment across millions of devices.",
      "image_url": "",
      "published": "Wed, 11 Feb 2026 15:48:15 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.079,
      "tier1_quick_score": 1.516
    },
    {
      "id": "05f8faf1ec331d76",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "How LinqAlpha assesses investment theses using Devil’s Advocate on Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-linqalpha-assesses-investment-theses-using-devils-advocate-on-amazon-bedrock/",
      "summary": "LinqAlpha is a Boston-based multi-agent AI system built specifically for institutional investors. The system supports and streamlines agentic workflows across company screening, primer generation, stock price catalyst mapping, and now, pressure-testing investment ideas through a new AI agent called Devil’s Advocate. In this post, we share how LinqAlpha uses Amazon Bedrock to build and scale Devil’s Advocate.",
      "image_url": "",
      "published": "Wed, 11 Feb 2026 15:45:30 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.079,
      "tier1_quick_score": 1.516
    },
    {
      "id": "3ca09597f05bfb41",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "How Amazon uses Amazon Nova models to automate operational readiness testing for new fulfillment centers",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-amazon-uses-amazon-nova-models-to-automate-operational-readiness-testing-for-new-fulfillment-centers/",
      "summary": "In this post, we discuss how&nbsp;Amazon Nova&nbsp;in&nbsp;Amazon Bedrock&nbsp;can be used to implement an AI-powered image recognition solution that automates the detection and validation of module components, significantly reducing manual verification efforts and improving accuracy.",
      "image_url": "",
      "published": "Tue, 10 Feb 2026 18:34:09 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.059,
      "tier1_quick_score": 1.496
    },
    {
      "id": "25156d2be403a27d",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Iberdrola enhances IT operations using Amazon Bedrock AgentCore",
      "url": "https://aws.amazon.com/blogs/machine-learning/iberdrola-enhances-it-operations-using-amazon-bedrock-agentcore/",
      "summary": "Iberdrola, one of the world’s largest utility companies, has embraced cutting-edge AI technology to revolutionize its IT operations in ServiceNow. Through its partnership with AWS, Iberdrola implemented different agentic architectures using Amazon Bedrock AgentCore, targeting three key areas: optimizing change request validation in the draft phase, enriching incident management with contextual intelligence, and simplifying change model selection using conversational AI. These innovations reduce bottlenecks, help teams accelerate ticket resolution, and deliver consistent and high-quality data handling throughout the organization.",
      "image_url": "",
      "published": "Tue, 10 Feb 2026 18:31:57 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.059,
      "tier1_quick_score": 1.496
    },
    {
      "id": "51378656ffa229b1",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Building real-time voice assistants with Amazon Nova Sonic compared to cascading architectures",
      "url": "https://aws.amazon.com/blogs/machine-learning/building-real-time-voice-assistants-with-amazon-nova-sonic-compared-to-cascading-architectures/",
      "summary": "Amazon Nova Sonic&nbsp;delivers real-time, human-like voice conversations through the bidirectional streaming interface. In this post, you learn how Amazon Nova Sonic can solve some of the challenges faced by cascaded approaches, simplify building voice AI agents, and provide natural conversational capabilities. We also provide guidance on when to choose each approach to help you make informed decisions for your voice AI projects.",
      "image_url": "",
      "published": "Tue, 10 Feb 2026 18:29:05 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.059,
      "tier1_quick_score": 1.496
    },
    {
      "id": "a075dbc7d77bca68",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Automated Reasoning checks rewriting chatbot reference implementation",
      "url": "https://aws.amazon.com/blogs/machine-learning/automated-reasoning-checks-rewriting-chatbot-reference-implementation/",
      "summary": "This blog post dives deeper into the implementation architecture for the Automated Reasoning checks rewriting chatbot.",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 19:34:05 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.043,
      "tier1_quick_score": 1.48
    },
    {
      "id": "22c42c0a18e7784d",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Scale LLM fine-tuning with Hugging Face and Amazon SageMaker AI",
      "url": "https://aws.amazon.com/blogs/machine-learning/scale-llm-fine-tuning-with-hugging-face-and-amazon-sagemaker-ai/",
      "summary": "In this post, we show how this integrated approach transforms enterprise LLM fine-tuning from a complex, resource-intensive challenge into a streamlined, scalable solution for achieving better model performance in domain-specific applications.",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 16:48:46 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.041,
      "tier1_quick_score": 1.478
    },
    {
      "id": "5e64b0122203008f",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "New Relic transforms productivity with generative AI on AWS",
      "url": "https://aws.amazon.com/blogs/machine-learning/new-relic-transforms-productivity-with-generative-ai-on-aws/",
      "summary": "Working with the Generative AI Innovation Center, New Relic NOVA (New Relic Omnipresence Virtual Assistant) evolved from a knowledge assistant into a comprehensive productivity engine. We explore the technical architecture, development journey, and key lessons learned in building an enterprise-grade AI solution that delivers measurable productivity gains at scale.",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 16:45:16 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.041,
      "tier1_quick_score": 1.478
    },
    {
      "id": "a98185712084d329",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Accelerate agentic application development with a full-stack starter template for Amazon Bedrock AgentCore",
      "url": "https://aws.amazon.com/blogs/machine-learning/accelerate-agentic-application-development-with-a-full-stack-starter-template-for-amazon-bedrock-agentcore/",
      "summary": "In this post, you will learn how to deploy Fullstack AgentCore Solution Template (FAST) to your Amazon Web Services (AWS) account, understand its architecture, and see how to extend it for your requirements. You will learn how to build your own agent while FAST handles authentication, infrastructure as code (IaC), deployment pipelines, and service integration.",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 16:40:58 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.041,
      "tier1_quick_score": 1.478
    },
    {
      "id": "da0df0d660b4dc54",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Agent-to-agent collaboration: Using Amazon Nova 2 Lite and Amazon Nova Act for multi-agent systems",
      "url": "https://aws.amazon.com/blogs/machine-learning/agent-to-agent-collaboration-using-amazon-nova-2-lite-and-amazon-nova-act-for-multi-agent-systems/",
      "summary": "This post walks through how agent-to-agent collaboration on Amazon Bedrock works in practice, using Amazon Nova 2 Lite for planning and Amazon Nova Act for browser interaction, to turn a fragile single-agent setup into a predictable multi-agent system.",
      "image_url": "",
      "published": "Mon, 09 Feb 2026 16:00:28 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.041,
      "tier1_quick_score": 1.478
    },
    {
      "id": "79370d2495557126",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Structured outputs on Amazon Bedrock: Schema-compliant AI responses",
      "url": "https://aws.amazon.com/blogs/machine-learning/structured-outputs-on-amazon-bedrock-schema-compliant-ai-responses/",
      "summary": "Today, we're announcing structured outputs on Amazon Bedrock—a capability that fundamentally transforms how you can obtain validated JSON responses from foundation models through constrained decoding for schema compliance. In this post, we explore the challenges of traditional JSON generation and how structured outputs solves them. We cover the two core mechanisms—JSON Schema output format and strict tool use—along with implementation details, best practices, and practical code examples.",
      "image_url": "",
      "published": "Fri, 06 Feb 2026 20:12:14 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.016,
      "tier1_quick_score": 1.453
    },
    {
      "id": "157be0691604e576",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Manage Amazon SageMaker HyperPod clusters using the HyperPod CLI and SDK",
      "url": "https://aws.amazon.com/blogs/machine-learning/manage-amazon-sagemaker-hyperpod-clusters-using-the-hyperpod-cli-and-sdk/",
      "summary": "In this post, we demonstrate how to use the CLI and the SDK to create and manage SageMaker HyperPod clusters in your AWS account. We walk through a practical example and dive deeper into the user workflow and parameter choices.",
      "image_url": "",
      "published": "Fri, 06 Feb 2026 19:27:45 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.016,
      "tier1_quick_score": 1.453
    },
    {
      "id": "c1e45689c97e4666",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "Evaluate generative AI models with an Amazon Nova rubric-based LLM judge on Amazon SageMaker AI (Part 2)",
      "url": "https://aws.amazon.com/blogs/machine-learning/evaluate-generative-ai-models-with-an-amazon-nova-rubric-based-llm-judge-on-amazon-sagemaker-ai-part-2/",
      "summary": "In this post, we explore the Amazon Nova rubric-based judge feature: what a rubric-based judge is, how the judge is trained, what metrics to consider, and how to calibrate the judge. We chare notebook code of the Amazon Nova rubric-based LLM-as-a-judge methodology to evaluate and compare the outputs of two different LLMs using SageMaker training jobs.",
      "image_url": "",
      "published": "Fri, 06 Feb 2026 16:29:45 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.015,
      "tier1_quick_score": 1.452
    },
    {
      "id": "3e9f373e384ab4b1",
      "source": "aws_ml_blog",
      "source_weight": 0.6,
      "title": "How Associa transforms document classification with the GenAI IDP Accelerator and Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-associa-transforms-document-classification-with-the-genai-idp-accelerator-and-amazon-bedrock/",
      "summary": "Associa collaborated with the AWS Generative AI Innovation Center to build a generative AI-powered document classification system aligning with Associa’s long-term vision of using generative AI to achieve operational efficiencies in document management. The solution automatically categorizes incoming documents with high accuracy, processes documents efficiently, and provides substantial cost savings while maintaining operational excellence. The document classification system, developed using the Generative AI Intelligent Document Processing (GenAI IDP) Accelerator, is designed to integrate seamlessly into existing workflows. It revolutionizes how employees interact with document management systems by reducing the time spent on manual classification tasks.",
      "image_url": "",
      "published": "Thu, 05 Feb 2026 20:41:52 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.011,
      "tier1_quick_score": 1.448
    },
    {
      "id": "6f3fb19c6ec9818d",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "New SemiAnalysis InferenceX Data Shows NVIDIA Blackwell Ultra Delivers up to 50x Better Performance and 35x Lower Costs for Agentic AI",
      "url": "https://blogs.nvidia.com/blog/data-blackwell-ultra-performance-lower-cost-agentic-ai/",
      "summary": "The NVIDIA Blackwell platform has been widely adopted by leading inference providers such as Baseten, DeepInfra, Fireworks AI and Together AI to reduce cost per token by up to 10x. Now, the NVIDIA Blackwell Ultra platform is taking this momentum further for agentic AI. AI agents and coding assistants are driving explosive growth in software-programming-related\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/data-blackwell-ultra-performance-lower-cost-agentic-ai/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/inference-charts-inferencemax-v1.5-perf-charts-4753570-r12_1-alt-scaled.png",
      "published": "Mon, 16 Feb 2026 17:00:40 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.426,
      "tier1_quick_score": 1.413
    },
    {
      "id": "6e2225d549ed5ae2",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.16.0",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0",
      "summary": "<h1>vLLM v0.16.0</h1>\n<h2>Highlights</h2>\n<p>This release features 440 commits from 203 contributors (7 new)!</p>\n<ul>\n<li><strong>PyTorch 2.10 upgrade</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>). This is a breaking change for environment dependency.</li>\n<li><strong>Async scheduling + Pipeline Parallelism</strong> is now fully supported, delivering <strong>30.8% E2E throughput improvement</strong> and <strong>31.8% TPOT improvement</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>).</li>\n<li><strong>Realtime API</strong>: A new WebSocket-based Realtime API enables streaming audio interactions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>), building on the Voxtral realtime infrastructure.</li>\n<li><strong>RLHF workflow improvements</strong>: Native NCCL-based weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise weight reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), and engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li><strong>Unified Parallel Drafting</strong> for speculative decoding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), plus spec decode now works with structured outputs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>) and penalty application in Model Runner V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>).</li>\n<li><strong>Major XPU platform overhaul</strong>: Deprecated IPEX in favor of vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>), adding MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), and FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>) support.</li>\n</ul>\n<h3>Model Support</h3>\n<ul>\n<li>New architectures: GLM-OCR with MTP (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33005\">#33005</a>), Qwen3-ASR (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33312\">#33312</a>), DeepSeek-OCR-2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33165\">#33165</a>), Intern-S1-Pro (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33636\">#33636</a>), MiniCPM-o 4.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33431\">#33431</a>), openPangu7B-VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32449\">#32449</a>), NemotronHPuzzle heterogeneous (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32549\">#32549</a>), MusicFlamingo (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32696\">#32696</a>), FunAudioChat (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/2\">#2</a>), ColBERT late interaction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33686\">#33686</a>), voyage-4-nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33720\">#33720</a>), GLM-5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34124\">#34124</a>).</li>\n<li>Speculative decoding: EAGLE3 for Hunyuan/HunyuanVL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33035\">#33035</a>), AFMoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33111\">#33111</a>), Mistral3 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a>).</li>\n<li>LoRA expansion: Gemma3 vision components (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32764\">#32764</a>), Nemotron-H MTP models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32265\">#32265</a>), Qwen3 output embedding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29816\">#29816</a>). Optimized fused MoE-LoRA kernel indexing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32770\">#32770</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32774\">#32774</a>), unpermute-aware fused MoE LoRA path (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32655\">#32655</a>), reduced kernel overhead for fewer active LoRAs with multiple CUDA graphs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32005\">#32005</a>).</li>\n<li>Features: Qwen3-Omni transcription (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29828\">#29828</a>), Mistral Large 3 with FlashInfer MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33174\">#33174</a>), LFM2 SigLIP2 intermediate encoder layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33370\">#33370</a>), Qwen3-Omni/GLM-4.xV MRoPE positioning fixes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33010\">#33010</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33039\">#33039</a>), embedding input for disabled modalities (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32493\">#32493</a>).</li>\n<li>Performance: GLM-4.7-GPTQ decode and MTP acceptance rate regression fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a>), DeepSeek V3.2 fast detokenization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33855\">#33855</a>), DeepSeek V3.2 tokenizer fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33832\">#33832</a>), GLM-5 MTP accuracy fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34385\">#34385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li>Async scheduling + Pipeline Parallelism: Full support with 30.8% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>), optimized spec decode + async scheduling with 1.5% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33612\">#33612</a>), deadlock fix for torchrun PP broadcast (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33701\">#33701</a>).</li>\n<li>Speculative decoding: Unified Parallel Drafting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), structured output support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>), penalty application in MRV2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>), skip softmax for all-greedy rejection sampling (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32852\">#32852</a>), correctness fix for spec tokens with prefill chunks (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33652\">#33652</a>).</li>\n<li>RLHF: Native NCCL weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li>Helion kernel framework: ConfigManager (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32740\">#32740</a>), kernel wrapper (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32964\">#32964</a>), kernel registry (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33203\">#33203</a>).</li>\n<li>PluggableLayer: Applied to linear layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33152\">#33152</a>) and Mamba layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33660\">#33660</a>).</li>\n<li>Batch invariance: Disable Cascade Attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32561\">#32561</a>), enable Triton attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33688\">#33688</a>).</li>\n<li>Performance: Grammar bitmask H2D copy on separate stream (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33059\">#33059</a>), zero-copy GQA for multimodal and CPU (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33732\">#33732</a>), early-reject oversized MM requests (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33502\">#33502</a>), CPU memory leak fix from Request reference cycle in prefix caching (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34183\">#34183</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<ul>\n<li><strong>NVIDIA</strong>: FlashInfer TRTLLM BF16 MoE integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32954\">#32954</a>), SM100 INT4 W4A16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32437\">#32437</a>), SM121 (DGX Spark) CUTLASS support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a>), MNNVL protocol for GB series (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33540\">#33540</a>), FlashInfer MLA concat optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31171\">#31171</a>), GDN attention layout optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33291\">#33291</a>), DeepGEMM FP8 MLA performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33568\">#33568</a>), wvSplitK_fp8 performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33527\">#33527</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33493\">#33493</a>), B200 MoE configs for Nemotron Nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32804\">#32804</a>), Super B200 TP2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33510\">#33510</a>), GLM 4.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32958\">#32958</a>), Mamba selective scan tuning for B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32873\">#32873</a>). Fix: DeepSeek R1 CUTLASS MLA on B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33637\">#33637</a>), QK Norm+RoPE fusion on B200+FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a>), CUTLASS FP8 blockwise on SM103a (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32224\">#32224</a>).</li>\n<li><strong>AMD ROCm</strong>: QWEN3-NEXT FP8 tunings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32042\">#32042</a>), AITER attention backend for Qwen3-Next (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32492\">#32492</a>), fused_add_rmsnorm_pad for GPT-OSS (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30976\">#30976</a>), Qwen3-Omni startup fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33077\">#33077</a>).</li>\n<li><strong>Intel XPU</strong>: Platform overhaul - deprecated IPEX, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>). New: unquantized MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>).</li>\n<li><strong>ARM CPU</strong>: KleidiAI INT4 dynamic quant with BF16 activations (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33122\">#33122</a>), NEON BFMMLA BF16 paged attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32263\">#32263</a>), vectorization backend optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30329\">#30329</a>), attention dispatch by head_dim alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32161\">#32161</a>).</li>\n<li><strong>IBM Z</strong>: BF16 kernel type for s390x (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33788\">#33788</a>).</li>\n<li><strong>torch.compile</strong>: Stop compiling identical artifacts (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34003\">#34003</a>), MoE cold start optimization option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33735\">#33735</a>), fix 32-bit indexing assumption (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33113\">#33113</a>), attention fusion pass fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33945\">#33945</a>).</li>\n<li><strong>Performance</strong>: Chat completion streaming optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33782\">#33782</a>), ORJSONResponse for faster API responses (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33548\">#33548</a>), MoE permute optimization for CUTLASS FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32892\">#32892</a>), shared/routed overlap for latent MoE on Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32790\">#32790</a>), FlashInfer autotune control flag (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34006\">#34006</a>).</li>\n</ul>\n<h3>Large Scale Serving</h3>\n<ul>\n<li>Disaggregated serving: Mooncake connector rework with bootstrap server (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31034\">#31034</a>), cross-layer KV cache layout at NIXL Connector V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33339\">#33339</a>), delay freeing blocks for aborted async loads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32255\">#32255</a>), async double-free fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33377\">#33377</a>), Ray multi-replica single-instance fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a>).</li>\n<li>EPLB: Capture logical experts with router replay (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33013\">#33013</a>), DP metadata fix for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32739\">#32739</a>).</li>\n<li>Metrics: KV offloading connector metrics (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/27942\">#27942</a>), labeled prompt token metrics for P/D disaggregation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33290\">#33290</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li>New: FP8 block quant for CompressedTensorsW8A16Fp8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33280\">#33280</a>), ModelOpt MXFP8 for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33786\">#33786</a>), NVFP4/FP8 on Turing GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33076\">#33076</a>), TP &gt; 4 for FP4 Gemm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31099\">#31099</a>).</li>\n<li>Bugfixes: FP8 online quantization memory fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31914\">#31914</a>), asymmetric W4A16 (ConchLinear) for CT (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33200\">#33200</a>), DeepSeek V3.2 NVFP4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33932\">#33932</a>), LoRA FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33879\">#33879</a>), quantized Falcon-H1 model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32728\">#32728</a>), quantized Mamba TP with n_groups=1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33257\">#33257</a>), CPU W8A8 with bias (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33582\">#33582</a>), CPU W8A8 3D input support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33727\">#33727</a>).</li>\n<li><strong>Deprecation</strong>: Removed BitBlas (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Realtime API</strong>: WebSocket-based streaming API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>) with Voxtral realtime support.</li>\n<li><strong>Responses API</strong>: Sampling parameters (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32609\">#32609</a>), return token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33212\">#33212</a>), return prompt token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33378\">#33378</a>), parser implementation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32712\">#32712</a>).</li>\n<li>Pooling API: Request schema consensus for ScoreRequest (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33060\">#33060</a>) and final standardization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31127\">#31127</a>).</li>\n<li>Tool calling: Fix multi-turn tool call ID preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32768\">#32768</a>), fix indexing double-counting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33141\">#33141</a>), GLM-4 incremental string streaming (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33218\">#33218</a>), DSV3.2 fast detokenization fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33964\">#33964</a>), MCP tools non-streaming fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32762\">#32762</a>).</li>\n<li>Structured outputs: Performance optimization with reasoning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33557\">#33557</a>), guidance vocab size fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33509\">#33509</a>).</li>\n<li>CLI: <code>--disable-access-log-for-endpoints</code> option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30011\">#30011</a>).</li>\n<li>UX: Nested configs in YAML files (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33193\">#33193</a>), GGUF <code>repo_id:quant_type</code> syntax (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33371\">#33371</a>), DeepSeek ReasoningParser with thinking enabled by default (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33221\">#33221</a>), remove noisy CT warning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33273\">#33273</a>), early tokenization validation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31366\">#31366</a>), reasoning_content backward compatibility (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33635\">#33635</a>), only include Authorization header when OPENAI_API_KEY is set (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33488\">#33488</a>).</li>\n<li>Features: run_batch transcription/translation support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33934\">#33934</a>), /server_info collect_env (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33246\">#33246</a>), OTEL tracing during model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31162\">#31162</a>), clear MM and encoder cache (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33452\">#33452</a>), HF Hub LoRA resolver (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/20320\">#20320</a>).</li>\n<li>Scoring: Fix multi-document scoring returning single result (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33837\">#33837</a>).</li>\n</ul>\n<h3>Security</h3>\n<ul>\n<li>Patch protobuf for <a href=\"https://github.com/advisories/GHSA-7gcm-g887-7qv7\" title=\"CVE-2026-0994\">CVE-2026-0994</a> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34253\">#34253</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li><strong>PyTorch 2.10</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>) - breaking change for environment dependency.</li>\n<li>huggingface-hub updates for Transformers v5 preparation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33473\">#33473</a>).</li>\n<li>Transformers v5 compatibility fixes across multiple models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33977\">#33977</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33683\">#33683</a>).</li>\n</ul>\n<h3>Deprecation &amp; Breaking Changes</h3>\n<ul>\n<li>Removed BitBlas quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n<li>Removed deprecated <code>reasoning_content</code> message field (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33402\">#33402</a>).</li>\n<li>Removed deprecated pooling items (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33477\">#33477</a>).</li>\n<li>Removed deprecated <code>VLLM_ALL2ALL_BACKEND</code> environment variable (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33535\">#33535</a>).</li>\n<li>Deprecated IPEX for XPU, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/aabbccddwasd\">@aabbccddwasd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Code4me2\">@Code4me2</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikchifo\">@ikchifo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jiangwu300\">@jiangwu300</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/pjs102793\">@pjs102793</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33963\">#33963</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sleepcoo\">@sleepcoo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33978\">#33978</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TundeAtSN\">@TundeAtSN</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a></li>\n</ul>",
      "image_url": "",
      "published": "2026-02-13T06:13:20Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.135,
      "tier1_quick_score": 1.222
    },
    {
      "id": "2cfc1c73d3671883",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.16.0rc3: [Bugfix] Fix MTP accuracy for GLM-5 (#34385)",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc3",
      "summary": "<p>Signed-off-by: mgoin <a href=\"mailto:mgoin64@gmail.com\">mgoin64@gmail.com</a><br />\n(cherry picked from commit <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/commit/ec12d39d44739bee408ec1473acc09e75daf1a5d\"><tt>ec12d39</tt></a>)</p>",
      "image_url": "",
      "published": "2026-02-12T04:54:27Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.095,
      "tier1_quick_score": 1.182
    },
    {
      "id": "b67a55266dc20649",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.65.0 corresponding to NGC container 26.01",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.65.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<div class=\"markdown-alert markdown-alert-note\"><p class=\"markdown-alert-title\"><svg class=\"octicon octicon-info mr-2\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg>Note</p><p>Windows support is deprecated, latest assets build for windows could be found in is release <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.51.0\">2.51.0 / 25.01</a></p>\n</div>\n  <details>\n    <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Exposed HTTP errors for non-JSON format response.</p>\n</li>\n<li>\n<p>Perf Analyzer dependencies were made optional when installing tritonclient via pip.</p>\n</li>\n<li>\n<p>Added support for <code>nv_inference_first_response_histogram_ms</code> metric when running models in coupled (aka non-decoupled) mode.</p>\n</li>\n<li>\n<p>Fixed an issue where a crashed Python backend stub process was not correctly detected leading to failed inference requests.</p>\n</li>\n<li>\n<p>Fixed an issue where a malicious HTTP request could exhaust all available system memory leading to a process crash or denial of service.</p>\n</li>\n<li>\n<p>Fixed an issue where loading a TensorRT engine larger than 30GB could cause out of memory errors despite sufficient memory to load the engine being available.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Known Issues</h2>\n<ul>\n<li>\n<p>If you are using the vllm backend to run models that accept video inputs, then do not upgrade to Triton 26.01 and wait for Triton 26.02</p>\n</li>\n<li>\n<p>Triton python package uses outdated dependency <code>starlette</code> package version.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.65.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r26.01#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n  </details>\n  <details>\n    <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.65.0/tritonserver2.65.0-igpu.tar\"><code>tritonserver2.65.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> [<code>2.10.0a0+a36e1d3](https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html), **Python** </code>3.12` and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.65.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n  </details>\n  <details>\n    <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.65.0/tritonserver2.65.0-agx.tar\"><code>tritonserver2.65.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.1</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+a36e1d3</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:26.01-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n  </details>\n<details>\n    <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.65.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.1.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n</details>\n<details>\n  <h2>ManyLinux Assets (early access)</h2>\n<p>This release was compiled with AlmaLinux 8.9 based out of <code>manylinux_2_28</code> and can be used on RHEL8  and later versions.<br />\nSee the included README.md for complete details about installation, verification, and support.<br />\nThis release supports CUDA 13, TensorRT 10.14.1.48, Onnx Runtime 1.23.2, PyTorch 2.10.0a0+b4e4ee8, Python 3.12 and supports ensembles.<br />\nSome optional backend features such as the PyTorch backend's TorchTRT extension are not currently supported.</p>\n</details>",
      "image_url": "",
      "published": "2026-02-11T22:21:05Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.087,
      "tier1_quick_score": 1.174
    },
    {
      "id": "1ef2c05d1093cdc8",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.64.0 corresponding to NGC container 25.12",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.64.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n  <details>\n    <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Fixed an issue with Triton Server’s Sagemaker Service which could result in a server crash resulting from a race condition, caused by unprotected access to the list of models.</p>\n</li>\n<li>\n<p>Extended the set of accelerated PyTorch libraries included with the Triton PyTorch backend.</p>\n</li>\n<li>\n<p>Upgraded Triton Client’s Golang dependencies to latest stable versions to resolve known issues with the previous version of the dependencies.</p>\n</li>\n<li>\n<p>The OpenAI-compatible frontend has transitioned from beta to a stable release.</p>\n</li>\n<li>\n<p>Added <code>echo</code> request parameter for TensorRT-LLM and Python backends to OpenAI-compatible API frontend <code>v1/completions</code> endpoint.</p>\n</li>\n<li>\n<p>Enabled OpenAI-compatible API frontend multi-LoRA support for TensorRT-LLM backend.</p>\n</li>\n<li>\n<p>Backends can now implement the new  <code>TRITONBACKEND_ModelInstanceReady</code> function to report accurate model readiness status.</p>\n</li>\n<li>\n<p>Updated the Python backend to accurately report model readiness.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Known Issues</h2>\n<ul>\n<li>\n<p>The error <code>'__init__(): incompatible function arguments</code> may occur when using TensorRT-LLM backend python models. To suppress the error temporarily, set input tensor <code>stream</code> with a boolean value explicitly in the request header.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.64.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.12#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n  </details>\n  <details>\n    <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.64.0/tritonserver2.64.0-igpu.tar\"><code>tritonserver2.64.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b4e4ee8</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.64.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n  </details>\n  <details>\n    <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.64.0/tritonserver2.64.0-agx.tar\"><code>tritonserver2.64.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.1</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b4e4ee8</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.12-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n  </details>\n  <details>\n    <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.64.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.1.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n  </details>\n  <details>\n    <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.12, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n  </details>\n<details>\n  <h2>ManyLinux Assets (early access)</h2>\n<p>This release was compiled with AlmaLinux 8.9 based out of <code>manylinux_2_28</code> and can be used on RHEL8  and later versions.<br />\nSee the included README.md for complete details about installation, verification, and support.<br />\nThis release supports CUDA 13, TensorRT 10.14.1.48, Onnx Runtime 1.23.2, PyTorch 2.10.0a0+b4e4ee8, Python 3.12 and supports ensembles.<br />\nSome optional backend features such as the PyTorch backend's TorchTRT extension are not currently supported.</p>\n</details>",
      "image_url": "",
      "published": "2026-02-11T22:17:34Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.087,
      "tier1_quick_score": 1.174
    },
    {
      "id": "9ae1e737b9ab2d0e",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.16.0rc2: Patch protobuf for CVE-2026-0994 (#34253)",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc2",
      "summary": "<p>Signed-off-by: Seiji Eicher <a href=\"mailto:seiji@anyscale.com\">seiji@anyscale.com</a><br />\nCo-authored-by: Kevin H. Luu <a href=\"mailto:khluu000@gmail.com\">khluu000@gmail.com</a><br />\n(cherry picked from commit <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/commit/5045d5c9831a3a4a423a409ccea521d299a43a9a\"><tt>5045d5c</tt></a>)</p>",
      "image_url": "",
      "published": "2026-02-11T10:33:40Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.074,
      "tier1_quick_score": 1.161
    },
    {
      "id": "bf4ad96790c2c64d",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.16.0rc1",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc1",
      "summary": "<p>[Frontend][last/5] Make pooling entrypoints request schema consensus.…</p>",
      "image_url": "",
      "published": "2026-02-09T06:42:38Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.036,
      "tier1_quick_score": 1.123
    },
    {
      "id": "d4ea9f8c41e92794",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "Code, Compute and Connection: Inside the Inaugural NVIDIA AI Day São Paulo",
      "url": "https://blogs.nvidia.com/blog/ai-day-sao-paulo/",
      "summary": "The worldwide tour of NVIDIA AI Days — bringing together AI enthusiasts, developers, researchers and startups — made its latest stop in São Paulo, Brazil.",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/sao-paulo-featured-1920x1080-2.jpg",
      "published": "Thu, 12 Feb 2026 22:00:58 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.12,
      "tier1_quick_score": 1.107
    },
    {
      "id": "0530f2ee25a8efae",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "Leading Inference Providers Cut AI Costs by up to 10x With Open Source Models on NVIDIA Blackwell",
      "url": "https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/",
      "summary": "A diagnostic insight in healthcare. A character’s dialogue in an interactive game. An autonomous resolution from a customer service agent. Each of these AI-powered interactions is built on the same unit of intelligence: a token. Scaling these AI interactions requires businesses to consider whether they can afford more tokens. The answer lies in better tokenomics\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/inference-press-moe-x-tokenomics-think-smart-blog-4779150-1280x680-1.jpg",
      "published": "Thu, 12 Feb 2026 16:00:46 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.111,
      "tier1_quick_score": 1.098
    },
    {
      "id": "ab7b6c6a13f1ac1c",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.15.2rc0: [Bugfix] Disable TRTLLM attention when KV transfer is enabled (#33192)",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.2rc0",
      "summary": "<p>Signed-off-by: Zhanqiu Hu <a href=\"mailto:zh338@cornell.edu\">zh338@cornell.edu</a></p>",
      "image_url": "",
      "published": "2026-02-05T00:49:18Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.009,
      "tier1_quick_score": 1.096
    },
    {
      "id": "82c84921d9c73a9e",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.15.1",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1",
      "summary": "<p>v0.15.1 is a patch release with security fixes, RTX Blackwell GPU fixes support, and bug fixes.</p>\n<h2>Security</h2>\n<ul>\n<li><strong><a href=\"https://github.com/advisories/GHSA-6mq8-rvhq-8wgg\" title=\"CVE-2025-69223\">CVE-2025-69223</a></strong>: Updated aiohttp dependency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33621\">#33621</a>)</li>\n<li><strong><a href=\"https://github.com/advisories/GHSA-7gcm-g887-7qv7\" title=\"CVE-2026-0994\">CVE-2026-0994</a></strong>: Updated Protobuf dependency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33619\">#33619</a>)</li>\n</ul>\n<h2>Highlights</h2>\n<h3>Bugfix Hardware Support</h3>\n<ul>\n<li><strong>RTX Blackwell (SM120)</strong>: Fixed NVFP4 MoE kernel support for RTX Blackwell workstation GPUs. Previously, NVFP4 MoE models would fail to load on these GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33417\">#33417</a>)</li>\n<li><strong>FP8 kernel selection</strong>: Fixed FP8 CUTLASS group GEMM to properly fall back to Triton kernels on SM120 GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33285\">#33285</a>)</li>\n</ul>\n<h3>Model Support</h3>\n<ul>\n<li><strong>Step-3.5-Flash</strong>: New model support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33523\">#33523</a>)</li>\n</ul>\n<h3>Bugfix Model Support</h3>\n<ul>\n<li><strong>Qwen3-VL-Reranker</strong>: Fixed model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33298\">#33298</a>)</li>\n<li><strong>Whisper</strong>: Fixed FlashAttention2 with full CUDA graphs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33360\">#33360</a>)</li>\n</ul>\n<h3>Performance</h3>\n<ul>\n<li><strong>torch.compile cold-start</strong>: Fixed regression that increased cold-start compilation time (Llama3-70B: ~88s → ~22s) (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33441\">#33441</a>)</li>\n<li><strong>MoE forward pass</strong>: Optimized by caching layer name computation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33184\">#33184</a>)</li>\n</ul>\n<h3>Bug Fixes</h3>\n<ul>\n<li>Fixed prefix cache hit rate of 0% with GPT-OSS style hybrid attention models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33524\">#33524</a>)</li>\n<li>Enabled Triton MoE backend for FP8 per-tensor dynamic quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33300\">#33300</a>)</li>\n<li>Disabled unsupported Renormalize routing methods for TRTLLM per-tensor FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33620\">#33620</a>)</li>\n<li>Fixed speculative decoding metrics crash when no tokens generated (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33729\">#33729</a>)</li>\n<li>Disabled fast MoE cold start optimization with speculative decoding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33624\">#33624</a>)</li>\n<li>Fixed ROCm skinny GEMM dispatch logic (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33366\">#33366</a>)</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li>Pinned LMCache &gt;= v0.3.9 for API compatibility (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33440\">#33440</a>)</li>\n</ul>\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/zaristei2\">@zaristei2</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33621\">#33621</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/compare/v0.15.0...v0.15.1\"><tt>v0.15.0...v0.15.1</tt></a></p>",
      "image_url": "",
      "published": "2026-02-05T01:01:39Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.009,
      "tier1_quick_score": 1.096
    },
    {
      "id": "bac9f0ac7445edad",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "NVIDIA DGX Spark Powers Big Projects in Higher Education",
      "url": "https://blogs.nvidia.com/blog/dgx-spark-higher-education/",
      "summary": "At leading institutions across the globe, the NVIDIA DGX Spark desktop supercomputer is bringing data‑center‑class AI to lab benches, faculty offices and students’ systems. There’s even a DGX Spark hard at work in the South Pole, at the IceCube Neutrino Observatory run by the University of Wisconsin-Madison. The compact supercomputer’s petaflop‑class performance enables local deployment\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/dgx-spark-higher-education/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/dgx-spark-higher-ed-featured-1280x680-1.jpg",
      "published": "Thu, 12 Feb 2026 15:00:23 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.109,
      "tier1_quick_score": 1.096
    },
    {
      "id": "b0bbdaed56ce378b",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "GeForce NOW Turns Screens Into a Gaming Machine",
      "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-amazon-fire-tv-app/",
      "summary": "The GeForce NOW sixth-anniversary festivities roll on this February, continuing a monthlong celebration of NVIDIA’s cloud gaming service. This week brings even more reasons to join the party, as GeForce NOW launches on a new platform with support for Amazon Fire TV devices, and eight new games to keep the streaming going strong. The new\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-amazon-fire-tv-app/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/gfn-thursday-ecosystem-nv-blog-1280x680-logo.jpg",
      "published": "Thu, 12 Feb 2026 14:00:58 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.108,
      "tier1_quick_score": 1.095
    },
    {
      "id": "d9c13dbb510e62f5",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.15.1rc1",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1rc1",
      "summary": "<p>[BugFix][Spec Decoding] Fix negative accepted tokens metric crash (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/3\">#3</a>…</p>",
      "image_url": "",
      "published": "2026-02-04T01:28:32Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 1.093
    },
    {
      "id": "131daa0d67d1bfbe",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.15.1rc0",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.1rc0",
      "summary": "<p>[torch.compile] Don't do the fast moe cold start optimization if ther…</p>",
      "image_url": "",
      "published": "2026-02-03T08:07:18Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.005,
      "tier1_quick_score": 1.092
    },
    {
      "id": "68b7491da1db14bf",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.16.0rc0: [Docs] Adding links and intro to Speculators and LLM Compressor (#32849)",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0rc0",
      "summary": "<p>Signed-off-by: Aidan Reilly <a href=\"mailto:aireilly@redhat.com\">aireilly@redhat.com</a><br />\nSigned-off-by: Harry Mellor <a href=\"mailto:19981378+hmellor@users.noreply.github.com\">19981378+hmellor@users.noreply.github.com</a><br />\nCo-authored-by: Harry Mellor <a href=\"mailto:19981378+hmellor@users.noreply.github.com\">19981378+hmellor@users.noreply.github.com</a></p>",
      "image_url": "",
      "published": "2026-01-29T22:12:35Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.088
    },
    {
      "id": "877d9d2f64c35601",
      "source": "vllm_releases",
      "source_weight": 0.25,
      "title": "v0.15.0",
      "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.0",
      "summary": "<h2>Highlights</h2>\n<p>This release features 335 commits from 158 contributors (39 new)!</p>\n<h3>Model Support</h3>\n<ul>\n<li><strong>New architectures</strong>: Kimi-K2.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33131\">#33131</a>), Molmo2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30997\">#30997</a>), Step3vl 10B (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32329\">#32329</a>), Step1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32511\">#32511</a>), GLM-Lite (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31386\">#31386</a>), Eagle2.5-8B VLM (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32456\">#32456</a>).</li>\n<li><strong>LoRA expansion</strong>: Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30802\">#30802</a>), InternVL2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32397\">#32397</a>), MiniMax M2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32763\">#32763</a>).</li>\n<li><strong>Speculative decoding</strong>: EAGLE3 for Pixtral/LlavaForConditionalGeneration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32542\">#32542</a>), Qwen3 VL MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32048\">#32048</a>), draft model support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/24322\">#24322</a>).</li>\n<li><strong>Embeddings</strong>: BGE-M3 sparse embeddings and ColBERT embeddings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/14526\">#14526</a>).</li>\n<li><strong>Model enhancements</strong>: Voxtral streaming architecture (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32861\">#32861</a>), SharedFusedMoE for Qwen3MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32082\">#32082</a>), dynamic resolution for Nemotron Nano VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32121\">#32121</a>), Molmo2 vision backbone quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32385\">#32385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li><strong>Async scheduling + Pipeline Parallelism</strong>: <code>--async-scheduling</code> now works with pipeline parallelism (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32359\">#32359</a>).</li>\n<li><strong>Mamba prefix caching</strong>: Block-aligned prefix caching for Mamba/hybrid models with <code>--enable-prefix-caching --mamba-cache-mode align</code>. Achieves ~2x speedup by caching Mamba states directly (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30877\">#30877</a>).</li>\n<li><strong>Session-based streaming input</strong>: New incremental input support for interactive workloads like ASR. Accepts async generators producing <code>StreamingInput</code> objects while maintaining KV cache alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28973\">#28973</a>).</li>\n<li><strong>Model Runner V2</strong>: VLM support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32546\">#32546</a>), architecture improvements.</li>\n<li><strong>LoRA</strong>: Inplace loading for memory efficiency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31326\">#31326</a>).</li>\n<li><strong>AOT compilation</strong>: torch.compile inductor artifacts support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/25205\">#25205</a>).</li>\n<li><strong>Performance</strong>: KV cache offloading redundant load prevention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29087\">#29087</a>), FlashAttn attention/cache update separation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/25954\">#25954</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<h4>NVIDIA</h4>\n<ul>\n<li><strong>Blackwell defaults</strong>: FlashInfer MLA is now the default MLA backend on Blackwell, with TRTLLM as default prefill (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32615\">#32615</a>).</li>\n<li><strong>MoE performance</strong>: 1.2-2% E2E throughput improvement via grouped topk kernel fusion (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32058\">#32058</a>), NVFP4 small-batch decoding improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30885\">#30885</a>), faster cold start for MoEs with torch.compile (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32805\">#32805</a>).</li>\n<li><strong>FP4 kernel optimization</strong>: Up to 65% faster FP4 quantization on Blackwell (SM100F) using 256-bit loads, ~4% E2E throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32520\">#32520</a>).</li>\n<li><strong>Kernel improvements</strong>: topk_sigmoid kernel for MoE routing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31246\">#31246</a>), atomics reduce counting for SplitK skinny GEMMs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29843\">#29843</a>), fused cat+quant for FP8 KV cache in MLA (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32950\">#32950</a>).</li>\n<li><strong>torch.compile</strong>: SiluAndMul and QuantFP8 CustomOp compilation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32806\">#32806</a>), Triton prefill attention performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32403\">#32403</a>).</li>\n</ul>\n<h4>AMD ROCm</h4>\n<ul>\n<li><strong>MoRI EP</strong>: High-performance all2all backend for Expert Parallel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28664\">#28664</a>).</li>\n<li><strong>Attention improvements</strong>: Shuffle KV cache layout and assembly paged attention kernel for AiterFlashAttentionBackend (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29887\">#29887</a>).</li>\n<li><strong>FP4 support</strong>: MLA projection GEMMs with dynamic quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32238\">#32238</a>).</li>\n<li><strong>Consumer GPU support</strong>: Flash Attention Triton backend on RDNA3/RDNA4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32944\">#32944</a>).</li>\n</ul>\n<h4>Other Platforms</h4>\n<ul>\n<li><strong>TPU</strong>: Pipeline parallelism support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28506\">#28506</a>), backend option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32438\">#32438</a>).</li>\n<li><strong>Intel XPU</strong>: AgRsAll2AllManager for distributed communication (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32654\">#32654</a>).</li>\n<li><strong>CPU</strong>: NUMA-aware acceleration for TP/DP inference on ARM (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32792\">#32792</a>), PyTorch 2.10 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32869\">#32869</a>).</li>\n<li><strong>Whisper</strong>: torch.compile support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30385\">#30385</a>).</li>\n<li><strong>WSL</strong>: Platform compatibility fix for Windows Subsystem for Linux (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32749\">#32749</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li><strong>MXFP4</strong>: W4A16 support for compressed-tensors MoE models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32285\">#32285</a>).</li>\n<li><strong>Non-gated MoE</strong>: Quantization support with Marlin, NVFP4 CUTLASS, FP8, INT8, and compressed-tensors (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32257\">#32257</a>).</li>\n<li><strong>Intel</strong>: Quantization Toolkit integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31716\">#31716</a>).</li>\n<li><strong>FP8 KV cache</strong>: Per-tensor and per-attention-head quantization via llmcompressor (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30141\">#30141</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Responses API</strong>: Partial message generation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32100\">#32100</a>), <code>include_stop_str_in_output</code> tuning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32383\">#32383</a>), <code>prompt_cache_key</code> support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32824\">#32824</a>).</li>\n<li><strong>OpenAI API</strong>: <code>skip_special_tokens</code> configuration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32345\">#32345</a>).</li>\n<li><strong>Score endpoint</strong>: Flexible input formats with <code>data_1</code>/<code>data_2</code> and <code>queries</code>/<code>documents</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32577\">#32577</a>).</li>\n<li><strong>Render endpoints</strong>: New endpoints for prompt preprocessing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32473\">#32473</a>).</li>\n<li><strong>Whisper API</strong>: <code>avg_logprob</code> and <code>compression_ratio</code> in verbose_json segments (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31059\">#31059</a>).</li>\n<li><strong>Security</strong>: FIPS 140-3 compliant hash option for enterprise/government users (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32386\">#32386</a>), <code>--ssl-ciphers</code> CLI argument (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30937\">#30937</a>).</li>\n<li><strong>UX improvements</strong>: Auto <code>api_server_count</code> based on <code>dp_size</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32525\">#32525</a>), wheel variant auto-detection during install (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32948\">#32948</a>), custom profiler URI schemes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32393\">#32393</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li>FlashInfer v0.6.1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30993\">#30993</a>)</li>\n<li>Transformers 4.57.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32287\">#32287</a>)</li>\n<li>PyTorch 2.10 for CPU backend (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32869\">#32869</a>)</li>\n<li>DeepGEMM newer version (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32479\">#32479</a>)</li>\n</ul>\n<h3>Breaking Changes &amp; Deprecations</h3>\n<ul>\n<li><strong>Metrics</strong>: Removed deprecated <code>vllm:time_per_output_token_seconds</code> metric - use <code>vllm:inter_token_latency_seconds</code> instead (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32661\">#32661</a>).</li>\n<li><strong>Environment variables</strong>: Removed deprecated environment variables (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32812\">#32812</a>).</li>\n<li><strong>Quantization</strong>: DeepSpeedFp8 removed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32679\">#32679</a>), RTN removed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32697\">#32697</a>), HQQ deprecated (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32681\">#32681</a>).</li>\n</ul>\n<h3>Bug Fixes</h3>\n<ul>\n<li><strong>Speculative decoding</strong>: Eagle draft_model_config fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31753\">#31753</a>).</li>\n<li><strong>DeepSeek</strong>: DeepSeek-V3.1 + DeepGEMM incompatible scale shapes fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32361\">#32361</a>).</li>\n<li><strong>Distributed</strong>: DP+MoE inference fix via CpuCommunicator (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31867\">#31867</a>), P/D with non-MoE DP fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33037\">#33037</a>).</li>\n<li><strong>EPLB</strong>: Possible deadlock fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32418\">#32418</a>).</li>\n<li><strong>NIXL</strong>: UCX memory leak fix by exporting UCX_MEM_MMAP_HOOK_MODE=none (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32181\">#32181</a>).</li>\n<li><strong>Structured output</strong>: Outlines byte fallback handling fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31391\">#31391</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/YunzhuLu\">@YunzhuLu</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32126\">#32126</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/emricksini-h\">@emricksini-h</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30784\">#30784</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/dsfaccini\">@dsfaccini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32289\">#32289</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ofirzaf\">@ofirzaf</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32312\">#32312</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/seekskyworld\">@seekskyworld</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32321\">#32321</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/brian033\">@brian033</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31715\">#31715</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TomerBN-Nvidia\">@TomerBN-Nvidia</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32257\">#32257</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/vanshilshah97\">@vanshilshah97</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32448\">#32448</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/George-Polya\">@George-Polya</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32385\">#32385</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/T1mn\">@T1mn</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32411\">#32411</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/mritunjaysharma394\">@mritunjaysharma394</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31492\">#31492</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/randzero\">@randzero</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32511\">#32511</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/DemingCheng\">@DemingCheng</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32556\">#32556</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/iboiko-habana\">@iboiko-habana</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32471\">#32471</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/honglyua-il\">@honglyua-il</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32462\">#32462</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/hyeongyun0916\">@hyeongyun0916</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32473\">#32473</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/DanielMe\">@DanielMe</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32560\">#32560</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/netanel-haber\">@netanel-haber</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32121\">#32121</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/longregen\">@longregen</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28784\">#28784</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jasonyanwenl\">@jasonyanwenl</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32749\">#32749</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Wauplin\">@Wauplin</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32788\">#32788</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikaadil\">@ikaadil</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32775\">#32775</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/alexsun07\">@alexsun07</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28664\">#28664</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/liranschour\">@liranschour</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30207\">#30207</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/AuYang261\">@AuYang261</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32844\">#32844</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/diviramon\">@diviramon</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32393\">#32393</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/RishabhSaini\">@RishabhSaini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32884\">#32884</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/MatteoFari\">@MatteoFari</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32397\">#32397</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/peakcrosser7\">@peakcrosser7</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30877\">#30877</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/orionr\">@orionr</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30443\">#30443</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/marksverdhei\">@marksverdhei</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32614\">#32614</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/joninco\">@joninco</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32935\">#32935</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/monajafi-amd\">@monajafi-amd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32944\">#32944</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ruizcrp\">@ruizcrp</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32988\">#32988</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sjhddh\">@sjhddh</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32983\">#32983</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/HirokenOvo\">@HirokenOvo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32646\">#32646</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Chenhao-Guan\">@Chenhao-Guan</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32763\">#32763</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/joshuadeng\">@joshuadeng</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28973\">#28973</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ZhanqiuHu\">@ZhanqiuHu</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33016\">#33016</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/compare/v0.14.1...v0.15.0\"><tt>v0.14.1...v0.15.0</tt></a></p>",
      "image_url": "",
      "published": "2026-01-29T10:21:01Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.088
    },
    {
      "id": "f26808a6663f56c4",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.63.0 corresponding to NGC container 25.11",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.63.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Enabled endpoint <code>v1/embeddings</code> for vLLM backend in OpenAI-compatible API server.</p>\n</li>\n<li>\n<p>Enabled <code>echo</code> parameter for TensorRT-LLM and Python backends in OpenAI-compatible API server.</p>\n</li>\n<li>\n<p>Improved error handling in OpenAI-compatible API server by providing more specific and OpenAI-compliant error codes.</p>\n</li>\n<li>\n<p>Upgraded the version of starlette used by OpenAI frontend.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka <a href=\"http://model.pt\" rel=\"nofollow\">‘model.pt’</a>). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.63.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.10#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.63.0/tritonserver2.63.0-igpu.tar\"><code>tritonserver2.63.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b558c98</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.63.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.63.0/tritonserver2.63.0-agx.tar\"><code>tritonserver2.63.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.14.1.48</code>, <strong>Onnx Runtime</strong> <code>1.23.2</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.10.0a0+b558c98</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.11-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.10 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.10-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.10-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.63.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.10. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.13.3.9</td>\n</tr>\n</tbody>\n</table>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>",
      "image_url": "",
      "published": "2026-01-30T17:03:42Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.088
    },
    {
      "id": "9c10e7e1095fc2e0",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.62.0 corresponding to NGC container 25.10",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.62.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Fixed a server crash issue caused by specially crafted request messages sent to <code>/v2/models/&lt;model_name&gt;/infer</code>.</p>\n</li>\n<li>\n<p>Fixed a server crash issue caused by incorrect handling of malformed HTTP requests.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Triton python package uses outdated dependency <code>starlette</code> package version.</p>\n</li>\n<li>\n<p>Since 25.10, vLLM backend uses V1 engine by default. You might see invalid characters in logprobs output and the bug has been reported to the vLLM team.</p>\n</li>\n<li>\n<p>Enabling vLLM metrics during inferences causes the engine to crash.</p>\n</li>\n<li>\n<p>PyTorch backend supports PyTorch 2.0 with the limitation that models must be provided as a serialized model file (aka ‘model.pt’). Please see Triton PyTorch Backend documentation for details.</p>\n</li>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.62.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.10#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.62.0/tritonserver2.62.0-igpu.tar\"><code>tritonserver2.62.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.1</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+145a3a7</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.10/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.62.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.62.0/tritonserver2.62.0-agx.tar\"><code>tritonserver2.62.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.3.9</code>, <strong>Onnx Runtime</strong> <code>1.23.1</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.10-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.06 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.62.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.06. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
      "image_url": "",
      "published": "2026-01-30T16:56:51Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 1.088
    },
    {
      "id": "7cd095f5f55dba53",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.61.0 corresponding to NGC container 25.09",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.61.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>\n<p>Static key authentication for OpenAI Frontend APIs</p>\n</li>\n<li>\n<p>Prevented models outside Triton’s repository being loaded from OpenAI Frontend.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM's v0 API and Ray are affected by vulnerabilities. Users should consider their own architecture and mitigation steps which may include but should not be limited to:</p>\n<ul>\n<li>Do not expose Ray executors and vLLM hosts to a network where any untrusted connections might reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that the port used is random.</li>\n</ul>\n</li>\n<li>\n<p>Perf Analyzer is no longer part of the “client” released archive and can be installed separately using <code>pip install perf-analyzer</code>.</p>\n</li>\n<li>\n<p>When using Valgrind or other leak detection tools on AGX-Thor or DGX-Spark systems, you might see memory leaks attributed to NvRmGpuLibOpen. The root cause has been identified and fixed in CUDA.</p>\n</li>\n<li>\n<p>Valgrind or other memory leak detection tools may occasionally report leaks related to DCGM. These reports are intermittent and often disappear on retry. The root cause is under investigation.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy.</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.61.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.09#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.61.0/tritonserver2.61.0-igpu.tar\"><code>tritonserver2.61.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.09/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.61.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.61.0/tritonserver2.61.0-agx.tar\"><code>tritonserver2.61.0-agx.tar</code></a>.</p>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.3.9</code>, <strong>Onnx Runtime</strong> <code>1.23.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.9.0a0+50eac81</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.09-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.06 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.61.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.06. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">1.0.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
      "image_url": "",
      "published": "2025-10-07T22:10:06Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.087
    },
    {
      "id": "0b6403585fa5c6b3",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.60.0 corresponding to NGC container 25.08",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.60.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Added CUDA 13 support.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>Triton ONNX Runtime Backend build uses <a href=\"https://github.com/microsoft/onnxruntime/commit/1d1712fdafb9e61b2d6d033c4433c1033395d7e7\">microsoft/onnxruntime/commit/1d1712fdaf</a> and may have some limitations on DGX Spark hardware which will be addressed in future versions.</p>\n</li>\n<li>\n<p>CuPy has issues with the CUDA 13 Device API in multithreaded contexts. Avoid using tritonclient cuda_shared_memory APIs in multithreaded environments until fixed by CuPy</p>\n</li>\n<li>\n<p>CuPy does not support CUDA 13 at the time of writing. Issues may be encountered when using CuPy before it officially supports CUDA 13, see <a href=\"https://github.com/triton-inference-server/server/tree/r25.08/python/openai#pre-requisites\">https://github.com/triton-inference-server/server/tree/r25.08/python/openai#pre-requisites</a> for more details</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.60.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.08#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.08, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.60.0/tritonserver2.60.0-igpu.tar\"><code>tritonserver2.60.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>CUDA</strong> <code>12.9</code>, <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.23.0+1d1712fdaf</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a+34c6371d24</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.07/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Jetson AGX Systems Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems\" rel=\"nofollow\">AGX Systems</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.60.0/tritonserver2.60.0-agx.tar\"><code>tritonserver2.60.0-agx.tar</code></a>.</p>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Jetson AGX release for 25.08, requires DCGM version 4 to be installed in order to use GPU metrics.<br />\nPlease use following command to install DCGM 4:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>curl -o /tmp/cuda-keyring.deb \\\n         https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb \\\n     &amp;&amp; apt install /tmp/cuda-keyring.deb \\\n     &amp;&amp; rm /tmp/cuda-keyring.deb \\\n     &amp;&amp; apt update \\\n     &amp;&amp; apt install --yes --no-install-recommends \\\n                  datacenter-gpu-manager-4-core=1:4.4.0-1\n</code></pre></div>\n</blockquote>\n<ul>\n<li>\n<p>This release supports <strong>CUDA</strong> <code>13.0</code>, <strong>TensorRT</strong> <code>10.13.2.6</code>, <strong>Onnx Runtime</strong> <code>1.23.0+1d1712fdaf</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+34c6371</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</p>\n</li>\n<li>\n<p>This package is a subset of <code>nvcr.io/nvidia/tritonserver:25.08-py3</code> ARM container image assets it.</p>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.06-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.06-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.60.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.21.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.11.0.33</td>\n</tr>\n</tbody>\n</table>\n</details>",
      "image_url": "",
      "published": "2025-08-26T22:15:33Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.087
    },
    {
      "id": "671420a2a130cda5",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.59.1 corresponding to NGC container 25.07",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.59.1",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Fixed vulnerabilities in the Triton Inference Server.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>There was no python wheels packages released as part of 25.07 release</p>\n</li>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.59.1_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.07, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.59.1/tritonserver2.59.1-igpu.tar\"><code>tritonserver2.59.1-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.6</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.07/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.04-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.04-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.59.1/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.20.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.10.0.31</td>\n</tr>\n</tbody>\n</table>\n</details>",
      "image_url": "",
      "published": "2025-07-29T21:50:01Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.087
    },
    {
      "id": "d62c6c9a740a083f",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.59.0 corresponding to NGC container 25.06",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.59.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Improved ensemble model performance in scenarios that allow out-of-order responses by increasing maximum throughput and reducing latency.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.59.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.06, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.59.0/tritonserver2.59.0-igpu.tar\"><code>tritonserver2.59.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.6</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.06/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.04-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.04-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.59.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.20.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.10.0.31</td>\n</tr>\n</tbody>\n</table>\n</details>",
      "image_url": "",
      "published": "2025-06-30T22:54:06Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.087
    },
    {
      "id": "ebc920b10350277a",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.58.0 corresponding to NGC container 25.05",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.58.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Optional “execution_context_allocation_strategy” parameter in the TensorRT backend configuration allows selection of memory allocation behavior.</li>\n<li>Support Tool calling functionality with Llama 3 and Mistral models in OpenAI frontend.</li>\n<li>Improvements around memory allocation and various bug fixes.</li>\n<li>GenAI-Perf now offers a new configuration file alongside the command line.</li>\n<li>GenAI-Perf now collects GPU metrics from /metrics endpoint exposed by DCGM Exporter.</li>\n<li>GenAI-Perf supports new Power, Utilization, Ecc, Errors and PCie metrics.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM backend for 25.05 might be unstable with the vLLM V1 architecture. We recommend switching to V0 for this release, by setting <code>VLLM_USE_V1</code> environment variable to 0. However, users should be aware that vLLM's V0 API is affected by vulnerabilities.</p>\n</li>\n<li>\n<p>vLLM containers include vllm version 0.8.4 which is affected by vulnerabilities.<br />\nWorkarounds:<br />\nPrior to the fix, your options include:</p>\n<ul>\n<li>Do not expose the vLLM host to a network where any untrusted connections may reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that port used is random.</li>\n</ul>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.58.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.05, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.58.0/tritonserver2.58.0-igpu.tar\"><code>tritonserver2.58.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.10.0.31</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.5</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.05/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.58.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.03 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.03-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.03-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.58.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.03. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.19.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.9.0.34</td>\n</tr>\n</tbody>\n</table>\n</details>",
      "image_url": "",
      "published": "2025-06-30T22:54:03Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.087
    },
    {
      "id": "d171b80d5d104921",
      "source": "triton_releases",
      "source_weight": 0.25,
      "title": "Release 2.57.0 corresponding to NGC container 25.04",
      "url": "https://github.com/triton-inference-server/server/releases/tag/v2.57.0",
      "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Exposed gRPC infer thread count as a server option.</li>\n<li>Improved server stability during the gRPC client cancellation.</li>\n<li>Improved server stability in tracing mode.</li>\n<li>Added BLS decoupled request cancellation in the Python Backend</li>\n<li>GenAI-Perf now offers a new configuration file alongside the command line.</li>\n<li>GenAI-Perf now supports the Huggingface TGI generated endpoint.</li>\n<li>GenAI-Perf added a Token per second per user (TPS/user) metric.</li>\n<li>GenAI-Perf metric parsing speed was increased by 60%.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>vLLM backend for 25.04 might be unstable with the vLLM V1 architecture. We recommend switching to V0 for this release, by setting <code>VLLM_USE_V1</code> environment variable to 0. However, users should be aware that vLLM's V0 API is affected by vulnerabilities.</p>\n</li>\n<li>\n<p>vLLM containers include vllm version 0.8.1 which is affected by new vulnerabilities.<br />\nWorkarounds:<br />\nPrior to the fix, your options include:</p>\n<ul>\n<li>Do not expose the vLLM host to a network where any untrusted connections may reach the host.</li>\n<li>Ensure that only the other vLLM hosts are able to connect to the TCP port used for the XPUB socket. Note that port used is random.</li>\n</ul>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model’s folder in the temporary directory, which is deleted upon server’s shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.57.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.04#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.04, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.57.0/tritonserver2.57.0-igpu.tar\"><code>tritonserver2.57.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.9.0.34</code>, <strong>Onnx Runtime</strong> <code>1.21.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.7.0a0+79aa17489c.nv25.4</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.04/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.57.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.03 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.03-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.03-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.57.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.03. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.18.2</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.9.0.34</td>\n</tr>\n</tbody>\n</table>\n</details>",
      "image_url": "",
      "published": "2025-06-30T22:54:00Z",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 1.087
    },
    {
      "id": "8010462db1e78b23",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "GeForce NOW Celebrates Six Years of Streaming With 24 Games in February",
      "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-feb-2026-games-list/",
      "summary": "Break out the cake and green sprinkles — GeForce NOW is turning six. Since launch, members have streamed over 1 billion hours, and the party’s just getting started. Throughout February, members can look forward to new games, fresh ways to play across more devices and even more ways to bring RTX power to every screen\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-feb-2026-games-list/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/gfn-thursday-6yr-anniversary-nv-blog-1280x680-logo.jpg",
      "published": "Thu, 05 Feb 2026 14:00:53 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.01,
      "tier1_quick_score": 0.997
    },
    {
      "id": "993f080f34b17b79",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "Nemotron Labs: How AI Agents Are Turning Documents Into Real-Time Business Intelligence",
      "url": "https://blogs.nvidia.com/blog/ai-agents-intelligent-document-processing/",
      "summary": "Businesses today face the challenge of uncovering valuable insights buried within a wide variety of documents — including reports, presentations, PDFs, web pages and spreadsheets.",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/nemotron-labs-featured.jpg",
      "published": "Wed, 04 Feb 2026 16:00:36 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.008,
      "tier1_quick_score": 0.995
    },
    {
      "id": "7cb2e5a4bd8e8e51",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "Everything Will Be Represented in a Virtual Twin, NVIDIA CEO Jensen Huang Says at 3DEXPERIENCE World",
      "url": "https://blogs.nvidia.com/blog/huang-3dexperience-2026/",
      "summary": "NVIDIA founder and CEO Jensen Huang and Dassault Systèmes CEO Pascal Daloz announced a partnership to build a shared industrial AI architecture, merging virtual twins with physics-based AI to redefine the future of design, engineering and manufacturing.",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/02/dassault-jhh-email-1280x680-28.jpg",
      "published": "Tue, 03 Feb 2026 22:14:51 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.006,
      "tier1_quick_score": 0.993
    },
    {
      "id": "7fcd2a2c35056be2",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "Mercedes-Benz Unveils New S-Class Built on NVIDIA DRIVE AV, Which Enables an L4-Ready Architecture",
      "url": "https://blogs.nvidia.com/blog/mercedes-benz-l4-s-class-drive-av-platform/",
      "summary": "Mercedes-Benz is marking 140 years of automotive innovation with a new S-Class built for the AI era, bringing together automotive safety and NVIDIA’s advanced autonomous driving platform to enable a level 4-ready architecture designed for trust. The new S-Class with MB.OS, which will be equipped with the NVIDIA DRIVE Hyperion architecture and full-stack NVIDIA DRIVE\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/mercedes-benz-l4-s-class-drive-av-platform/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/01/benz-nv-1280x680-1.jpg",
      "published": "Thu, 29 Jan 2026 18:00:31 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 0.988
    },
    {
      "id": "23548019f4b64ecc",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "Into the Omniverse: Physical AI Open Models and Frameworks Advance Robots and Autonomous Systems",
      "url": "https://blogs.nvidia.com/blog/physical-ai-open-models-robot-autonomous-systems-omniverse/",
      "summary": "Open source has become essential for driving innovation in robotics and autonomy. By providing access to critical infrastructure — from simulation frameworks to AI models — NVIDIA is enabling collaborative development that accelerates the path to safer, more capable autonomous systems.",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/01/ito-jan-2026-1280x680-1.jpg",
      "published": "Thu, 29 Jan 2026 17:00:21 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 0.988
    },
    {
      "id": "cd990fc767cf9d3d",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "GeForce NOW Brings GeForce RTX Gaming to Linux PCs",
      "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-linux/",
      "summary": "Get ready to game — the native GeForce NOW app for Linux PCs is now available in beta, letting Linux desktops tap directly into GeForce RTX performance from the cloud. Alongside the expansion comes ten new games, including The Bard’s Tale IV: Director’s Cut and The Bard’s Tale Trilogy for a leveled-up gaming weekend. And\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-linux/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/01/gfn-thursday-1-6-nv-blog-1280x680-logo-1.jpg",
      "published": "Thu, 29 Jan 2026 14:00:49 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 0.988
    },
    {
      "id": "7a29dda9cde09e1e",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "Accelerating Science: A Blueprint for a Renewed National Quantum Initiative",
      "url": "https://blogs.nvidia.com/blog/national-quantum-initiative/",
      "summary": "Quantum technologies are rapidly emerging as foundational capabilities for economic competitiveness, national security and scientific leadership in the 21st century. Sustained U.S. leadership in quantum information science is critical to ensuring that breakthroughs in computing, sensing, networking and materials translate into secure technologies and industries, a skilled domestic workforce and long-term strategic advantage. To secure\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/national-quantum-initiative/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2025/09/nvidiaheadquarters.jpg",
      "published": "Wed, 28 Jan 2026 18:17:53 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.001,
      "tier1_quick_score": 0.988
    },
    {
      "id": "7a6926c10a8e13bc",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "NVIDIA Launches Earth-2 Family of Open Models — the World’s First Fully Open, Accelerated Set of Models and Tools for AI Weather",
      "url": "https://blogs.nvidia.com/blog/nvidia-earth-2-open-models/",
      "summary": "At the American Meteorological Society’s Annual Meeting, NVIDIA today unveiled a new NVIDIA Earth-2 family of open models, libraries and frameworks for weather and climate AI, offering the world’s first fully open, production-ready weather AI software stack.",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/01/hpc-corp-blog-earth-2-model-1920x1080-1.jpg",
      "published": "Mon, 26 Jan 2026 14:00:53 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 0.987
    },
    {
      "id": "484509bdb7334e78",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "NVIDIA DRIVE AV Raises the Bar for Vehicle Safety as Mercedes-Benz CLA Earns Top Euro NCAP Award",
      "url": "https://blogs.nvidia.com/blog/drive-av-mercedes-benz-cla-euro-ncap-safety-award/",
      "summary": "AI-powered driver assistance technologies are becoming standard equipment, fundamentally changing how vehicle safety is assessed and validated. The recent recognition of the Mercedes-Benz CLA as Euro NCAP&#8217;s Best Performer of 2025 underscores this shift, as the vehicle combines traditional passive safety features with NVIDIA DRIVE AV software to achieve the highest overall safety score of\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/drive-av-mercedes-benz-cla-euro-ncap-safety-award/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/01/mb-cla-euro-ncap-2.jpg",
      "published": "Thu, 22 Jan 2026 18:21:49 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 0.987
    },
    {
      "id": "b8b6e98e2c15bfbe",
      "source": "nvidia_blog",
      "source_weight": 0.15,
      "title": "How to Get Started With Visual Generative AI on NVIDIA RTX PCs",
      "url": "https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/",
      "summary": "AI-powered content generation is now embedded in everyday tools like Adobe and Canva, with a slew of agencies and studios incorporating the technology into their workflows. Image models now deliver photorealistic results consistently, video models can generate long and coherent clips, and both can follow creative directions. Creators are increasingly running these workflows locally on\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "image_url": "https://blogs.nvidia.com/wp-content/uploads/2026/01/2026-01-22-nv-blog-1280x680-1.jpg",
      "published": "Thu, 22 Jan 2026 14:00:57 +0000",
      "collected_at": "2026-02-19T06:30:18.594850+00:00",
      "ingest_batch_id": "20260219-063018",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 0.837,
      "freshness": 0.0,
      "tier1_quick_score": 0.987
    }
  ]
}