{
  "5eb1dc4c2b27f35b::v:e55a95b831": {
    "platform_relevant": false,
    "novelty": 3,
    "practicality": 2,
    "hype": 2,
    "why_1line": "Robotics VLA verification research; not applicable to coding-agent platforms or software delivery automation.",
    "__label_source": "llm"
  },
  "e4aef8221107f2b4::v:e55a95b831": {
    "platform_relevant": false,
    "novelty": 4,
    "practicality": 2,
    "hype": 3,
    "why_1line": "Multimodal CoT scaling is theoretically interesting but lacks deployment patterns, evals strategy, or software delivery automation for coding agents.",
    "__label_source": "llm"
  },
  "ff6af3576c9906a8::v:e55a95b831": {
    "platform_relevant": false,
    "novelty": 3,
    "practicality": 2,
    "hype": 2,
    "why_1line": "RAG retrieval optimization paper; lacks coding-agent integration, deployment patterns, or eval harness specifics for production agentic systems.",
    "__label_source": "llm"
  },
  "8c7ff1a869d5d3eb::v:e55a95b831": {
    "platform_relevant": true,
    "novelty": 4,
    "practicality": 4,
    "hype": 3,
    "why_1line": "Confidence-aware compute allocation for multi-step agents—directly addresses reliability/efficiency tradeoff in production agentic systems.",
    "__label_source": "llm"
  },
  "68a3a1d3bce91643::v:e55a95b831": {
    "platform_relevant": false,
    "novelty": 3,
    "practicality": 1,
    "hype": 2,
    "why_1line": "Legal/theoretical copyright framework; not actionable for platform engineers building coding agents or production systems.",
    "__label_source": "llm"
  },
  "5eb1dc4c2b27f35b::v:60cab3992b": {
    "platform_relevant": false,
    "novelty": 3,
    "practicality": 2,
    "hype": 2,
    "why_1line": "Robotics VLA verification research; misaligned with coding-agent automation and software delivery priorities.",
    "__label_source": "llm"
  },
  "e4aef8221107f2b4::v:60cab3992b": {
    "platform_relevant": false,
    "novelty": 4,
    "practicality": 2,
    "hype": 3,
    "why_1line": "Multimodal CoT test-time scaling research; lacks concrete deployment patterns, harness design, or coding-agent engineering signal.",
    "__label_source": "llm"
  },
  "ff6af3576c9906a8::v:60cab3992b": {
    "platform_relevant": false,
    "novelty": 3,
    "practicality": 2,
    "hype": 2,
    "why_1line": "RAG retrieval improvement interesting but orthogonal to agentic coding automation and software delivery pipeline priorities.",
    "__label_source": "llm"
  },
  "8c7ff1a869d5d3eb::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 4,
    "practicality": 4,
    "hype": 3,
    "why_1line": "Confidence-aware dynamic compute allocation for multi-step agents reduces token usage 2.3x while improving reliability—directly applicable to production agent harness optimization.",
    "__label_source": "llm"
  },
  "68a3a1d3bce91643::v:60cab3992b": {
    "platform_relevant": false,
    "novelty": 3,
    "practicality": 1,
    "hype": 2,
    "why_1line": "Copyright theory paper; not relevant to agentic coding automation, production ML systems, or developer tooling.",
    "__label_source": "llm"
  },
  "6e2225d549ed5ae2::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 4,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "877d9d2f64c35601::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 4,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "d62c6c9a740a083f::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 5,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "d171b80d5d104921::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "82c84921d9c73a9e::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 2,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "8bed79565182bbc6::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "b67a55266dc20649::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "1ef2c05d1093cdc8::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "f26808a6663f56c4::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "9c10e7e1095fc2e0::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "7cd095f5f55dba53::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "0b6403585fa5c6b3::v:60cab3992b": {
    "platform_relevant": true,
    "novelty": 3,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Relevant to AI platform engineering workflows.",
    "__label_source": "heuristic"
  },
  "6e2225d549ed5ae2::v:8774c39aa8": {
    "platform_relevant": true,
    "novelty": 4,
    "practicality": 5,
    "hype": 3,
    "why_1line": "Production inference serving: 30.8% throughput gains via async+pipeline parallelism, speculative decode structured outputs, RLHF engine pause/resume for training loops.",
    "__label_source": "llm"
  },
  "877d9d2f64c35601::v:8774c39aa8": {
    "platform_relevant": true,
    "novelty": 4,
    "practicality": 4,
    "hype": 3,
    "why_1line": "vLLM v0.15 delivers production-critical inference optimizations (Mamba prefix caching 2x speedup, async scheduling + pipeline parallelism, FP4 kernel 65% faster) and expanded model/hardware support enabling robust agentic coding workload deployment.",
    "__label_source": "llm"
  },
  "d62c6c9a740a083f::v:8774c39aa8": {
    "platform_relevant": false,
    "novelty": 2,
    "practicality": 2,
    "hype": 1,
    "why_1line": "Generic inference server release notes; lacks agentic coding, tooling orchestration, or software delivery automation relevance.",
    "__label_source": "llm"
  },
  "d171b80d5d104921::v:8774c39aa8": {
    "platform_relevant": false,
    "novelty": 2,
    "practicality": 2,
    "hype": 1,
    "why_1line": "Inference server release notes with infra improvements but zero relevance to agentic coding, software delivery automation, or agent eval/reliability.",
    "__label_source": "llm"
  },
  "82c84921d9c73a9e::v:8774c39aa8": {
    "platform_relevant": true,
    "novelty": 2,
    "practicality": 4,
    "hype": 1,
    "why_1line": "Production inference stability patch: Blackwell GPU fixes, 4x cold-start speedup, security deps—direct serving reliability win.",
    "__label_source": "llm"
  },
  "8bed79565182bbc6::v:8774c39aa8": {
    "platform_relevant": false,
    "novelty": 2,
    "practicality": 2,
    "hype": 1,
    "why_1line": "Generic dependency bump & bug fixes; no agent engineering, eval harness, or software delivery automation signal.",
    "__label_source": "llm"
  },
  "b67a55266dc20649::v:8774c39aa8": {
    "platform_relevant": false,
    "novelty": 2,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Standard inference server release notes with bug fixes; not relevant to agentic coding automation or software delivery loops.",
    "__label_source": "llm"
  },
  "1ef2c05d1093cdc8::v:8774c39aa8": {
    "platform_relevant": false,
    "novelty": 2,
    "practicality": 3,
    "hype": 1,
    "why_1line": "Infrastructure release notes for inference serving; lacks agentic coding automation, agent eval harness, or software delivery workflow signal.",
    "__label_source": "llm"
  }
}