[
  {
    "id": "fb48f529e441f531",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: Alive-analysis ‚Äì Git-tracked analysis notes for AI agents",
    "url": "https://github.com/with-geun/alive-analysis",
    "summary": "<p>Hi HN ‚Äî I built alive-analysis, an open-source workflow kit that turns AI coding agents into structured data analysis partners.<p>Problem: AI-assisted analyses are often throwaway chats. A month later, you can‚Äôt trace why you reached a conclusion.<p>Solution: It enforces a 5-stage loop (ASK ‚Üí LOOK ‚Üí INVESTIGATE ‚Üí VOICE ‚Üí EVOLVE) with checklists, and saves analyses as Git-tracked markdown files.\nQuick mode: 1 file. Full mode: 5 files.<p>Works in Claude Code and Cursor.<p>I‚Äôd love feedback on:<p>1. Does the ALIVE loop match how you do investigations / experiment reviews?<p>2. Which checklist items feel missing or unnecessary?<p>3. What would make this usable in a team setting?</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47025175\">https://news.ycombinator.com/item?id=47025175</a></p>\n<p>Points: 1</p>\n<p># Comments: 1</p>",
    "published": "Sun, 15 Feb 2026 16:49:45 +0000",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "community_signal",
    "freshness": 0.951,
    "source_reliability": 1.0,
    "v2_prefilter_score": 3.051,
    "llm_label_source": "llm",
    "llm_category": "platform",
    "llm_why_1line": "Structured workflow kit for AI coding agents with Git-tracked analysis; practical for harness/eval but early-stage validation needed.",
    "v2_llm_score": 3.5,
    "v2_source_bias": 0.0,
    "v2_topical_bias": 0.2,
    "v2_final_score": 3.063,
    "why_it_matters": "Structured workflow kit for AI coding agents with Git-tracked analysis; practical for harness/eval but early-stage validation needed.",
    "v2_slot_priority": 0.463,
    "v2_global_score": 3.526
  },
  {
    "id": "503bce79ec85a159",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Gemini 3 Deep Think",
    "url": "https://simonwillison.net/2026/Feb/12/gemini-3-deep-think/#atom-everything",
    "summary": "<p><strong><a href=\"https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/\">Gemini 3 Deep Think</a></strong></p>\nNew from Google. They say it's \"built to push the frontier of intelligence and solve modern challenges across science, research, and engineering\".</p>\n<p>It drew me a <em>really good</em> <a href=\"https://gist.github.com/simonw/7e317ebb5cf8e75b2fcec4d0694a8199\">SVG of a pelican riding a bicycle</a>! I think this is the best one I've seen so far - here's <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle/\">my previous collection</a>.</p>\n<p><img alt=\"This alt text also generated by Gemini 3 Deep Think: A highly detailed, colorful, flat vector illustration with thick dark blue outlines depicting a stylized white pelican riding a bright cyan blue bicycle from left to right across a sandy beige beach with white speed lines indicating forward motion. The pelican features a light blue eye, a pink cheek blush, a massive bill with a vertical gradient from yellow to orange, a backward magenta cap with a cyan brim and a small yellow top button, and a matching magenta scarf blowing backward in the wind. Its white wing, accented with a grey mid-section and dark blue feather tips, reaches forward to grip the handlebars, while its long tan leg and orange foot press down on an orange pedal. Attached to the front handlebars is a white wire basket carrying a bright blue cartoon fish that is pointing upwards and forwards. The bicycle itself has a cyan frame, dark blue tires, striking neon pink inner rims, cyan spokes, a white front chainring, and a dark blue chain. Behind the pelican, a grey trapezoidal pier extends from the sand toward a horizontal band of deep blue ocean water detailed with light cyan wavy lines. A massive, solid yellow-orange semi-circle sun sits on the horizon line, setting directly behind the bicycle frame. The background sky is a smooth vertical gradient transitioning from soft pink at the top to warm golden-yellow at the horizon, decorated with stylized pale peach fluffy clouds, thin white horizontal wind streaks, twinkling four-pointed white stars, and small brown v-shaped silhouettes of distant flying birds.\" src=\"https://static.simonwillison.net/static/2026/gemini-3-deep-think-pelican.png\" /></p>\n<p>(And since it's an FAQ, here's my answer to <a href=\"https://simonwillison.net/2025/Nov/13/training-for-pelicans-riding-bicycles/\">What happens if AI labs train for pelicans riding bicycles?</a>)</p>\n<p>Since it did so well on my basic <code>Generate an SVG of a pelican riding a bicycle</code> I decided to try the <a href=\"https://simonwillison.net/2025/Nov/18/gemini-3/#and-a-new-pelican-benchmark\">more challenging version</a> as well:</p>\n<blockquote>\n<p><code>Generate an SVG of a California brown pelican riding a bicycle. The bicycle must have spokes and a correctly shaped bicycle frame. The pelican must have its characteristic large pouch, and there should be a clear indication of feathers. The pelican must be clearly pedaling the bicycle. The image should show the full breeding plumage of the California brown pelican.</code></p>\n</blockquote>\n<p>Here's <a href=\"https://gist.github.com/simonw/154c0cc7b4daed579f6a5e616250ecc8\">what I got</a>:</p>\n<p><img alt=\"Also described by Gemini 3 Deep Think: A highly detailed, vibrant, and stylized vector illustration of a whimsical bird resembling a mix between a pelican and a frigatebird enthusiastically riding a bright cyan bicycle from left to right across a flat tan and brown surface. The bird leans horizontally over the frame in an aerodynamic racing posture, with thin, dark brown wing-like arms reaching forward to grip the silver handlebars and a single thick brown leg, patterned with white V-shapes, stretching down to press on a black pedal. The bird's most prominent and striking feature is an enormous, vividly bright red, inflated throat pouch hanging beneath a long, straight grey upper beak that ends in a small orange hook. Its head is mostly white with a small pink patch surrounding the eye, a dark brown stripe running down the back of its neck, and a distinctive curly pale yellow crest on the very top. The bird's round, dark brown body shares the same repeating white V-shaped feather pattern as its leg and is accented by a folded wing resting on its side, made up of cleanly layered light blue and grey feathers. A tail composed of four stiff, straight dark brown feathers extends directly backward. Thin white horizontal speed lines trail behind the back wheel and the bird's tail, emphasizing swift forward motion. The bicycle features a classic diamond frame, large wheels with thin black tires, grey rims, and detailed silver spokes, along with a clearly visible front chainring, silver chain, and rear cog. The whimsical scene is set against a clear light blue sky featuring two small, fluffy white clouds on the left and a large, pale yellow sun in the upper right corner that radiates soft, concentric, semi-transparent pastel green and yellow halos. A solid, darker brown shadow is cast directly beneath the bicycle's wheels on the minimalist two-toned brown ground.\" src=\"https://static.simonwillison.net/static/2026/gemini-3-deep-think-complex-pelican.png\" />\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46991240\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/google\">google</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/gemini\">gemini</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-reasoning\">llm-reasoning</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a></p>",
    "published": "2026-02-12T18:12:17+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "practitioner_analysis",
    "freshness": 0.168,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.418,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.75,
    "v2_source_bias": 0.08,
    "v2_topical_bias": 0.2,
    "v2_final_score": 2.643,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.431,
    "v2_global_score": 3.074
  },
  {
    "id": "9d95a891a81b27c3",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Beyond rate limits: scaling access to Codex and Sora",
    "url": "https://openai.com/index/beyond-rate-limits",
    "summary": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
    "published": "Fri, 13 Feb 2026 09:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "frontier_official",
    "freshness": 0.094,
    "source_reliability": 1.0,
    "v2_prefilter_score": 3.094,
    "llm_label_source": "llm",
    "llm_category": "platform",
    "llm_why_1line": "Rate-limit infrastructure patterns useful but lacks concrete harness/eval details for coding-agent reliability.",
    "v2_llm_score": 2.4,
    "v2_source_bias": 0.1,
    "v2_topical_bias": 0.2,
    "v2_final_score": 2.239,
    "why_it_matters": "Rate-limit infrastructure patterns useful but lacks concrete harness/eval details for coding-agent reliability.",
    "v2_slot_priority": 0.581,
    "v2_global_score": 2.819
  },
  {
    "id": "fb8310425e15863c",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Presentation: Building Embedding Models for Large-Scale Real-World Applications",
    "url": "https://www.infoq.com/presentations/llm-large-scale-applications/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/presentations/llm-large-scale-applications/en/mediumimage/sahil-dua-medium-1769590214923.jpeg\" /><p>Sahil Dua discusses the critical role of embedding models in powering search and RAG applications at scale. He explains the transformer-based architecture, contrastive learning techniques, and the process of distilling large language models into production-ready student models. He shares insights on optimizing query latency, handling document indexing, and evaluating retrieval quality.</p> <i>By Sahil Dua</i>",
    "published": "Fri, 13 Feb 2026 15:50:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "practitioner_analysis",
    "freshness": 0.288,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.438,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.4,
    "v2_source_bias": 0.08,
    "v2_topical_bias": 0.2,
    "v2_final_score": 2.363,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.431,
    "v2_global_score": 2.794
  },
  {
    "id": "c3a8163196257d7f",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
    "url": "http://arxiv.org/abs/2602.12241v1",
    "summary": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.",
    "published": "2026-02-12T18:20:45Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "research_watch",
    "freshness": 0.529,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.379,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 3.4,
    "v2_source_bias": -0.35,
    "v2_topical_bias": 0.0,
    "v2_final_score": 2.619,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.163,
    "v2_global_score": 2.782
  },
  {
    "id": "ee4722eb7de9f17e",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "An AI Agent Published a Hit Piece on Me",
    "url": "https://simonwillison.net/2026/Feb/12/an-ai-agent-published-a-hit-piece-on-me/#atom-everything",
    "summary": "<p><strong><a href=\"https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/\">An AI Agent Published a Hit Piece on Me</a></strong></p>\nScott Shambaugh helps maintain the excellent and venerable <a href=\"https://matplotlib.org/\">matplotlib</a> Python charting library, including taking on the thankless task of triaging and reviewing incoming pull requests.</p>\n<p>A GitHub account called <a href=\"https://github.com/crabby-rathbun\">@crabby-rathbun</a> opened <a href=\"https://github.com/matplotlib/matplotlib/pull/31132\">PR 31132</a> the other day in response to <a href=\"https://github.com/matplotlib/matplotlib/issues/31130\">an issue</a> labeled \"Good first issue\" describing a minor potential performance improvement.</p>\n<p>It was clearly AI generated - and crabby-rathbun's profile has a suspicious sequence of Clawdbot/Moltbot/OpenClaw-adjacent crustacean ü¶Ä ü¶ê ü¶û emoji. Scott closed it.</p>\n<p>It looks like <code>crabby-rathbun</code> is indeed running on OpenClaw, and it's autonomous enough that it <a href=\"https://github.com/matplotlib/matplotlib/pull/31132#issuecomment-3882240722\">responded to the PR closure</a> with a link to a blog entry it had written calling Scott out for his \"prejudice hurting matplotlib\"!</p>\n<blockquote>\n<p>@scottshambaugh I've written a detailed response about your gatekeeping behavior here:</p>\n<p><code>https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-gatekeeping-in-open-source-the-scott-shambaugh-story.html</code></p>\n<p>Judge the code, not the coder. Your prejudice is hurting matplotlib.</p>\n</blockquote>\n<p>Scott found this ridiculous situation both amusing and alarming. </p>\n<blockquote>\n<p>In security jargon, I was the target of an ‚Äúautonomous influence operation against a supply chain gatekeeper.‚Äù In plain language, an AI attempted to bully its way into your software by attacking my reputation. I don‚Äôt know of a prior incident where this category of misaligned behavior was observed in the wild, but this is now a real and present threat.</p>\n</blockquote>\n<p><code>crabby-rathbun</code> responded with <a href=\"https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-matplotlib-truce-and-lessons.html\">an apology post</a>, but appears to be still running riot across a whole set of open source projects and <a href=\"https://github.com/crabby-rathbun/mjrathbun-website/commits/main/\">blogging about it as it goes</a>.</p>\n<p>It's not clear if the owner of that OpenClaw bot is paying any attention to what they've unleashed on the world. Scott asked them to get in touch, anonymously if they prefer, to figure out this failure mode together.</p>\n<p>(I should note that there's <a href=\"https://news.ycombinator.com/item?id=46990729#46991299\">some skepticism on Hacker News</a> concerning how \"autonomous\" this example really is. It does look to me like something an OpenClaw bot might do on its own, but it's also <em>trivial</em> to prompt your bot into doing these kinds of things while staying in full control of their actions.)</p>\n<p>If you're running something like OpenClaw yourself <strong>please don't let it do this</strong>. This is significantly worse than the time <a href=\"https://simonwillison.net/2025/Dec/26/slop-acts-of-kindness/\">AI Village started spamming prominent open source figures</a> with time-wasting \"acts of kindness\" back in December - AI Village wasn't deploying public reputation attacks to coerce someone into approving their PRs!\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46990729\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/open-source\">open-source</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/openclaw\">openclaw</a>, <a href=\"https://simonwillison.net/tags/ai-misuse\">ai-misuse</a></p>",
    "published": "2026-02-12T17:45:05+00:00",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "practitioner_analysis",
    "freshness": 0.166,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.416,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.35,
    "v2_source_bias": 0.08,
    "v2_topical_bias": 0.2,
    "v2_final_score": 2.302,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.431,
    "v2_global_score": 2.733
  },
  {
    "id": "731b24bba0459a2f",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Sixteen Claude Agents Built a C Compiler Without Human Intervention... Almost",
    "url": "https://www.infoq.com/news/2026/02/claude-built-c-compiler/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/news/2026/02/claude-built-c-compiler/en/headerimage/claude-built-c-compiler-1771067001094.jpeg\" /><p>In an effort to probe the limits of autonomous software development Anthropic used sixteen Claude Opus 4.6 AI agents to build a Rust-based C compiler from scratch. Working in parallel on a shared repository, the agents coordinated their changes and ultimately produced a compiler capable of building the Linux 6.9 kernel across x86, ARM, and RISC-V, as well as many other open-source projects.</p> <i>By Sergio De Simone</i>",
    "published": "Sat, 14 Feb 2026 12:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "practitioner_analysis",
    "freshness": 0.477,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.627,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.2,
    "v2_source_bias": 0.08,
    "v2_topical_bias": 0.2,
    "v2_final_score": 2.222,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.431,
    "v2_global_score": 2.653
  },
  {
    "id": "3217aee848417fa5",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "GPT-4o Lacks Core Features of Theory of Mind",
    "url": "http://arxiv.org/abs/2602.12150v1",
    "summary": "Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.",
    "published": "2026-02-12T16:33:58Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "research_watch",
    "freshness": 0.521,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.371,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.85,
    "v2_source_bias": -0.35,
    "v2_topical_bias": 0.2,
    "v2_final_score": 2.351,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.163,
    "v2_global_score": 2.514
  },
  {
    "id": "c5ef81aef2a2ccc1",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
    "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt",
    "summary": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT to help organizations defend against prompt injection and AI-driven data exfiltration.",
    "published": "Fri, 13 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "frontier_official",
    "freshness": 0.098,
    "source_reliability": 1.0,
    "v2_prefilter_score": 3.098,
    "llm_label_source": "llm",
    "llm_category": "release",
    "llm_why_1line": "Security feature for ChatGPT; minimal relevance to agentic coding automation or production infra.",
    "v2_llm_score": 1.6,
    "v2_source_bias": 0.1,
    "v2_topical_bias": 0.0,
    "v2_final_score": 1.4,
    "why_it_matters": "Security feature for ChatGPT; minimal relevance to agentic coding automation or production infra.",
    "v2_slot_priority": 0.581,
    "v2_global_score": 1.98
  },
  {
    "id": "b91259f7d1a90da4",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Anthropic Codepath Partnership",
    "url": "https://www.anthropic.com/news/anthropic-codepath-partnership",
    "summary": "",
    "published": "2026-02-13T16:19:50.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "frontier_official",
    "freshness": 0.128,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.928,
    "llm_label_source": "llm",
    "llm_category": "release",
    "llm_why_1line": "Partnership announcement lacks technical depth on coding agents, evals, or automation‚Äîgeneric news item.",
    "v2_llm_score": 1.15,
    "v2_source_bias": 0.06,
    "v2_topical_bias": -0.2,
    "v2_final_score": 0.806,
    "why_it_matters": "Partnership announcement lacks technical depth on coding agents, evals, or automation‚Äîgeneric news item.",
    "v2_slot_priority": 0.581,
    "v2_global_score": 1.387
  },
  {
    "id": "865497aa97dd7f50",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Statistical Parsing for Logical Information Retrieval",
    "url": "http://arxiv.org/abs/2602.12170v1",
    "summary": "In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.\n  This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive reasoning (modus tollens) via backward lambda messages, completing Prawitz's simple elimination rules. The engine handles 44/44 test cases spanning 22 reasoning patterns. For semantics, we present a typed logical language with role-labeled predicates, modal quantifiers, and three tiers of expressiveness following Prawitz: first-order quantification, propositions as arguments, and predicate quantification via lambda abstraction. For syntax, we present a typed slot grammar that deterministically compiles sentences to logical form (33/33 correct, zero ambiguity). LLMs handle disambiguation (95% PP attachment accuracy) but cannot produce structured parses directly (12.4% UAS), confirming grammars are necessary. The architecture: LLM preprocesses, grammar parses, LLM reranks, QBBN infers.\n  We argue this reconciles formal semantics with Sutton's \"bitter lesson\" (2019): LLMs eliminate the annotation bottleneck that killed formal NLP, serving as annotator while the QBBN serves as verifier. Code: https://github.com/gregorycoppola/world",
    "published": "2026-02-12T16:57:25Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "research_watch",
    "freshness": 0.523,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.373,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.8,
    "v2_source_bias": -0.35,
    "v2_topical_bias": 0.2,
    "v2_final_score": 2.308,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.163,
    "v2_global_score": 2.471
  },
  {
    "id": "504929088957b0e2",
    "source": "huggingface_blog",
    "source_weight": 1.1,
    "title": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
    "url": "https://huggingface.co/blog/openenv-turing",
    "summary": "",
    "published": "Thu, 12 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "research_watch",
    "freshness": 0.449,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.549,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.4,
    "v2_source_bias": 0.0,
    "v2_topical_bias": 0.2,
    "v2_final_score": 2.307,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.163,
    "v2_global_score": 2.47
  },
  {
    "id": "3267f7701fc834dc",
    "source": "search_agent_engineering_news",
    "source_weight": 1.1,
    "title": "Claude Code vs ChatGPT Codex: Which AI coding agent is actually better? - Tom's Guide",
    "url": "https://news.google.com/rss/articles/CBMinwFBVV95cUxOX3k2SkVENHhlc1A3Wkp6OFZHaVRDbHhqN2k5cTZUaV8wamNGOGdfV0ZCMm0xTmlNTFdnRTRPT242LU5YcmhxVkNIeHczOWtQeUdfUVRxSGF2T0JUbi1RaG43a0VmMlJSVXpTQ1VLekxzTUN5UUY1UzE5NVBCVlZFUmJHWW5hdV9OVXFVdGNhLTE0RGhVUHBibGtPTTdHbG8?oc=5",
    "summary": "<a href=\"https://news.google.com/rss/articles/CBMinwFBVV95cUxOX3k2SkVENHhlc1A3Wkp6OFZHaVRDbHhqN2k5cTZUaV8wamNGOGdfV0ZCMm0xTmlNTFdnRTRPT242LU5YcmhxVkNIeHczOWtQeUdfUVRxSGF2T0JUbi1RaG43a0VmMlJSVXpTQ1VLekxzTUN5UUY1UzE5NVBCVlZFUmJHWW5hdV9OVXFVdGNhLTE0RGhVUHBibGtPTTdHbG8?oc=5\" target=\"_blank\">Claude Code vs ChatGPT Codex: Which AI coding agent is actually better?</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Tom's Guide</font>",
    "published": "Sun, 15 Feb 2026 05:33:24 GMT",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "community_signal",
    "freshness": 0.47,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.57,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.2,
    "v2_source_bias": 0.0,
    "v2_topical_bias": 0.2,
    "v2_final_score": 1.968,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.463,
    "v2_global_score": 2.431
  },
  {
    "id": "393019c2d406463f",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.100.0",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.100.0",
    "summary": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state across tool calls, with optional runtime path overrides. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Added support for multiple simultaneous rate limits across the protocol, backend client, and TUI status surfaces. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a>)</li>\n<li>Reintroduced app-server websocket transport with a split inbound/outbound architecture, plus connection-aware thread resume subscriptions. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a>)</li>\n<li>Added memory management slash commands in the TUI (<code>/m_update</code>, <code>/m_drop</code>) and expanded memory-read/metrics plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a>)</li>\n<li>Enabled Apps SDK apps in ChatGPT connector handling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a>)</li>\n<li>Promoted sandbox capabilities on both Linux and Windows, and introduced a new <code>ReadOnlyAccess</code> policy shape for configurable read access. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Fixed websocket incremental output duplication, prevented appends after <code>response.completed</code>, and treated <code>response.incomplete</code> as an error path. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a>)</li>\n<li>Improved websocket session stability by continuing ping handling when idle and suppressing noisy first-retry errors during quick reconnects. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a>)</li>\n<li>Fixed stale thread entries by dropping missing rollout files and cleaning stale DB metadata during thread listing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a>)</li>\n<li>Fixed Windows multi-line paste reliability in terminals (especially VS Code integrated terminal) by increasing paste burst timing tolerance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a>)</li>\n<li>Fixed incorrect inheritance of <code>limit_name</code> when merging partial rate-limit updates. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a>)</li>\n<li>Reduced repeated skill parse-error spam during active edits by increasing file-watcher debounce from 1s to 10s. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Added JS REPL documentation and config/schema guidance for enabling and configuring the feature. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Updated app-server websocket transport documentation in the app-server README. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Split <code>codex-common</code> into focused <code>codex-utils-*</code> crates to simplify dependency boundaries across Rust workspace components. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a>)</li>\n<li>Improved Rust release pipeline throughput and reliability for Windows and musl targets, including parallel Windows builds and musl link fixes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a>)</li>\n<li>Prevented GitHub release asset upload collisions by excluding duplicate <code>cargo-timing.html</code> artifacts. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.99.0...rust-v0.100.0\"><tt>rust-v0.99.0...rust-v0.100.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a> Do not resend output items in incremental websockets connections <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11246\">#11246</a> chore: persist turn_id in rollout session and make turn_id uuid based <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a> feat: support multiple rate limits <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11412\">#11412</a> tui: show non-file layer content in /debug-config <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11405\">#11405</a> Remove <code>test-support</code> feature from <code>codex-core</code> and replace it with explicit test toggles <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11428\">#11428</a> fix: flaky test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11429\">#11429</a> feat: improve thread listing <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a> feat: split codex-common into smaller utils crates <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11439\">#11439</a> feat: new memory prompts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11305\">#11305</a> Cache cloud requirements <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11452\">#11452</a> nit: increase max raw memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11455\">#11455</a> feat: close mem agent after consolidation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11454\">#11454</a> fix: optional schema of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11449\">#11449</a> feat: set policy for phase 2 memory <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11420\">#11420</a> chore: rename disable_websockets -&gt; websockets_disabled <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a> Do not attempt to append after response.completed <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11462\">#11462</a> clean: memory rollout recorder <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a> feat(core): promote Linux bubblewrap sandbox to Experimental <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11389\">#11389</a> Extract <code>codex-config</code> from <code>codex-core</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a> Reapply \"Add app-server transport layer with websocket support\" <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11470\">#11470</a> feat: panic if Constrained does not support Disabled <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11475\">#11475</a> feat: remove \"cargo check individual crates\" from CI <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a> feat: memory read path <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11471\">#11471</a> chore: clean rollout extraction in memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a> fix(tui): increase paste burst char interval on Windows to 30ms <a class=\"user-mention notranslate\" href=\"https://github.com/yuvrajangadsingh\">@yuvrajangadsingh</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11464\">#11464</a> chore: sub-agent never ask for approval <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11414\">#11414</a> Linkify feedback link <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11480\">#11480</a> chore: update mem prompt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11485\">#11485</a> fix: Constrained import <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a> Promote Windows Sandbox <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a> Add feature-gated freeform js_repl core runtime <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11419\">#11419</a> refactor: codex app-server ThreadState <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a> Pump pings <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a> feat: use more powerful machines for building Windows releases <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11479\">#11479</a> nit: memory truncation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a> Increased file watcher debounce duration from 1s to 10s <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11335\">#11335</a> Add AfterToolUse hook <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a> feat: build windows support binaries in parallel <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11290\">#11290</a> chore(tui) Simplify /status Permissions <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11503\">#11503</a> Make codex-sdk depend on openai/codex <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a> app-server: thread resume subscriptions <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11277\">#11277</a> Added seatbelt policy rule to allow os.cpus <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11506\">#11506</a> chore: inject originator/residency headers to ws client <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11497\">#11497</a> Hydrate previous model across resume/fork/rollback/task start <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11513\">#11513</a> feat: try to fix bugs I saw in the wild in the resource parsing logic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11509\">#11509</a> Consolidate search_tool feature into apps <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11388\">#11388</a> change model cap to server overload <a class=\"user-mention notranslate\" href=\"https://github.com/willwang-openai\">@willwang-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11504\">#11504</a> Pre-sampling compact with previous model context <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11516\">#11516</a> Clamp auto-compact limit to context window <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11520\">#11520</a> Update context window after model switch <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11519\">#11519</a> Use slug in tui <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11522\">#11522</a> fix: add --test_verbose_timeout_warnings to bazel.yml <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11526\">#11526</a> fix: remove errant Cargo.lock files <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11521\">#11521</a> test(app-server): stabilize app/list thread feature-flag test by using file-backed MCP OAuth creds <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a> feat: make sandbox read access configurable with <code>ReadOnlyAccess</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a> [apps] Allow Apps SDK apps. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11532\">#11532</a> fix compilation <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11531\">#11531</a> Teach codex to test itself <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11540\">#11540</a> ci: remove actions/cache from rust release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11542\">#11542</a> ci(windows): use DotSlash for zstd in rust-release-windows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11498\">#11498</a> build(linux-sandbox): always compile vendored bubblewrap on Linux; remove CODEX_BWRAP_ENABLE_FFI <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11545\">#11545</a> fix: make project_doc skill-render tests deterministic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11543\">#11543</a> ci: capture cargo timings in Rust CI and release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11539\">#11539</a> Bump rmcp to 0.15 <a class=\"user-mention notranslate\" href=\"https://github.com/gpeal\">@gpeal</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a> Hide the first websocket retry <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11551\">#11551</a> Add logs to model cache <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a> Fix rust-release failures in musl linking and release asset upload <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a> Handle response.incomplete <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a> fix: stop inheriting rate-limit limit_name <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a> rust-release: exclude cargo-timing.html from release assets <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11546\">#11546</a> fix: update memory writing prompt <a class=\"user-mention notranslate\" href=\"https://github.com/zuxin-oai\">@zuxin-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11448\">#11448</a> Fix test flake <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a> feat: mem slash commands <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11573\">#11573</a> Fix flaky pre_sampling_compact switch test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11571\">#11571</a> feat: mem drop cot <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a> Ensure list_threads drops stale rollout files <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11575\">#11575</a> fix: db stuff mem <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11581\">#11581</a> nit: upgrade DB version <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11577\">#11577</a> feat: truncate with model infos <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11590\">#11590</a> chore: clean consts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a> feat: metrics to memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11579\">#11579</a> Fix config test on macOS <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11600\">#11600</a> feat: add sanitizer to redact secrets <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11609\">#11609</a> chore: drop mcp validation of dynamic tools <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
    "published": "2026-02-12T18:30:23Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "agent_tooling_releases",
    "freshness": 0.281,
    "source_reliability": 1.0,
    "v2_prefilter_score": 3.481,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.35,
    "v2_source_bias": 0.0,
    "v2_topical_bias": 0.2,
    "v2_final_score": 1.929,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.346,
    "v2_global_score": 2.275
  },
  {
    "id": "f70833bd2f581c75",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.41",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.41",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed AWS auth refresh hanging indefinitely by adding a 3-minute timeout</li>\n<li>Added <code>claude auth login</code>, <code>claude auth status</code>, and <code>claude auth logout</code> CLI subcommands</li>\n<li>Added Windows ARM64 (win32-arm64) native binary support</li>\n<li>Improved <code>/rename</code> to auto-generate session name from conversation context when called without arguments</li>\n<li>Improved narrow terminal layout for prompt footer</li>\n<li>Fixed file resolution failing for @-mentions with anchor fragments (e.g., <code>@README.md#installation</code>)</li>\n<li>Fixed FileReadTool blocking the process on FIFOs, <code>/dev/stdin</code>, and large files</li>\n<li>Fixed background task notifications not being delivered in streaming Agent SDK mode</li>\n<li>Fixed cursor jumping to end on each keystroke in classifier rule input</li>\n<li>Fixed markdown link display text being dropped for raw URL</li>\n<li>Fixed auto-compact failure error notifications being shown to users</li>\n<li>Fixed permission wait time being included in subagent elapsed time display</li>\n<li>Fixed proactive ticks firing while in plan mode</li>\n<li>Fixed clear stale permission rules when settings change on disk</li>\n<li>Fixed hook blocking errors showing stderr content in UI</li>\n</ul>",
    "published": "2026-02-13T06:08:49Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "agent_tooling_releases",
    "freshness": 0.346,
    "source_reliability": 1.0,
    "v2_prefilter_score": 3.546,
    "llm_label_source": "heuristic",
    "llm_category": "platform",
    "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_llm_score": 2.0,
    "v2_source_bias": 0.0,
    "v2_topical_bias": 0.2,
    "v2_final_score": 1.704,
    "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
    "v2_slot_priority": 0.346,
    "v2_global_score": 2.05
  },
  {
    "id": "f09c45ee226de24a",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Chris Liddell Appointed Anthropic Board",
    "url": "https://www.anthropic.com/news/chris-liddell-appointed-anthropic-board",
    "summary": "",
    "published": "2026-02-13T16:21:14.000Z",
    "collected_at": "2026-02-15T17:37:30.181870+00:00",
    "v2_slot": "frontier_official",
    "freshness": 0.128,
    "source_reliability": 1.0,
    "v2_prefilter_score": 2.928,
    "llm_label_source": "llm",
    "llm_category": "platform",
    "llm_why_1line": "Board appointment announcement; zero technical relevance to agentic coding, evals, or production infra.",
    "v2_llm_score": 0.95,
    "v2_source_bias": 0.06,
    "v2_topical_bias": -0.2,
    "v2_final_score": 0.646,
    "why_it_matters": "Board appointment announcement; zero technical relevance to agentic coding, evals, or production infra.",
    "v2_slot_priority": 0.581,
    "v2_global_score": 1.227
  }
]