[
  {
    "id": "8bed79565182bbc6",
    "source": "llamaindex_releases",
    "source_weight": 0.95,
    "title": "v0.14.14",
    "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.14",
    "summary": "<h1>Release Notes</h1>\n<h2>[2026-02-10]</h2>\n<h3>llama-index-callbacks-wandb [0.4.2]</h3>\n<ul>\n<li>Fix potential crashes and improve security defaults in core components (<a href=\"https://github.com/run-llama/llama_index/pull/20610\">#20610</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.14]</h3>\n<ul>\n<li>fix: catch pydantic ValidationError in VectorStoreQueryOutputParser (<a href=\"https://github.com/run-llama/llama_index/pull/20450\">#20450</a>)</li>\n<li>fix: distinguish empty string from None in MediaResource.hash (<a href=\"https://github.com/run-llama/llama_index/pull/20451\">#20451</a>)</li>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n<li>fix(core): fallback to bundled nltk cache if env var missing (<a href=\"https://github.com/run-llama/llama_index/pull/20528\">#20528</a>)</li>\n<li>feat(callbacks): add TokenBudgetHandler for cost governance (<a href=\"https://github.com/run-llama/llama_index/pull/20546\">#20546</a>)</li>\n<li>fix(core):handled a edge case in truncate_text function (<a href=\"https://github.com/run-llama/llama_index/pull/20551\">#20551</a>)</li>\n<li>fix(core):fix in types Thread passing None when target is None instead of copy_context().run (<a href=\"https://github.com/run-llama/llama_index/pull/20553\">#20553</a>)</li>\n<li>chore: bump llama-index lockfile, and minor test tweaks (<a href=\"https://github.com/run-llama/llama_index/pull/20556\">#20556</a>)</li>\n<li>Compatibility for workflows context changes (<a href=\"https://github.com/run-llama/llama_index/pull/20557\">#20557</a>)</li>\n<li>test(core): fix cache dir path test for Windows compatibility (<a href=\"https://github.com/run-llama/llama_index/pull/20566\">#20566</a>)</li>\n<li>fix(tests): enforce utf-8 encoding in json reader tests for windows compatibility (<a href=\"https://github.com/run-llama/llama_index/pull/20576\">#20576</a>)</li>\n<li>Fix BM25Retriever mapping in upgrade tool / 修复升级工具中的 BM25Retriever 映射 (<a href=\"https://github.com/run-llama/llama_index/pull/20582\">#20582</a>)</li>\n<li>fix(agent): handle empty LLM responses with retry logic and add test cases (<a href=\"https://github.com/run-llama/llama_index/pull/20596\">#20596</a>)</li>\n<li>fix: add show_progress parameter to run_transformations to prevent unexpected keyword argument error (<a href=\"https://github.com/run-llama/llama_index/pull/20608\">#20608</a>)</li>\n<li>Fix potential crashes and improve security defaults in core components (<a href=\"https://github.com/run-llama/llama_index/pull/20610\">#20610</a>)</li>\n<li>Add core 3.14 tests (<a href=\"https://github.com/run-llama/llama_index/pull/20619\">#20619</a>)</li>\n</ul>\n<h3>llama-index-embeddings-cohere [0.7.0]</h3>\n<ul>\n<li>fix(embeddings-cohere): add retry logic with tenacity (<a href=\"https://github.com/run-llama/llama_index/pull/20592\">#20592</a>)</li>\n</ul>\n<h3>llama-index-embeddings-google-genai [0.3.2]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n</ul>\n<h3>llama-index-embeddings-siliconflow [0.3.2]</h3>\n<ul>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n</ul>\n<h3>llama-index-embeddings-upstage [0.5.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-graph-stores-falkordb [0.4.2]</h3>\n<ul>\n<li>fix(falkordb): Fix MENTIONS relationship creation with triplet_source_id (<a href=\"https://github.com/run-llama/llama_index/pull/20650\">#20650</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.8]</h3>\n<ul>\n<li>chore: Update cacheable Anthropic models (<a href=\"https://github.com/run-llama/llama_index/pull/20581\">#20581</a>)</li>\n<li>chore: add support for opus 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20635\">#20635</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.8]</h3>\n<ul>\n<li>fix bedrock converse empty tool config issue (<a href=\"https://github.com/run-llama/llama_index/pull/20571\">#20571</a>)</li>\n<li>fix(llms-bedrock-converse): improve bedrock converse retry handling (<a href=\"https://github.com/run-llama/llama_index/pull/20590\">#20590</a>)</li>\n<li>feat(bedrock-converse): Add support for Claude Opus 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20637\">#20637</a>)</li>\n<li>Add support for adaptive thinking in Bedrock (<a href=\"https://github.com/run-llama/llama_index/pull/20659\">#20659</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-llms-cohere [0.7.1]</h3>\n<ul>\n<li>Feat: add custom base_url support to Cohere LLM (<a href=\"https://github.com/run-llama/llama_index/pull/20534\">#20534</a>)</li>\n<li>fix(llms-cohere): handle additional error types in retry logic (<a href=\"https://github.com/run-llama/llama_index/pull/20591\">#20591</a>)</li>\n</ul>\n<h3>llama-index-llms-dashscope [0.5.2]</h3>\n<ul>\n<li>fix(dashscope): remove empty tool_calls from assistant messages (<a href=\"https://github.com/run-llama/llama_index/pull/20535\">#20535</a>)</li>\n</ul>\n<h3>llama-index-llms-google-genai [0.8.7]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n<li>fix(decorator):adds logic to llm_retry_decorator for async methods. (<a href=\"https://github.com/run-llama/llama_index/pull/20588\">#20588</a>)</li>\n<li>Fix/google genai cleanup (<a href=\"https://github.com/run-llama/llama_index/pull/20607\">#20607</a>)</li>\n<li>fix(google-genai): skip model meta fetch when not needed (<a href=\"https://github.com/run-llama/llama_index/pull/20639\">#20639</a>)</li>\n</ul>\n<h3>llama-index-llms-huggingface-api [0.6.2]</h3>\n<ul>\n<li>Update sensible default provider for huggingface inference api (<a href=\"https://github.com/run-llama/llama_index/pull/20589\">#20589</a>)</li>\n</ul>\n<h3>llama-index-llms-langchain [0.7.1]</h3>\n<ul>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n</ul>\n<h3>llama-index-llms-openai [0.6.18]</h3>\n<ul>\n<li>OpenAI response fix (<a href=\"https://github.com/run-llama/llama_index/pull/20538\">#20538</a>)</li>\n<li>feat: Add support for gpt-5.2-chat model (<a href=\"https://github.com/run-llama/llama_index/pull/20549\">#20549</a>)</li>\n<li>fix(openai): make image_url detail optional in message dict (<a href=\"https://github.com/run-llama/llama_index/pull/20609\">#20609</a>)</li>\n<li>Add new reasoning types (<a href=\"https://github.com/run-llama/llama_index/pull/20612\">#20612</a>)</li>\n<li>fix(openai): exclude unsupported params for all reasoning models (<a href=\"https://github.com/run-llama/llama_index/pull/20627\">#20627</a>)</li>\n</ul>\n<h3>llama-index-llms-openai-like [0.6.0]</h3>\n<ul>\n<li>make transformers an optional dependency for openai-like (<a href=\"https://github.com/run-llama/llama_index/pull/20580\">#20580</a>)</li>\n</ul>\n<h3>llama-index-llms-openrouter [0.4.4]</h3>\n<ul>\n<li>make transformers an optional dependency for openai-like (<a href=\"https://github.com/run-llama/llama_index/pull/20580\">#20580</a>)</li>\n</ul>\n<h3>llama-index-llms-siliconflow [0.4.3]</h3>\n<ul>\n<li>Fix DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated (<a href=\"https://github.com/run-llama/llama_index/pull/20517\">#20517</a>)</li>\n</ul>\n<h3>llama-index-llms-upstage [0.7.0]</h3>\n<ul>\n<li>add new upstage model(solar-pro3) (<a href=\"https://github.com/run-llama/llama_index/pull/20544\">#20544</a>)</li>\n</ul>\n<h3>llama-index-llms-vllm [0.6.2]</h3>\n<ul>\n<li>feat: add openai-like server mode for VllmServer (<a href=\"https://github.com/run-llama/llama_index/pull/20537\">#20537</a>)</li>\n</ul>\n<h3>llama-index-memory-bedrock-agentcore [0.1.2]</h3>\n<ul>\n<li>Add event and memory record deletion methods in bedrock-agentcorememory (<a href=\"https://github.com/run-llama/llama_index/pull/20428\">#20428</a>)</li>\n<li>chore(deps): update llama-index-core dependency lock to include 0.14.x (<a href=\"https://github.com/run-llama/llama_index/pull/20483\">#20483</a>)</li>\n</ul>\n<h3>llama-index-memory-mem0 [1.0.0]</h3>\n<ul>\n<li>fix: mem0 integration cleanup + refactor (<a href=\"https://github.com/run-llama/llama_index/pull/20532\">#20532</a>)</li>\n</ul>\n<h3>llama-index-node-parser-chonkie [0.1.1]</h3>\n<ul>\n<li>feat: add chonkie integration (<a href=\"https://github.com/run-llama/llama_index/pull/20622\">#20622</a>)</li>\n<li>update readme (<a href=\"https://github.com/run-llama/llama_index/pull/20656\">#20656</a>)</li>\n</ul>\n<h3>llama-index-node-parser-docling [0.4.2]</h3>\n<ul>\n<li>fix: catch pydantic ValidationError in VectorStoreQueryOutputParser (<a href=\"https://github.com/run-llama/llama_index/pull/20450\">#20450</a>)</li>\n</ul>\n<h3>llama-index-packs-code-hierarchy [0.6.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-gmail-openai-agent [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-multidoc-autoretrieval [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-panel-chatbot [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-recursive-retriever [0.7.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-packs-resume-screener [0.9.3]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-retry-engine-weaviate [0.5.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-streamlit-chatbot [0.5.2]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-sub-question-weaviate [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-packs-timescale-vector-autoretrieval [0.4.1]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 12 directories with 14 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20578\">#20578</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-cohere-rerank [0.6.0]</h3>\n<ul>\n<li>fix(cohere-rerank): add retry logic and tenacity dependency to cohere rerank (<a href=\"https://github.com/run-llama/llama_index/pull/20593\">#20593</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-nvidia-rerank [0.5.4]</h3>\n<ul>\n<li>fix(nvidia-rerank): fix initialization logic for on-prem auth (<a href=\"https://github.com/run-llama/llama_index/pull/20560\">#20560</a>)</li>\n<li>fix(nvidia-rerank): correct private attribute reference (<a href=\"https://github.com/run-llama/llama_index/pull/20570\">#20570</a>)</li>\n<li>fix(nvidia-rerank): Fix POST request url for locally hosted NIM rerankers (<a href=\"https://github.com/run-llama/llama_index/pull/20579\">#20579</a>)</li>\n</ul>\n<h3>llama-index-postprocessor-tei-rerank [0.4.2]</h3>\n<ul>\n<li>fix(tei-rerank): use index field from API response for correct score … (<a href=\"https://github.com/run-llama/llama_index/pull/20599\">#20599</a>)</li>\n<li>test(tei-rerank): add test coverage for rerank retry coverage (<a href=\"https://github.com/run-llama/llama_index/pull/20600\">#20600</a>)</li>\n</ul>\n<h3>llama-index-protocols-ag-ui [0.2.4]</h3>\n<ul>\n<li>fix: avoid ValueError in ag-ui message conversion for multi-block ChatMessages (<a href=\"https://github.com/run-llama/llama_index/pull/20648\">#20648</a>)</li>\n</ul>\n<h3>llama-index-readers-datasets [0.1.0]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-readers-microsoft-sharepoint [0.7.0]</h3>\n<ul>\n<li>Sharepoint page support events (<a href=\"https://github.com/run-llama/llama_index/pull/20572\">#20572</a>)</li>\n</ul>\n<h3>llama-index-readers-obsidian [0.6.1]</h3>\n<ul>\n<li>Langchain1.x support (<a href=\"https://github.com/run-llama/llama_index/pull/20472\">#20472</a>)</li>\n</ul>\n<h3>llama-index-readers-service-now [0.2.2]</h3>\n<ul>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.6]</h3>\n<ul>\n<li>feat: implement partial_params support to McpToolSpec (<a href=\"https://github.com/run-llama/llama_index/pull/20554\">#20554</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp-discovery [0.1.0]</h3>\n<ul>\n<li>Add llama-index-tools-mcp-discovery integration (<a href=\"https://github.com/run-llama/llama_index/pull/20502\">#20502</a>)</li>\n</ul>\n<h3>llama-index-tools-moss [0.1.0]</h3>\n<ul>\n<li>feat(tools): add Moss search engine integration (<a href=\"https://github.com/run-llama/llama_index/pull/20615\">#20615</a>)</li>\n</ul>\n<h3>llama-index-tools-seltz [0.1.0]</h3>\n<ul>\n<li>feat(tools): add Seltz web knowledge tool integration (<a href=\"https://github.com/run-llama/llama_index/pull/20626\">#20626</a>)</li>\n</ul>\n<h3>llama-index-tools-typecast [0.1.0]</h3>\n<ul>\n<li>Migrate Typecast tool to V2 API for voices endpoints (<a href=\"https://github.com/run-llama/llama_index/pull/20548\">#20548</a>)</li>\n</ul>\n<h3>llama-index-tools-wolfram-alpha [0.5.0]</h3>\n<ul>\n<li>feat(wolfram-alpha): switch to LLM API with bearer auth (<a href=\"https://github.com/run-llama/llama_index/pull/20586\">#20586</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-clickhouse [0.6.2]</h3>\n<ul>\n<li>fix(clickhouse): Add drop_existing_table parameter to prevent data loss (<a href=\"https://github.com/run-llama/llama_index/pull/20651\">#20651</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [0.9.6]</h3>\n<ul>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-mongodb [0.9.1]</h3>\n<ul>\n<li>Update MongoDB vector store tests to use newer model (<a href=\"https://github.com/run-llama/llama_index/pull/20515\">#20515</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-oceanbase [0.4.0]</h3>\n<ul>\n<li>feat(oceanbase): add sparse/fulltext/hybrid search (<a href=\"https://github.com/run-llama/llama_index/pull/20524\">#20524</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-opensearch [1.0.0]</h3>\n<ul>\n<li>Changed OpenSearch engine default from deprecated <code>nmslib</code> to <code>faiss</code> (<a href=\"https://github.com/run-llama/llama_index/pull/20507\">#20507</a>)</li>\n<li>chore(deps): bump the uv group across 4 directories with 4 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20531\">#20531</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-postgres [0.7.3]</h3>\n<ul>\n<li>fix(postgres): disable bitmap scan for vector queries (<a href=\"https://github.com/run-llama/llama_index/pull/20514\">#20514</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-yugabytedb [0.5.4]</h3>\n<ul>\n<li>Add YugabyteDB as a Vector Store (<a href=\"https://github.com/run-llama/llama_index/pull/20559\">#20559</a>)</li>\n<li>chore(deps): bump the pip group across 2 directories with 7 updates (<a href=\"https://github.com/run-llama/llama_index/pull/20662\">#20662</a>)</li>\n</ul>\n<h3>llama-index-voice-agents-gemini-live [0.2.2]</h3>\n<ul>\n<li>Add client headers to Gemini API requests (<a href=\"https://github.com/run-llama/llama_index/pull/20519\">#20519</a>)</li>\n</ul>",
    "published": "2026-02-10T23:08:46Z",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "release",
    "score": 7.962,
    "source_reliability": 1.0,
    "freshness": 0.012,
    "platform_hits": 4,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "cost",
      "agent",
      "vllm"
    ],
    "why_it_matters": "Generic dependency-bumping and bug-fix release with no agentic coding, eval, or automation harness signal.",
    "llm_platform_relevant": false,
    "llm_novelty": 2,
    "llm_practicality": 2,
    "llm_hype": 1,
    "llm_why_1line": "Generic dependency-bumping and bug-fix release with no agentic coding, eval, or automation harness signal.",
    "llm_label_source": "llm"
  },
  {
    "id": "c3a8163196257d7f",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
    "url": "http://arxiv.org/abs/2602.12241v1",
    "summary": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.",
    "published": "2026-02-12T18:20:45Z",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "paper",
    "score": 7.922,
    "source_reliability": 1.0,
    "freshness": 0.072,
    "platform_hits": 3,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "latency",
      "cost"
    ],
    "why_it_matters": "Likely impact on inference, latency, cost workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "7bd3c882dcf7e77a",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing GPT-5.3-Codex",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex",
    "summary": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
    "published": "Thu, 05 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "news",
    "score": 5.4,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "c82ec79fbf9f9804",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "GPT-5 lowers the cost of cell-free protein synthesis",
    "url": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
    "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
    "published": "Thu, 05 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "news",
    "score": 5.4,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "cost"
    ],
    "why_it_matters": "Likely impact on cost workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "393019c2d406463f",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.100.0",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.100.0",
    "summary": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state across tool calls, with optional runtime path overrides. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Added support for multiple simultaneous rate limits across the protocol, backend client, and TUI status surfaces. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a>)</li>\n<li>Reintroduced app-server websocket transport with a split inbound/outbound architecture, plus connection-aware thread resume subscriptions. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a>)</li>\n<li>Added memory management slash commands in the TUI (<code>/m_update</code>, <code>/m_drop</code>) and expanded memory-read/metrics plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a>)</li>\n<li>Enabled Apps SDK apps in ChatGPT connector handling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a>)</li>\n<li>Promoted sandbox capabilities on both Linux and Windows, and introduced a new <code>ReadOnlyAccess</code> policy shape for configurable read access. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Fixed websocket incremental output duplication, prevented appends after <code>response.completed</code>, and treated <code>response.incomplete</code> as an error path. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a>)</li>\n<li>Improved websocket session stability by continuing ping handling when idle and suppressing noisy first-retry errors during quick reconnects. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a>)</li>\n<li>Fixed stale thread entries by dropping missing rollout files and cleaning stale DB metadata during thread listing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a>)</li>\n<li>Fixed Windows multi-line paste reliability in terminals (especially VS Code integrated terminal) by increasing paste burst timing tolerance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a>)</li>\n<li>Fixed incorrect inheritance of <code>limit_name</code> when merging partial rate-limit updates. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a>)</li>\n<li>Reduced repeated skill parse-error spam during active edits by increasing file-watcher debounce from 1s to 10s. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Added JS REPL documentation and config/schema guidance for enabling and configuring the feature. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Updated app-server websocket transport documentation in the app-server README. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Split <code>codex-common</code> into focused <code>codex-utils-*</code> crates to simplify dependency boundaries across Rust workspace components. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a>)</li>\n<li>Improved Rust release pipeline throughput and reliability for Windows and musl targets, including parallel Windows builds and musl link fixes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a>)</li>\n<li>Prevented GitHub release asset upload collisions by excluding duplicate <code>cargo-timing.html</code> artifacts. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.99.0...rust-v0.100.0\"><tt>rust-v0.99.0...rust-v0.100.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a> Do not resend output items in incremental websockets connections <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11246\">#11246</a> chore: persist turn_id in rollout session and make turn_id uuid based <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a> feat: support multiple rate limits <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11412\">#11412</a> tui: show non-file layer content in /debug-config <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11405\">#11405</a> Remove <code>test-support</code> feature from <code>codex-core</code> and replace it with explicit test toggles <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11428\">#11428</a> fix: flaky test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11429\">#11429</a> feat: improve thread listing <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a> feat: split codex-common into smaller utils crates <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11439\">#11439</a> feat: new memory prompts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11305\">#11305</a> Cache cloud requirements <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11452\">#11452</a> nit: increase max raw memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11455\">#11455</a> feat: close mem agent after consolidation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11454\">#11454</a> fix: optional schema of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11449\">#11449</a> feat: set policy for phase 2 memory <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11420\">#11420</a> chore: rename disable_websockets -&gt; websockets_disabled <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a> Do not attempt to append after response.completed <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11462\">#11462</a> clean: memory rollout recorder <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a> feat(core): promote Linux bubblewrap sandbox to Experimental <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11389\">#11389</a> Extract <code>codex-config</code> from <code>codex-core</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a> Reapply \"Add app-server transport layer with websocket support\" <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11470\">#11470</a> feat: panic if Constrained does not support Disabled <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11475\">#11475</a> feat: remove \"cargo check individual crates\" from CI <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a> feat: memory read path <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11471\">#11471</a> chore: clean rollout extraction in memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a> fix(tui): increase paste burst char interval on Windows to 30ms <a class=\"user-mention notranslate\" href=\"https://github.com/yuvrajangadsingh\">@yuvrajangadsingh</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11464\">#11464</a> chore: sub-agent never ask for approval <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11414\">#11414</a> Linkify feedback link <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11480\">#11480</a> chore: update mem prompt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11485\">#11485</a> fix: Constrained import <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a> Promote Windows Sandbox <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a> Add feature-gated freeform js_repl core runtime <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11419\">#11419</a> refactor: codex app-server ThreadState <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a> Pump pings <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a> feat: use more powerful machines for building Windows releases <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11479\">#11479</a> nit: memory truncation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a> Increased file watcher debounce duration from 1s to 10s <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11335\">#11335</a> Add AfterToolUse hook <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a> feat: build windows support binaries in parallel <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11290\">#11290</a> chore(tui) Simplify /status Permissions <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11503\">#11503</a> Make codex-sdk depend on openai/codex <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a> app-server: thread resume subscriptions <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11277\">#11277</a> Added seatbelt policy rule to allow os.cpus <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11506\">#11506</a> chore: inject originator/residency headers to ws client <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11497\">#11497</a> Hydrate previous model across resume/fork/rollback/task start <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11513\">#11513</a> feat: try to fix bugs I saw in the wild in the resource parsing logic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11509\">#11509</a> Consolidate search_tool feature into apps <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11388\">#11388</a> change model cap to server overload <a class=\"user-mention notranslate\" href=\"https://github.com/willwang-openai\">@willwang-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11504\">#11504</a> Pre-sampling compact with previous model context <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11516\">#11516</a> Clamp auto-compact limit to context window <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11520\">#11520</a> Update context window after model switch <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11519\">#11519</a> Use slug in tui <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11522\">#11522</a> fix: add --test_verbose_timeout_warnings to bazel.yml <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11526\">#11526</a> fix: remove errant Cargo.lock files <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11521\">#11521</a> test(app-server): stabilize app/list thread feature-flag test by using file-backed MCP OAuth creds <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a> feat: make sandbox read access configurable with <code>ReadOnlyAccess</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a> [apps] Allow Apps SDK apps. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11532\">#11532</a> fix compilation <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11531\">#11531</a> Teach codex to test itself <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11540\">#11540</a> ci: remove actions/cache from rust release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11542\">#11542</a> ci(windows): use DotSlash for zstd in rust-release-windows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11498\">#11498</a> build(linux-sandbox): always compile vendored bubblewrap on Linux; remove CODEX_BWRAP_ENABLE_FFI <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11545\">#11545</a> fix: make project_doc skill-render tests deterministic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11543\">#11543</a> ci: capture cargo timings in Rust CI and release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11539\">#11539</a> Bump rmcp to 0.15 <a class=\"user-mention notranslate\" href=\"https://github.com/gpeal\">@gpeal</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a> Hide the first websocket retry <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11551\">#11551</a> Add logs to model cache <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a> Fix rust-release failures in musl linking and release asset upload <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a> Handle response.incomplete <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a> fix: stop inheriting rate-limit limit_name <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a> rust-release: exclude cargo-timing.html from release assets <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11546\">#11546</a> fix: update memory writing prompt <a class=\"user-mention notranslate\" href=\"https://github.com/zuxin-oai\">@zuxin-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11448\">#11448</a> Fix test flake <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a> feat: mem slash commands <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11573\">#11573</a> Fix flaky pre_sampling_compact switch test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11571\">#11571</a> feat: mem drop cot <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a> Ensure list_threads drops stale rollout files <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11575\">#11575</a> fix: db stuff mem <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11581\">#11581</a> nit: upgrade DB version <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11577\">#11577</a> feat: truncate with model infos <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11590\">#11590</a> chore: clean consts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a> feat: metrics to memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11579\">#11579</a> Fix config test on macOS <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11600\">#11600</a> feat: add sanitizer to redact secrets <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11609\">#11609</a> chore: drop mcp validation of dynamic tools <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
    "published": "2026-02-12T18:30:23Z",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "release",
    "score": 7.472,
    "source_reliability": 1.0,
    "freshness": 0.072,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "throughput",
      "agent"
    ],
    "why_it_matters": "Likely impact on throughput, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "51c533e894830874",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Running Pydantic's Monty Rust sandboxed Python subset in WebAssembly",
    "url": "https://simonwillison.net/2026/Feb/6/pydantic-monty/#atom-everything",
    "summary": "<p>There's a jargon-filled headline for you! Everyone's <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing\">building sandboxes</a> for running untrusted code right now, and Pydantic's latest attempt, <a href=\"https://github.com/pydantic/monty\">Monty</a>, provides a custom Python-like language (a subset of Python) in Rust and makes it available as both a Rust library and a Python package. I got it working in WebAssembly, providing a sandbox-in-a-sandbox.</p>\n<p>Here's <a href=\"https://github.com/pydantic/monty\">how they describe Monty</a>:</p>\n<blockquote>\n<p>Monty avoids the cost, latency, complexity and general faff of using full container based sandbox for running LLM generated code.</p>\n<p>Instead, it let's you safely run Python code written by an LLM embedded in your agent, with startup times measured in single digit microseconds not hundreds of milliseconds.</p>\n<p>What Monty <strong>can</strong> do:</p>\n<ul>\n<li>Run a reasonable subset of Python code - enough for your agent to express what it wants to do</li>\n<li>Completely block access to the host environment: filesystem, env variables and network access are all implemented via external function calls the developer can control</li>\n<li>Call functions on the host - only functions you give it access to [...]</li>\n</ul>\n</blockquote>\n<p>A quick way to try it out is via <a href=\"https://github.com/astral-sh/uv\">uv</a>:</p>\n<pre><code>uv run --with pydantic-monty python -m asyncio\n</code></pre>\n<p>Then paste this into the Python interactive prompt - the <code>-m asyncio</code> enables top-level await:</p>\n<pre><span>import</span> <span>pydantic_monty</span>\n<span>code</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>'print(\"hello \" + str(4 * 5))'</span>)\n<span>await</span> <span>pydantic_monty</span>.<span>run_monty_async</span>(<span>code</span>)</pre>\n<p>Monty supports a <em>very</em> small subset of Python - it doesn't even support class declarations yet!</p>\n<p>But, given its target use-case, that's not actually a problem.</p>\n<p>The neat thing about providing tools like this for LLMs is that they're really good at iterating against error messages. A coding agent can run some Python code, get an error message telling it that classes aren't supported and then try again with a different approach.</p>\n<p>I wanted to try this in a browser, so I fired up <a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/\">a code research task</a> in Claude Code for web and kicked it off with the following:</p>\n<blockquote>\n<p>Clone <a href=\"https://github.com/pydantic/monty\">https://github.com/pydantic/monty</a> to /tmp and figure out how to compile it into a python WebAssembly wheel that can then be loaded in Pyodide. The wheel file itself should be checked into the repo along with build scripts and passing pytest playwright test scripts that load Pyodide from a CDN and the wheel from a “python -m http.server” localhost and demonstrate it working</p>\n</blockquote>\n<p>Then a little later:</p>\n<blockquote>\n<p>I want an additional WASM file that works independently of Pyodide, which is also usable in a web browser - build that too along with playwright tests that show it working. Also build two HTML files - one called demo.html and one called pyodide-demo.html - these should work similar to <a href=\"https://tools.simonwillison.net/micropython\">https://tools.simonwillison.net/micropython</a> (download that code with curl to inspect it) - one should load the WASM build, the other should load Pyodide and have it use the WASM wheel. These will be served by GitHub Pages so they can load the WASM and wheel from a relative path since the .html files will be served from the same folder as the wheel and WASM file</p>\n</blockquote>\n<p>Here's <a href=\"https://gisthost.github.io/?22d88e6367d7e002c4fb383c213c2df2/page-001.html\">the transcript</a>, and the <a href=\"https://github.com/simonw/research/tree/main/monty-wasm-pyodide\">final research report</a> it produced.</p>\n<p>I now have the Monty Rust code compiled to WebAssembly in two different shapes - as a <code>.wasm</code> bundle you can load and call from JavaScript, and as a <code>monty-wasm-pyodide/pydantic_monty-0.0.3-cp313-cp313-emscripten_4_0_9_wasm32.whl</code> wheel file which can be loaded into <a href=\"https://pyodide.org/\">Pyodide</a> and then called from Python in Pyodide in WebAssembly in a browser.</p>\n<p>Here are those two demos, hosted on GitHub Pages:</p>\n<ul>\n<li>\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/demo.html\">Monty WASM demo</a> - a UI over JavaScript that loads the Rust WASM module directly.</li>\n<li>\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/pyodide-demo.html\">Monty Pyodide demo</a> - this one provides an identical interface but here the code is <a href=\"https://github.com/simonw/research/blob/3add1ffec70b530711fa237d91f546da5bcf1f1c/monty-wasm-pyodide/pyodide-demo.html#L257-L280\">loading Pyodide and then installing the Monty WASM wheel</a>.</li>\n</ul>\n<p><img alt=\"Screenshot of a web app titled &quot;Monty via Pyodide&quot; with description &quot;Run Monty (a sandboxed Python interpreter by Pydantic) inside Pyodide (CPython compiled to WebAssembly). This loads the pydantic-monty wheel and uses its full Python API. Code is saved in the URL for sharing.&quot; A green banner reads &quot;Code executed successfully!&quot; Below are example buttons labeled &quot;Basic&quot;, &quot;Inputs&quot;, &quot;Reuse&quot;, &quot;Error Handling&quot;, &quot;Fibonacci&quot;, and &quot;Classes&quot;. A code editor labeled &quot;Python Code (runs inside Monty sandbox via Pyodide):&quot; contains: &quot;import pydantic_monty\\n\\n# Create interpreter with input variables\\nm = pydantic_monty.Monty('x + y', inputs=['x', 'y'])\\n\\n# Run with different inputs\\nresult1 = m.run(inputs={&quot;x&quot;: 10, &quot;y&quot;: 20})\\nprint(f&quot;10 + 20 = {result1}&quot;)\\n\\nresult2 = m.run(inputs={&quot;x&quot;: 100, &quot;y&quot;: 200})&quot; with &quot;Run Code&quot; and &quot;Clear&quot; buttons. The Output section shows &quot;10 + 20 = 30&quot; and &quot;100 + 200 = 300&quot; with a &quot;Copy&quot; button. Footer reads &quot;Executed in 4.0ms&quot;.\" src=\"https://static.simonwillison.net/static/2026/monty-pyodide.jpg\" /></p>\n<p>As a connoisseur of sandboxes - the more options the better! - this new entry from Pydantic ticks a lot of my boxes. It's small, fast, widely available (thanks to Rust and WebAssembly) and provides strict limits on memory usage, CPU time and access to disk and network.</p>\n<p>It was also a great excuse to spin up another demo showing how easy it is these days to turn compiled code like C or Rust into WebAssembly that runs in both a browser and a Pyodide environment.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/rust\">rust</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/pyodide\">pyodide</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/pydantic\">pydantic</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
    "published": "2026-02-06T22:31:31+00:00",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "news",
    "score": 7.45,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 3,
    "hype_hits": 1,
    "maturity": "production-ready",
    "tags": [
      "latency",
      "cost",
      "agent"
    ],
    "why_it_matters": "Likely impact on latency, cost, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "df155614d3a4c98b",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: Agent Lens – Code assistant observability in VSCode",
    "url": "https://github.com/23min/agent-lens",
    "summary": "<p>Article URL: <a href=\"https://github.com/23min/agent-lens\">https://github.com/23min/agent-lens</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47021758\">https://news.ycombinator.com/item?id=47021758</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 07:33:19 +0000",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "news",
    "score": 7.218,
    "source_reliability": 1.0,
    "freshness": 0.918,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "observability",
      "agent"
    ],
    "why_it_matters": "Likely impact on observability, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "0530f2ee25a8efae",
    "source": "nvidia_blog",
    "source_weight": 0.15,
    "title": "Leading Inference Providers Cut AI Costs by up to 10x With Open Source Models on NVIDIA Blackwell",
    "url": "https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/",
    "summary": "A diagnostic insight in healthcare. A character’s dialogue in an interactive game. An autonomous resolution from a customer service agent. Each of these AI-powered interactions is built on the same unit of intelligence: a token. Scaling these AI interactions requires businesses to consider whether they can afford more tokens. The answer lies in better tokenomics\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 12 Feb 2026 16:00:46 +0000",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "news",
    "score": 7.215,
    "source_reliability": 1.0,
    "freshness": 0.065,
    "platform_hits": 3,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "cost",
      "agent"
    ],
    "why_it_matters": "Likely impact on inference, cost, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "edab237724839010",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Covering electricity price increases from our data centers",
    "url": "https://simonwillison.net/2026/Feb/12/covering-electricity-price-increases/#atom-everything",
    "summary": "<p><strong><a href=\"https://www.anthropic.com/news/covering-electricity-price-increases\">Covering electricity price increases from our data centers</a></strong></p>\nOne of the sub-threads of the AI energy usage discourse has been the impact new data centers have on the cost of electricity to nearby residents. Here's <a href=\"https://www.bloomberg.com/graphics/2025-ai-data-centers-electricity-prices/\">detailed analysis from Bloomberg in September</a> reporting \"Wholesale electricity costs as much as 267% more than it did five years ago in areas near data centers\".</p>\n<p>Anthropic appear to be taking on this aspect of the problem directly, promising to cover 100% of necessary grid upgrade costs and also saying:</p>\n<blockquote>\n<p>We will work to bring net-new power generation online to match our data centers’ electricity needs. Where new generation isn’t online, we’ll work with utilities and external experts to estimate and cover demand-driven price effects from our data centers.</p>\n</blockquote>\n<p>I look forward to genuine energy industry experts picking this apart to judge if it will actually have the claimed impact on consumers.</p>\n<p>As always, I remain frustrated at the refusal of the major AI labs to fully quantify their energy usage. The best data we've had on this still comes from Mistral's report <a href=\"https://simonwillison.net/2025/Jul/22/mistral-environmental-standard/\">last July</a> and even that lacked key data such as the breakdown between energy usage for training vs inference.\n\n    <p><small></small>Via <a href=\"https://x.com/anthropicai/status/2021694494215901314\">@anthropicai</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-energy-usage\">ai-energy-usage</a></p>",
    "published": "2026-02-12T20:01:23+00:00",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "news",
    "score": 6.527,
    "source_reliability": 1.0,
    "freshness": 0.077,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "inference",
      "cost"
    ],
    "why_it_matters": "Likely impact on inference, cost workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "fb8310425e15863c",
    "source": "infoq_ai_ml",
    "source_weight": 1.15,
    "title": "Presentation: Building Embedding Models for Large-Scale Real-World Applications",
    "url": "https://www.infoq.com/presentations/llm-large-scale-applications/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
    "summary": "<img src=\"https://res.infoq.com/presentations/llm-large-scale-applications/en/mediumimage/sahil-dua-medium-1769590214923.jpeg\" /><p>Sahil Dua discusses the critical role of embedding models in powering search and RAG applications at scale. He explains the transformer-based architecture, contrastive learning techniques, and the process of distilling large language models into production-ready student models. He shares insights on optimizing query latency, handling document indexing, and evaluating retrieval quality.</p> <i>By Sahil Dua</i>",
    "published": "Fri, 13 Feb 2026 15:50:00 GMT",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "news",
    "score": 6.525,
    "source_reliability": 1.0,
    "freshness": 0.175,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "latency",
      "rag"
    ],
    "why_it_matters": "Likely impact on latency, rag workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "6e2225d549ed5ae2",
    "source": "vllm_releases",
    "source_weight": 0.25,
    "title": "v0.16.0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0",
    "summary": "<h1>vLLM v0.16.0</h1>\n<h2>Highlights</h2>\n<p>This release features 440 commits from 203 contributors (7 new)!</p>\n<ul>\n<li><strong>PyTorch 2.10 upgrade</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>). This is a breaking change for environment dependency.</li>\n<li><strong>Async scheduling + Pipeline Parallelism</strong> is now fully supported, delivering <strong>30.8% E2E throughput improvement</strong> and <strong>31.8% TPOT improvement</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>).</li>\n<li><strong>Realtime API</strong>: A new WebSocket-based Realtime API enables streaming audio interactions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>), building on the Voxtral realtime infrastructure.</li>\n<li><strong>RLHF workflow improvements</strong>: Native NCCL-based weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise weight reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), and engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li><strong>Unified Parallel Drafting</strong> for speculative decoding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), plus spec decode now works with structured outputs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>) and penalty application in Model Runner V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>).</li>\n<li><strong>Major XPU platform overhaul</strong>: Deprecated IPEX in favor of vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>), adding MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), and FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>) support.</li>\n</ul>\n<h3>Model Support</h3>\n<ul>\n<li>New architectures: GLM-OCR with MTP (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33005\">#33005</a>), Qwen3-ASR (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33312\">#33312</a>), DeepSeek-OCR-2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33165\">#33165</a>), Intern-S1-Pro (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33636\">#33636</a>), MiniCPM-o 4.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33431\">#33431</a>), openPangu7B-VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32449\">#32449</a>), NemotronHPuzzle heterogeneous (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32549\">#32549</a>), MusicFlamingo (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32696\">#32696</a>), FunAudioChat (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/2\">#2</a>), ColBERT late interaction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33686\">#33686</a>), voyage-4-nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33720\">#33720</a>), GLM-5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34124\">#34124</a>).</li>\n<li>Speculative decoding: EAGLE3 for Hunyuan/HunyuanVL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33035\">#33035</a>), AFMoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33111\">#33111</a>), Mistral3 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a>).</li>\n<li>LoRA expansion: Gemma3 vision components (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32764\">#32764</a>), Nemotron-H MTP models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32265\">#32265</a>), Qwen3 output embedding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29816\">#29816</a>). Optimized fused MoE-LoRA kernel indexing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32770\">#32770</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32774\">#32774</a>), unpermute-aware fused MoE LoRA path (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32655\">#32655</a>), reduced kernel overhead for fewer active LoRAs with multiple CUDA graphs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32005\">#32005</a>).</li>\n<li>Features: Qwen3-Omni transcription (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29828\">#29828</a>), Mistral Large 3 with FlashInfer MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33174\">#33174</a>), LFM2 SigLIP2 intermediate encoder layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33370\">#33370</a>), Qwen3-Omni/GLM-4.xV MRoPE positioning fixes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33010\">#33010</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33039\">#33039</a>), embedding input for disabled modalities (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32493\">#32493</a>).</li>\n<li>Performance: GLM-4.7-GPTQ decode and MTP acceptance rate regression fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a>), DeepSeek V3.2 fast detokenization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33855\">#33855</a>), DeepSeek V3.2 tokenizer fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33832\">#33832</a>), GLM-5 MTP accuracy fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34385\">#34385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li>Async scheduling + Pipeline Parallelism: Full support with 30.8% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>), optimized spec decode + async scheduling with 1.5% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33612\">#33612</a>), deadlock fix for torchrun PP broadcast (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33701\">#33701</a>).</li>\n<li>Speculative decoding: Unified Parallel Drafting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), structured output support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>), penalty application in MRV2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>), skip softmax for all-greedy rejection sampling (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32852\">#32852</a>), correctness fix for spec tokens with prefill chunks (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33652\">#33652</a>).</li>\n<li>RLHF: Native NCCL weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li>Helion kernel framework: ConfigManager (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32740\">#32740</a>), kernel wrapper (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32964\">#32964</a>), kernel registry (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33203\">#33203</a>).</li>\n<li>PluggableLayer: Applied to linear layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33152\">#33152</a>) and Mamba layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33660\">#33660</a>).</li>\n<li>Batch invariance: Disable Cascade Attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32561\">#32561</a>), enable Triton attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33688\">#33688</a>).</li>\n<li>Performance: Grammar bitmask H2D copy on separate stream (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33059\">#33059</a>), zero-copy GQA for multimodal and CPU (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33732\">#33732</a>), early-reject oversized MM requests (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33502\">#33502</a>), CPU memory leak fix from Request reference cycle in prefix caching (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34183\">#34183</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<ul>\n<li><strong>NVIDIA</strong>: FlashInfer TRTLLM BF16 MoE integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32954\">#32954</a>), SM100 INT4 W4A16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32437\">#32437</a>), SM121 (DGX Spark) CUTLASS support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a>), MNNVL protocol for GB series (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33540\">#33540</a>), FlashInfer MLA concat optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31171\">#31171</a>), GDN attention layout optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33291\">#33291</a>), DeepGEMM FP8 MLA performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33568\">#33568</a>), wvSplitK_fp8 performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33527\">#33527</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33493\">#33493</a>), B200 MoE configs for Nemotron Nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32804\">#32804</a>), Super B200 TP2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33510\">#33510</a>), GLM 4.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32958\">#32958</a>), Mamba selective scan tuning for B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32873\">#32873</a>). Fix: DeepSeek R1 CUTLASS MLA on B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33637\">#33637</a>), QK Norm+RoPE fusion on B200+FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a>), CUTLASS FP8 blockwise on SM103a (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32224\">#32224</a>).</li>\n<li><strong>AMD ROCm</strong>: QWEN3-NEXT FP8 tunings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32042\">#32042</a>), AITER attention backend for Qwen3-Next (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32492\">#32492</a>), fused_add_rmsnorm_pad for GPT-OSS (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30976\">#30976</a>), Qwen3-Omni startup fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33077\">#33077</a>).</li>\n<li><strong>Intel XPU</strong>: Platform overhaul - deprecated IPEX, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>). New: unquantized MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>).</li>\n<li><strong>ARM CPU</strong>: KleidiAI INT4 dynamic quant with BF16 activations (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33122\">#33122</a>), NEON BFMMLA BF16 paged attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32263\">#32263</a>), vectorization backend optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30329\">#30329</a>), attention dispatch by head_dim alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32161\">#32161</a>).</li>\n<li><strong>IBM Z</strong>: BF16 kernel type for s390x (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33788\">#33788</a>).</li>\n<li><strong>torch.compile</strong>: Stop compiling identical artifacts (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34003\">#34003</a>), MoE cold start optimization option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33735\">#33735</a>), fix 32-bit indexing assumption (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33113\">#33113</a>), attention fusion pass fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33945\">#33945</a>).</li>\n<li><strong>Performance</strong>: Chat completion streaming optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33782\">#33782</a>), ORJSONResponse for faster API responses (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33548\">#33548</a>), MoE permute optimization for CUTLASS FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32892\">#32892</a>), shared/routed overlap for latent MoE on Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32790\">#32790</a>), FlashInfer autotune control flag (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34006\">#34006</a>).</li>\n</ul>\n<h3>Large Scale Serving</h3>\n<ul>\n<li>Disaggregated serving: Mooncake connector rework with bootstrap server (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31034\">#31034</a>), cross-layer KV cache layout at NIXL Connector V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33339\">#33339</a>), delay freeing blocks for aborted async loads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32255\">#32255</a>), async double-free fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33377\">#33377</a>), Ray multi-replica single-instance fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a>).</li>\n<li>EPLB: Capture logical experts with router replay (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33013\">#33013</a>), DP metadata fix for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32739\">#32739</a>).</li>\n<li>Metrics: KV offloading connector metrics (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/27942\">#27942</a>), labeled prompt token metrics for P/D disaggregation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33290\">#33290</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li>New: FP8 block quant for CompressedTensorsW8A16Fp8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33280\">#33280</a>), ModelOpt MXFP8 for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33786\">#33786</a>), NVFP4/FP8 on Turing GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33076\">#33076</a>), TP &gt; 4 for FP4 Gemm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31099\">#31099</a>).</li>\n<li>Bugfixes: FP8 online quantization memory fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31914\">#31914</a>), asymmetric W4A16 (ConchLinear) for CT (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33200\">#33200</a>), DeepSeek V3.2 NVFP4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33932\">#33932</a>), LoRA FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33879\">#33879</a>), quantized Falcon-H1 model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32728\">#32728</a>), quantized Mamba TP with n_groups=1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33257\">#33257</a>), CPU W8A8 with bias (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33582\">#33582</a>), CPU W8A8 3D input support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33727\">#33727</a>).</li>\n<li><strong>Deprecation</strong>: Removed BitBlas (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Realtime API</strong>: WebSocket-based streaming API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>) with Voxtral realtime support.</li>\n<li><strong>Responses API</strong>: Sampling parameters (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32609\">#32609</a>), return token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33212\">#33212</a>), return prompt token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33378\">#33378</a>), parser implementation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32712\">#32712</a>).</li>\n<li>Pooling API: Request schema consensus for ScoreRequest (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33060\">#33060</a>) and final standardization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31127\">#31127</a>).</li>\n<li>Tool calling: Fix multi-turn tool call ID preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32768\">#32768</a>), fix indexing double-counting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33141\">#33141</a>), GLM-4 incremental string streaming (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33218\">#33218</a>), DSV3.2 fast detokenization fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33964\">#33964</a>), MCP tools non-streaming fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32762\">#32762</a>).</li>\n<li>Structured outputs: Performance optimization with reasoning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33557\">#33557</a>), guidance vocab size fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33509\">#33509</a>).</li>\n<li>CLI: <code>--disable-access-log-for-endpoints</code> option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30011\">#30011</a>).</li>\n<li>UX: Nested configs in YAML files (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33193\">#33193</a>), GGUF <code>repo_id:quant_type</code> syntax (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33371\">#33371</a>), DeepSeek ReasoningParser with thinking enabled by default (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33221\">#33221</a>), remove noisy CT warning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33273\">#33273</a>), early tokenization validation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31366\">#31366</a>), reasoning_content backward compatibility (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33635\">#33635</a>), only include Authorization header when OPENAI_API_KEY is set (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33488\">#33488</a>).</li>\n<li>Features: run_batch transcription/translation support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33934\">#33934</a>), /server_info collect_env (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33246\">#33246</a>), OTEL tracing during model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31162\">#31162</a>), clear MM and encoder cache (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33452\">#33452</a>), HF Hub LoRA resolver (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/20320\">#20320</a>).</li>\n<li>Scoring: Fix multi-document scoring returning single result (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33837\">#33837</a>).</li>\n</ul>\n<h3>Security</h3>\n<ul>\n<li>Patch protobuf for <a href=\"https://github.com/advisories/GHSA-7gcm-g887-7qv7\" title=\"CVE-2026-0994\">CVE-2026-0994</a> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34253\">#34253</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li><strong>PyTorch 2.10</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>) - breaking change for environment dependency.</li>\n<li>huggingface-hub updates for Transformers v5 preparation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33473\">#33473</a>).</li>\n<li>Transformers v5 compatibility fixes across multiple models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33977\">#33977</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33683\">#33683</a>).</li>\n</ul>\n<h3>Deprecation &amp; Breaking Changes</h3>\n<ul>\n<li>Removed BitBlas quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n<li>Removed deprecated <code>reasoning_content</code> message field (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33402\">#33402</a>).</li>\n<li>Removed deprecated pooling items (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33477\">#33477</a>).</li>\n<li>Removed deprecated <code>VLLM_ALL2ALL_BACKEND</code> environment variable (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33535\">#33535</a>).</li>\n<li>Deprecated IPEX for XPU, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors 🎉</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/aabbccddwasd\">@aabbccddwasd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Code4me2\">@Code4me2</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikchifo\">@ikchifo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jiangwu300\">@jiangwu300</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/pjs102793\">@pjs102793</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33963\">#33963</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sleepcoo\">@sleepcoo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33978\">#33978</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TundeAtSN\">@TundeAtSN</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a></li>\n</ul>",
    "published": "2026-02-13T06:13:20Z",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "release",
    "score": 13.768,
    "source_reliability": 1.0,
    "freshness": 0.118,
    "platform_hits": 6,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "serving",
      "throughput",
      "optimization",
      "quantization",
      "triton"
    ],
    "why_it_matters": "Production LLM serving infra: async+pipeline parallelism, speculative decode, RLHF tooling, and multi-hardware optimization directly impact coding-agent deployment.",
    "llm_platform_relevant": true,
    "llm_novelty": 4,
    "llm_practicality": 5,
    "llm_hype": 3,
    "llm_why_1line": "Production LLM serving infra: async+pipeline parallelism, speculative decode, RLHF tooling, and multi-hardware optimization directly impact coding-agent deployment.",
    "llm_label_source": "llm"
  },
  {
    "id": "4bffd18b76c47d13",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Multi Agent Research System",
    "url": "https://www.anthropic.com/engineering/multi-agent-research-system",
    "summary": "",
    "published": "2026-01-06T15:09:32.000Z",
    "collected_at": "2026-02-15T09:36:26.399592+00:00",
    "type": "news",
    "score": 5.4,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  }
]