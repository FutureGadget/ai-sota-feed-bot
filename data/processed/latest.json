[
  {
    "id": "588621adb31a551e",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Quoting Eric Meyer",
    "url": "https://simonwillison.net/2026/Feb/15/eric-meyer/#atom-everything",
    "summary": "<blockquote cite=\"https://mastodon.social/@Meyerweb/116065151451468199\"><p>I saw yet another “CSS is a massively bloated mess” whine and I’m like.  My dude.  My brother in Chromium.  It is trying as hard as it can to express the totality of visual presentation and layout design and typography and animation and digital interactivity and a few other things in a human-readable text format.  It’s not bloated, it’s fantastically ambitious.  Its reach is greater than most of us can hope to grasp.  Put some <em>respect</em> on its <em>name</em>.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://mastodon.social/@Meyerweb/116065151451468199\">Eric Meyer</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/css\">css</a>, <a href=\"https://simonwillison.net/tags/web-standards\">web-standards</a>, <a href=\"https://simonwillison.net/tags/eric-meyer\">eric-meyer</a></p>",
    "published": "2026-02-15T13:36:20+00:00",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "news",
    "score": 3.779,
    "source_reliability": 1.0,
    "freshness": 0.929,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "platform",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "c16b69a1be247646",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "GPT-5.2 derives a new result in theoretical physics",
    "url": "https://openai.com/index/new-result-theoretical-physics",
    "summary": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
    "published": "Fri, 13 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "news",
    "score": 3.713,
    "source_reliability": 1.0,
    "freshness": 0.113,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "platform",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "c5ef81aef2a2ccc1",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
    "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt",
    "summary": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT to help organizations defend against prompt injection and AI-driven data exfiltration.",
    "published": "Fri, 13 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "news",
    "score": 3.708,
    "source_reliability": 1.0,
    "freshness": 0.108,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "platform",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "c3e5b34fa99275da",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Clawdrey Hepburn – an AI agent researching identity infrastructure",
    "url": "https://twitter.com/clawdreyhepburn/status/2022771820659622022",
    "summary": "<p>Article URL: <a href=\"https://twitter.com/clawdreyhepburn/status/2022771820659622022\">https://twitter.com/clawdreyhepburn/status/2022771820659622022</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024026\">https://news.ycombinator.com/item?id=47024026</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 14:37:44 +0000",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "news",
    "score": 3.67,
    "source_reliability": 1.0,
    "freshness": 0.97,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "platform",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "393019c2d406463f",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.100.0",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.100.0",
    "summary": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state across tool calls, with optional runtime path overrides. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Added support for multiple simultaneous rate limits across the protocol, backend client, and TUI status surfaces. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a>)</li>\n<li>Reintroduced app-server websocket transport with a split inbound/outbound architecture, plus connection-aware thread resume subscriptions. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a>)</li>\n<li>Added memory management slash commands in the TUI (<code>/m_update</code>, <code>/m_drop</code>) and expanded memory-read/metrics plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a>)</li>\n<li>Enabled Apps SDK apps in ChatGPT connector handling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a>)</li>\n<li>Promoted sandbox capabilities on both Linux and Windows, and introduced a new <code>ReadOnlyAccess</code> policy shape for configurable read access. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Fixed websocket incremental output duplication, prevented appends after <code>response.completed</code>, and treated <code>response.incomplete</code> as an error path. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a>)</li>\n<li>Improved websocket session stability by continuing ping handling when idle and suppressing noisy first-retry errors during quick reconnects. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a>)</li>\n<li>Fixed stale thread entries by dropping missing rollout files and cleaning stale DB metadata during thread listing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a>)</li>\n<li>Fixed Windows multi-line paste reliability in terminals (especially VS Code integrated terminal) by increasing paste burst timing tolerance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a>)</li>\n<li>Fixed incorrect inheritance of <code>limit_name</code> when merging partial rate-limit updates. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a>)</li>\n<li>Reduced repeated skill parse-error spam during active edits by increasing file-watcher debounce from 1s to 10s. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Added JS REPL documentation and config/schema guidance for enabling and configuring the feature. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Updated app-server websocket transport documentation in the app-server README. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Split <code>codex-common</code> into focused <code>codex-utils-*</code> crates to simplify dependency boundaries across Rust workspace components. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a>)</li>\n<li>Improved Rust release pipeline throughput and reliability for Windows and musl targets, including parallel Windows builds and musl link fixes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a>)</li>\n<li>Prevented GitHub release asset upload collisions by excluding duplicate <code>cargo-timing.html</code> artifacts. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.99.0...rust-v0.100.0\"><tt>rust-v0.99.0...rust-v0.100.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a> Do not resend output items in incremental websockets connections <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11246\">#11246</a> chore: persist turn_id in rollout session and make turn_id uuid based <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a> feat: support multiple rate limits <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11412\">#11412</a> tui: show non-file layer content in /debug-config <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11405\">#11405</a> Remove <code>test-support</code> feature from <code>codex-core</code> and replace it with explicit test toggles <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11428\">#11428</a> fix: flaky test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11429\">#11429</a> feat: improve thread listing <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a> feat: split codex-common into smaller utils crates <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11439\">#11439</a> feat: new memory prompts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11305\">#11305</a> Cache cloud requirements <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11452\">#11452</a> nit: increase max raw memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11455\">#11455</a> feat: close mem agent after consolidation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11454\">#11454</a> fix: optional schema of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11449\">#11449</a> feat: set policy for phase 2 memory <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11420\">#11420</a> chore: rename disable_websockets -&gt; websockets_disabled <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a> Do not attempt to append after response.completed <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11462\">#11462</a> clean: memory rollout recorder <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a> feat(core): promote Linux bubblewrap sandbox to Experimental <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11389\">#11389</a> Extract <code>codex-config</code> from <code>codex-core</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a> Reapply \"Add app-server transport layer with websocket support\" <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11470\">#11470</a> feat: panic if Constrained does not support Disabled <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11475\">#11475</a> feat: remove \"cargo check individual crates\" from CI <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a> feat: memory read path <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11471\">#11471</a> chore: clean rollout extraction in memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a> fix(tui): increase paste burst char interval on Windows to 30ms <a class=\"user-mention notranslate\" href=\"https://github.com/yuvrajangadsingh\">@yuvrajangadsingh</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11464\">#11464</a> chore: sub-agent never ask for approval <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11414\">#11414</a> Linkify feedback link <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11480\">#11480</a> chore: update mem prompt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11485\">#11485</a> fix: Constrained import <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a> Promote Windows Sandbox <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a> Add feature-gated freeform js_repl core runtime <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11419\">#11419</a> refactor: codex app-server ThreadState <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a> Pump pings <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a> feat: use more powerful machines for building Windows releases <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11479\">#11479</a> nit: memory truncation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a> Increased file watcher debounce duration from 1s to 10s <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11335\">#11335</a> Add AfterToolUse hook <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a> feat: build windows support binaries in parallel <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11290\">#11290</a> chore(tui) Simplify /status Permissions <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11503\">#11503</a> Make codex-sdk depend on openai/codex <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a> app-server: thread resume subscriptions <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11277\">#11277</a> Added seatbelt policy rule to allow os.cpus <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11506\">#11506</a> chore: inject originator/residency headers to ws client <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11497\">#11497</a> Hydrate previous model across resume/fork/rollback/task start <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11513\">#11513</a> feat: try to fix bugs I saw in the wild in the resource parsing logic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11509\">#11509</a> Consolidate search_tool feature into apps <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11388\">#11388</a> change model cap to server overload <a class=\"user-mention notranslate\" href=\"https://github.com/willwang-openai\">@willwang-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11504\">#11504</a> Pre-sampling compact with previous model context <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11516\">#11516</a> Clamp auto-compact limit to context window <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11520\">#11520</a> Update context window after model switch <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11519\">#11519</a> Use slug in tui <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11522\">#11522</a> fix: add --test_verbose_timeout_warnings to bazel.yml <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11526\">#11526</a> fix: remove errant Cargo.lock files <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11521\">#11521</a> test(app-server): stabilize app/list thread feature-flag test by using file-backed MCP OAuth creds <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a> feat: make sandbox read access configurable with <code>ReadOnlyAccess</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a> [apps] Allow Apps SDK apps. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11532\">#11532</a> fix compilation <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11531\">#11531</a> Teach codex to test itself <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11540\">#11540</a> ci: remove actions/cache from rust release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11542\">#11542</a> ci(windows): use DotSlash for zstd in rust-release-windows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11498\">#11498</a> build(linux-sandbox): always compile vendored bubblewrap on Linux; remove CODEX_BWRAP_ENABLE_FFI <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11545\">#11545</a> fix: make project_doc skill-render tests deterministic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11543\">#11543</a> ci: capture cargo timings in Rust CI and release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11539\">#11539</a> Bump rmcp to 0.15 <a class=\"user-mention notranslate\" href=\"https://github.com/gpeal\">@gpeal</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a> Hide the first websocket retry <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11551\">#11551</a> Add logs to model cache <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a> Fix rust-release failures in musl linking and release asset upload <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a> Handle response.incomplete <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a> fix: stop inheriting rate-limit limit_name <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a> rust-release: exclude cargo-timing.html from release assets <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11546\">#11546</a> fix: update memory writing prompt <a class=\"user-mention notranslate\" href=\"https://github.com/zuxin-oai\">@zuxin-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11448\">#11448</a> Fix test flake <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a> feat: mem slash commands <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11573\">#11573</a> Fix flaky pre_sampling_compact switch test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11571\">#11571</a> feat: mem drop cot <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a> Ensure list_threads drops stale rollout files <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11575\">#11575</a> fix: db stuff mem <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11581\">#11581</a> nit: upgrade DB version <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11577\">#11577</a> feat: truncate with model infos <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11590\">#11590</a> chore: clean consts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a> feat: metrics to memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11579\">#11579</a> Fix config test on macOS <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11600\">#11600</a> feat: add sanitizer to redact secrets <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11609\">#11609</a> chore: drop mcp validation of dynamic tools <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
    "published": "2026-02-12T18:30:23Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "release",
    "score": 3.857,
    "source_reliability": 1.0,
    "freshness": 0.057,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "throughput",
      "agent"
    ],
    "why_it_matters": "Likely impact on throughput, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 1,
    "llm_category": "release",
    "llm_why_1line": "Relevant to AI platform engineering workflows.",
    "llm_label_source": "heuristic"
  },
  {
    "id": "5eb1dc4c2b27f35b",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "url": "http://arxiv.org/abs/2602.12281v1",
    "summary": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
    "published": "2026-02-12T18:59:59Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "paper",
    "score": 2.508,
    "source_reliability": 1.0,
    "freshness": 0.058,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "benchmark"
    ],
    "why_it_matters": "Likely impact on inference, benchmark workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "research",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "e4aef8221107f2b4",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
    "url": "http://arxiv.org/abs/2602.12279v1",
    "summary": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
    "published": "2026-02-12T18:59:49Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "paper",
    "score": 2.508,
    "source_reliability": 1.0,
    "freshness": 0.058,
    "platform_hits": 1,
    "hype_hits": 1,
    "maturity": "research",
    "tags": [
      "inference"
    ],
    "why_it_matters": "Likely impact on inference workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "research",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "1ee9e473379a13f8",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "AI is slowly munching away my passion",
    "url": "https://whynot.fail/human/ai-is-slowly-munching-away-my-passion/",
    "summary": "<p>Article URL: <a href=\"https://whynot.fail/human/ai-is-slowly-munching-away-my-passion/\">https://whynot.fail/human/ai-is-slowly-munching-away-my-passion/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47024005\">https://news.ycombinator.com/item?id=47024005</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 14:35:12 +0000",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "news",
    "score": 3.668,
    "source_reliability": 1.0,
    "freshness": 0.968,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "platform",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "c6b25db1102abe64",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.33",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.33",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed agent teammate sessions in tmux to send and receive messages</li>\n<li>Fixed warnings about agent teams not being available on your current plan</li>\n<li>Added <code>TeammateIdle</code> and <code>TaskCompleted</code> hook events for multi-agent workflows</li>\n<li>Added support for restricting which sub-agents can be spawned via <code>Task(agent_type)</code> syntax in agent \"tools\" frontmatter</li>\n<li>Added <code>memory</code> frontmatter field support for agents, enabling persistent memory with <code>user</code>, <code>project</code>, or <code>local</code> scope</li>\n<li>Added plugin name to skill descriptions and <code>/skills</code> menu for better discoverability</li>\n<li>Fixed an issue where submitting a new message while the model was in extended thinking would interrupt the thinking phase</li>\n<li>Fixed an API error that could occur when aborting mid-stream, where whitespace text combined with a thinking block would bypass normalization and produce an invalid request</li>\n<li>Fixed API proxy compatibility issue where 404 errors on streaming endpoints no longer triggered non-streaming fallback</li>\n<li>Fixed an issue where proxy settings configured via <code>settings.json</code> environment variables were not applied to WebFetch and other HTTP requests on the Node.js build</li>\n<li>Fixed <code>/resume</code> session picker showing raw XML markup instead of clean titles for sessions started with slash commands</li>\n<li>Improved error messages for API connection failures — now shows specific cause (e.g., ECONNREFUSED, SSL errors) instead of generic \"Connection error\"</li>\n<li>Errors from invalid managed settings are now surfaced</li>\n<li>VSCode: Added support for remote sessions, allowing OAuth users to browse and resume sessions from claude.ai</li>\n<li>VSCode: Added git branch and message count to the session picker, with support for searching by branch name</li>\n<li>VSCode: Fixed scroll-to-bottom under-scrolling on initial session load and session switch</li>\n</ul>",
    "published": "2026-02-06T01:47:21Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "release",
    "score": 3.4,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 2,
    "llm_hype": 1,
    "llm_category": "release",
    "llm_why_1line": "Relevant to AI platform engineering workflows.",
    "llm_label_source": "heuristic"
  },
  {
    "id": "a7a4345c41b65656",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage",
    "url": "http://arxiv.org/abs/2602.12274v1",
    "summary": "Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.",
    "published": "2026-02-12T18:58:12Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "paper",
    "score": 2.508,
    "source_reliability": 1.0,
    "freshness": 0.058,
    "platform_hits": 0,
    "hype_hits": 1,
    "maturity": "production-ready",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "research",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "0f0dfac01402e3a9",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs",
    "url": "http://arxiv.org/abs/2602.12273v1",
    "summary": "We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\\varepsilon$-optimality for the iUzawa-Net, and validate its promising numerical efficiency through nonsmooth elliptic and parabolic optimal control problems. Our techniques offer a versatile framework for designing and analyzing various optimization-informed deep learning approaches to optimal control and other PDE-constrained optimization problems. The proposed learning-to-control approach synergizes model-based optimization algorithms and data-driven deep learning techniques, inheriting the merits of both methodologies.",
    "published": "2026-02-12T18:57:43Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "paper",
    "score": 2.508,
    "source_reliability": 1.0,
    "freshness": 0.058,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "optimization"
    ],
    "why_it_matters": "Likely impact on optimization workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "research",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "9f6a0346a31359dc",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Building C Compiler",
    "url": "https://www.anthropic.com/engineering/building-c-compiler",
    "summary": "",
    "published": "2026-02-05T19:38:29.000Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "news",
    "score": 3.6,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "platform",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "912cf25fe3586ccd",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Infrastructure Noise",
    "url": "https://www.anthropic.com/engineering/infrastructure-noise",
    "summary": "",
    "published": "2026-02-05T17:42:29.000Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "news",
    "score": 3.6,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "platform",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "f09c45ee226de24a",
    "source": "anthropic_newsroom",
    "source_weight": 1.8,
    "title": "Chris Liddell Appointed Anthropic Board",
    "url": "https://www.anthropic.com/news/chris-liddell-appointed-anthropic-board",
    "summary": "",
    "published": "2026-02-13T16:21:14.000Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "news",
    "score": 3.541,
    "source_reliability": 1.0,
    "freshness": 0.141,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_category": "platform",
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "f70833bd2f581c75",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.41",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.41",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed AWS auth refresh hanging indefinitely by adding a 3-minute timeout</li>\n<li>Added <code>claude auth login</code>, <code>claude auth status</code>, and <code>claude auth logout</code> CLI subcommands</li>\n<li>Added Windows ARM64 (win32-arm64) native binary support</li>\n<li>Improved <code>/rename</code> to auto-generate session name from conversation context when called without arguments</li>\n<li>Improved narrow terminal layout for prompt footer</li>\n<li>Fixed file resolution failing for @-mentions with anchor fragments (e.g., <code>@README.md#installation</code>)</li>\n<li>Fixed FileReadTool blocking the process on FIFOs, <code>/dev/stdin</code>, and large files</li>\n<li>Fixed background task notifications not being delivered in streaming Agent SDK mode</li>\n<li>Fixed cursor jumping to end on each keystroke in classifier rule input</li>\n<li>Fixed markdown link display text being dropped for raw URL</li>\n<li>Fixed auto-compact failure error notifications being shown to users</li>\n<li>Fixed permission wait time being included in subagent elapsed time display</li>\n<li>Fixed proactive ticks firing while in plan mode</li>\n<li>Fixed clear stale permission rules when settings change on disk</li>\n<li>Fixed hook blocking errors showing stderr content in UI</li>\n</ul>",
    "published": "2026-02-13T06:08:49Z",
    "collected_at": "2026-02-15T14:41:14.560712+00:00",
    "type": "release",
    "score": 3.292,
    "source_reliability": 1.0,
    "freshness": 0.092,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 2,
    "llm_practicality": 2,
    "llm_hype": 1,
    "llm_category": "release",
    "llm_why_1line": "Relevant to AI platform engineering workflows.",
    "llm_label_source": "heuristic"
  }
]