[
  {
    "id": "6e2225d549ed5ae2",
    "source": "vllm_releases",
    "source_weight": 0.75,
    "title": "v0.16.0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.16.0",
    "summary": "<h1>vLLM v0.16.0</h1>\n<h2>Highlights</h2>\n<p>This release features 440 commits from 203 contributors (7 new)!</p>\n<ul>\n<li><strong>PyTorch 2.10 upgrade</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>). This is a breaking change for environment dependency.</li>\n<li><strong>Async scheduling + Pipeline Parallelism</strong> is now fully supported, delivering <strong>30.8% E2E throughput improvement</strong> and <strong>31.8% TPOT improvement</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>).</li>\n<li><strong>Realtime API</strong>: A new WebSocket-based Realtime API enables streaming audio interactions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>), building on the Voxtral realtime infrastructure.</li>\n<li><strong>RLHF workflow improvements</strong>: Native NCCL-based weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise weight reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), and engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li><strong>Unified Parallel Drafting</strong> for speculative decoding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), plus spec decode now works with structured outputs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>) and penalty application in Model Runner V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>).</li>\n<li><strong>Major XPU platform overhaul</strong>: Deprecated IPEX in favor of vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>), adding MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), and FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>) support.</li>\n</ul>\n<h3>Model Support</h3>\n<ul>\n<li>New architectures: GLM-OCR with MTP (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33005\">#33005</a>), Qwen3-ASR (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33312\">#33312</a>), DeepSeek-OCR-2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33165\">#33165</a>), Intern-S1-Pro (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33636\">#33636</a>), MiniCPM-o 4.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33431\">#33431</a>), openPangu7B-VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32449\">#32449</a>), NemotronHPuzzle heterogeneous (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32549\">#32549</a>), MusicFlamingo (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32696\">#32696</a>), FunAudioChat (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/2\">#2</a>), ColBERT late interaction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33686\">#33686</a>), voyage-4-nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33720\">#33720</a>), GLM-5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34124\">#34124</a>).</li>\n<li>Speculative decoding: EAGLE3 for Hunyuan/HunyuanVL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33035\">#33035</a>), AFMoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33111\">#33111</a>), Mistral3 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a>).</li>\n<li>LoRA expansion: Gemma3 vision components (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32764\">#32764</a>), Nemotron-H MTP models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32265\">#32265</a>), Qwen3 output embedding (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29816\">#29816</a>). Optimized fused MoE-LoRA kernel indexing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32770\">#32770</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32774\">#32774</a>), unpermute-aware fused MoE LoRA path (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32655\">#32655</a>), reduced kernel overhead for fewer active LoRAs with multiple CUDA graphs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32005\">#32005</a>).</li>\n<li>Features: Qwen3-Omni transcription (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29828\">#29828</a>), Mistral Large 3 with FlashInfer MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33174\">#33174</a>), LFM2 SigLIP2 intermediate encoder layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33370\">#33370</a>), Qwen3-Omni/GLM-4.xV MRoPE positioning fixes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33010\">#33010</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33039\">#33039</a>), embedding input for disabled modalities (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32493\">#32493</a>).</li>\n<li>Performance: GLM-4.7-GPTQ decode and MTP acceptance rate regression fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a>), DeepSeek V3.2 fast detokenization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33855\">#33855</a>), DeepSeek V3.2 tokenizer fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33832\">#33832</a>), GLM-5 MTP accuracy fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34385\">#34385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li>Async scheduling + Pipeline Parallelism: Full support with 30.8% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32618\">#32618</a>), optimized spec decode + async scheduling with 1.5% throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33612\">#33612</a>), deadlock fix for torchrun PP broadcast (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33701\">#33701</a>).</li>\n<li>Speculative decoding: Unified Parallel Drafting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32887\">#32887</a>), structured output support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33374\">#33374</a>), penalty application in MRV2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33251\">#33251</a>), skip softmax for all-greedy rejection sampling (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32852\">#32852</a>), correctness fix for spec tokens with prefill chunks (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33652\">#33652</a>).</li>\n<li>RLHF: Native NCCL weight syncing API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31943\">#31943</a>), layerwise reloading for QeRL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32133\">#32133</a>), engine pause/resume with request preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32351\">#32351</a>).</li>\n<li>Helion kernel framework: ConfigManager (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32740\">#32740</a>), kernel wrapper (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32964\">#32964</a>), kernel registry (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33203\">#33203</a>).</li>\n<li>PluggableLayer: Applied to linear layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33152\">#33152</a>) and Mamba layers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33660\">#33660</a>).</li>\n<li>Batch invariance: Disable Cascade Attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32561\">#32561</a>), enable Triton attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33688\">#33688</a>).</li>\n<li>Performance: Grammar bitmask H2D copy on separate stream (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33059\">#33059</a>), zero-copy GQA for multimodal and CPU (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33732\">#33732</a>), early-reject oversized MM requests (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33502\">#33502</a>), CPU memory leak fix from Request reference cycle in prefix caching (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34183\">#34183</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<ul>\n<li><strong>NVIDIA</strong>: FlashInfer TRTLLM BF16 MoE integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32954\">#32954</a>), SM100 INT4 W4A16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32437\">#32437</a>), SM121 (DGX Spark) CUTLASS support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a>), MNNVL protocol for GB series (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33540\">#33540</a>), FlashInfer MLA concat optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31171\">#31171</a>), GDN attention layout optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33291\">#33291</a>), DeepGEMM FP8 MLA performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33568\">#33568</a>), wvSplitK_fp8 performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33527\">#33527</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33493\">#33493</a>), B200 MoE configs for Nemotron Nano (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32804\">#32804</a>), Super B200 TP2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33510\">#33510</a>), GLM 4.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32958\">#32958</a>), Mamba selective scan tuning for B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32873\">#32873</a>). Fix: DeepSeek R1 CUTLASS MLA on B200 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33637\">#33637</a>), QK Norm+RoPE fusion on B200+FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a>), CUTLASS FP8 blockwise on SM103a (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32224\">#32224</a>).</li>\n<li><strong>AMD ROCm</strong>: QWEN3-NEXT FP8 tunings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32042\">#32042</a>), AITER attention backend for Qwen3-Next (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32492\">#32492</a>), fused_add_rmsnorm_pad for GPT-OSS (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30976\">#30976</a>), Qwen3-Omni startup fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33077\">#33077</a>).</li>\n<li><strong>Intel XPU</strong>: Platform overhaul - deprecated IPEX, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>). New: unquantized MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33659\">#33659</a>), MXFP4 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33679\">#33679</a>), WNA16 kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33973\">#33973</a>), scaled_mm kernel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34117\">#34117</a>), FP8 MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34202\">#34202</a>).</li>\n<li><strong>ARM CPU</strong>: KleidiAI INT4 dynamic quant with BF16 activations (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33122\">#33122</a>), NEON BFMMLA BF16 paged attention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32263\">#32263</a>), vectorization backend optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30329\">#30329</a>), attention dispatch by head_dim alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32161\">#32161</a>).</li>\n<li><strong>IBM Z</strong>: BF16 kernel type for s390x (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33788\">#33788</a>).</li>\n<li><strong>torch.compile</strong>: Stop compiling identical artifacts (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34003\">#34003</a>), MoE cold start optimization option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33735\">#33735</a>), fix 32-bit indexing assumption (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33113\">#33113</a>), attention fusion pass fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33945\">#33945</a>).</li>\n<li><strong>Performance</strong>: Chat completion streaming optimization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33782\">#33782</a>), ORJSONResponse for faster API responses (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33548\">#33548</a>), MoE permute optimization for CUTLASS FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32892\">#32892</a>), shared/routed overlap for latent MoE on Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32790\">#32790</a>), FlashInfer autotune control flag (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34006\">#34006</a>).</li>\n</ul>\n<h3>Large Scale Serving</h3>\n<ul>\n<li>Disaggregated serving: Mooncake connector rework with bootstrap server (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31034\">#31034</a>), cross-layer KV cache layout at NIXL Connector V2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33339\">#33339</a>), delay freeing blocks for aborted async loads (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32255\">#32255</a>), async double-free fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33377\">#33377</a>), Ray multi-replica single-instance fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a>).</li>\n<li>EPLB: Capture logical experts with router replay (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33013\">#33013</a>), DP metadata fix for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32739\">#32739</a>).</li>\n<li>Metrics: KV offloading connector metrics (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/27942\">#27942</a>), labeled prompt token metrics for P/D disaggregation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33290\">#33290</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li>New: FP8 block quant for CompressedTensorsW8A16Fp8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33280\">#33280</a>), ModelOpt MXFP8 for dense models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33786\">#33786</a>), NVFP4/FP8 on Turing GPUs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33076\">#33076</a>), TP &gt; 4 for FP4 Gemm (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31099\">#31099</a>).</li>\n<li>Bugfixes: FP8 online quantization memory fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31914\">#31914</a>), asymmetric W4A16 (ConchLinear) for CT (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33200\">#33200</a>), DeepSeek V3.2 NVFP4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33932\">#33932</a>), LoRA FP8 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33879\">#33879</a>), quantized Falcon-H1 model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32728\">#32728</a>), quantized Mamba TP with n_groups=1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33257\">#33257</a>), CPU W8A8 with bias (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33582\">#33582</a>), CPU W8A8 3D input support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33727\">#33727</a>).</li>\n<li><strong>Deprecation</strong>: Removed BitBlas (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Realtime API</strong>: WebSocket-based streaming API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33187\">#33187</a>) with Voxtral realtime support.</li>\n<li><strong>Responses API</strong>: Sampling parameters (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32609\">#32609</a>), return token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33212\">#33212</a>), return prompt token IDs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33378\">#33378</a>), parser implementation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32712\">#32712</a>).</li>\n<li>Pooling API: Request schema consensus for ScoreRequest (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33060\">#33060</a>) and final standardization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31127\">#31127</a>).</li>\n<li>Tool calling: Fix multi-turn tool call ID preservation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32768\">#32768</a>), fix indexing double-counting (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33141\">#33141</a>), GLM-4 incremental string streaming (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33218\">#33218</a>), DSV3.2 fast detokenization fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33964\">#33964</a>), MCP tools non-streaming fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32762\">#32762</a>).</li>\n<li>Structured outputs: Performance optimization with reasoning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33557\">#33557</a>), guidance vocab size fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33509\">#33509</a>).</li>\n<li>CLI: <code>--disable-access-log-for-endpoints</code> option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30011\">#30011</a>).</li>\n<li>UX: Nested configs in YAML files (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33193\">#33193</a>), GGUF <code>repo_id:quant_type</code> syntax (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33371\">#33371</a>), DeepSeek ReasoningParser with thinking enabled by default (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33221\">#33221</a>), remove noisy CT warning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33273\">#33273</a>), early tokenization validation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31366\">#31366</a>), reasoning_content backward compatibility (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33635\">#33635</a>), only include Authorization header when OPENAI_API_KEY is set (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33488\">#33488</a>).</li>\n<li>Features: run_batch transcription/translation support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33934\">#33934</a>), /server_info collect_env (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33246\">#33246</a>), OTEL tracing during model loading (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31162\">#31162</a>), clear MM and encoder cache (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33452\">#33452</a>), HF Hub LoRA resolver (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/20320\">#20320</a>).</li>\n<li>Scoring: Fix multi-document scoring returning single result (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33837\">#33837</a>).</li>\n</ul>\n<h3>Security</h3>\n<ul>\n<li>Patch protobuf for <a href=\"https://github.com/advisories/GHSA-7gcm-g887-7qv7\" title=\"CVE-2026-0994\">CVE-2026-0994</a> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/34253\">#34253</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li><strong>PyTorch 2.10</strong> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30525\">#30525</a>) - breaking change for environment dependency.</li>\n<li>huggingface-hub updates for Transformers v5 preparation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33473\">#33473</a>).</li>\n<li>Transformers v5 compatibility fixes across multiple models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33977\">#33977</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33683\">#33683</a>).</li>\n</ul>\n<h3>Deprecation &amp; Breaking Changes</h3>\n<ul>\n<li>Removed BitBlas quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32683\">#32683</a>) and Marlin 24 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32688\">#32688</a>).</li>\n<li>Removed deprecated <code>reasoning_content</code> message field (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33402\">#33402</a>).</li>\n<li>Removed deprecated pooling items (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33477\">#33477</a>).</li>\n<li>Removed deprecated <code>VLLM_ALL2ALL_BACKEND</code> environment variable (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33535\">#33535</a>).</li>\n<li>Deprecated IPEX for XPU, switched to vllm-xpu-kernels (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33379\">#33379</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors üéâ</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/aabbccddwasd\">@aabbccddwasd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33771\">#33771</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Code4me2\">@Code4me2</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33517\">#33517</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikchifo\">@ikchifo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33967\">#33967</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jiangwu300\">@jiangwu300</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33604\">#33604</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/pjs102793\">@pjs102793</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33963\">#33963</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sleepcoo\">@sleepcoo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33978\">#33978</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TundeAtSN\">@TundeAtSN</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33939\">#33939</a></li>\n</ul>",
    "published": "2026-02-13T06:13:20Z",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "release",
    "score": 14.268,
    "source_reliability": 1.0,
    "freshness": 0.118,
    "platform_hits": 6,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "serving",
      "throughput",
      "optimization",
      "quantization",
      "triton"
    ],
    "why_it_matters": "Production LLM serving infra: async+pipeline parallelism, speculative decode, RLHF tooling, and multi-hardware optimization directly impact coding-agent deployment.",
    "llm_platform_relevant": true,
    "llm_novelty": 4,
    "llm_practicality": 5,
    "llm_hype": 3,
    "llm_why_1line": "Production LLM serving infra: async+pipeline parallelism, speculative decode, RLHF tooling, and multi-hardware optimization directly impact coding-agent deployment.",
    "llm_label_source": "llm"
  },
  {
    "id": "c3a8163196257d7f",
    "source": "arxiv_cs_lg",
    "source_weight": 0.85,
    "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
    "url": "http://arxiv.org/abs/2602.12241v1",
    "summary": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent \"encode-the-whole-utterance\" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.",
    "published": "2026-02-12T18:20:45Z",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "paper",
    "score": 7.922,
    "source_reliability": 1.0,
    "freshness": 0.072,
    "platform_hits": 3,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "latency",
      "cost"
    ],
    "why_it_matters": "Likely impact on inference, latency, cost workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "0530f2ee25a8efae",
    "source": "nvidia_blog",
    "source_weight": 0.4,
    "title": "Leading Inference Providers Cut AI Costs by up to 10x With Open Source Models on NVIDIA Blackwell",
    "url": "https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/",
    "summary": "A diagnostic insight in healthcare. A character‚Äôs dialogue in an interactive game. An autonomous resolution from a customer service agent. Each of these AI-powered interactions is built on the same unit of intelligence: a token. Scaling these AI interactions requires businesses to consider whether they can afford more tokens. The answer lies in better tokenomics\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
    "published": "Thu, 12 Feb 2026 16:00:46 +0000",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "news",
    "score": 7.466,
    "source_reliability": 1.0,
    "freshness": 0.066,
    "platform_hits": 3,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "cost",
      "agent"
    ],
    "why_it_matters": "Likely impact on inference, cost, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "51c533e894830874",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Running Pydantic's Monty Rust sandboxed Python subset in WebAssembly",
    "url": "https://simonwillison.net/2026/Feb/6/pydantic-monty/#atom-everything",
    "summary": "<p>There's a jargon-filled headline for you! Everyone's <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing\">building sandboxes</a> for running untrusted code right now, and Pydantic's latest attempt, <a href=\"https://github.com/pydantic/monty\">Monty</a>, provides a custom Python-like language (a subset of Python) in Rust and makes it available as both a Rust library and a Python package. I got it working in WebAssembly, providing a sandbox-in-a-sandbox.</p>\n<p>Here's <a href=\"https://github.com/pydantic/monty\">how they describe Monty</a>:</p>\n<blockquote>\n<p>Monty avoids the cost, latency, complexity and general faff of using full container based sandbox for running LLM generated code.</p>\n<p>Instead, it let's you safely run Python code written by an LLM embedded in your agent, with startup times measured in single digit microseconds not hundreds of milliseconds.</p>\n<p>What Monty <strong>can</strong> do:</p>\n<ul>\n<li>Run a reasonable subset of Python code - enough for your agent to express what it wants to do</li>\n<li>Completely block access to the host environment: filesystem, env variables and network access are all implemented via external function calls the developer can control</li>\n<li>Call functions on the host - only functions you give it access to [...]</li>\n</ul>\n</blockquote>\n<p>A quick way to try it out is via <a href=\"https://github.com/astral-sh/uv\">uv</a>:</p>\n<pre><code>uv run --with pydantic-monty python -m asyncio\n</code></pre>\n<p>Then paste this into the Python interactive prompt - the <code>-m asyncio</code> enables top-level await:</p>\n<pre><span>import</span> <span>pydantic_monty</span>\n<span>code</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>'print(\"hello \" + str(4 * 5))'</span>)\n<span>await</span> <span>pydantic_monty</span>.<span>run_monty_async</span>(<span>code</span>)</pre>\n<p>Monty supports a <em>very</em> small subset of Python - it doesn't even support class declarations yet!</p>\n<p>But, given its target use-case, that's not actually a problem.</p>\n<p>The neat thing about providing tools like this for LLMs is that they're really good at iterating against error messages. A coding agent can run some Python code, get an error message telling it that classes aren't supported and then try again with a different approach.</p>\n<p>I wanted to try this in a browser, so I fired up <a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/\">a code research task</a> in Claude Code for web and kicked it off with the following:</p>\n<blockquote>\n<p>Clone <a href=\"https://github.com/pydantic/monty\">https://github.com/pydantic/monty</a> to /tmp and figure out how to compile it into a python WebAssembly wheel that can then be loaded in Pyodide. The wheel file itself should be checked into the repo along with build scripts and passing pytest playwright test scripts that load Pyodide from a CDN and the wheel from a ‚Äúpython -m http.server‚Äù localhost and demonstrate it working</p>\n</blockquote>\n<p>Then a little later:</p>\n<blockquote>\n<p>I want an additional WASM file that works independently of Pyodide, which is also usable in a web browser - build that too along with playwright tests that show it working. Also build two HTML files - one called demo.html and one called pyodide-demo.html - these should work similar to <a href=\"https://tools.simonwillison.net/micropython\">https://tools.simonwillison.net/micropython</a> (download that code with curl to inspect it) - one should load the WASM build, the other should load Pyodide and have it use the WASM wheel. These will be served by GitHub Pages so they can load the WASM and wheel from a relative path since the .html files will be served from the same folder as the wheel and WASM file</p>\n</blockquote>\n<p>Here's <a href=\"https://gisthost.github.io/?22d88e6367d7e002c4fb383c213c2df2/page-001.html\">the transcript</a>, and the <a href=\"https://github.com/simonw/research/tree/main/monty-wasm-pyodide\">final research report</a> it produced.</p>\n<p>I now have the Monty Rust code compiled to WebAssembly in two different shapes - as a <code>.wasm</code> bundle you can load and call from JavaScript, and as a <code>monty-wasm-pyodide/pydantic_monty-0.0.3-cp313-cp313-emscripten_4_0_9_wasm32.whl</code> wheel file which can be loaded into <a href=\"https://pyodide.org/\">Pyodide</a> and then called from Python in Pyodide in WebAssembly in a browser.</p>\n<p>Here are those two demos, hosted on GitHub Pages:</p>\n<ul>\n<li>\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/demo.html\">Monty WASM demo</a> - a UI over JavaScript that loads the Rust WASM module directly.</li>\n<li>\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/pyodide-demo.html\">Monty Pyodide demo</a> - this one provides an identical interface but here the code is <a href=\"https://github.com/simonw/research/blob/3add1ffec70b530711fa237d91f546da5bcf1f1c/monty-wasm-pyodide/pyodide-demo.html#L257-L280\">loading Pyodide and then installing the Monty WASM wheel</a>.</li>\n</ul>\n<p><img alt=\"Screenshot of a web app titled &quot;Monty via Pyodide&quot; with description &quot;Run Monty (a sandboxed Python interpreter by Pydantic) inside Pyodide (CPython compiled to WebAssembly). This loads the pydantic-monty wheel and uses its full Python API. Code is saved in the URL for sharing.&quot; A green banner reads &quot;Code executed successfully!&quot; Below are example buttons labeled &quot;Basic&quot;, &quot;Inputs&quot;, &quot;Reuse&quot;, &quot;Error Handling&quot;, &quot;Fibonacci&quot;, and &quot;Classes&quot;. A code editor labeled &quot;Python Code (runs inside Monty sandbox via Pyodide):&quot; contains: &quot;import pydantic_monty\\n\\n# Create interpreter with input variables\\nm = pydantic_monty.Monty('x + y', inputs=['x', 'y'])\\n\\n# Run with different inputs\\nresult1 = m.run(inputs={&quot;x&quot;: 10, &quot;y&quot;: 20})\\nprint(f&quot;10 + 20 = {result1}&quot;)\\n\\nresult2 = m.run(inputs={&quot;x&quot;: 100, &quot;y&quot;: 200})&quot; with &quot;Run Code&quot; and &quot;Clear&quot; buttons. The Output section shows &quot;10 + 20 = 30&quot; and &quot;100 + 200 = 300&quot; with a &quot;Copy&quot; button. Footer reads &quot;Executed in 4.0ms&quot;.\" src=\"https://static.simonwillison.net/static/2026/monty-pyodide.jpg\" /></p>\n<p>As a connoisseur of sandboxes - the more options the better! - this new entry from Pydantic ticks a lot of my boxes. It's small, fast, widely available (thanks to Rust and WebAssembly) and provides strict limits on memory usage, CPU time and access to disk and network.</p>\n<p>It was also a great excuse to spin up another demo showing how easy it is these days to turn compiled code like C or Rust into WebAssembly that runs in both a browser and a Pyodide environment.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/rust\">rust</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/pyodide\">pyodide</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/pydantic\">pydantic</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
    "published": "2026-02-06T22:31:31+00:00",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "news",
    "score": 7.45,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 3,
    "hype_hits": 1,
    "maturity": "production-ready",
    "tags": [
      "latency",
      "cost",
      "agent"
    ],
    "why_it_matters": "Likely impact on latency, cost, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "df155614d3a4c98b",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "Show HN: Agent Lens ‚Äì Code assistant observability in VSCode",
    "url": "https://github.com/23min/agent-lens",
    "summary": "<p>Article URL: <a href=\"https://github.com/23min/agent-lens\">https://github.com/23min/agent-lens</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47021758\">https://news.ycombinator.com/item?id=47021758</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 07:33:19 +0000",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "news",
    "score": 7.225,
    "source_reliability": 1.0,
    "freshness": 0.925,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "observability",
      "agent"
    ],
    "why_it_matters": "Likely impact on observability, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "877d9d2f64c35601",
    "source": "vllm_releases",
    "source_weight": 0.75,
    "title": "v0.15.0",
    "url": "https://github.com/vllm-project/vllm/releases/tag/v0.15.0",
    "summary": "<h2>Highlights</h2>\n<p>This release features 335 commits from 158 contributors (39 new)!</p>\n<h3>Model Support</h3>\n<ul>\n<li><strong>New architectures</strong>: Kimi-K2.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33131\">#33131</a>), Molmo2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30997\">#30997</a>), Step3vl 10B (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32329\">#32329</a>), Step1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32511\">#32511</a>), GLM-Lite (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31386\">#31386</a>), Eagle2.5-8B VLM (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32456\">#32456</a>).</li>\n<li><strong>LoRA expansion</strong>: Nemotron-H (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30802\">#30802</a>), InternVL2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32397\">#32397</a>), MiniMax M2 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32763\">#32763</a>).</li>\n<li><strong>Speculative decoding</strong>: EAGLE3 for Pixtral/LlavaForConditionalGeneration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32542\">#32542</a>), Qwen3 VL MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32048\">#32048</a>), draft model support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/24322\">#24322</a>).</li>\n<li><strong>Embeddings</strong>: BGE-M3 sparse embeddings and ColBERT embeddings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/14526\">#14526</a>).</li>\n<li><strong>Model enhancements</strong>: Voxtral streaming architecture (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32861\">#32861</a>), SharedFusedMoE for Qwen3MoE (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32082\">#32082</a>), dynamic resolution for Nemotron Nano VL (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32121\">#32121</a>), Molmo2 vision backbone quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32385\">#32385</a>).</li>\n</ul>\n<h3>Engine Core</h3>\n<ul>\n<li><strong>Async scheduling + Pipeline Parallelism</strong>: <code>--async-scheduling</code> now works with pipeline parallelism (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32359\">#32359</a>).</li>\n<li><strong>Mamba prefix caching</strong>: Block-aligned prefix caching for Mamba/hybrid models with <code>--enable-prefix-caching --mamba-cache-mode align</code>. Achieves ~2x speedup by caching Mamba states directly (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30877\">#30877</a>).</li>\n<li><strong>Session-based streaming input</strong>: New incremental input support for interactive workloads like ASR. Accepts async generators producing <code>StreamingInput</code> objects while maintaining KV cache alignment (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28973\">#28973</a>).</li>\n<li><strong>Model Runner V2</strong>: VLM support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32546\">#32546</a>), architecture improvements.</li>\n<li><strong>LoRA</strong>: Inplace loading for memory efficiency (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31326\">#31326</a>).</li>\n<li><strong>AOT compilation</strong>: torch.compile inductor artifacts support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/25205\">#25205</a>).</li>\n<li><strong>Performance</strong>: KV cache offloading redundant load prevention (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29087\">#29087</a>), FlashAttn attention/cache update separation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/25954\">#25954</a>).</li>\n</ul>\n<h3>Hardware &amp; Performance</h3>\n<h4>NVIDIA</h4>\n<ul>\n<li><strong>Blackwell defaults</strong>: FlashInfer MLA is now the default MLA backend on Blackwell, with TRTLLM as default prefill (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32615\">#32615</a>).</li>\n<li><strong>MoE performance</strong>: 1.2-2% E2E throughput improvement via grouped topk kernel fusion (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32058\">#32058</a>), NVFP4 small-batch decoding improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30885\">#30885</a>), faster cold start for MoEs with torch.compile (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32805\">#32805</a>).</li>\n<li><strong>FP4 kernel optimization</strong>: Up to 65% faster FP4 quantization on Blackwell (SM100F) using 256-bit loads, ~4% E2E throughput improvement (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32520\">#32520</a>).</li>\n<li><strong>Kernel improvements</strong>: topk_sigmoid kernel for MoE routing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31246\">#31246</a>), atomics reduce counting for SplitK skinny GEMMs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29843\">#29843</a>), fused cat+quant for FP8 KV cache in MLA (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32950\">#32950</a>).</li>\n<li><strong>torch.compile</strong>: SiluAndMul and QuantFP8 CustomOp compilation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32806\">#32806</a>), Triton prefill attention performance (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32403\">#32403</a>).</li>\n</ul>\n<h4>AMD ROCm</h4>\n<ul>\n<li><strong>MoRI EP</strong>: High-performance all2all backend for Expert Parallel (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28664\">#28664</a>).</li>\n<li><strong>Attention improvements</strong>: Shuffle KV cache layout and assembly paged attention kernel for AiterFlashAttentionBackend (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/29887\">#29887</a>).</li>\n<li><strong>FP4 support</strong>: MLA projection GEMMs with dynamic quantization (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32238\">#32238</a>).</li>\n<li><strong>Consumer GPU support</strong>: Flash Attention Triton backend on RDNA3/RDNA4 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32944\">#32944</a>).</li>\n</ul>\n<h4>Other Platforms</h4>\n<ul>\n<li><strong>TPU</strong>: Pipeline parallelism support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28506\">#28506</a>), backend option (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32438\">#32438</a>).</li>\n<li><strong>Intel XPU</strong>: AgRsAll2AllManager for distributed communication (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32654\">#32654</a>).</li>\n<li><strong>CPU</strong>: NUMA-aware acceleration for TP/DP inference on ARM (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32792\">#32792</a>), PyTorch 2.10 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32869\">#32869</a>).</li>\n<li><strong>Whisper</strong>: torch.compile support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30385\">#30385</a>).</li>\n<li><strong>WSL</strong>: Platform compatibility fix for Windows Subsystem for Linux (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32749\">#32749</a>).</li>\n</ul>\n<h3>Quantization</h3>\n<ul>\n<li><strong>MXFP4</strong>: W4A16 support for compressed-tensors MoE models (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32285\">#32285</a>).</li>\n<li><strong>Non-gated MoE</strong>: Quantization support with Marlin, NVFP4 CUTLASS, FP8, INT8, and compressed-tensors (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32257\">#32257</a>).</li>\n<li><strong>Intel</strong>: Quantization Toolkit integration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31716\">#31716</a>).</li>\n<li><strong>FP8 KV cache</strong>: Per-tensor and per-attention-head quantization via llmcompressor (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30141\">#30141</a>).</li>\n</ul>\n<h3>API &amp; Frontend</h3>\n<ul>\n<li><strong>Responses API</strong>: Partial message generation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32100\">#32100</a>), <code>include_stop_str_in_output</code> tuning (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32383\">#32383</a>), <code>prompt_cache_key</code> support (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32824\">#32824</a>).</li>\n<li><strong>OpenAI API</strong>: <code>skip_special_tokens</code> configuration (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32345\">#32345</a>).</li>\n<li><strong>Score endpoint</strong>: Flexible input formats with <code>data_1</code>/<code>data_2</code> and <code>queries</code>/<code>documents</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32577\">#32577</a>).</li>\n<li><strong>Render endpoints</strong>: New endpoints for prompt preprocessing (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32473\">#32473</a>).</li>\n<li><strong>Whisper API</strong>: <code>avg_logprob</code> and <code>compression_ratio</code> in verbose_json segments (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31059\">#31059</a>).</li>\n<li><strong>Security</strong>: FIPS 140-3 compliant hash option for enterprise/government users (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32386\">#32386</a>), <code>--ssl-ciphers</code> CLI argument (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30937\">#30937</a>).</li>\n<li><strong>UX improvements</strong>: Auto <code>api_server_count</code> based on <code>dp_size</code> (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32525\">#32525</a>), wheel variant auto-detection during install (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32948\">#32948</a>), custom profiler URI schemes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32393\">#32393</a>).</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li>FlashInfer v0.6.1 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30993\">#30993</a>)</li>\n<li>Transformers 4.57.5 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32287\">#32287</a>)</li>\n<li>PyTorch 2.10 for CPU backend (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32869\">#32869</a>)</li>\n<li>DeepGEMM newer version (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32479\">#32479</a>)</li>\n</ul>\n<h3>Breaking Changes &amp; Deprecations</h3>\n<ul>\n<li><strong>Metrics</strong>: Removed deprecated <code>vllm:time_per_output_token_seconds</code> metric - use <code>vllm:inter_token_latency_seconds</code> instead (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32661\">#32661</a>).</li>\n<li><strong>Environment variables</strong>: Removed deprecated environment variables (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32812\">#32812</a>).</li>\n<li><strong>Quantization</strong>: DeepSpeedFp8 removed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32679\">#32679</a>), RTN removed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32697\">#32697</a>), HQQ deprecated (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32681\">#32681</a>).</li>\n</ul>\n<h3>Bug Fixes</h3>\n<ul>\n<li><strong>Speculative decoding</strong>: Eagle draft_model_config fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31753\">#31753</a>).</li>\n<li><strong>DeepSeek</strong>: DeepSeek-V3.1 + DeepGEMM incompatible scale shapes fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32361\">#32361</a>).</li>\n<li><strong>Distributed</strong>: DP+MoE inference fix via CpuCommunicator (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31867\">#31867</a>), P/D with non-MoE DP fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33037\">#33037</a>).</li>\n<li><strong>EPLB</strong>: Possible deadlock fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32418\">#32418</a>).</li>\n<li><strong>NIXL</strong>: UCX memory leak fix by exporting UCX_MEM_MMAP_HOOK_MODE=none (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32181\">#32181</a>).</li>\n<li><strong>Structured output</strong>: Outlines byte fallback handling fix (<a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31391\">#31391</a>).</li>\n</ul>\n<hr />\n<h2>New Contributors üéâ</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/YunzhuLu\">@YunzhuLu</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32126\">#32126</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/emricksini-h\">@emricksini-h</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30784\">#30784</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/dsfaccini\">@dsfaccini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32289\">#32289</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ofirzaf\">@ofirzaf</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32312\">#32312</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/seekskyworld\">@seekskyworld</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32321\">#32321</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/brian033\">@brian033</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31715\">#31715</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/TomerBN-Nvidia\">@TomerBN-Nvidia</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32257\">#32257</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/vanshilshah97\">@vanshilshah97</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32448\">#32448</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/George-Polya\">@George-Polya</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32385\">#32385</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/T1mn\">@T1mn</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32411\">#32411</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/mritunjaysharma394\">@mritunjaysharma394</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/31492\">#31492</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/randzero\">@randzero</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32511\">#32511</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/DemingCheng\">@DemingCheng</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32556\">#32556</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/iboiko-habana\">@iboiko-habana</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32471\">#32471</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/honglyua-il\">@honglyua-il</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32462\">#32462</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/hyeongyun0916\">@hyeongyun0916</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32473\">#32473</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/DanielMe\">@DanielMe</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32560\">#32560</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/netanel-haber\">@netanel-haber</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32121\">#32121</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/longregen\">@longregen</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28784\">#28784</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/jasonyanwenl\">@jasonyanwenl</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32749\">#32749</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Wauplin\">@Wauplin</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32788\">#32788</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ikaadil\">@ikaadil</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32775\">#32775</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/alexsun07\">@alexsun07</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28664\">#28664</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/liranschour\">@liranschour</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30207\">#30207</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/AuYang261\">@AuYang261</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32844\">#32844</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/diviramon\">@diviramon</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32393\">#32393</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/RishabhSaini\">@RishabhSaini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32884\">#32884</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/MatteoFari\">@MatteoFari</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32397\">#32397</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/peakcrosser7\">@peakcrosser7</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30877\">#30877</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/orionr\">@orionr</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/30443\">#30443</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/marksverdhei\">@marksverdhei</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32614\">#32614</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/joninco\">@joninco</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32935\">#32935</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/monajafi-amd\">@monajafi-amd</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32944\">#32944</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ruizcrp\">@ruizcrp</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32988\">#32988</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/sjhddh\">@sjhddh</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32983\">#32983</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/HirokenOvo\">@HirokenOvo</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32646\">#32646</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Chenhao-Guan\">@Chenhao-Guan</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/32763\">#32763</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/joshuadeng\">@joshuadeng</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/28973\">#28973</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/ZhanqiuHu\">@ZhanqiuHu</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/pull/33016\">#33016</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a class=\"commit-link\" href=\"https://github.com/vllm-project/vllm/compare/v0.14.1...v0.15.0\"><tt>v0.14.1...v0.15.0</tt></a></p>",
    "published": "2026-01-29T10:21:01Z",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "release",
    "score": 14.15,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 6,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "throughput",
      "optimization",
      "quantization",
      "triton"
    ],
    "why_it_matters": "vLLM v0.15.0 delivers production-critical inference optimizations (2x Mamba speedup, 65% FP4 kernel gains) and async streaming for agentic workflows, directly improving deployment reliability and latency.",
    "llm_platform_relevant": true,
    "llm_novelty": 4,
    "llm_practicality": 5,
    "llm_hype": 3,
    "llm_why_1line": "vLLM v0.15.0 delivers production-critical inference optimizations (2x Mamba speedup, 65% FP4 kernel gains) and async streaming for agentic workflows, directly improving deployment reliability and latency.",
    "llm_label_source": "llm"
  },
  {
    "id": "d62c6c9a740a083f",
    "source": "triton_releases",
    "source_weight": 0.7,
    "title": "Release 2.59.0 corresponding to NGC container 25.06",
    "url": "https://github.com/triton-inference-server/server/releases/tag/v2.59.0",
    "summary": "<h1>Triton Inference Server</h1>\n<p>The Triton Inference Server provides a cloud inferencing solution optimized for both CPUs and GPUs. The server provides an inference service via an HTTP or GRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server. For edge deployments, Triton Server is also available as a shared library with an API that allows the full functionality of the server to be included directly in an application.</p>\n<details>\n  <h2>New Features and Improvements</h2>\n<ul>\n<li>Improved ensemble model performance in scenarios that allow out-of-order responses by increasing maximum throughput and reducing latency.</li>\n</ul>\n</details>\n<details>\n  <h2>Known Issues</h2>\n<ul>\n<li>\n<p>TensorRT calibration cache may require size adjustment in some cases, which was observed for the IGX platform.</p>\n</li>\n<li>\n<p>The core Python binding may incur an additional D2H and H2D copy if the backend and frontend both specify device memory to be used for response tensors.</p>\n</li>\n<li>\n<p>A segmentation fault related to DCGM and NSCQ may be encountered during server shutdown on NVSwitch systems. A possible workaround for this issue is to disable the collection of GPU metrics <code>tritonserver --allow-gpu-metrics false ...</code></p>\n</li>\n<li>\n<p>vLLM backend currently does not take advantage of the <a href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\" rel=\"nofollow\">vLLM v0.6</a> performance improvement when metrics are enabled.</p>\n</li>\n<li>\n<p>When using TensorRT models, if auto-complete configuration is disabled and <code>is_non_linear_format_io:true</code> for <a href=\"https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md#non-linear-io-formats\">reformat-free tensors</a> is not provided in the model configuration, the model may not load successfully.</p>\n</li>\n<li>\n<p>When using Python models in <a href=\"https://github.com/triton-inference-server/python_backend/tree/main?tab=readme-ov-file#decoupled-mode\">decoupled mode</a>, users need to ensure that the <code>ResponseSender</code> goes out of scope or is properly cleaned up before unloading the model to guarantee that the unloading process executes correctly.</p>\n</li>\n<li>\n<p>Restart support was temporarily removed for Python models.</p>\n</li>\n<li>\n<p>Triton Inference Server with vLLM backend currently does not support running vLLM models with tensor parallelism sizes greater than 1 and the default \"distributed_executor_backend\" setting when using explicit model control mode. In attempt to load a vllm model (tp &gt; 1) in explicit mode, users could potentially  see failure at <code>initialize</code> step: <code>could not acquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown, possibly due to daemon threads</code>. For the default model control mode, after server shutdown, vllm related sub-processes are not killed. Related vllm issue: <a class=\"issue-link js-issue-link\" href=\"https://github.com/vllm-project/vllm/issues/6766\">vllm-project/vllm#6766</a> . Please specify  \"distributed_executor_backend\":\"ray\" in the <code>model.json</code> when deploying vllm models with tensor parallelism &gt; 1.</p>\n</li>\n<li>\n<p>When loading models with file override, multiple model configuration files are not supported. Users must  provide the model configuration by setting parameter <code>\"config\" : \"&lt;JSON&gt;\"</code> instead of custom configuration file in the following format: <code>\"file:configs/&lt;model-config-name&gt;.pbtxt\" : \"&lt;base64-encoded-file-content&gt;\"</code>.</p>\n</li>\n<li>\n<p>TensorRT-LLM <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">backend</a> provides limited support of Triton extensions and features.</p>\n</li>\n<li>\n<p>The TensorRT-LLM backend may core dump on server shutdown. This impacts server teardown only and will not impact inferencing.</p>\n</li>\n<li>\n<p>The Java CAPI is known to have intermittent segfaults.</p>\n</li>\n<li>\n<p>Some systems which implement <code>malloc()</code> may not release memory back to the operating system right away causing a false memory leak. This can be mitigated by using a different malloc implementation. <code>TCMalloc</code> and <code>jemalloc</code> are installed in the Triton container and can be <a href=\"https://github.com/triton-inference-server/server/blob/r25.01/docs/user_guide/model_management.md\">used by specifying the library in LD_PRELOAD</a>. NVIDIA recommends experimenting with both <code>tcmalloc</code> and <code>jemalloc</code> to determine which one works better for your use case.</p>\n</li>\n<li>\n<p>Auto-complete may cause an increase in server start time. To avoid a start time increase, users can provide the full model configuration and launch the server with <code>--disable-auto-complete-config</code>.</p>\n</li>\n<li>\n<p>Auto-complete does not support PyTorch models due to lack of metadata in the model. It can only verify that the number of inputs and the input names matches what is specified in the model configuration. There is no model metadata about the number of outputs and datatypes. Related PyTorch bug:<a href=\"https://github.com/pytorch/pytorch/issues/38273\"> https://github.com/pytorch/pytorch/issues/38273</a></p>\n</li>\n<li>\n<p>Triton Client PIP wheels for ARM SBSA are not available from PyPI and pip will install an incorrect Jetson version of Triton Client library for Arm SBSA. The correct client wheel file can be pulled directly from the Arm SBSA SDK image and manually installed.</p>\n</li>\n<li>\n<p>Traced models in PyTorch seem to create overflows when int8 tensor values are transformed to int32 on the GPU. Refer to <a href=\"https://github.com/pytorch/pytorch/issues/66930\">pytorch/pytorch#66930</a> for more information.</p>\n</li>\n<li>\n<p>Triton cannot retrieve GPU metrics with <a href=\"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-gpus\" rel=\"nofollow\">MIG-enabled GPU devices</a>.</p>\n</li>\n<li>\n<p>Triton metrics might not work if the host machine is running a separate DCGM agent on bare-metal or in a container.</p>\n</li>\n<li>\n<p>When cloud storage (AWS, GCS, AZURE) is used as a model repository and a model has multiple versions, Triton creates an extra local copy of the cloud model‚Äôs folder in the temporary directory, which is deleted upon server‚Äôs shutdown.</p>\n</li>\n<li>\n<p>Python backend support for Windows is limited and does not currently support the following features:</p>\n<ul>\n<li>GPU tensors</li>\n<li>CPU and GPU-related metrics</li>\n<li>Custom execution environments</li>\n<li>The model load/unload APIs</li>\n</ul>\n</li>\n</ul>\n</details>\n<details>\n  <h2>Client Libraries and Examples</h2>\n<p>Ubuntu 24.04 builds of the client libraries and examples are included in this release in the attached <code>v2.59.0_ubuntu2404.clients.tar.gz</code> file.  The SDK is also available for as an Ubuntu 24.04 based <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags\" rel=\"nofollow\">NGC Container</a>. The SDK container includes the client libraries and examples, Performance Analyzer and Model Analyzer. Some components are also available in the tritonclient pip package. See <a href=\"https://github.com/triton-inference-server/client/tree/r25.05#getting-the-client-libraries-and-examples\">Getting the Client Libraries</a> for more information on each of these options.</p>\n</details>\n<details>\n  <h2>Windows Support</h2>\n<blockquote>\n<p>[!NOTE]<br />\nThere is no Windows release for 25.06, the latest release is <a href=\"https://github.com/triton-inference-server/server/releases/tag/v2.54.0\">25.01</a>.</p>\n</blockquote>\n</details>\n<details>\n  <h2>Jetson iGPU Support</h2>\n<p>A release of Triton for <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\" rel=\"nofollow\">IGX</a> is provided in the attached tar file: <a href=\"https://github.com/triton-inference-server/server/releases/download/v2.59.0/tritonserver2.59.0-igpu.tar\"><code>tritonserver2.59.0-igpu.tar</code></a>.</p>\n<ul>\n<li>This release supports <strong>TensorRT</strong> <code>10.11.0.33</code>, <strong>Onnx Runtime</strong> <code>1.22.0</code>, <strong>PyTorch</strong> <a href=\"https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html\" rel=\"nofollow\"><code>2.8.0a0+5228986c39.nv25.6</code></a>, <strong>Python</strong> <code>3.12</code> and as well as <em>ensembles</em>.</li>\n<li>ONNX Runtime backend does not support the OpenVINO and TensorRT execution providers. The CUDA execution provider is in Beta.</li>\n<li>System shared memory is supported on Jetson. CUDA shared memory is not supported.</li>\n<li>GPU metrics, GCS storage, S3 storage and Azure storage are not supported.</li>\n</ul>\n<p>The tar file contains the Triton server executable and shared libraries and also the C++ and Python client libraries and examples. For more information on how to install and use Triton on JetPack refer to <a href=\"https://github.com/triton-inference-server/server/blob/r25.06/docs/user_guide/jetson.md\"><code>jetson.md</code></a>.</p>\n<p>The wheel for the Python client library is present in the tar file and can be installed by running the following command:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code>python3 -m pip install --upgrade clients/python/tritonclient-2.59.0-py3-none-manylinux2014_aarch64.whl[all]\n</code></pre></div>\n</details>\n<details>\n  <h2>Triton TRT-LLM Container Support Matrix</h2>\n<p>The Triton TensorRT-LLM container is built from the 25.04 image <a href=\"http://nvcr.io/nvidia/tritonserver:25.04-py3-min\" rel=\"nofollow\"><code>nvcr.io/nvidia/tritonserver:25.04-py3-min</code></a>. Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html\" rel=\"nofollow\">support matrix</a> and <a href=\"https://github.com/triton-inference-server/server/blob/v2.59.0/docs/introduction/compatibility.md#container-name-trtllm-python-py3\">compatibility.md</a> for all dependency versions related to 25.04. However, the packages listed below have different versions than those specified in the support matrix.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dependency</th>\n<th align=\"center\">Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">TensorRT-LLM</td>\n<td align=\"center\">0.20.0</td>\n</tr>\n<tr>\n<td align=\"center\">TensorRT</td>\n<td align=\"center\">10.10.0.31</td>\n</tr>\n</tbody>\n</table>\n</details>",
    "published": "2025-06-30T22:54:06Z",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "release",
    "score": 10.5,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 6,
    "hype_hits": 1,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "latency",
      "throughput",
      "agent",
      "triton"
    ],
    "why_it_matters": "Inference server release notes lack coding-agent automation, agentic harness, or software delivery pipeline relevance.",
    "llm_platform_relevant": false,
    "llm_novelty": 2,
    "llm_practicality": 2,
    "llm_hype": 1,
    "llm_why_1line": "Inference server release notes lack coding-agent automation, agentic harness, or software delivery pipeline relevance.",
    "llm_label_source": "llm"
  },
  {
    "id": "393019c2d406463f",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.100.0",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.100.0",
    "summary": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state across tool calls, with optional runtime path overrides. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Added support for multiple simultaneous rate limits across the protocol, backend client, and TUI status surfaces. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a>)</li>\n<li>Reintroduced app-server websocket transport with a split inbound/outbound architecture, plus connection-aware thread resume subscriptions. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a>)</li>\n<li>Added memory management slash commands in the TUI (<code>/m_update</code>, <code>/m_drop</code>) and expanded memory-read/metrics plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a>)</li>\n<li>Enabled Apps SDK apps in ChatGPT connector handling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a>)</li>\n<li>Promoted sandbox capabilities on both Linux and Windows, and introduced a new <code>ReadOnlyAccess</code> policy shape for configurable read access. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Fixed websocket incremental output duplication, prevented appends after <code>response.completed</code>, and treated <code>response.incomplete</code> as an error path. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a>)</li>\n<li>Improved websocket session stability by continuing ping handling when idle and suppressing noisy first-retry errors during quick reconnects. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a>)</li>\n<li>Fixed stale thread entries by dropping missing rollout files and cleaning stale DB metadata during thread listing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a>)</li>\n<li>Fixed Windows multi-line paste reliability in terminals (especially VS Code integrated terminal) by increasing paste burst timing tolerance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a>)</li>\n<li>Fixed incorrect inheritance of <code>limit_name</code> when merging partial rate-limit updates. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a>)</li>\n<li>Reduced repeated skill parse-error spam during active edits by increasing file-watcher debounce from 1s to 10s. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Added JS REPL documentation and config/schema guidance for enabling and configuring the feature. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Updated app-server websocket transport documentation in the app-server README. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Split <code>codex-common</code> into focused <code>codex-utils-*</code> crates to simplify dependency boundaries across Rust workspace components. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a>)</li>\n<li>Improved Rust release pipeline throughput and reliability for Windows and musl targets, including parallel Windows builds and musl link fixes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a>)</li>\n<li>Prevented GitHub release asset upload collisions by excluding duplicate <code>cargo-timing.html</code> artifacts. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.99.0...rust-v0.100.0\"><tt>rust-v0.99.0...rust-v0.100.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a> Do not resend output items in incremental websockets connections <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11246\">#11246</a> chore: persist turn_id in rollout session and make turn_id uuid based <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a> feat: support multiple rate limits <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11412\">#11412</a> tui: show non-file layer content in /debug-config <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11405\">#11405</a> Remove <code>test-support</code> feature from <code>codex-core</code> and replace it with explicit test toggles <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11428\">#11428</a> fix: flaky test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11429\">#11429</a> feat: improve thread listing <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a> feat: split codex-common into smaller utils crates <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11439\">#11439</a> feat: new memory prompts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11305\">#11305</a> Cache cloud requirements <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11452\">#11452</a> nit: increase max raw memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11455\">#11455</a> feat: close mem agent after consolidation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11454\">#11454</a> fix: optional schema of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11449\">#11449</a> feat: set policy for phase 2 memory <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11420\">#11420</a> chore: rename disable_websockets -&gt; websockets_disabled <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a> Do not attempt to append after response.completed <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11462\">#11462</a> clean: memory rollout recorder <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a> feat(core): promote Linux bubblewrap sandbox to Experimental <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11389\">#11389</a> Extract <code>codex-config</code> from <code>codex-core</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a> Reapply \"Add app-server transport layer with websocket support\" <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11470\">#11470</a> feat: panic if Constrained does not support Disabled <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11475\">#11475</a> feat: remove \"cargo check individual crates\" from CI <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a> feat: memory read path <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11471\">#11471</a> chore: clean rollout extraction in memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a> fix(tui): increase paste burst char interval on Windows to 30ms <a class=\"user-mention notranslate\" href=\"https://github.com/yuvrajangadsingh\">@yuvrajangadsingh</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11464\">#11464</a> chore: sub-agent never ask for approval <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11414\">#11414</a> Linkify feedback link <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11480\">#11480</a> chore: update mem prompt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11485\">#11485</a> fix: Constrained import <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a> Promote Windows Sandbox <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a> Add feature-gated freeform js_repl core runtime <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11419\">#11419</a> refactor: codex app-server ThreadState <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a> Pump pings <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a> feat: use more powerful machines for building Windows releases <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11479\">#11479</a> nit: memory truncation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a> Increased file watcher debounce duration from 1s to 10s <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11335\">#11335</a> Add AfterToolUse hook <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a> feat: build windows support binaries in parallel <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11290\">#11290</a> chore(tui) Simplify /status Permissions <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11503\">#11503</a> Make codex-sdk depend on openai/codex <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a> app-server: thread resume subscriptions <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11277\">#11277</a> Added seatbelt policy rule to allow os.cpus <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11506\">#11506</a> chore: inject originator/residency headers to ws client <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11497\">#11497</a> Hydrate previous model across resume/fork/rollback/task start <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11513\">#11513</a> feat: try to fix bugs I saw in the wild in the resource parsing logic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11509\">#11509</a> Consolidate search_tool feature into apps <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11388\">#11388</a> change model cap to server overload <a class=\"user-mention notranslate\" href=\"https://github.com/willwang-openai\">@willwang-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11504\">#11504</a> Pre-sampling compact with previous model context <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11516\">#11516</a> Clamp auto-compact limit to context window <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11520\">#11520</a> Update context window after model switch <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11519\">#11519</a> Use slug in tui <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11522\">#11522</a> fix: add --test_verbose_timeout_warnings to bazel.yml <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11526\">#11526</a> fix: remove errant Cargo.lock files <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11521\">#11521</a> test(app-server): stabilize app/list thread feature-flag test by using file-backed MCP OAuth creds <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a> feat: make sandbox read access configurable with <code>ReadOnlyAccess</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a> [apps] Allow Apps SDK apps. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11532\">#11532</a> fix compilation <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11531\">#11531</a> Teach codex to test itself <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11540\">#11540</a> ci: remove actions/cache from rust release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11542\">#11542</a> ci(windows): use DotSlash for zstd in rust-release-windows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11498\">#11498</a> build(linux-sandbox): always compile vendored bubblewrap on Linux; remove CODEX_BWRAP_ENABLE_FFI <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11545\">#11545</a> fix: make project_doc skill-render tests deterministic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11543\">#11543</a> ci: capture cargo timings in Rust CI and release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11539\">#11539</a> Bump rmcp to 0.15 <a class=\"user-mention notranslate\" href=\"https://github.com/gpeal\">@gpeal</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a> Hide the first websocket retry <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11551\">#11551</a> Add logs to model cache <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a> Fix rust-release failures in musl linking and release asset upload <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a> Handle response.incomplete <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a> fix: stop inheriting rate-limit limit_name <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a> rust-release: exclude cargo-timing.html from release assets <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11546\">#11546</a> fix: update memory writing prompt <a class=\"user-mention notranslate\" href=\"https://github.com/zuxin-oai\">@zuxin-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11448\">#11448</a> Fix test flake <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a> feat: mem slash commands <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11573\">#11573</a> Fix flaky pre_sampling_compact switch test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11571\">#11571</a> feat: mem drop cot <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a> Ensure list_threads drops stale rollout files <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11575\">#11575</a> fix: db stuff mem <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11581\">#11581</a> nit: upgrade DB version <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11577\">#11577</a> feat: truncate with model infos <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11590\">#11590</a> chore: clean consts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a> feat: metrics to memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11579\">#11579</a> Fix config test on macOS <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11600\">#11600</a> feat: add sanitizer to redact secrets <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11609\">#11609</a> chore: drop mcp validation of dynamic tools <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
    "published": "2026-02-12T18:30:23Z",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "release",
    "score": 7.473,
    "source_reliability": 1.0,
    "freshness": 0.073,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "throughput",
      "agent"
    ],
    "why_it_matters": "Likely impact on throughput, agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "edab237724839010",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Covering electricity price increases from our data centers",
    "url": "https://simonwillison.net/2026/Feb/12/covering-electricity-price-increases/#atom-everything",
    "summary": "<p><strong><a href=\"https://www.anthropic.com/news/covering-electricity-price-increases\">Covering electricity price increases from our data centers</a></strong></p>\nOne of the sub-threads of the AI energy usage discourse has been the impact new data centers have on the cost of electricity to nearby residents. Here's <a href=\"https://www.bloomberg.com/graphics/2025-ai-data-centers-electricity-prices/\">detailed analysis from Bloomberg in September</a> reporting \"Wholesale electricity costs as much as 267% more than it did five years ago in areas near data centers\".</p>\n<p>Anthropic appear to be taking on this aspect of the problem directly, promising to cover 100% of necessary grid upgrade costs and also saying:</p>\n<blockquote>\n<p>We will work to bring net-new power generation online to match our data centers‚Äô electricity needs. Where new generation isn‚Äôt online, we‚Äôll work with utilities and external experts to estimate and cover demand-driven price effects from our data centers.</p>\n</blockquote>\n<p>I look forward to genuine energy industry experts picking this apart to judge if it will actually have the claimed impact on consumers.</p>\n<p>As always, I remain frustrated at the refusal of the major AI labs to fully quantify their energy usage. The best data we've had on this still comes from Mistral's report <a href=\"https://simonwillison.net/2025/Jul/22/mistral-environmental-standard/\">last July</a> and even that lacked key data such as the breakdown between energy usage for training vs inference.\n\n    <p><small></small>Via <a href=\"https://x.com/anthropicai/status/2021694494215901314\">@anthropicai</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-energy-usage\">ai-energy-usage</a></p>",
    "published": "2026-02-12T20:01:23+00:00",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "news",
    "score": 6.527,
    "source_reliability": 1.0,
    "freshness": 0.077,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "inference",
      "cost"
    ],
    "why_it_matters": "Likely impact on inference, cost workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "7bd3c882dcf7e77a",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing GPT-5.3-Codex",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex",
    "summary": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
    "published": "Thu, 05 Feb 2026 00:00:00 GMT",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "news",
    "score": 5.4,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "c82ec79fbf9f9804",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "GPT-5 lowers the cost of cell-free protein synthesis",
    "url": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
    "summary": "An autonomous lab combining OpenAI‚Äôs GPT-5 with Ginkgo Bioworks‚Äô cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
    "published": "Thu, 05 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "news",
    "score": 5.4,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "cost"
    ],
    "why_it_matters": "Likely impact on cost workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "4bffd18b76c47d13",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Multi Agent Research System",
    "url": "https://www.anthropic.com/engineering/multi-agent-research-system",
    "summary": "",
    "published": "2026-01-06T15:09:32.000Z",
    "collected_at": "2026-02-15T09:25:06.205609+00:00",
    "type": "news",
    "score": 5.4,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  }
]