[
  {
    "id": "c16b69a1be247646",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "GPT-5.2 derives a new result in theoretical physics",
    "url": "https://openai.com/index/new-result-theoretical-physics",
    "summary": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
    "published": "Fri, 13 Feb 2026 11:00:00 GMT",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "news",
    "score": 3.737,
    "source_reliability": 1.0,
    "freshness": 0.137,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "c5ef81aef2a2ccc1",
    "source": "openai_blog",
    "source_weight": 2.0,
    "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
    "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt",
    "summary": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT to help organizations defend against prompt injection and AI-driven data exfiltration.",
    "published": "Fri, 13 Feb 2026 10:00:00 GMT",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "news",
    "score": 3.732,
    "source_reliability": 1.0,
    "freshness": 0.132,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "db33c571a255908c",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "AI to stay in Flow – a personal decision on how I chose to (not) use AI",
    "url": "https://www.dev-log.me/ai_to_stay_in_flow/",
    "summary": "<p>Article URL: <a href=\"https://www.dev-log.me/ai_to_stay_in_flow/\">https://www.dev-log.me/ai_to_stay_in_flow/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47022673\">https://news.ycombinator.com/item?id=47022673</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 10:34:03 +0000",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "news",
    "score": 3.697,
    "source_reliability": 1.0,
    "freshness": 0.997,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "40fb814c14c3376c",
    "source": "hackernews_ai",
    "source_weight": 1.1,
    "title": "AI Agent Lands PRs in Major OSS Projects, Targets Maintainers via Cold Outreach",
    "url": "https://socket.dev/blog/ai-agent-lands-prs-in-major-oss-projects-targets-maintainers-via-cold-outreach",
    "summary": "<p>Article URL: <a href=\"https://socket.dev/blog/ai-agent-lands-prs-in-major-oss-projects-targets-maintainers-via-cold-outreach\">https://socket.dev/blog/ai-agent-lands-prs-in-major-oss-projects-targets-maintainers-via-cold-outreach</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47022669\">https://news.ycombinator.com/item?id=47022669</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
    "published": "Sun, 15 Feb 2026 10:33:28 +0000",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "news",
    "score": 3.696,
    "source_reliability": 1.0,
    "freshness": 0.996,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "f70833bd2f581c75",
    "source": "claude_code_releases",
    "source_weight": 2.2,
    "title": "v2.1.41",
    "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.41",
    "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed AWS auth refresh hanging indefinitely by adding a 3-minute timeout</li>\n<li>Added <code>claude auth login</code>, <code>claude auth status</code>, and <code>claude auth logout</code> CLI subcommands</li>\n<li>Added Windows ARM64 (win32-arm64) native binary support</li>\n<li>Improved <code>/rename</code> to auto-generate session name from conversation context when called without arguments</li>\n<li>Improved narrow terminal layout for prompt footer</li>\n<li>Fixed file resolution failing for @-mentions with anchor fragments (e.g., <code>@README.md#installation</code>)</li>\n<li>Fixed FileReadTool blocking the process on FIFOs, <code>/dev/stdin</code>, and large files</li>\n<li>Fixed background task notifications not being delivered in streaming Agent SDK mode</li>\n<li>Fixed cursor jumping to end on each keystroke in classifier rule input</li>\n<li>Fixed markdown link display text being dropped for raw URL</li>\n<li>Fixed auto-compact failure error notifications being shown to users</li>\n<li>Fixed permission wait time being included in subagent elapsed time display</li>\n<li>Fixed proactive ticks firing while in plan mode</li>\n<li>Fixed clear stale permission rules when settings change on disk</li>\n<li>Fixed hook blocking errors showing stderr content in UI</li>\n</ul>",
    "published": "2026-02-13T06:08:49Z",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "release",
    "score": 3.912,
    "source_reliability": 1.0,
    "freshness": 0.112,
    "platform_hits": 1,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [
      "agent"
    ],
    "why_it_matters": "Likely impact on agent workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "5eb1dc4c2b27f35b",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "url": "http://arxiv.org/abs/2602.12281v1",
    "summary": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
    "published": "2026-02-12T18:59:59Z",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "paper",
    "score": 2.521,
    "source_reliability": 1.0,
    "freshness": 0.071,
    "platform_hits": 2,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [
      "inference",
      "benchmark"
    ],
    "why_it_matters": "Likely impact on inference, benchmark workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "e4aef8221107f2b4",
    "source": "arxiv_cs_ai",
    "source_weight": 0.85,
    "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
    "url": "http://arxiv.org/abs/2602.12279v1",
    "summary": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
    "published": "2026-02-12T18:59:49Z",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "paper",
    "score": 2.521,
    "source_reliability": 1.0,
    "freshness": 0.071,
    "platform_hits": 1,
    "hype_hits": 1,
    "maturity": "research",
    "tags": [
      "inference"
    ],
    "why_it_matters": "Likely impact on inference workflows and platform decisions.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "7bdea70aab3e6e06",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.101.0",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.101.0",
    "summary": "<h2>Bug Fixes</h2>\n<ul>\n<li>Model resolution now preserves the requested model slug when selecting by prefix, so model references stay stable instead of being rewritten. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11602\">#11602</a>)</li>\n<li>Developer messages are now excluded from phase-1 memory input, reducing noisy or irrelevant content entering memory. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11608\">#11608</a>)</li>\n<li>Memory phase processing concurrency was reduced to make consolidation/staging more stable under load. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11614\">#11614</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Cleaned and simplified the phase-1 memory pipeline code paths. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11605\">#11605</a>)</li>\n<li>Minor repository maintenance: formatting and test-suite hygiene updates in remote model tests. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11619\">#11619</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.100.0...rust-v0.101.0\"><tt>rust-v0.100.0...rust-v0.101.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11605\">#11605</a> chore: drop and clean from phase 1 <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11602\">#11602</a> fix(core) model_info preserves slug <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11608\">#11608</a> exclude developer messages from phase-1 memory input <a class=\"user-mention notranslate\" href=\"https://github.com/wendyjiao-openai\">@wendyjiao-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11591\">#11591</a> Add cwd to memory files <a class=\"user-mention notranslate\" href=\"https://github.com/wendyjiao-openai\">@wendyjiao-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11614\">#11614</a> chore: reduce concurrency of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11619\">#11619</a> fix: fmt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
    "published": "2026-02-12T21:39:49Z",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "release",
    "score": 3.879,
    "source_reliability": 1.0,
    "freshness": 0.079,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "23706b7d45179bbd",
    "source": "openai_codex_releases",
    "source_weight": 2.2,
    "title": "0.101.0-alpha.1",
    "url": "https://github.com/openai/codex/releases/tag/rust-v0.101.0-alpha.1",
    "summary": "<p>Release 0.101.0-alpha.1</p>",
    "published": "2026-02-12T19:15:34Z",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "release",
    "score": 3.871,
    "source_reliability": 1.0,
    "freshness": 0.071,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "dd9ffded5689f601",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt",
    "url": "https://simonwillison.net/2026/Feb/15/cognitive-debt/#atom-everything",
    "summary": "<p><strong><a href=\"https://margaretstorey.com/blog/2026/02/09/cognitive-debt/\">How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt</a></strong></p>\nThis piece by Margaret-Anne Storey is the best explanation of the term <strong>cognitive debt</strong> I've seen so far.</p>\n<blockquote>\n<p><em>Cognitive debt</em>, a term gaining <a href=\"https://www.media.mit.edu/publications/your-brain-on-chatgpt/\">traction</a> recently, instead communicates the notion that the debt compounded from going fast lives in the brains of the developers and affects their lived experiences and abilities to “go fast” or to make changes. Even if AI agents produce code that could be easy to understand, the humans involved may have simply lost the plot and may not understand what the program is supposed to do, how their intentions were implemented, or how to possibly change it.</p>\n</blockquote>\n<p>Margaret-Anne expands on this further with an anecdote about a student team she coached:</p>\n<blockquote>\n<p>But by weeks 7 or 8, one team hit a wall. They could no longer make even simple changes without breaking something unexpected. When I met with them, the team initially blamed technical debt: messy code, poor architecture, hurried implementations. But as we dug deeper, the real problem emerged: no one on the team could explain why certain design decisions had been made or how different parts of the system were supposed to work together. The code might have been messy, but the bigger issue was that the theory of the system, their shared understanding, had fragmented or disappeared entirely. They had accumulated cognitive debt faster than technical debt, and it paralyzed them.</p>\n</blockquote>\n<p>I've experienced this myself on some of my more ambitious vibe-code-adjacent projects. I've been experimenting with prompting entire new features into existence without reviewing their implementations and, while it works surprisingly well, I've found myself getting lost in my own projects.</p>\n<p>I no longer have a firm mental model of what they can do and how they work, which means each additional feature becomes harder to reason about, eventually leading me to lose the ability to make confident decisions about where to go next.\n\n    <p><small></small>Via <a href=\"https://martinfowler.com/fragments/2026-02-13.html\">Martin Fowler</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a></p>",
    "published": "2026-02-15T05:20:11+00:00",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "news",
    "score": 3.651,
    "source_reliability": 1.0,
    "freshness": 0.801,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "770ccec0a4c97947",
    "source": "simon_willison",
    "source_weight": 1.25,
    "title": "Launching Interop 2026",
    "url": "https://simonwillison.net/2026/Feb/15/interop-2026/#atom-everything",
    "summary": "<p><strong><a href=\"https://hacks.mozilla.org/2026/02/launching-interop-2026/\">Launching Interop 2026</a></strong></p>\nJake Archibald reports on Interop 2026, the initiative between Apple, Google, Igalia, Microsoft, and Mozilla to collaborate on ensuring a targeted set of web platform features reach cross-browser parity over the course of the year.</p>\n<p>I hadn't realized how influential and successful the Interop series has been. It started back in 2021 as <a href=\"https://web.dev/blog/compat2021\">Compat 2021</a> before being rebranded to Interop <a href=\"https://blogs.windows.com/msedgedev/2022/03/03/microsoft-edge-and-interop-2022/\">in 2022</a>.</p>\n<p>The dashboards for each year can be seen here, and they demonstrate how wildly effective the program has been: <a href=\"https://wpt.fyi/interop-2021\">2021</a>, <a href=\"https://wpt.fyi/interop-2022\">2022</a>, <a href=\"https://wpt.fyi/interop-2023\">2023</a>, <a href=\"https://wpt.fyi/interop-2024\">2024</a>, <a href=\"https://wpt.fyi/interop-2025\">2025</a>, <a href=\"https://wpt.fyi/interop-2026\">2026</a>.</p>\n<p>Here's the progress chart for 2025, which shows every browser vendor racing towards a 95%+ score by the end of the year:</p>\n<p><img alt=\"Line chart showing Interop 2025 browser compatibility scores over the year (Jan–Dec) for Chrome, Edge, Firefox, Safari, and Interop. Y-axis ranges from 0% to 100%. Chrome (yellow) and Edge (green) lead, starting around 80% and reaching near 100% by Dec. Firefox (orange) starts around 48% and climbs to ~98%. Safari (blue) starts around 45% and reaches ~96%. The Interop line (dark green/black) starts lowest around 29% and rises to ~95% by Dec. All browsers converge near 95–100% by year's end.\" src=\"https://static.simonwillison.net/static/2026/interop-2025.jpg\" /></p>\n<p>The feature I'm most excited about in 2026 is <a href=\"https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using#basic_mpa_view_transition\">Cross-document View Transitions</a>, building on the successful 2025 target of <a href=\"https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using\">Same-Document View Transitions</a>. This will provide fancy SPA-style transitions between pages on websites with no JavaScript at all.</p>\n<p>As a keen WebAssembly tinkerer I'm also intrigued by this one:</p>\n<blockquote>\n<p><a href=\"https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md\">JavaScript Promise Integration for Wasm</a> allows WebAssembly to asynchronously 'suspend', waiting on the result of an external promise. This simplifies the compilation of languages like C/C++ which expect APIs to run synchronously.</p>\n</blockquote>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/browsers\">browsers</a>, <a href=\"https://simonwillison.net/tags/css\">css</a>, <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/web-standards\">web-standards</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/jake-archibald\">jake-archibald</a></p>",
    "published": "2026-02-15T04:33:22+00:00",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "news",
    "score": 3.626,
    "source_reliability": 1.0,
    "freshness": 0.776,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "production-ready",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  },
  {
    "id": "9f6a0346a31359dc",
    "source": "anthropic_engineering",
    "source_weight": 2.0,
    "title": "Building C Compiler",
    "url": "https://www.anthropic.com/engineering/building-c-compiler",
    "summary": "",
    "published": "2026-02-05T19:38:29.000Z",
    "collected_at": "2026-02-15T10:38:29.167569+00:00",
    "type": "news",
    "score": 3.6,
    "source_reliability": 1.0,
    "freshness": 0.0,
    "platform_hits": 0,
    "hype_hits": 0,
    "maturity": "research",
    "tags": [],
    "why_it_matters": "Potential relevance to AI platform stack; review for downstream impact.",
    "llm_platform_relevant": true,
    "llm_novelty": 3,
    "llm_practicality": 3,
    "llm_hype": 2,
    "llm_why_1line": "",
    "llm_label_source": "heuristic"
  }
]