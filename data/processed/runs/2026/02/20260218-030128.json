{
  "run_at": "2026-02-18T03:01:28.845985+00:00",
  "item_count": 15,
  "items": [
    {
      "id": "f9febfff147e7f3d",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Agent Audit Kit v0.1 – deterministic replay + stress for LLM agents",
      "url": "https://github.com/helpfuldolphin/AgentAuditKit/releases/tag/aak-v0.1.0-e3",
      "summary": "<p>Article URL: <a href=\"https://github.com/helpfuldolphin/AgentAuditKit/releases/tag/aak-v0.1.0-e3\">https://github.com/helpfuldolphin/AgentAuditKit/releases/tag/aak-v0.1.0-e3</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47056581\">https://news.ycombinator.com/item?id=47056581</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Wed, 18 Feb 2026 02:54:26 +0000",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "community_signal",
      "freshness": 0.993,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.093,
      "type": "news",
      "llm_label_source": "llm",
      "llm_category": "release",
      "llm_summary_1line": "Agent Audit Kit v0.1 enables deterministic replay and stress testing for LLM agents—critical for debugging and reliability validation in production workflows.",
      "llm_why_1line": "Directly addresses agent harness/eval gap; deterministic replay + stress testing are core reliability patterns. Low HN signal (0 comments) limits evidence depth.",
      "v2_llm_score": 4.25,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 3.636,
      "summary_1line": "Agent Audit Kit v0.1 enables deterministic replay and stress testing for LLM agents, addressing reproducibility and reliability validation in production workflows.",
      "why_it_matters": "Directly addresses agent harness/eval gap; deterministic replay + stress testing are core reliability patterns. Low HN signal (0 comments) limits evidence depth.",
      "v2_slot_priority": 0.673,
      "v2_global_score": 4.309
    },
    {
      "id": "8af6289d4ecb1da2",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Introducing Claude Sonnet 4.6",
      "url": "https://simonwillison.net/2026/Feb/17/claude-sonnet-46/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.anthropic.com/news/claude-sonnet-4-6\">Introducing Claude Sonnet 4.6</a></strong></p>\nSonnet 4.6 is out today, and Anthropic claim it offers similar performance to <a href=\"https://simonwillison.net/2025/Nov/24/claude-opus/\">November's Opus 4.5</a> while maintaining the Sonnet pricing of $3/million input and $15/million output tokens (the Opus models are $5/$25). Here's <a href=\"https://www-cdn.anthropic.com/78073f739564e986ff3e28522761a7a0b4484f84.pdf\">the system card PDF</a>.</p>\n<p>Sonnet 4.6 has a \"reliable knowledge cutoff\" of August 2025, compared to Opus 4.6's May 2025 and Haiku 4.5's February 2025. Both Opus and Sonnet default to 200,000 max input tokens but can stretch to 1 million in beta and at a higher cost.</p>\n<p>I just released <a href=\"https://github.com/simonw/llm-anthropic/releases/tag/0.24\">llm-anthropic 0.24</a> with support for both Sonnet 4.6 and Opus 4.6. Claude Code <a href=\"https://github.com/simonw/llm-anthropic/pull/65\">did most of the work</a> - the new models had a fiddly amount of extra details around adaptive thinking and no longer supporting prefixes, as described <a href=\"https://platform.claude.com/docs/en/about-claude/models/migration-guide\">in Anthropic's migration guide</a>.</p>\n<p>Here's <a href=\"https://gist.github.com/simonw/b185576a95e9321b441f0a4dfc0e297c\">what I got</a> from:</p>\n<pre><code>uvx --with llm-anthropic llm 'Generate an SVG of a pelican riding a bicycle' -m claude-sonnet-4.6\n</code></pre>\n<p><img alt=\"The pelican has a jaunty top hat with a red band. There is a string between the upper and lower beaks for some reason. The bicycle frame is warped in the wrong way.\" src=\"https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png\" /></p>\n<p>The SVG comments include:</p>\n<pre><code>&lt;!-- Hat (fun accessory) --&gt;\n</code></pre>\n<p>I tried a second time and also got a top hat. Sonnet 4.6 apparently loves top hats!</p>\n<p>For comparison, here's the pelican Opus 4.5 drew me <a href=\"https://simonwillison.net/atom/everything/(https:/simonwillison.net/2025/Nov/24/claude-opus/)\">in November</a>:</p>\n<p><img alt=\"The pelican is cute and looks pretty good. The bicycle is not great - the frame is wrong and the pelican is facing backwards when the handlebars appear to be forwards.There is also something that looks a bit like an egg on the handlebars.\" src=\"https://static.simonwillison.net/static/2025/claude-opus-4.5-pelican.jpg\" /></p>\n<p>And here's Anthropic's current best pelican, drawn by Opus 4.6 <a href=\"https://simonwillison.net/2026/Feb/5/two-new-models/\">on February 5th</a>:</p>\n<p><img alt=\"Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers.\" src=\"https://static.simonwillison.net/static/2026/opus-4.6-pelican.png\" /></p>\n<p>Opus 4.6 produces the best pelican beak/pouch. I do think the top hat from Sonnet 4.6 is a nice touch though.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=47050488\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/llm\">llm</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/llm-pricing\">llm-pricing</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png",
      "published": "2026-02-17T23:58:58+00:00",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.927,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.177,
      "type": "news",
      "llm_label_source": "llm",
      "llm_category": "release",
      "llm_summary_1line": "Claude Sonnet 4.6 matches Opus 4.5 performance at Sonnet pricing ($3/$15M tokens); 200K context default, 1M beta, August 2025 knowledge cutoff.",
      "llm_why_1line": "Pricing parity with stronger model is actionable for cost optimization, but limited agentic-specific improvements documented; primarily an economics play.",
      "v2_llm_score": 3.05,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 3.012,
      "summary_1line": "Claude Sonnet 4.6 matches Opus 4.5 performance at half the cost ($3/$15 vs $5/$25 per million tokens) with August 2025 knowledge cutoff.",
      "why_it_matters": "Pricing parity with stronger model is actionable for cost optimization, but limited agentic-specific improvements documented; primarily an economics play.",
      "v2_slot_priority": 0.549,
      "v2_global_score": 3.562
    },
    {
      "id": "1801a1e24942383b",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Qwen3.5: Towards Native Multimodal Agents",
      "url": "https://simonwillison.net/2026/Feb/17/qwen35/#atom-everything",
      "summary": "<p><strong><a href=\"https://qwen.ai/blog?id=qwen3.5\">Qwen3.5: Towards Native Multimodal Agents</a></strong></p>\nAlibaba's Qwen just released the first two models in the Qwen 3.5 series - one open weights, one proprietary. Both are multi-modal for vision input.</p>\n<p>The open weight one is a Mixture of Experts model called Qwen3.5-397B-A17B. Interesting to see Qwen call out serving efficiency as a benefit of that architecture:</p>\n<blockquote>\n<p>Built on an innovative hybrid architecture that fuses linear attention (via Gated Delta Networks) with a sparse mixture-of-experts, the model attains remarkable inference efficiency: although it comprises 397 billion total parameters, just 17 billion are activated per forward pass, optimizing both speed and cost without sacrificing capability.</p>\n</blockquote>\n<p>It's <a href=\"https://huggingface.co/Qwen/Qwen3.5-397B-A17B\">807GB on Hugging Face</a>, and Unsloth have a <a href=\"https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF\">collection of smaller GGUFs</a> ranging in size from 94.2GB 1-bit to 462GB Q8_K_XL.</p>\n<p>I got this <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle/\">pelican</a> from the <a href=\"https://openrouter.ai/qwen/qwen3.5-397b-a17b\">OpenRouter hosted model</a> (<a href=\"https://gist.github.com/simonw/625546cf6b371f9c0040e64492943b82\">transcript</a>):</p>\n<p><img alt=\"Pelican is quite good although the neck lacks an outline for some reason. Bicycle is very basic with an incomplete frame\" src=\"https://static.simonwillison.net/static/2026/qwen3.5-397b-a17b.png\" /></p>\n<p>The proprietary hosted model is called Qwen3.5 Plus 2026-02-15, and is a little confusing. Qwen researcher <a href=\"https://twitter.com/JustinLin610/status/2023340126479569140\">Junyang Lin  says</a>:</p>\n<blockquote>\n<p>Qwen3-Plus is a hosted API version of 397B. As the model natively supports 256K tokens, Qwen3.5-Plus supports 1M token context length. Additionally it supports search and code interpreter, which you can use on Qwen Chat with Auto mode.</p>\n</blockquote>\n<p>Here's <a href=\"https://gist.github.com/simonw/9507dd47483f78dc1195117735273e20\">its pelican</a>, which is similar in quality to the open weights model:</p>\n<p><img alt=\"Similar quality pelican. The bicycle is taller and has a better frame shape. They are visually quite similar.\" src=\"https://static.simonwillison.net/static/2026/qwen3.5-plus-02-15.png\" />\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/vision-llms\">vision-llms</a>, <a href=\"https://simonwillison.net/tags/qwen\">qwen</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/openrouter\">openrouter</a>, <a href=\"https://simonwillison.net/tags/ai-in-china\">ai-in-china</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/qwen3.5-397b-a17b.png",
      "published": "2026-02-17T04:30:57+00:00",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.57,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.82,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Qwen3.5: Towards Native Multimodal Agents Alibaba's Qwen just released the first two models in the Qwen 3.5 series - one open weights, one proprietary. Both are multi-modal for vision input. The open weight one is a M...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.8,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.746,
      "summary_1line": "Qwen3.5: Towards Native Multimodal Agents Alibaba's Qwen just released the first two models in the Qwen 3.5 series - one open weights, one proprietary. Both are multi-modal for vision input. The open weight one is a M...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.549,
      "v2_global_score": 3.296
    },
    {
      "id": "2cef63a7c25d947d",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Moonshot AI Releases Open-Weight Kimi K2.5 Model with Vision and Agent Swarm Capabilities",
      "url": "https://www.infoq.com/news/2026/02/kimi-k25-swarm/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/kimi-k25-swarm/en/headerimage/generatedHeaderImage-1771079384813.jpg\" /><p>Moonshot AI released Kimi K2.5, their latest open-weight multimodal LLM. K2.5 excels at coding tasks, with benchmark scores comparable to frontier models such as GPT-5 and Gemini. It also features an agent swarm mode, which can direct up to 100 sub-agents for attacking problems with parallel workflow.</p> <i>By Anthony Alford</i>",
      "image_url": "https://res.infoq.com/news/2026/02/kimi-k25-swarm/en/headerimage/generatedHeaderImage-1771079384813.jpg",
      "published": "Tue, 17 Feb 2026 14:00:00 GMT",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.722,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.872,
      "type": "release",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Moonshot AI released Kimi K2.5, their latest open-weight multimodal LLM. K2.5 excels at coding tasks, with benchmark scores comparable to frontier models such as GPT-5 and Gemini. It also features an agent swarm mode,...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.6,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.598,
      "summary_1line": "Moonshot AI released Kimi K2.5, their latest open-weight multimodal LLM. K2.5 excels at coding tasks, with benchmark scores comparable to frontier models such as GPT-5 and Gemini. It also features an agent swarm mode,...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.549,
      "v2_global_score": 3.147
    },
    {
      "id": "2caaa6f56b6bd4a1",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation",
      "url": "http://arxiv.org/abs/2602.15724v1",
      "summary": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.",
      "image_url": "",
      "published": "2026-02-17T17:00:11Z",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.914,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.764,
      "type": "paper",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 3.2,
      "v2_source_bias": -0.35,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.707,
      "summary_1line": "Retrieval-augmented framework improves LLM efficiency in vision-language navigation by selecting exemplars and pruning candidate actions.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.382,
      "v2_global_score": 3.089
    },
    {
      "id": "3f5e03016b1baa5d",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Does AI Make the Agile Manifesto Obsolete?",
      "url": "https://www.infoq.com/news/2026/02/ai-agile-manifesto-debate/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/ai-agile-manifesto-debate/en/headerimage/generatedHeaderImage-1770724261142.jpg\" /><p>Capgemini's Steve Jones argues AI agents building apps in hours have killed the Agile Manifesto, as its human-centric principles don't fit agentic SDLCs. While Forrester reports 95% still find Agile relevant, Kent Beck proposes \"augmented coding\" and AWS suggests \"Intent Design\" over sprint planning. The debate: Is Agile dead, or evolving for AI collaboration?</p> <i>By Steef-Jan Wiggers</i>",
      "image_url": "https://res.infoq.com/news/2026/02/ai-agile-manifesto-debate/en/headerimage/generatedHeaderImage-1770724261142.jpg",
      "published": "Tue, 17 Feb 2026 10:34:00 GMT",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.663,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.813,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Capgemini's Steve Jones argues AI agents building apps in hours have killed the Agile Manifesto, as its human-centric principles don't fit agentic SDLCs. While Forrester reports 95% still find Agile relevant, Kent Bec...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.249,
      "summary_1line": "Capgemini's Steve Jones argues AI agents building apps in hours have killed the Agile Manifesto, as its human-centric principles don't fit agentic SDLCs. While Forrester reports 95% still find Agile relevant, Kent Bec...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.549,
      "v2_global_score": 2.799
    },
    {
      "id": "fe7dbebc903fef59",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "India Brief Economic Index",
      "url": "https://www.anthropic.com/research/india-brief-economic-index",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T19:36:33.000Z",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.936,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.336,
      "type": "research",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "India Brief Economic Index",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.4,
      "v2_topical_bias": 0.0,
      "v2_final_score": 2.24,
      "summary_1line": "India Brief Economic Index",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.382,
      "v2_global_score": 2.622
    },
    {
      "id": "071cd72262b5c321",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.103.0",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.103.0",
      "summary": "<h2>New Features</h2>\n<ul>\n<li>App listing responses now include richer app details (<code>app_metadata</code>, branding, and labels), so clients can render more complete app cards without extra requests. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11706\">#11706</a>)</li>\n<li>Commit co-author attribution now uses a Codex-managed <code>prepare-commit-msg</code> hook, with <code>command_attribution</code> override support (default label, custom label, or disable). (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11617\">#11617</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Removed the <code>remote_models</code> feature flag to prevent fallback model metadata when it was disabled, improving model selection reliability and performance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11699\">#11699</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Updated Rust dependencies (<code>clap</code>, <code>env_logger</code>, <code>arc-swap</code>) and refreshed Bazel lock state as routine maintenance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11888\">#11888</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11889\">#11889</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11890\">#11890</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12032\">#12032</a>)</li>\n<li>Reverted the Rust toolchain bump to <code>1.93.1</code> after CI breakage. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11886\">#11886</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12035\">#12035</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.102.0...rust-v0.103.0\"><tt>rust-v0.102.0...rust-v0.103.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11699\">#11699</a> chore: rm remote models fflag <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11706\">#11706</a> [apps] Expose more fields from apps listing endpoints. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11890\">#11890</a> chore(deps): bump arc-swap from 1.8.0 to 1.8.2 in /codex-rs <a class=\"user-mention notranslate\" href=\"https://github.com/dependabot\">@dependabot</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11886\">#11886</a> chore(deps): bump rust-toolchain from 1.93.0 to 1.93.1 in /codex-rs <a class=\"user-mention notranslate\" href=\"https://github.com/dependabot\">@dependabot</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12032\">#12032</a> chore: just bazel-lock-update <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11888\">#11888</a> chore(deps): bump clap from 4.5.56 to 4.5.58 in /codex-rs <a class=\"user-mention notranslate\" href=\"https://github.com/dependabot\">@dependabot</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11889\">#11889</a> chore(deps): bump env_logger from 0.11.8 to 0.11.9 in /codex-rs <a class=\"user-mention notranslate\" href=\"https://github.com/dependabot\">@dependabot</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11617\">#11617</a> Use prompt-based co-author attribution with config override <a class=\"user-mention notranslate\" href=\"https://github.com/gabec-openai\">@gabec-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12035\">#12035</a> Revert \"chore(deps): bump rust-toolchain from 1.93.0 to 1.93.1 in /co…dex-rs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11886\">#11886</a>)\" <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n</ul>",
      "image_url": "",
      "published": "2026-02-17T23:03:06Z",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.932,
      "source_reliability": 1.0,
      "v2_prefilter_score": 4.132,
      "type": "release",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "New Features App listing responses now include richer app details ( app_metadata , branding, and labels), so clients can render more complete app cards without extra requests. ( #11706 ) Commit co-author attribution n...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.35,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.125,
      "summary_1line": "New Features App listing responses now include richer app details ( app_metadata , branding, and labels), so clients can render more complete app cards without extra requests. ( #11706 ) Commit co-author attribution n...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.466,
      "v2_global_score": 2.591
    },
    {
      "id": "42710d92908034f2",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Opus 4 6",
      "url": "https://www.anthropic.com/news/claude-opus-4-6",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T17:46:31.000Z",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.68,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.48,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Claude Opus 4 6",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.06,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.796,
      "summary_1line": "Claude Opus 4 6",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.77,
      "v2_global_score": 2.566
    },
    {
      "id": "6ed48b697f4e1625",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Sonnet 4 6",
      "url": "https://www.anthropic.com/news/claude-sonnet-4-6",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T17:45:22.000Z",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.68,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.48,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Claude Sonnet 4 6",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.06,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.796,
      "summary_1line": "Claude Sonnet 4 6",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.77,
      "v2_global_score": 2.566
    },
    {
      "id": "5e217b99ccfdf091",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "CAMEL: An ECG Language Model for Forecasting Cardiac Events",
      "url": "http://arxiv.org/abs/2602.15677v1",
      "summary": "Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).",
      "image_url": "",
      "published": "2026-02-17T16:02:52Z",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.907,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.757,
      "type": "paper",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classificat...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.75,
      "v2_source_bias": -0.35,
      "v2_topical_bias": 0.0,
      "v2_final_score": 2.124,
      "summary_1line": "CAMEL combines ECG signal encoding with LLMs for cardiac event forecasting; demonstrates SOTA on ECG benchmarks via curriculum learning.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.382,
      "v2_global_score": 2.506
    },
    {
      "id": "5983155f3e279e8e",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Qwen3.5-397B-A17B: the smallest Open-Opus class, very efficient model",
      "url": "https://www.latent.space/p/ainews-qwen35-397b-a17b-the-smallest",
      "summary": "Congrats Qwen team!",
      "image_url": "https://substackcdn.com/image/fetch/$s_!1fDP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0472c69a-cd07-4bde-8b10-61bc1d0702a7_2444x1704.png",
      "published": "Tue, 17 Feb 2026 04:22:56 GMT",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.568,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.768,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Congrats Qwen team!",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.955,
      "summary_1line": "Congrats Qwen team!",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.549,
      "v2_global_score": 2.505
    },
    {
      "id": "6b18d450bbd59ae6",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.45",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.45",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Added support for Claude Sonnet 4.6</li>\n<li>Added support for reading <code>enabledPlugins</code> and <code>extraKnownMarketplaces</code> from <code>--add-dir</code> directories</li>\n<li>Added <code>spinnerTipsOverride</code> setting to customize spinner tips — configure <code>tips</code> with an array of custom tip strings, and optionally set <code>excludeDefault: true</code> to show only your custom tips instead of the built-in ones</li>\n<li>Added <code>SDKRateLimitInfo</code> and <code>SDKRateLimitEvent</code> types to the SDK, enabling consumers to receive rate limit status updates including utilization, reset times, and overage information</li>\n<li>Fixed Agent Teams teammates failing on Bedrock, Vertex, and Foundry by propagating API provider environment variables to tmux-spawned processes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/23561\">#23561</a>)</li>\n<li>Fixed sandbox \"operation not permitted\" errors when writing temporary files on macOS by using the correct per-user temp directory (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/21654\">#21654</a>)</li>\n<li>Fixed Task tool (backgrounded agents) crashing with a <code>ReferenceError</code> on completion (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/22087\">#22087</a>)</li>\n<li>Fixed autocomplete suggestions not being accepted on Enter when images are pasted in the input</li>\n<li>Fixed skills invoked by subagents incorrectly appearing in main session context after compaction</li>\n<li>Fixed excessive <code>.claude.json.backup</code> files accumulating on every startup</li>\n<li>Fixed plugin-provided commands, agents, and hooks not being available immediately after installation without requiring a restart</li>\n<li>Improved startup performance by removing eager loading of session history for stats caching</li>\n<li>Improved memory usage for shell commands that produce large output — RSS no longer grows unboundedly with command output size</li>\n<li>Improved collapsed read/search groups to show the current file or search pattern being processed beneath the summary line while active</li>\n<li>[VSCode] Improved permission destination choice (project/user/session) to persist across sessions</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-17T18:53:52Z",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.865,
      "source_reliability": 1.0,
      "v2_prefilter_score": 4.065,
      "type": "release",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "What's changed Added support for Claude Sonnet 4.6 Added support for reading enabledPlugins and extraKnownMarketplaces from --add-dir directories Added spinnerTipsOverride setting to customize spinner tips — configure...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.15,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 1.965,
      "summary_1line": "What's changed Added support for Claude Sonnet 4.6 Added support for reading enabledPlugins and extraKnownMarketplaces from --add-dir directories Added spinnerTipsOverride setting to customize spinner tips — configure...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.466,
      "v2_global_score": 2.431
    },
    {
      "id": "bef44b20b585b2ba",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.37",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.37",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.44</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.37/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.37/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.37</pre></div>",
      "image_url": "",
      "published": "2026-02-16T21:51:27Z",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.594,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.894,
      "type": "release",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Internal/Other Changes Updated bundled Claude CLI to version 2.1.44 PyPI: https://pypi.org/project/claude-agent-sdk/0.1.37/ pip install claude-agent-sdk==0.1.37",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 1.778,
      "summary_1line": "Internal/Other Changes Updated bundled Claude CLI to version 2.1.44 PyPI: https://pypi.org/project/claude-agent-sdk/0.1.37/ pip install claude-agent-sdk==0.1.37",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.466,
      "v2_global_score": 2.244
    },
    {
      "id": "61d49c9c5c9b00a3",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "NVIDIA Nemotron 2 Nano 9B Japanese: 日本のソブリンAIを支える最先端小規模言語モデル",
      "url": "https://huggingface.co/blog/nvidia/nemotron-nano-9b-v2-japanese-ja",
      "summary": "",
      "image_url": "",
      "published": "Tue, 17 Feb 2026 23:28:52 GMT",
      "collected_at": "2026-02-18T03:00:06.047322+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.969,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.069,
      "type": "research",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "NVIDIA Nemotron 2 Nano 9B Japanese: 日本のソブリンAIを支える最先端小規模言語モデル",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.845,
      "summary_1line": "NVIDIA Nemotron 2 Nano 9B Japanese: 日本のソブリンAIを支える最先端小規模言語モデル",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.382,
      "v2_global_score": 2.227
    }
  ]
}