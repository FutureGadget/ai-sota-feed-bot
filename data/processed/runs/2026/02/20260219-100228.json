{
  "run_at": "2026-02-19T10:02:28.052219+00:00",
  "item_count": 21,
  "items": [
    {
      "id": "8af6289d4ecb1da2",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Introducing Claude Sonnet 4.6",
      "url": "https://simonwillison.net/2026/Feb/17/claude-sonnet-46/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.anthropic.com/news/claude-sonnet-4-6\">Introducing Claude Sonnet 4.6</a></strong></p>\nSonnet 4.6 is out today, and Anthropic claim it offers similar performance to <a href=\"https://simonwillison.net/2025/Nov/24/claude-opus/\">November's Opus 4.5</a> while maintaining the Sonnet pricing of $3/million input and $15/million output tokens (the Opus models are $5/$25). Here's <a href=\"https://www-cdn.anthropic.com/78073f739564e986ff3e28522761a7a0b4484f84.pdf\">the system card PDF</a>.</p>\n<p>Sonnet 4.6 has a \"reliable knowledge cutoff\" of August 2025, compared to Opus 4.6's May 2025 and Haiku 4.5's February 2025. Both Opus and Sonnet default to 200,000 max input tokens but can stretch to 1 million in beta and at a higher cost.</p>\n<p>I just released <a href=\"https://github.com/simonw/llm-anthropic/releases/tag/0.24\">llm-anthropic 0.24</a> with support for both Sonnet 4.6 and Opus 4.6. Claude Code <a href=\"https://github.com/simonw/llm-anthropic/pull/65\">did most of the work</a> - the new models had a fiddly amount of extra details around adaptive thinking and no longer supporting prefixes, as described <a href=\"https://platform.claude.com/docs/en/about-claude/models/migration-guide\">in Anthropic's migration guide</a>.</p>\n<p>Here's <a href=\"https://gist.github.com/simonw/b185576a95e9321b441f0a4dfc0e297c\">what I got</a> from:</p>\n<pre><code>uvx --with llm-anthropic llm 'Generate an SVG of a pelican riding a bicycle' -m claude-sonnet-4.6\n</code></pre>\n<p><img alt=\"The pelican has a jaunty top hat with a red band. There is a string between the upper and lower beaks for some reason. The bicycle frame is warped in the wrong way.\" src=\"https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png\" /></p>\n<p>The SVG comments include:</p>\n<pre><code>&lt;!-- Hat (fun accessory) --&gt;\n</code></pre>\n<p>I tried a second time and also got a top hat. Sonnet 4.6 apparently loves top hats!</p>\n<p>For comparison, here's the pelican Opus 4.5 drew me <a href=\"https://simonwillison.net/atom/everything/(https:/simonwillison.net/2025/Nov/24/claude-opus/)\">in November</a>:</p>\n<p><img alt=\"The pelican is cute and looks pretty good. The bicycle is not great - the frame is wrong and the pelican is facing backwards when the handlebars appear to be forwards.There is also something that looks a bit like an egg on the handlebars.\" src=\"https://static.simonwillison.net/static/2025/claude-opus-4.5-pelican.jpg\" /></p>\n<p>And here's Anthropic's current best pelican, drawn by Opus 4.6 <a href=\"https://simonwillison.net/2026/Feb/5/two-new-models/\">on February 5th</a>:</p>\n<p><img alt=\"Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers.\" src=\"https://static.simonwillison.net/static/2026/opus-4.6-pelican.png\" /></p>\n<p>Opus 4.6 produces the best pelican beak/pouch. I do think the top hat from Sonnet 4.6 is a nice touch though.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=47050488\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/llm\">llm</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/llm-pricing\">llm-pricing</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
      "image_url": "https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png",
      "published": "2026-02-17T23:58:58+00:00",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.427,
      "tier1_quick_score": 2.966,
      "v2_slot": "practitioner_analysis",
      "v2_prefilter_score": 2.677,
      "llm_label_source": "llm",
      "llm_category": "release",
      "llm_summary_1line": "Claude Sonnet 4.6 matches Opus 4.5 performance at Sonnet pricing ($3/$15M tokens); 200K context default, 1M beta, August 2025 knowledge cutoff.",
      "llm_why_1line": "Pricing parity with stronger model is actionable for cost optimization, but limited agentic-specific improvements documented; primarily an economics play.",
      "v2_llm_score": 3.05,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.937,
      "summary_1line": "Claude Sonnet 4.6 matches Opus 4.5 performance at half the cost ($3/$15 vs $5/$25 per million tokens) with August 2025 knowledge cutoff.",
      "why_it_matters": "Pricing parity with stronger model is actionable for cost optimization, but limited agentic-specific improvements documented; primarily an economics play.",
      "v2_slot_priority": 0.512,
      "v2_global_score": 3.449
    },
    {
      "id": "13463e1f08e717b4",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "The A.I. Disruption We’ve Been Waiting for Has Arrived",
      "url": "https://simonwillison.net/2026/Feb/18/the-ai-disruption/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.nytimes.com/2026/02/18/opinion/ai-software.html?unlocked_article_code=1.NFA.UkLv.r-XczfzYRdXJ&amp;smid=url-share\">The A.I. Disruption We’ve Been Waiting for Has Arrived</a></strong></p>\nNew opinion piece from Paul Ford in the New York Times. Unsurprisingly for a piece by Paul it's packed with quoteworthy snippets, but a few stood out for me in particular.</p>\n<p>Paul describes the <a href=\"https://simonwillison.net/2026/Jan/4/inflection/\">November moment</a> that so many other programmers have observed, and highlights Claude Code's ability to revive old side projects:</p>\n<blockquote>\n<p>[Claude Code] was always a helpful coding assistant, but in November it suddenly got much better, and ever since I’ve been knocking off side projects that had sat in folders for a decade or longer. It’s fun to see old ideas come to life, so I keep a steady flow. Maybe it adds up to a half-hour a day of my time, and an hour of Claude’s.</p>\n<p>November was, for me and many others in tech, a great surprise. Before, A.I. coding tools were often useful, but halting and clumsy. Now, the bot can run for a full hour and make whole, designed websites and apps that may be flawed, but credible. I spent an entire session of therapy talking about it.</p>\n</blockquote>\n<p>And as the former CEO of a respected consultancy firm (Postlight) he's well positioned to evaluate the potential impact:</p>\n<blockquote>\n<p>When you watch a large language model slice through some horrible, expensive problem — like migrating data from an old platform to a modern one — you feel the earth shifting. I was the chief executive of a software services firm, which made me a professional software cost estimator. When I rebooted my messy personal website a few weeks ago, I realized: I would have paid $25,000 for someone else to do this. When a friend asked me to convert a large, thorny data set, I downloaded it, cleaned it up and made it pretty and easy to explore. In the past I would have charged $350,000.</p>\n<p>That last price is full 2021 retail — it implies a product manager, a designer, two engineers (one senior) and four to six months of design, coding and testing. Plus maintenance. Bespoke software is joltingly expensive. Today, though, when the stars align and my prompts work out, I can do hundreds of thousands of dollars worth of work for fun (fun for me) over weekends and evenings, for the price of the Claude $200-a-month plan.</p>\n</blockquote>\n<p>He also neatly captures the inherent community tension involved in exploring this technology:</p>\n<blockquote>\n<p>All of the people I love hate this stuff, and all the people I hate love it. And yet, likely because of the same personality flaws that drew me to technology in the first place, I am annoyingly excited.</p>\n</blockquote>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/new-york-times\">new-york-times</a>, <a href=\"https://simonwillison.net/tags/paul-ford\">paul-ford</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
      "image_url": "",
      "published": "2026-02-18T17:07:31+00:00",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.655,
      "tier1_quick_score": 3.159,
      "v2_slot": "practitioner_analysis",
      "v2_prefilter_score": 2.905,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "The A.I. Disruption We’ve Been Waiting for Has Arrived New opinion piece from Paul Ford in the New York Times. Unsurprisingly for a piece by Paul it's packed with quoteworthy snippets, but a few stood out for me in pa...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.6,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.588,
      "summary_1line": "The A.I. Disruption We’ve Been Waiting for Has Arrived New opinion piece from Paul Ford in the New York Times. Unsurprisingly for a piece by Paul it's packed with quoteworthy snippets, but a few stood out for me in pa...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.512,
      "v2_global_score": 3.1
    },
    {
      "id": "bf60b833f693787c",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "GitHub Agentic Workflows Unleash AI-Driven Repository Automation",
      "url": "https://www.infoq.com/news/2026/02/github-agentic-workflows/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/github-agentic-workflows/en/headerimage/github-agentic-workflows-1771415135302.jpeg\" /><p>Recently launched in technical preview, GitHub Agentic Workflows introduce a way to automate complex, repetitive repository tasks using coding agents that understand context and intent, GitHub says. This enables workflows such as automatic issue triage and labeling, documentation updates, CI troubleshooting, test improvements, and reporting.</p> <i>By Sergio De Simone</i>",
      "image_url": "https://res.infoq.com/news/2026/02/github-agentic-workflows/en/headerimage/github-agentic-workflows-1771415135302.jpeg",
      "published": "Wed, 18 Feb 2026 12:00:00 GMT",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.576,
      "tier1_quick_score": 2.996,
      "v2_slot": "practitioner_analysis",
      "v2_prefilter_score": 2.726,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Recently launched in technical preview, GitHub Agentic Workflows introduce a way to automate complex, repetitive repository tasks using coding agents that understand context and intent, GitHub says. This enables workf...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.6,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.576,
      "summary_1line": "Recently launched in technical preview, GitHub Agentic Workflows introduce a way to automate complex, repetitive repository tasks using coding agents that understand context and intent, GitHub says. This enables workf...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.512,
      "v2_global_score": 3.088
    },
    {
      "id": "86624d7aa186ea4d",
      "source": "llamaindex_releases",
      "source_weight": 0.95,
      "title": "v0.14.15",
      "url": "https://github.com/run-llama/llama_index/releases/tag/v0.14.15",
      "summary": "<h1>Release Notes</h1>\n<h2>[2026-02-18]</h2>\n<h3>llama-index-agent-agentmesh [0.1.0]</h3>\n<ul>\n<li>[Integration] AgentMesh: Trust Layer for LlamaIndex Agents (<a href=\"https://github.com/run-llama/llama_index/pull/20644\">#20644</a>)</li>\n</ul>\n<h3>llama-index-core [0.14.15]</h3>\n<ul>\n<li>Support basic operations for multimodal types (<a href=\"https://github.com/run-llama/llama_index/pull/20640\">#20640</a>)</li>\n<li>Feat recursive llm type support (<a href=\"https://github.com/run-llama/llama_index/pull/20642\">#20642</a>)</li>\n<li>fix: remove redundant metadata_seperator field from TextNode (<a href=\"https://github.com/run-llama/llama_index/pull/20649\">#20649</a>)</li>\n<li>fix(tests): update mock prompt type in mock_prompts.py (<a href=\"https://github.com/run-llama/llama_index/pull/20661\">#20661</a>)</li>\n<li>Feat multimodal template var formatting (<a href=\"https://github.com/run-llama/llama_index/pull/20682\">#20682</a>)</li>\n<li>Feat multimodal prompt templates (<a href=\"https://github.com/run-llama/llama_index/pull/20683\">#20683</a>)</li>\n<li>Feat multimodal chat prompt helper (<a href=\"https://github.com/run-llama/llama_index/pull/20684\">#20684</a>)</li>\n<li>Add retry and error handling to BaseExtractor (<a href=\"https://github.com/run-llama/llama_index/pull/20693\">#20693</a>)</li>\n<li>ensure at least one message/content block is returned by the old memory (<a href=\"https://github.com/run-llama/llama_index/pull/20729\">#20729</a>)</li>\n</ul>\n<h3>llama-index-embeddings-ibm [0.6.0.post1]</h3>\n<ul>\n<li>chore: Remove persistent_connection parameter support, update (<a href=\"https://github.com/run-llama/llama_index/pull/20714\">#20714</a>)</li>\n<li>docs: Update IBM docs (<a href=\"https://github.com/run-llama/llama_index/pull/20718\">#20718</a>)</li>\n</ul>\n<h3>llama-index-llms-anthropic [0.10.9]</h3>\n<ul>\n<li>Sonnet 4-6 addition (<a href=\"https://github.com/run-llama/llama_index/pull/20723\">#20723</a>)</li>\n</ul>\n<h3>llama-index-llms-bedrock-converse [0.12.10]</h3>\n<ul>\n<li>fix(bedrock-converse): ensure thinking_delta is populated in all chat modes (<a href=\"https://github.com/run-llama/llama_index/pull/20664\">#20664</a>)</li>\n<li>feat(bedrock-converse): Add support for Claude Sonnet 4.6 (<a href=\"https://github.com/run-llama/llama_index/pull/20726\">#20726</a>)</li>\n</ul>\n<h3>llama-index-llms-ibm [0.7.0.post1]</h3>\n<ul>\n<li>chore: Remove persistent_connection parameter support, update (<a href=\"https://github.com/run-llama/llama_index/pull/20714\">#20714</a>)</li>\n<li>docs: Update IBM docs (<a href=\"https://github.com/run-llama/llama_index/pull/20718\">#20718</a>)</li>\n</ul>\n<h3>llama-index-llms-mistralai [0.10.0]</h3>\n<ul>\n<li>Rrubini/mistral azure sdk (<a href=\"https://github.com/run-llama/llama_index/pull/20668\">#20668</a>)</li>\n</ul>\n<h3>llama-index-llms-oci-data-science [1.0.0]</h3>\n<ul>\n<li>Add support for new OCI DataScience endpoint /predictWithStream for streaming use case (<a href=\"https://github.com/run-llama/llama_index/pull/20545\">#20545</a>)</li>\n</ul>\n<h3>llama-index-observability-otel [0.3.0]</h3>\n<ul>\n<li>improve otel data serialization by flattening dicts (<a href=\"https://github.com/run-llama/llama_index/pull/20719\">#20719</a>)</li>\n<li>feat: support custom span processor; refactor: use llama-index-instrumentation instead of llama-index-core (<a href=\"https://github.com/run-llama/llama_index/pull/20732\">#20732</a>)</li>\n</ul>\n<h3>llama-index-program-evaporate [0.5.2]</h3>\n<ul>\n<li>Sandbox LLM-generated code execution in EvaporateExtractor (<a href=\"https://github.com/run-llama/llama_index/pull/20676\">#20676</a>)</li>\n</ul>\n<h3>llama-index-readers-bitbucket [0.4.2]</h3>\n<ul>\n<li>fix: replace mutable default argument in load_all_file_paths (<a href=\"https://github.com/run-llama/llama_index/pull/20698\">#20698</a>)</li>\n</ul>\n<h3>llama-index-readers-github [0.10.0]</h3>\n<ul>\n<li>feat: Enhance GitHubRepoReader with selective file fetching and deduplication (Issue <a class=\"issue-link js-issue-link\" href=\"https://github.com/run-llama/llama_index/issues/20471\">#20471</a>) (<a href=\"https://github.com/run-llama/llama_index/pull/20550\">#20550</a>)</li>\n</ul>\n<h3>llama-index-readers-layoutir [0.1.1]</h3>\n<ul>\n<li>feat: Add LayoutIR reader integration (<a href=\"https://github.com/run-llama/llama_index/pull/20708\">#20708</a>)</li>\n<li>fix(layoutir): hotfix for output_dir crash and Block extraction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/run-llama/llama_index/pull/20708\">#20708</a> follow-up) (<a href=\"https://github.com/run-llama/llama_index/pull/20715\">#20715</a>)</li>\n<li>fix(layoutir): restrict requires-python to &gt;=3.12 to match layoutir dependency (<a href=\"https://github.com/run-llama/llama_index/pull/20733\">#20733</a>)</li>\n</ul>\n<h3>llama-index-readers-microsoft-sharepoint [0.8.0]</h3>\n<ul>\n<li>Add pagination support for Microsoft Graph API calls in SharePoint reader (<a href=\"https://github.com/run-llama/llama_index/pull/20704\">#20704</a>)</li>\n</ul>\n<h3>llama-index-readers-whatsapp [0.4.2]</h3>\n<ul>\n<li>fix: Update WhatsAppChatLoader to retrieve DataFrame in pandas format (<a href=\"https://github.com/run-llama/llama_index/pull/20722\">#20722</a>)</li>\n</ul>\n<h3>llama-index-tools-mcp [0.4.7]</h3>\n<ul>\n<li>feat: propagate partial_params to get_tools_from_mcp utils (<a href=\"https://github.com/run-llama/llama_index/pull/20669\">#20669</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-faiss [0.5.3]</h3>\n<ul>\n<li>Replace eval() with json.loads in FaissMapVectorStore persistence (<a href=\"https://github.com/run-llama/llama_index/pull/20675\">#20675</a>)</li>\n</ul>\n<h3>llama-index-vector-stores-milvus [1.0.0]</h3>\n<ul>\n<li>Fix: remove ORM Collection mix-usage with MilvusClient in Milvus vector store (<a href=\"https://github.com/run-llama/llama_index/pull/20687\">#20687</a>)</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-18T19:06:42Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 1.0,
      "freshness": 0.766,
      "tier1_quick_score": 2.884,
      "v2_slot": "agent_tooling_releases",
      "v2_prefilter_score": 2.716,
      "llm_label_source": "heuristic",
      "llm_category": "release",
      "llm_summary_1line": "Release Notes [2026-02-18] llama-index-agent-agentmesh [0.1.0] [Integration] AgentMesh: Trust Layer for LlamaIndex Agents ( #20644 ) llama-index-core [0.14.15] Support basic operations for multimodal types ( #20640 )...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 3.0,
      "v2_source_bias": 0.05,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.58,
      "summary_1line": "Release Notes [2026-02-18] llama-index-agent-agentmesh [0.1.0] [Integration] AgentMesh: Trust Layer for LlamaIndex Agents ( #20644 ) llama-index-core [0.14.15] Support basic operations for multimodal types ( #20640 )...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.498,
      "v2_global_score": 3.078
    },
    {
      "id": "2caaa6f56b6bd4a1",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation",
      "url": "http://arxiv.org/abs/2602.15724v1",
      "summary": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.",
      "image_url": "",
      "published": "2026-02-17T17:00:11Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 1.0,
      "freshness": 0.693,
      "tier1_quick_score": 2.5,
      "v2_slot": "research_watch",
      "v2_prefilter_score": 2.543,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 3.2,
      "v2_source_bias": -0.35,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.674,
      "summary_1line": "Retrieval-augmented framework improves LLM efficiency in vision-language navigation by selecting exemplars and pruning candidate actions.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.351,
      "v2_global_score": 3.025
    },
    {
      "id": "ef58b4d6ca17a3ee",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: NSED is public – Mixture-of-Models to Hit SOTA using self-hosted AI",
      "url": "https://github.com/peeramid-labs/nsed",
      "summary": "<p>Hey HN,\nWe're open-sourcing (source-available, BSL 1.1, patent pending) the orchestrator behind our paper benchmark results. NSED (N-Way Self-Evaluating Deliberation) is a Rust binary that coordinates multiple LLMs through structured rounds of proposals and cross-evaluation, using quadratic voting to prevent any single model from dominating the consensus.<p>The result: Three open-weight models (20B, 8B, 12B) on consumer GPUs — 64GB total VRAM, ~$7K hardware — score 84% on AIME 2025. The same models individually or with naive majority voting score ~54%. That's frontier-model performance on hardware you can buy at Micro Center.<p>How it works:<p>Each agent independently proposes a solution\nEvery agent evaluates every other agent's work\nScores aggregate via quadratic voting (cost of influence grows quadratically → no single model can dominate)\nRepeat. Agents see prior results, refine, re-evaluate\nSystem converges toward the highest-quality answer through adversarial cross-checking<p>It's provider-agnostic — mix Ollama, vLLM, OpenAI, Anthropic, or any OpenAI-compatible endpoint in the same deliberation. Everything streams over NATS JetStream with full persistence: every proposal, evaluation, score, and reasoning trace is logged and streamable via SSE.<p>Paper: arxiv.org/abs/2601.16863\nHappy to answer questions about the architecture, the quadratic voting mechanism, benchmark methodology, or anything else.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47067772\">https://news.ycombinator.com/item?id=47067772</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "image_url": "",
      "published": "Wed, 18 Feb 2026 23:21:15 +0000",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.513,
      "tier1_quick_score": 3.091,
      "v2_slot": "community_signal",
      "v2_prefilter_score": 2.613,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Hey HN, We're open-sourcing (source-available, BSL 1.1, patent pending) the orchestrator behind our paper benchmark results. NSED (N-Way Self-Evaluating Deliberation) is a Rust binary that coordinates multiple LLMs th...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.85,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.466,
      "summary_1line": "Hey HN, We're open-sourcing (source-available, BSL 1.1, patent pending) the orchestrator behind our paper benchmark results. NSED (N-Way Self-Evaluating Deliberation) is a Rust binary that coordinates multiple LLMs th...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.413,
      "v2_global_score": 2.879
    },
    {
      "id": "4d640e64a53f865c",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "GLM-5: from Vibe Coding to Agentic Engineering",
      "url": "http://arxiv.org/abs/2602.15763v1",
      "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
      "image_url": "",
      "published": "2026-02-17T17:50:56Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "paper",
      "source_reliability": 1.0,
      "freshness": 0.698,
      "tier1_quick_score": 2.508,
      "v2_slot": "research_watch",
      "v2_prefilter_score": 2.548,
      "llm_label_source": "heuristic",
      "llm_category": "research",
      "llm_summary_1line": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, G...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 3.0,
      "v2_source_bias": -0.35,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.505,
      "summary_1line": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, G...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.351,
      "v2_global_score": 2.856
    },
    {
      "id": "e090493a0ff267ce",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing GPT-5.3-Codex-Spark",
      "url": "https://openai.com/index/introducing-gpt-5-3-codex-spark",
      "summary": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
      "image_url": "",
      "published": "Thu, 12 Feb 2026 10:00:00 GMT",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.122,
      "tier1_quick_score": 3.111,
      "v2_slot": "frontier_official",
      "v2_prefilter_score": 3.122,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.1,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.084,
      "summary_1line": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.686,
      "v2_global_score": 2.77
    },
    {
      "id": "9d95a891a81b27c3",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Beyond rate limits: scaling access to Codex and Sora",
      "url": "https://openai.com/index/beyond-rate-limits",
      "summary": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.163,
      "tier1_quick_score": 3.153,
      "v2_slot": "frontier_official",
      "v2_prefilter_score": 3.163,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.1,
      "v2_topical_bias": 0.2,
      "v2_final_score": 1.933,
      "summary_1line": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.686,
      "v2_global_score": 2.619
    },
    {
      "id": "42710d92908034f2",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Opus 4 6",
      "url": "https://www.anthropic.com/news/claude-opus-4-6",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T17:46:31.000Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.605,
      "tier1_quick_score": 3.457,
      "v2_slot": "frontier_official",
      "v2_prefilter_score": 3.405,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Claude Opus 4 6",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.06,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.781,
      "summary_1line": "Claude Opus 4 6",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.686,
      "v2_global_score": 2.467
    },
    {
      "id": "9fa6421b6aa531b8",
      "source": "langchain_blog",
      "source_weight": 1.05,
      "title": "monday Service + LangSmith: Building a Code-First Evaluation Strategy from Day 1",
      "url": "https://blog.langchain.com/customers-monday/",
      "summary": "Learn how monday Service developed an eval-driven development framework for their customer-facing service agents.",
      "image_url": "https://blog.langchain.com/content/images/2026/02/Monday-Service-case-study.png",
      "published": "Wed, 18 Feb 2026 08:05:53 GMT",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.523,
      "tier1_quick_score": 2.852,
      "v2_slot": "practitioner_analysis",
      "v2_prefilter_score": 2.573,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Learn how monday Service developed an eval-driven development framework for their customer-facing service agents.",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.4,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.318,
      "summary_1line": "Learn how monday Service developed an eval-driven development framework for their customer-facing service agents.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.512,
      "v2_global_score": 2.83
    },
    {
      "id": "0f43cce4717b58ca",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "Measuring Agent Autonomy",
      "url": "https://www.anthropic.com/research/measuring-agent-autonomy",
      "summary": "",
      "image_url": "",
      "published": "2026-02-18T20:26:31.000Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "research",
      "source_reliability": 1.0,
      "freshness": 0.886,
      "tier1_quick_score": 3.352,
      "v2_slot": "research_watch",
      "v2_prefilter_score": 3.286,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Measuring Agent Autonomy",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.4,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.433,
      "summary_1line": "Measuring Agent Autonomy",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.351,
      "v2_global_score": 2.784
    },
    {
      "id": "5713ae1ccd7185d1",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.47",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.47",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed FileWriteTool line counting to preserve intentional trailing blank lines instead of stripping them with <code>trimEnd()</code>.</li>\n<li>Fixed Windows terminal rendering bugs caused by <code>os.EOL</code> (<code>\\r\\n</code>) in display code — line counts now show correct values instead of always showing 1 on Windows.</li>\n<li>Improved VS Code plan preview: auto-updates as Claude iterates, enables commenting only when the plan is ready for review, and keeps the preview open when rejecting so Claude can revise.</li>\n<li>Fixed a bug where bold and colored text in markdown output could shift to the wrong characters on Windows due to <code>\\r\\n</code> line endings.</li>\n<li>Fixed compaction failing when conversation contains many PDF documents by stripping document blocks alongside images before sending to the compaction API (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26188\">#26188</a>)</li>\n<li>Improved memory usage in long-running sessions by releasing API stream buffers, agent context, and skill state after use</li>\n<li>Improved startup performance by deferring SessionStart hook execution, reducing time-to-interactive by ~500ms.</li>\n<li>Fixed an issue where bash tool output was silently discarded on Windows when using MSYS2 or Cygwin shells.</li>\n<li>Improved performance of <code>@</code> file mentions - file suggestions now appear faster by pre-warming the index on startup and using session-based caching with background refresh.</li>\n<li>Improved memory usage by trimming agent task message history after tasks complete</li>\n<li>Improved memory usage during long agent sessions by eliminating O(n²) message accumulation in progress updates</li>\n<li>Fixed the bash permission classifier to validate that returned match descriptions correspond to actual input rules, preventing hallucinated descriptions from incorrectly granting permissions</li>\n<li>Fixed user-defined agents only loading one file on NFS/FUSE filesystems that report zero inodes (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26044\">#26044</a>)</li>\n<li>Fixed plugin agent skills silently failing to load when referenced by bare name instead of fully-qualified plugin name (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25834\">#25834</a>)</li>\n<li>Search patterns in collapsed tool results are now displayed in quotes for clarity</li>\n<li>Windows: Fixed CWD tracking temp files never being cleaned up, causing them to accumulate indefinitely (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/17600\">#17600</a>)</li>\n<li>Use <code>ctrl+f</code> to kill all background agents instead of double-pressing ESC. Background agents now continue running when you press ESC to cancel the main thread, giving you more control over agent lifecycle.</li>\n<li>Fixed API 400 errors (\"thinking blocks cannot be modified\") that occurred in sessions with concurrent agents, caused by interleaved streaming content blocks preventing proper message merging.</li>\n<li>Simplified teammate navigation to use only Shift+Down (with wrapping) instead of both Shift+Up and Shift+Down.</li>\n<li>Fixed an issue where a single file write/edit error would abort all other parallel file write/edit operations. Independent file mutations now complete even when a sibling fails.</li>\n<li>Added <code>last_assistant_message</code> field to Stop and SubagentStop hook inputs, providing the final assistant response text so hooks can access it without parsing transcript files.</li>\n<li>Fixed custom session titles set via <code>/rename</code> being lost after resuming a conversation (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/23610\">#23610</a>)</li>\n<li>Fixed collapsed read/search hint text overflowing on narrow terminals by truncating from the start.</li>\n<li>Fixed an issue where bash commands with backslash-newline continuation lines (e.g., long commands split across multiple lines with <code>\\</code>) would produce spurious empty arguments, potentially breaking command execution.</li>\n<li>Fixed built-in slash commands (<code>/help</code>, <code>/model</code>, <code>/compact</code>, etc.) being hidden from the autocomplete dropdown when many user skills are installed (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/22020\">#22020</a>)</li>\n<li>Fixed MCP servers not appearing in the MCP Management Dialog after deferred loading</li>\n<li>Fixed session name persisting in status bar after <code>/clear</code> command (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26082\">#26082</a>)</li>\n<li>Fixed crash when a skill's <code>name</code> or <code>description</code> in SKILL.md frontmatter is a bare number (e.g., <code>name: 3000</code>) — the value is now properly coerced to a string (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25837\">#25837</a>)</li>\n<li>Fixed /resume silently dropping sessions when the first message exceeds 16KB or uses array-format content (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25721\">#25721</a>)</li>\n<li>Added <code>chat:newline</code> keybinding action for configurable multi-line input (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26075\">#26075</a>)</li>\n<li>Added <code>added_dirs</code> to the statusline JSON <code>workspace</code> section, exposing directories added via <code>/add-dir</code> to external scripts (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26096\">#26096</a>)</li>\n<li>Fixed <code>claude doctor</code> misclassifying mise and asdf-managed installations as native installs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26033\">#26033</a>)</li>\n<li>Fixed zsh heredoc failing with \"read-only file system\" error in sandboxed commands (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25990\">#25990</a>)</li>\n<li>Fixed agent progress indicator showing inflated tool use count (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26023\">#26023</a>)</li>\n<li>Fixed image pasting not working on WSL2 systems where Windows copies images as BMP format (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25935\">#25935</a>)</li>\n<li>Fixed background agent results returning raw transcript data instead of the agent's final answer (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26012\">#26012</a>)</li>\n<li>Fixed Warp terminal incorrectly prompting for Shift+Enter setup when it supports it natively (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25957\">#25957</a>)</li>\n<li>Fixed CJK wide characters causing misaligned timestamps and layout elements in the TUI (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26084\">#26084</a>)</li>\n<li>Fixed custom agent <code>model</code> field in <code>.claude/agents/*.md</code> being ignored when spawning team teammates (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26064\">#26064</a>)</li>\n<li>Fixed plan mode being lost after context compaction, causing the model to switch from planning to implementation mode (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26061\">#26061</a>)</li>\n<li>Fixed <code>alwaysThinkingEnabled: true</code> in settings.json not enabling thinking mode on Bedrock and Vertex providers (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26074\">#26074</a>)</li>\n<li>Fixed <code>tool_decision</code> OTel telemetry event not being emitted in headless/SDK mode (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26059\">#26059</a>)</li>\n<li>Fixed session name being lost after context compaction — renamed sessions now preserve their custom title through compaction (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26121\">#26121</a>)</li>\n<li>Increased initial session count in resume picker from 10 to 50 for faster session discovery (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26123\">#26123</a>)</li>\n<li>Windows: fixed worktree session matching when drive letter casing differs (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26123\">#26123</a>)</li>\n<li>Fixed <code>/resume &lt;session-id&gt;</code> failing to find sessions whose first message exceeds 16KB (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25920\">#25920</a>)</li>\n<li>Fixed \"Always allow\" on multiline bash commands creating invalid permission patterns that corrupt settings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25909\">#25909</a>)</li>\n<li>Fixed React crash (error <a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/31\">#31</a>) when a skill's <code>argument-hint</code> in SKILL.md frontmatter uses YAML sequence syntax (e.g., <code>[topic: foo | bar]</code>) — the value is now properly coerced to a string (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25826\">#25826</a>)</li>\n<li>Fixed crash when using <code>/fork</code> on sessions that used web search — null entries in search results from transcript deserialization are now handled gracefully (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25811\">#25811</a>)</li>\n<li>Fixed read-only git commands triggering FSEvents file watcher loops on macOS by adding --no-optional-locks flag (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25750\">#25750</a>)</li>\n<li>Fixed custom agents and skills not being discovered when running from a git worktree — project-level <code>.claude/agents/</code> and <code>.claude/skills/</code> from the main repository are now included (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25816\">#25816</a>)</li>\n<li>Fixed non-interactive subcommands like <code>claude doctor</code> and <code>claude plugin validate</code> being blocked inside nested Claude sessions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25803\">#25803</a>)</li>\n<li>Windows: Fixed the same CLAUDE.md file being loaded twice when drive letter casing differs between paths (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25756\">#25756</a>)</li>\n<li>Fixed inline code spans in markdown being incorrectly parsed as bash commands (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25792\">#25792</a>)</li>\n<li>Fixed teammate spinners not respecting custom spinnerVerbs from settings (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25748\">#25748</a>)</li>\n<li>Fixed shell commands permanently failing after a command deletes its own working directory (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26136\">#26136</a>)</li>\n<li>Fixed hooks (PreToolUse, PostToolUse) silently failing to execute on Windows by using Git Bash instead of cmd.exe (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25981\">#25981</a>)</li>\n<li>Fixed LSP <code>findReferences</code> and other location-based operations returning results from gitignored files (e.g., <code>node_modules/</code>, <code>venv/</code>) (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26051\">#26051</a>)</li>\n<li>Moved config backup files from home directory root to <code>~/.claude/backups/</code> to reduce home directory clutter (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26130\">#26130</a>)</li>\n<li>Fixed sessions with large first prompts (&gt;16KB) disappearing from the /resume list (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26140\">#26140</a>)</li>\n<li>Fixed shell functions with double-underscore prefixes (e.g., <code>__git_ps1</code>) not being preserved across shell sessions (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25824\">#25824</a>)</li>\n<li>Fixed spinner showing \"0 tokens\" counter before any tokens have been received (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26105\">#26105</a>)</li>\n<li>VSCode: Fixed conversation messages appearing dimmed while the AskUserQuestion dialog is open (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26078\">#26078</a>)</li>\n<li>Fixed background tasks failing in git worktrees due to remote URL resolution reading from worktree-specific gitdir instead of the main repository config (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26065\">#26065</a>)</li>\n<li>Fixed Right Alt key leaving visible <code>[25~</code> escape sequence residue in the input field on Windows/Git Bash terminals (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25943\">#25943</a>)</li>\n<li>The <code>/rename</code> command now updates the terminal tab title by default (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/25789\">#25789</a>)</li>\n<li>Fixed Edit tool silently corrupting Unicode curly quotes (\\u201c\\u201d \\u2018\\u2019) by replacing them with straight quotes when making edits (<a class=\"issue-link js-issue-link\" href=\"https://github.com/anthropics/claude-code/issues/26141\">#26141</a>)</li>\n<li>Fixed OSC 8 hyperlinks only being clickable on the first line when link text wraps across multiple terminal lines.</li>\n</ul>",
      "image_url": "",
      "published": "2026-02-18T21:38:45Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 1.0,
      "freshness": 0.801,
      "tier1_quick_score": 4.168,
      "v2_slot": "agent_tooling_releases",
      "v2_prefilter_score": 4.001,
      "llm_label_source": "heuristic",
      "llm_category": "release",
      "llm_summary_1line": "What's changed Fixed FileWriteTool line counting to preserve intentional trailing blank lines instead of stripping them with trimEnd() . Fixed Windows terminal rendering bugs caused by os.EOL ( \\r\\n ) in display code...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.6,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.26,
      "summary_1line": "What's changed Fixed FileWriteTool line counting to preserve intentional trailing blank lines instead of stripping them with trimEnd() . Fixed Windows terminal rendering bugs caused by os.EOL ( \\r\\n ) in display code...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.498,
      "v2_global_score": 2.758
    },
    {
      "id": "87ab2257aa76cec8",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "How Dropbox Built a Scalable Context Engine for Enterprise Knowledge Search",
      "url": "https://www.infoq.com/news/2026/02/dropbox-context-engine/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/dropbox-context-engine/en/headerimage/generatedHeaderImage-1771284814159.jpg\" /><p>Dropbox engineers have detailed how the company built the context engine behind Dropbox Dash, revealing a shift toward index-based retrieval, knowledge graph-derived context, and continuous evaluation to support enterprise AI at scale</p> <i>By Matt Foster</i>",
      "image_url": "https://res.infoq.com/news/2026/02/dropbox-context-engine/en/headerimage/generatedHeaderImage-1771284814159.jpg",
      "published": "Wed, 18 Feb 2026 07:23:00 GMT",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.514,
      "tier1_quick_score": 2.944,
      "v2_slot": "practitioner_analysis",
      "v2_prefilter_score": 2.664,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Dropbox engineers have detailed how the company built the context engine behind Dropbox Dash, revealing a shift toward index-based retrieval, knowledge graph-derived context, and continuous evaluation to support enter...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.227,
      "summary_1line": "Dropbox engineers have detailed how the company built the context engine behind Dropbox Dash, revealing a shift toward index-based retrieval, knowledge graph-derived context, and continuous evaluation to support enter...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.512,
      "v2_global_score": 2.739
    },
    {
      "id": "c58d7596e7e871ef",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.104.0",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.104.0",
      "summary": "<h2>New Features</h2>\n<ul>\n<li>Added <code>WS_PROXY</code>/<code>WSS_PROXY</code> environment support (including lowercase variants) for websocket proxying in the network proxy. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11784\">#11784</a>)</li>\n<li>App-server v2 now emits notifications when threads are archived or unarchived, enabling clients to react without polling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12030\">#12030</a>)</li>\n<li>Protocol/core now carry distinct approval IDs for command approvals to support multiple approvals within a single shell command execution flow. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12051\">#12051</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li><code>Ctrl+C</code>/<code>Ctrl+D</code> now cleanly exits the cwd-change prompt during resume/fork flows instead of implicitly selecting an option. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12040\">#12040</a>)</li>\n<li>Reduced false-positive safety-check downgrade behavior by relying on the response header model (and websocket top-level events) rather than the response body model slug. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12061\">#12061</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Updated docs and schemas to cover websocket proxy configuration, new thread archive/unarchive notifications, and the command approval ID plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11784\">#11784</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12030\">#12030</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12051\">#12051</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Made the Rust release workflow resilient to <code>npm publish</code> attempts for an already-published version. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12044\">#12044</a>)</li>\n<li>Standardized remote compaction test mocking and refreshed related snapshots to align with the default production-shaped behavior. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12050\">#12050</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.103.0...rust-v0.104.0\"><tt>rust-v0.103.0...rust-v0.104.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11784\">#11784</a> feat(network-proxy): add websocket proxy env support <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12044\">#12044</a> don't fail if an npm publish attempt is for an existing version. <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12040\">#12040</a> tui: exit session on Ctrl+C in cwd change prompt <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12030\">#12030</a> app-server: Emit thread archive/unarchive notifications <a class=\"user-mention notranslate\" href=\"https://github.com/euroelessar\">@euroelessar</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12061\">#12061</a> Chore: remove response model check and rely on header model for downgrade <a class=\"user-mention notranslate\" href=\"https://github.com/shijie-oai\">@shijie-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12051\">#12051</a> feat(core): plumb distinct approval ids for command approvals <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/12050\">#12050</a> Unify remote compaction snapshot mocks around default endpoint behavior <a class=\"user-mention notranslate\" href=\"https://github.com/charley-oai\">@charley-oai</a></li>\n</ul>",
      "image_url": "",
      "published": "2026-02-18T07:13:02Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 1.0,
      "freshness": 0.619,
      "tier1_quick_score": 3.992,
      "v2_slot": "agent_tooling_releases",
      "v2_prefilter_score": 3.819,
      "llm_label_source": "heuristic",
      "llm_category": "release",
      "llm_summary_1line": "New Features Added WS_PROXY / WSS_PROXY environment support (including lowercase variants) for websocket proxying in the network proxy. ( #11784 ) App-server v2 now emits notifications when threads are archived or una...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.6,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.206,
      "summary_1line": "New Features Added WS_PROXY / WSS_PROXY environment support (including lowercase variants) for websocket proxying in the network proxy. ( #11784 ) App-server v2 now emits notifications when threads are archived or una...",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.498,
      "v2_global_score": 2.704
    },
    {
      "id": "c35812ccca3ce7be",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Harness engineering: leveraging Codex in an agent-first world",
      "url": "https://openai.com/index/harness-engineering",
      "summary": "By Ryan Lopopolo, Member of the Technical Staff",
      "image_url": "",
      "published": "Wed, 11 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.09,
      "tier1_quick_score": 3.079,
      "v2_slot": "frontier_official",
      "v2_prefilter_score": 3.09,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "By Ryan Lopopolo, Member of the Technical Staff",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.1,
      "v2_topical_bias": 0.2,
      "v2_final_score": 1.918,
      "summary_1line": "By Ryan Lopopolo, Member of the Technical Staff",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.686,
      "v2_global_score": 2.604
    },
    {
      "id": "c16b69a1be247646",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "GPT-5.2 derives a new result in theoretical physics",
      "url": "https://openai.com/index/new-result-theoretical-physics",
      "summary": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
      "image_url": "",
      "published": "Fri, 13 Feb 2026 11:00:00 GMT",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.167,
      "tier1_quick_score": 3.158,
      "v2_slot": "frontier_official",
      "v2_prefilter_score": 3.167,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.1,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.893,
      "summary_1line": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.686,
      "v2_global_score": 2.579
    },
    {
      "id": "b7dd9d05bcdf917e",
      "source": "claude_agent_sdk_python_releases",
      "source_weight": 1.3,
      "title": "v0.1.38",
      "url": "https://github.com/anthropics/claude-agent-sdk-python/releases/tag/v0.1.38",
      "summary": "<h3>Internal/Other Changes</h3>\n<ul>\n<li>Updated bundled Claude CLI to version 2.1.47</li>\n</ul>\n<hr />\n<p><strong>PyPI:</strong> <a href=\"https://pypi.org/project/claude-agent-sdk/0.1.38/\" rel=\"nofollow\">https://pypi.org/project/claude-agent-sdk/0.1.38/</a></p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\"><pre>pip install claude-agent-sdk==0.1.38</pre></div>",
      "image_url": "",
      "published": "2026-02-18T21:57:56Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "release",
      "source_reliability": 1.0,
      "freshness": 0.806,
      "tier1_quick_score": 3.272,
      "v2_slot": "agent_tooling_releases",
      "v2_prefilter_score": 3.106,
      "llm_label_source": "heuristic",
      "llm_category": "release",
      "llm_summary_1line": "Internal/Other Changes Updated bundled Claude CLI to version 2.1.47 PyPI: https://pypi.org/project/claude-agent-sdk/0.1.38/ pip install claude-agent-sdk==0.1.38",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.25,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.017,
      "summary_1line": "Internal/Other Changes Updated bundled Claude CLI to version 2.1.47 PyPI: https://pypi.org/project/claude-agent-sdk/0.1.38/ pip install claude-agent-sdk==0.1.38",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.498,
      "v2_global_score": 2.515
    },
    {
      "id": "6ed48b697f4e1625",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude Sonnet 4 6",
      "url": "https://www.anthropic.com/news/claude-sonnet-4-6",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T17:45:22.000Z",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.604,
      "tier1_quick_score": 3.457,
      "v2_slot": "frontier_official",
      "v2_prefilter_score": 3.404,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Claude Sonnet 4 6",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.06,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.781,
      "summary_1line": "Claude Sonnet 4 6",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.686,
      "v2_global_score": 2.467
    },
    {
      "id": "f8127c71f0f00b70",
      "source": "claude_blog",
      "source_weight": 1.15,
      "title": "Improved Web Search With Dynamic Filtering",
      "url": "https://claude.com/blog/improved-web-search-with-dynamic-filtering",
      "summary": "",
      "image_url": "",
      "published": "2026-02-17T00:00:00+00:00",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "news",
      "source_reliability": 1.0,
      "freshness": 0.484,
      "tier1_quick_score": 2.663,
      "v2_slot": "frontier_official",
      "v2_prefilter_score": 2.634,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Improved Web Search With Dynamic Filtering",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.777,
      "summary_1line": "Improved Web Search With Dynamic Filtering",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.686,
      "v2_global_score": 2.463
    },
    {
      "id": "5ded8551ab4e9f3f",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "IBM and UC Berkeley Diagnose Why Enterprise Agents Fail Using IT-Bench and MAST",
      "url": "https://huggingface.co/blog/ibm-research/itbenchandmast",
      "summary": "",
      "image_url": "",
      "published": "Wed, 18 Feb 2026 16:15:45 GMT",
      "collected_at": "2026-02-19T00:00:08.275543+00:00",
      "ingest_batch_id": "20260219-000008",
      "tier": "tier1",
      "type": "research",
      "source_reliability": 1.0,
      "freshness": 0.853,
      "tier1_quick_score": 2.998,
      "v2_slot": "research_watch",
      "v2_prefilter_score": 2.953,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "IBM and UC Berkeley Diagnose Why Enterprise Agents Fail Using IT-Bench and MAST",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.028,
      "summary_1line": "IBM and UC Berkeley Diagnose Why Enterprise Agents Fail Using IT-Bench and MAST",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.351,
      "v2_global_score": 2.379
    }
  ]
}