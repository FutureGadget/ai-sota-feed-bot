{
  "run_at": "2026-02-16T10:53:21.184282+00:00",
  "item_count": 16,
  "items": [
    {
      "id": "c6e0c1c50f17e587",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Deep Blue",
      "url": "https://simonwillison.net/2026/Feb/15/deep-blue/#atom-everything",
      "summary": "<p>We coined a new term on the <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/\">Oxide and Friends podcast</a> last month (primary credit to Adam Leventhal) covering the sense of psychological ennui leading into existential dread that many software developers are feeling thanks to the encroachment of generative AI into their field of work.</p>\n<p>We're calling it <strong>Deep Blue</strong>.</p>\n<p>You can listen to it being coined in real time <a href=\"https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=2835s\">from 47:15 in the episode</a>. I've included <a href=\"https://simonwillison.net/2026/Feb/15/deep-blue/#transcript\">a transcript below</a>.</p>\n<p>Deep Blue is a very real issue.</p>\n<p>Becoming a professional software engineer is <em>hard</em>. Getting good enough for people to pay you money to write software takes years of dedicated work. The rewards are significant: this is a well compensated career which opens up a lot of great opportunities.</p>\n<p>It's also a career that's mostly free from gatekeepers and expensive prerequisites. You don't need an expensive degree or accreditation. A laptop, an internet connection and a lot of time and curiosity is enough to get you started.</p>\n<p>And it rewards the nerds! Spending your teenage years tinkering with computers turned out to be a very smart investment in your future.</p>\n<p>The idea that this could all be stripped away by a chatbot is <em>deeply</em> upsetting.</p>\n<p>I've seen signs of Deep Blue in most of the online communities I spend time in. I've even faced accusations from my peers that I am actively harming their future careers through my work helping people understand how well AI-assisted programming can work.</p>\n<p>I think this is an issue which is causing genuine mental anguish for a lot of people in our community. Giving it a name makes it easier for us to have conversations about it.</p>\n<h4 id=\"my-experiences-of-deep-blue\">My experiences of Deep Blue</h4>\n<p>I distinctly remember my first experience of Deep Blue. For me it was triggered by ChatGPT Code Interpreter back in early 2023.</p>\n<p>My primary project is <a href=\"https://datasette.io/\">Datasette</a>, an ecosystem of open source tools for telling stories with data. I had dedicated myself to the challenge of helping people (initially focusing on journalists) clean up, analyze and find meaning in data, in all sorts of shapes and sizes.</p>\n<p>I expected I would need to build a lot of software for this! It felt like a challenge that could keep me happily engaged for many years to come.</p>\n<p>Then I tried uploading a CSV file of <a href=\"https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data\">San Francisco Police Department Incident Reports</a> - hundreds of thousands of rows - to ChatGPT Code Interpreter and... it did every piece of data cleanup and analysis I had on my napkin roadmap for the next few years with a couple of prompts.</p>\n<p>It even converted the data into a neatly normalized SQLite database and let me download the result!</p>\n<p>I remember having two competing thoughts in parallel.</p>\n<p>On the one hand, as somebody who wants journalists to be able to do more with data, this felt like a <em>huge</em> breakthrough. Imagine giving every journalist in the world an on-demand analyst who could help them tackle any data question they could think of!</p>\n<p>But on the other hand... <em>what was I even for</em>? My confidence in the value of my own projects took a painful hit. Was the path I'd chosen for myself suddenly a dead end?</p>\n<p>I've had some further pangs of Deep Blue just in the past few weeks, thanks to the Claude Opus 4.5/4.6 and GPT-5.2/5.3 coding agent effect. As many other people are also observing, the latest generation of coding agents, given the right prompts, really can churn away for a few minutes to several hours and produce working, documented and fully tested software that exactly matches the criteria they were given.</p>\n<p>\"The code they write isn't any good\" doesn't really cut it any more.</p>\n<h4 id=\"transcript\">A lightly edited transcript</h4>\n<blockquote>\n<p><strong>Bryan</strong>: I think that we're going to see a real problem with AI induced ennui where software engineers in particular get listless because the AI can do anything. Simon, what do you think about that?</p>\n<p><strong>Simon</strong>: Definitely. Anyone who's paying close attention to coding agents is feeling some of that already. There's an extent where you sort of get over it when you realize that you're still useful, even though your ability to memorize the syntax of program languages is completely irrelevant now.</p>\n<p>Something I see a lot of is people out there who are having existential crises and are very, very unhappy because they're like, \"I dedicated my career to learning this thing and now it just does it. What am I even for?\". I will very happily try and convince those people that they are for a whole bunch of things and that none of that experience they've accumulated has gone to waste, but psychologically it's a difficult time for software engineers.</p>\n<p>[...]</p>\n<p><strong>Bryan</strong>: Okay, so I'm going to predict that we name that. Whatever that is, we have a name for that kind of feeling and that kind of, whether you want to call it a blueness or a loss of purpose, and that we're kind of trying to address it collectively in a directed way.</p>\n<p><strong>Adam</strong>: Okay, this is your big moment. Pick the name. If you call your shot from here, this is you pointing to the stands. You know, I – Like deep blue, you know.</p>\n<p><strong>Bryan</strong>: Yeah, deep blue. I like that. I like deep blue. Deep blue. Oh, did you walk me into that, you bastard? You just blew out the candles on my birthday cake.</p>\n<p>It wasn't my big moment at all. That was your big moment. No, that is, Adam, that is very good. That is deep blue.</p>\n<p><strong>Simon</strong>: All of the chess players and the Go players went through this a decade ago and they have come out stronger.</p>\n</blockquote>\n<p>Turns out it was more than a decade ago: <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov\">Deep Blue defeated Garry Kasparov in 1997</a>.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/oxide\">oxide</a>, <a href=\"https://simonwillison.net/tags/bryan-cantrill\">bryan-cantrill</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a></p>",
      "published": "2026-02-15T21:06:44+00:00",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.709,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.959,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "We coined a new term on the Oxide and Friends podcast last month (primary credit to Adam Leventhal) covering the sense of psychological ennui leading into existential dread that many software developers are feeling th...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.75,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.724,
      "summary_1line": "Simon Willison names \"Deep Blue\"—existential dread software engineers feel as coding agents automate away their core work; reflects on psychological toll of AI capability advances.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.54,
      "v2_global_score": 3.264
    },
    {
      "id": "b4a1f65fd2c36642",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Google Explores Scaling Principles for Multi-agent Coordination",
      "url": "https://www.infoq.com/news/2026/02/google-agent-scaling-principles/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/google-agent-scaling-principles/en/headerimage/google-scaling-agents-principles-1771231654834.jpeg\" /><p>Google Research tried to answer the question of how to design agent systems for optimal performance by running a controlled evaluation of 180 agent configurations. From this, the team derived what they call the \"first quantitative scaling principles for AI agent systems\", showing that multi-agent coordination does not reliably improve results and can even reduce performance.</p> <i>By Sergio De Simone</i>",
      "published": "Mon, 16 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.955,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.105,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Google Research tried to answer the question of how to design agent systems for optimal performance by running a controlled evaluation of 180 agent configurations. From this, the team derived what they call the \"first...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.6,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.633,
      "summary_1line": "Google derives quantitative scaling principles from 180 agent configs, finding multi-agent coordination doesn't reliably boost performance—challenges the 'more agents = better' assumption.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.54,
      "v2_global_score": 3.173
    },
    {
      "id": "cdb2c4ef400220ae",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
      "url": "http://arxiv.org/abs/2602.13165v1",
      "summary": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.",
      "published": "2026-02-13T18:25:00Z",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.563,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.413,
      "type": "paper",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 3.25,
      "v2_source_bias": -0.35,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.697,
      "summary_1line": "Krites: async LLM-judged semantic caching boosts static cache hit rates 3.9x for conversational AI without impacting critical-path latency.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.322,
      "v2_global_score": 3.019
    },
    {
      "id": "28aeb3b7d9a1e43f",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Article: Architecting Agentic MLOps: A Layered Protocol Strategy with A2A and MCP",
      "url": "https://www.infoq.com/articles/architecting-agentic-mlops-a2a-mcp/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/articles/architecting-agentic-mlops-a2a-mcp/en/headerimage/architecting-agentic-mlops-a2a-mcp-header-1770303550343.jpg\" /><p>In this article, the authors outline protocols for building extensible multi-agent MLOps systems. The core architecture deliberately decouples orchestration from execution, allowing teams to incrementally add capabilities via discovery and evolve operations from static pipelines toward intelligent, adaptive coordination.</p> <i>By Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora</i>",
      "published": "Mon, 16 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.955,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.105,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "In this article, the authors outline protocols for building extensible multi-agent MLOps systems. The core architecture deliberately decouples orchestration from execution, allowing teams to incrementally add capabili...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.4,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.463,
      "summary_1line": "MLOps architecture decouples orchestration from execution via A2A and MCP protocols, enabling multi-agent systems to evolve from static pipelines to adaptive coordination.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.54,
      "v2_global_score": 3.003
    },
    {
      "id": "8fdd8b7d12d6ce56",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Train AI Agents to Write Better Playwright Tests",
      "url": "https://testdino.com/blog/playwright-skill/",
      "summary": "<p>AI agents often generate inconsistent Playwright tests because they do not understand your application’s specific workflows, UI patterns, or constraints.<p>The Playwright Skill provides more than 70 structured markdown guides that teach patterns for locators, authentication, visual testing, CI configuration, and framework migration so agents can apply consistent solutions instead of guessing.<p>You install it with: npx skills add testdino-hq/playwright-skill.<p>The material is organized into five packs: core testing (46 guides), Playwright CLI usage for token‑efficient automation (10), Page Object Model patterns (2), CI/CD setup for major providers (9), and migrations from Cypress or Selenium (2).<p>Each guide follows the same structure—when to use a pattern, when to avoid it, quick reference code, and complete implementations—so learners can move from concept to reliable tests step by step.<p>The skill works with tools such as Claude Code, GitHub Copilot, Cursor, and any agent that implements the skills protocol, and it is MIT‑licensed so teams can adapt the content to their own standards and practices.<p>For a deeper walkthrough of the guides and structure, see the full article at <a href=\"https://testdino.com/blog/playwright-skill/\" rel=\"nofollow\">https://testdino.com/blog/playwright-skill/</a>.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47032774\">https://news.ycombinator.com/item?id=47032774</a></p>\n<p>Points: 2</p>\n<p># Comments: 0</p>",
      "published": "Mon, 16 Feb 2026 09:18:10 +0000",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "community_signal",
      "freshness": 0.908,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.008,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "AI agents often generate inconsistent Playwright tests because they do not understand your application’s specific workflows, UI patterns, or constraints. The Playwright Skill provides more than 70 structured markdown...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.75,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.49,
      "summary_1line": "70+ structured Playwright guides as a reusable skill for AI agents to write consistent, production-ready tests across Claude Code, Copilot, and Cursor.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.502,
      "v2_global_score": 2.992
    },
    {
      "id": "69224219498a0b50",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Quantization-Aware Collaborative Inference for Large Embodied AI Models",
      "url": "http://arxiv.org/abs/2602.13052v1",
      "summary": "Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.",
      "published": "2026-02-13T16:08:19Z",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.551,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.401,
      "type": "paper",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significa...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 3.05,
      "v2_source_bias": -0.35,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.525,
      "summary_1line": "Quantization-aware inference optimization for embodied AI on edge devices via rate-distortion bounds and joint bitwidth-frequency design.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.322,
      "v2_global_score": 2.847
    },
    {
      "id": "71f2751bde88bac5",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Three months of OpenClaw",
      "url": "https://simonwillison.net/2026/Feb/15/openclaw/#atom-everything",
      "summary": "<p>It's wild that the first commit to OpenClaw was <a href=\"https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712ccc11\">on November 25th 2025</a>, and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an extremely vague <a href=\"https://www.youtube.com/watch?v=n7I-D4YXbzg\">Super Bowl commercial for AI.com</a>.</p>\n<p>Quoting AI.com founder <a href=\"https://twitter.com/kris/status/2020663711015514399\">Kris Marszalek</a>, purchaser of the <a href=\"https://www.theregister.com/2026/02/09/70m_aicom_domain_sale/\">most expensive domain in history</a> for $70m:</p>\n<blockquote>\n<p>ai.com is the world’s first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.</p>\n</blockquote>\n<p>Looks like vaporware to me - all you can do right now is reserve a handle - but it's still remarkable to see an open source project get to <em>that</em> level of hype in such a short space of time.</p>\n<p><strong>Update</strong>: OpenClaw creator Peter Steinberger <a href=\"https://steipete.me/posts/2026/openclaw\">just announced</a> that he's joining OpenAI and plans to transfer ownership of OpenClaw to a new independent foundation.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/openclaw\">openclaw</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/open-source\">open-source</a>, <a href=\"https://simonwillison.net/tags/domains\">domains</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/peter-steinberger\">peter-steinberger</a></p>",
      "published": "2026-02-15T17:23:28+00:00",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.646,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.896,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "It's wild that the first commit to OpenClaw was on November 25th 2025 , and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an ex...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.247,
      "summary_1line": "OpenClaw hit 196K GitHub stars in 3 months, but the piece is gossip about hype cycles and domain speculation rather than technical depth on the framework itself.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.54,
      "v2_global_score": 2.787
    },
    {
      "id": "c16b69a1be247646",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "GPT-5.2 derives a new result in theoretical physics",
      "url": "https://openai.com/index/new-result-theoretical-physics",
      "summary": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
      "published": "Fri, 13 Feb 2026 11:00:00 GMT",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.05,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.05,
      "type": "news",
      "llm_label_source": "llm",
      "llm_category": "research",
      "llm_summary_1line": "GPT-5.2 proposed a novel gluon amplitude formula later formally verified, but lacks concrete automation or coding-harness implications.",
      "llm_why_1line": "Theoretical physics proof-of-concept, not applicable to production coding agents or delivery automation workflows.",
      "v2_llm_score": 1.5,
      "v2_source_bias": 0.1,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.31,
      "summary_1line": "GPT-5.2 discovers new theoretical physics formula, but lacks concrete relevance to production coding automation or agent harness development.",
      "why_it_matters": "Theoretical physics proof-of-concept, not applicable to production coding agents or delivery automation workflows.",
      "v2_slot_priority": 0.535,
      "v2_global_score": 1.845
    },
    {
      "id": "b91259f7d1a90da4",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Codepath Partnership",
      "url": "https://www.anthropic.com/news/anthropic-codepath-partnership",
      "summary": "",
      "published": "2026-02-13T16:19:50.000Z",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.063,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.863,
      "type": "news",
      "llm_label_source": "llm",
      "llm_category": "platform",
      "llm_summary_1line": "Anthropic partners with Codepath on education initiatives—likely a marketing announcement without technical depth on agentic coding or production infrastructure.",
      "llm_why_1line": "Partnership announcements typically lack concrete harness patterns, eval strategies, or automation architecture relevant to platform engineers.",
      "v2_llm_score": 1.15,
      "v2_source_bias": 0.06,
      "v2_topical_bias": -0.2,
      "v2_final_score": 0.793,
      "summary_1line": "Anthropic partners with CodePath on education initiatives; limited technical depth for production agentic systems.",
      "why_it_matters": "Partnership announcements typically lack concrete harness patterns, eval strategies, or automation architecture relevant to platform engineers.",
      "v2_slot_priority": 0.535,
      "v2_global_score": 1.328
    },
    {
      "id": "f09c45ee226de24a",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Chris Liddell Appointed Anthropic Board",
      "url": "https://www.anthropic.com/news/chris-liddell-appointed-anthropic-board",
      "summary": "",
      "published": "2026-02-13T16:21:14.000Z",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.063,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.863,
      "type": "news",
      "llm_label_source": "llm",
      "llm_category": "platform",
      "llm_summary_1line": "Chris Liddell joins Anthropic's board as COO. Corporate governance update with no direct technical implications for agentic platforms.",
      "llm_why_1line": "Board appointments are org-level news; no engineering deliverables, tooling, or research applicable to coding automation.",
      "v2_llm_score": 0.95,
      "v2_source_bias": 0.06,
      "v2_topical_bias": -0.2,
      "v2_final_score": 0.633,
      "summary_1line": "Chris Liddell joins Anthropic's board as independent director—organizational news with no technical impact on coding agents or production infrastructure.",
      "why_it_matters": "Board appointments are org-level news; no engineering deliverables, tooling, or research applicable to coding automation.",
      "v2_slot_priority": 0.535,
      "v2_global_score": 1.168
    },
    {
      "id": "504929088957b0e2",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
      "url": "https://huggingface.co/blog/openenv-turing",
      "summary": "",
      "published": "Thu, 12 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.385,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.485,
      "type": "research",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.4,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.298,
      "summary_1line": "OpenEnv framework for evaluating tool-using agents in realistic environments; limited technical depth in available excerpt.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.322,
      "v2_global_score": 2.62
    },
    {
      "id": "3748da72fd68d49d",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.42",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.42",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed /resume showing interrupt messages as session titles</li>\n<li>Fixed Opus 4.6 launch announcement showing for Bedrock/Vertex/Foundry users</li>\n<li>Improved error message for many-image dimension limit errors with /compact suggestion</li>\n</ul>",
      "published": "2026-02-13T19:56:33Z",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.325,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.525,
      "type": "release",
      "llm_label_source": "llm",
      "llm_category": "release",
      "llm_summary_1line": "Claude Code v2.1.42 ships UX fixes for resume sessions, model announcements, and image handling—minor polish iteration.",
      "llm_why_1line": "Incremental release; minor UX improvements, no new agentic capabilities or eval/harness features.",
      "v2_llm_score": 3.05,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.0,
      "v2_final_score": 2.232,
      "summary_1line": "Claude Code v2.1.42 patches session resume UI, deployment announcement visibility, and image handling errors—minor bug fixes with no feature additions.",
      "why_it_matters": "Incremental release; minor UX improvements, no new agentic capabilities or eval/harness features.",
      "v2_slot_priority": 0.382,
      "v2_global_score": 2.614
    },
    {
      "id": "fe7dbebc903fef59",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "India Brief Economic Index",
      "url": "https://www.anthropic.com/research/india-brief-economic-index",
      "summary": "",
      "published": "2026-02-16T07:23:21.000Z",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.97,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.37,
      "type": "research",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "India Brief Economic Index",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.4,
      "v2_topical_bias": 0.0,
      "v2_final_score": 2.245,
      "summary_1line": "Economic index research on India; no relevance to agentic coding, LLM infrastructure, or software automation.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.322,
      "v2_global_score": 2.567
    },
    {
      "id": "d7c065e7fd03c9f2",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Why OpenAI Should Build Slack",
      "url": "https://www.latent.space/p/ainews-why-openai-should-build-slack",
      "summary": "a quiet day lets us answer a Sam Altman question: what should he build next?",
      "published": "Sat, 14 Feb 2026 07:48:54 GMT",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.279,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.479,
      "type": "news",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "a quiet day lets us answer a Sam Altman question: what should he build next?",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.912,
      "summary_1line": "Opinion piece speculating on OpenAI's next product direction; lacks technical depth or agentic relevance.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.54,
      "v2_global_score": 2.452
    },
    {
      "id": "393019c2d406463f",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.100.0",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.100.0",
      "summary": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state across tool calls, with optional runtime path overrides. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Added support for multiple simultaneous rate limits across the protocol, backend client, and TUI status surfaces. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a>)</li>\n<li>Reintroduced app-server websocket transport with a split inbound/outbound architecture, plus connection-aware thread resume subscriptions. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a>)</li>\n<li>Added memory management slash commands in the TUI (<code>/m_update</code>, <code>/m_drop</code>) and expanded memory-read/metrics plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a>)</li>\n<li>Enabled Apps SDK apps in ChatGPT connector handling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a>)</li>\n<li>Promoted sandbox capabilities on both Linux and Windows, and introduced a new <code>ReadOnlyAccess</code> policy shape for configurable read access. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Fixed websocket incremental output duplication, prevented appends after <code>response.completed</code>, and treated <code>response.incomplete</code> as an error path. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a>)</li>\n<li>Improved websocket session stability by continuing ping handling when idle and suppressing noisy first-retry errors during quick reconnects. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a>)</li>\n<li>Fixed stale thread entries by dropping missing rollout files and cleaning stale DB metadata during thread listing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a>)</li>\n<li>Fixed Windows multi-line paste reliability in terminals (especially VS Code integrated terminal) by increasing paste burst timing tolerance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a>)</li>\n<li>Fixed incorrect inheritance of <code>limit_name</code> when merging partial rate-limit updates. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a>)</li>\n<li>Reduced repeated skill parse-error spam during active edits by increasing file-watcher debounce from 1s to 10s. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Added JS REPL documentation and config/schema guidance for enabling and configuring the feature. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Updated app-server websocket transport documentation in the app-server README. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Split <code>codex-common</code> into focused <code>codex-utils-*</code> crates to simplify dependency boundaries across Rust workspace components. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a>)</li>\n<li>Improved Rust release pipeline throughput and reliability for Windows and musl targets, including parallel Windows builds and musl link fixes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a>)</li>\n<li>Prevented GitHub release asset upload collisions by excluding duplicate <code>cargo-timing.html</code> artifacts. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.99.0...rust-v0.100.0\"><tt>rust-v0.99.0...rust-v0.100.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a> Do not resend output items in incremental websockets connections <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11246\">#11246</a> chore: persist turn_id in rollout session and make turn_id uuid based <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a> feat: support multiple rate limits <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11412\">#11412</a> tui: show non-file layer content in /debug-config <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11405\">#11405</a> Remove <code>test-support</code> feature from <code>codex-core</code> and replace it with explicit test toggles <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11428\">#11428</a> fix: flaky test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11429\">#11429</a> feat: improve thread listing <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a> feat: split codex-common into smaller utils crates <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11439\">#11439</a> feat: new memory prompts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11305\">#11305</a> Cache cloud requirements <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11452\">#11452</a> nit: increase max raw memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11455\">#11455</a> feat: close mem agent after consolidation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11454\">#11454</a> fix: optional schema of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11449\">#11449</a> feat: set policy for phase 2 memory <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11420\">#11420</a> chore: rename disable_websockets -&gt; websockets_disabled <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a> Do not attempt to append after response.completed <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11462\">#11462</a> clean: memory rollout recorder <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a> feat(core): promote Linux bubblewrap sandbox to Experimental <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11389\">#11389</a> Extract <code>codex-config</code> from <code>codex-core</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a> Reapply \"Add app-server transport layer with websocket support\" <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11470\">#11470</a> feat: panic if Constrained does not support Disabled <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11475\">#11475</a> feat: remove \"cargo check individual crates\" from CI <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a> feat: memory read path <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11471\">#11471</a> chore: clean rollout extraction in memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a> fix(tui): increase paste burst char interval on Windows to 30ms <a class=\"user-mention notranslate\" href=\"https://github.com/yuvrajangadsingh\">@yuvrajangadsingh</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11464\">#11464</a> chore: sub-agent never ask for approval <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11414\">#11414</a> Linkify feedback link <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11480\">#11480</a> chore: update mem prompt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11485\">#11485</a> fix: Constrained import <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a> Promote Windows Sandbox <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a> Add feature-gated freeform js_repl core runtime <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11419\">#11419</a> refactor: codex app-server ThreadState <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a> Pump pings <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a> feat: use more powerful machines for building Windows releases <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11479\">#11479</a> nit: memory truncation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a> Increased file watcher debounce duration from 1s to 10s <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11335\">#11335</a> Add AfterToolUse hook <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a> feat: build windows support binaries in parallel <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11290\">#11290</a> chore(tui) Simplify /status Permissions <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11503\">#11503</a> Make codex-sdk depend on openai/codex <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a> app-server: thread resume subscriptions <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11277\">#11277</a> Added seatbelt policy rule to allow os.cpus <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11506\">#11506</a> chore: inject originator/residency headers to ws client <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11497\">#11497</a> Hydrate previous model across resume/fork/rollback/task start <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11513\">#11513</a> feat: try to fix bugs I saw in the wild in the resource parsing logic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11509\">#11509</a> Consolidate search_tool feature into apps <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11388\">#11388</a> change model cap to server overload <a class=\"user-mention notranslate\" href=\"https://github.com/willwang-openai\">@willwang-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11504\">#11504</a> Pre-sampling compact with previous model context <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11516\">#11516</a> Clamp auto-compact limit to context window <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11520\">#11520</a> Update context window after model switch <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11519\">#11519</a> Use slug in tui <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11522\">#11522</a> fix: add --test_verbose_timeout_warnings to bazel.yml <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11526\">#11526</a> fix: remove errant Cargo.lock files <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11521\">#11521</a> test(app-server): stabilize app/list thread feature-flag test by using file-backed MCP OAuth creds <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a> feat: make sandbox read access configurable with <code>ReadOnlyAccess</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a> [apps] Allow Apps SDK apps. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11532\">#11532</a> fix compilation <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11531\">#11531</a> Teach codex to test itself <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11540\">#11540</a> ci: remove actions/cache from rust release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11542\">#11542</a> ci(windows): use DotSlash for zstd in rust-release-windows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11498\">#11498</a> build(linux-sandbox): always compile vendored bubblewrap on Linux; remove CODEX_BWRAP_ENABLE_FFI <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11545\">#11545</a> fix: make project_doc skill-render tests deterministic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11543\">#11543</a> ci: capture cargo timings in Rust CI and release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11539\">#11539</a> Bump rmcp to 0.15 <a class=\"user-mention notranslate\" href=\"https://github.com/gpeal\">@gpeal</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a> Hide the first websocket retry <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11551\">#11551</a> Add logs to model cache <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a> Fix rust-release failures in musl linking and release asset upload <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a> Handle response.incomplete <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a> fix: stop inheriting rate-limit limit_name <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a> rust-release: exclude cargo-timing.html from release assets <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11546\">#11546</a> fix: update memory writing prompt <a class=\"user-mention notranslate\" href=\"https://github.com/zuxin-oai\">@zuxin-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11448\">#11448</a> Fix test flake <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a> feat: mem slash commands <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11573\">#11573</a> Fix flaky pre_sampling_compact switch test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11571\">#11571</a> feat: mem drop cot <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a> Ensure list_threads drops stale rollout files <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11575\">#11575</a> fix: db stuff mem <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11581\">#11581</a> nit: upgrade DB version <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11577\">#11577</a> feat: truncate with model infos <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11590\">#11590</a> chore: clean consts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a> feat: metrics to memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11579\">#11579</a> Fix config test on macOS <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11600\">#11600</a> feat: add sanitizer to redact secrets <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11609\">#11609</a> chore: drop mcp validation of dynamic tools <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
      "published": "2026-02-12T18:30:23Z",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.206,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.406,
      "type": "release",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "New Features Added an experimental, feature-gated JavaScript REPL runtime ( js_repl ) that can persist state across tool calls, with optional runtime path overrides. ( #10674 ) Added support for multiple simultaneous...",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.35,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 1.907,
      "summary_1line": "Claude Code 0.100.0 adds experimental JS REPL with state persistence, multi-rate-limit support, websocket stability fixes, and expanded memory management—improving agent reliability for production coding workflows.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.382,
      "v2_global_score": 2.289
    },
    {
      "id": "df23a683c09c1437",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-sdk==0.3.6",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.6",
      "summary": "<p>Changes since sdk==0.3.5</p>\n<ul>\n<li>release(sdk-py): 0.3.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6805\">#6805</a>)</li>\n<li>chore: update to add prune method (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6804\">#6804</a>)</li>\n<li>chore: Re-organize client files. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6787\">#6787</a>)</li>\n</ul>",
      "published": "2026-02-14T19:46:16Z",
      "collected_at": "2026-02-16T10:50:58.231242+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.498,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.448,
      "type": "release",
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Changes since sdk==0.3.5 release(sdk-py): 0.3.6 ( #6805 ) chore: update to add prune method ( #6804 ) chore: Re-organize client files. ( #6787 )",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.06,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.609,
      "summary_1line": "LangGraph SDK 0.3.6 adds pruning methods and reorganizes client files—minor maintenance update.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.382,
      "v2_global_score": 1.991
    }
  ]
}