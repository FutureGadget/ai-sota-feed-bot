{
  "run_at": "2026-02-16T08:00:57.136856+00:00",
  "item_count": 17,
  "items": [
    {
      "id": "c6e0c1c50f17e587",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Deep Blue",
      "url": "https://simonwillison.net/2026/Feb/15/deep-blue/#atom-everything",
      "summary": "<p>We coined a new term on the <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/\">Oxide and Friends podcast</a> last month (primary credit to Adam Leventhal) covering the sense of psychological ennui leading into existential dread that many software developers are feeling thanks to the encroachment of generative AI into their field of work.</p>\n<p>We're calling it <strong>Deep Blue</strong>.</p>\n<p>You can listen to it being coined in real time <a href=\"https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=2835s\">from 47:15 in the episode</a>. I've included <a href=\"https://simonwillison.net/2026/Feb/15/deep-blue/#transcript\">a transcript below</a>.</p>\n<p>Deep Blue is a very real issue.</p>\n<p>Becoming a professional software engineer is <em>hard</em>. Getting good enough for people to pay you money to write software takes years of dedicated work. The rewards are significant: this is a well compensated career which opens up a lot of great opportunities.</p>\n<p>It's also a career that's mostly free from gatekeepers and expensive prerequisites. You don't need an expensive degree or accreditation. A laptop, an internet connection and a lot of time and curiosity is enough to get you started.</p>\n<p>And it rewards the nerds! Spending your teenage years tinkering with computers turned out to be a very smart investment in your future.</p>\n<p>The idea that this could all be stripped away by a chatbot is <em>deeply</em> upsetting.</p>\n<p>I've seen signs of Deep Blue in most of the online communities I spend time in. I've even faced accusations from my peers that I am actively harming their future careers through my work helping people understand how well AI-assisted programming can work.</p>\n<p>I think this is an issue which is causing genuine mental anguish for a lot of people in our community. Giving it a name makes it easier for us to have conversations about it.</p>\n<h4 id=\"my-experiences-of-deep-blue\">My experiences of Deep Blue</h4>\n<p>I distinctly remember my first experience of Deep Blue. For me it was triggered by ChatGPT Code Interpreter back in early 2023.</p>\n<p>My primary project is <a href=\"https://datasette.io/\">Datasette</a>, an ecosystem of open source tools for telling stories with data. I had dedicated myself to the challenge of helping people (initially focusing on journalists) clean up, analyze and find meaning in data, in all sorts of shapes and sizes.</p>\n<p>I expected I would need to build a lot of software for this! It felt like a challenge that could keep me happily engaged for many years to come.</p>\n<p>Then I tried uploading a CSV file of <a href=\"https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data\">San Francisco Police Department Incident Reports</a> - hundreds of thousands of rows - to ChatGPT Code Interpreter and... it did every piece of data cleanup and analysis I had on my napkin roadmap for the next few years with a couple of prompts.</p>\n<p>It even converted the data into a neatly normalized SQLite database and let me download the result!</p>\n<p>I remember having two competing thoughts in parallel.</p>\n<p>On the one hand, as somebody who wants journalists to be able to do more with data, this felt like a <em>huge</em> breakthrough. Imagine giving every journalist in the world an on-demand analyst who could help them tackle any data question they could think of!</p>\n<p>But on the other hand... <em>what was I even for</em>? My confidence in the value of my own projects took a painful hit. Was the path I'd chosen for myself suddenly a dead end?</p>\n<p>I've had some further pangs of Deep Blue just in the past few weeks, thanks to the Claude Opus 4.5/4.6 and GPT-5.2/5.3 coding agent effect. As many other people are also observing, the latest generation of coding agents, given the right prompts, really can churn away for a few minutes to several hours and produce working, documented and fully tested software that exactly matches the criteria they were given.</p>\n<p>\"The code they write isn't any good\" doesn't really cut it any more.</p>\n<h4 id=\"transcript\">A lightly edited transcript</h4>\n<blockquote>\n<p><strong>Bryan</strong>: I think that we're going to see a real problem with AI induced ennui where software engineers in particular get listless because the AI can do anything. Simon, what do you think about that?</p>\n<p><strong>Simon</strong>: Definitely. Anyone who's paying close attention to coding agents is feeling some of that already. There's an extent where you sort of get over it when you realize that you're still useful, even though your ability to memorize the syntax of program languages is completely irrelevant now.</p>\n<p>Something I see a lot of is people out there who are having existential crises and are very, very unhappy because they're like, \"I dedicated my career to learning this thing and now it just does it. What am I even for?\". I will very happily try and convince those people that they are for a whole bunch of things and that none of that experience they've accumulated has gone to waste, but psychologically it's a difficult time for software engineers.</p>\n<p>[...]</p>\n<p><strong>Bryan</strong>: Okay, so I'm going to predict that we name that. Whatever that is, we have a name for that kind of feeling and that kind of, whether you want to call it a blueness or a loss of purpose, and that we're kind of trying to address it collectively in a directed way.</p>\n<p><strong>Adam</strong>: Okay, this is your big moment. Pick the name. If you call your shot from here, this is you pointing to the stands. You know, I – Like deep blue, you know.</p>\n<p><strong>Bryan</strong>: Yeah, deep blue. I like that. I like deep blue. Deep blue. Oh, did you walk me into that, you bastard? You just blew out the candles on my birthday cake.</p>\n<p>It wasn't my big moment at all. That was your big moment. No, that is, Adam, that is very good. That is deep blue.</p>\n<p><strong>Simon</strong>: All of the chess players and the Go players went through this a decade ago and they have come out stronger.</p>\n</blockquote>\n<p>Turns out it was more than a decade ago: <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov\">Deep Blue defeated Garry Kasparov in 1997</a>.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/oxide\">oxide</a>, <a href=\"https://simonwillison.net/tags/bryan-cantrill\">bryan-cantrill</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a></p>",
      "published": "2026-02-15T21:06:44+00:00",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.762,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.012,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "<p>We coined a new term on the <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/\">Oxide and Friends podcast</a> last m",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.75,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.732,
      "summary_1line": "<p>We coined a new term on the <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/\">Oxide and Friends podcast</a> last m",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.472,
      "v2_global_score": 3.204
    },
    {
      "id": "cdb2c4ef400220ae",
      "source": "arxiv_cs_ai",
      "source_weight": 0.85,
      "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
      "url": "http://arxiv.org/abs/2602.13165v1",
      "summary": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.",
      "published": "2026-02-13T18:25:00Z",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.577,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.427,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential fo",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 3.25,
      "v2_source_bias": -0.35,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.699,
      "summary_1line": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential fo",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.326,
      "v2_global_score": 3.025
    },
    {
      "id": "1756b534c0c6ce67",
      "source": "hackernews_ai",
      "source_weight": 1.1,
      "title": "Show HN: Gulama – Security-first open-source AI agent (OpenClaw alternative)",
      "url": "https://github.com/san-techie21/gulama-bot",
      "summary": "<p>Hi HN,<p>I'm a security engineer with 15+ years in enterprise security. After watching OpenClaw explode to 180K stars while binding to 0.0.0.0 by default, shipping no encryption, and accumulating 512 CVEs — I decided to build what I think a personal AI agent should look like when security comes first.<p>Gulama is an open-source personal AI agent with 15+ security mechanisms built into the core:<p>- AES-256-GCM encryption for all credentials and memories (never plaintext)\n- Sandboxed execution via bubblewrap/Docker (same sandbox Anthropic uses for Claude Code)\n- Ed25519-signed skills (no unsigned code runs — unlike ClawHub's 230+ malicious skills)\n- Cedar-inspired policy engine for deterministic authorization\n- Canary tokens for prompt injection detection\n- Egress filtering + DLP to prevent data exfiltration\n- Gateway binds 127.0.0.1 ONLY by default (not 0.0.0.0)\n- Cryptographic hash-chain audit trail<p>Beyond security, it's a full-featured agent:<p>- 100+ LLM providers via LiteLLM (Anthropic, OpenAI, DeepSeek, Ollama, etc.)\n- 19 built-in skills (files, shell, web, browser, email, calendar, GitHub, Notion, Spotify, voice, MCP bridge, and more)\n- 10 communication channels (CLI, Telegram, Discord, Slack, WhatsApp, Matrix, Teams, Web UI, Voice Wake)\n- Full MCP server + client support\n- Multi-agent orchestration with background sub-agents\n- RAG-powered memory via ChromaDB\n- Self-modifying: the agent writes its own new skills at runtime (sandboxed)\n- 5 autonomy levels from \"ask before everything\" to full autopilot<p>Install: pip install gulama && gulama setup && gulama chat<p>Stack: Python 3.12+, FastAPI, LiteLLM, SQLite, ChromaDB, Click\nPyPI: <a href=\"https://pypi.org/project/gulama/\" rel=\"nofollow\">https://pypi.org/project/gulama/</a><p>Happy to answer any questions about the security architecture or design decisions.</p>\n<hr />\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47031982\">https://news.ycombinator.com/item?id=47031982</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>",
      "published": "Mon, 16 Feb 2026 07:27:20 +0000",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "community_signal",
      "freshness": 0.966,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.066,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "<p>Hi HN,<p>I'm a security engineer with 15+ years in enterprise security. After watching OpenClaw explode to 180K stars while binding to 0.",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.75,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.504,
      "summary_1line": "<p>Hi HN,<p>I'm a security engineer with 15+ years in enterprise security. After watching OpenClaw explode to 180K stars while binding to 0.",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.516,
      "v2_global_score": 3.021
    },
    {
      "id": "69224219498a0b50",
      "source": "arxiv_cs_lg",
      "source_weight": 0.85,
      "title": "Quantization-Aware Collaborative Inference for Large Embodied AI Models",
      "url": "http://arxiv.org/abs/2602.13052v1",
      "summary": "Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.",
      "published": "2026-02-13T16:08:19Z",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.565,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.415,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However,",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 3.05,
      "v2_source_bias": -0.35,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.527,
      "summary_1line": "Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However,",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.326,
      "v2_global_score": 2.853
    },
    {
      "id": "9d95a891a81b27c3",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Beyond rate limits: scaling access to Codex and Sora",
      "url": "https://openai.com/index/beyond-rate-limits",
      "summary": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
      "published": "Fri, 13 Feb 2026 09:00:00 GMT",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.052,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.052,
      "llm_label_source": "llm",
      "llm_category": "platform",
      "llm_summary_1line": "OpenAI describes rate-limiting, usage tracking, and credit system for Codex/Sora access",
      "llm_why_1line": "Relevant to production inference infra but lacks technical depth on harness integration or agent eval patterns",
      "v2_llm_score": 2.4,
      "v2_source_bias": 0.1,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.23,
      "summary_1line": "OpenAI describes rate-limiting, usage tracking, and credit system for Codex/Sora access",
      "why_it_matters": "Relevant to production inference infra but lacks technical depth on harness integration or agent eval patterns",
      "v2_slot_priority": 0.578,
      "v2_global_score": 2.808
    },
    {
      "id": "71f2751bde88bac5",
      "source": "simon_willison",
      "source_weight": 1.25,
      "title": "Three months of OpenClaw",
      "url": "https://simonwillison.net/2026/Feb/15/openclaw/#atom-everything",
      "summary": "<p>It's wild that the first commit to OpenClaw was <a href=\"https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712ccc11\">on November 25th 2025</a>, and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an extremely vague <a href=\"https://www.youtube.com/watch?v=n7I-D4YXbzg\">Super Bowl commercial for AI.com</a>.</p>\n<p>Quoting AI.com founder <a href=\"https://twitter.com/kris/status/2020663711015514399\">Kris Marszalek</a>, purchaser of the <a href=\"https://www.theregister.com/2026/02/09/70m_aicom_domain_sale/\">most expensive domain in history</a> for $70m:</p>\n<blockquote>\n<p>ai.com is the world’s first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.</p>\n</blockquote>\n<p>Looks like vaporware to me - all you can do right now is reserve a handle - but it's still remarkable to see an open source project get to <em>that</em> level of hype in such a short space of time.</p>\n<p><strong>Update</strong>: OpenClaw creator Peter Steinberger <a href=\"https://steipete.me/posts/2026/openclaw\">just announced</a> that he's joining OpenAI and plans to transfer ownership of OpenClaw to a new independent foundation.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/openclaw\">openclaw</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/open-source\">open-source</a>, <a href=\"https://simonwillison.net/tags/domains\">domains</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/peter-steinberger\">peter-steinberger</a></p>",
      "published": "2026-02-15T17:23:28+00:00",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.694,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.944,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "<p>It's wild that the first commit to OpenClaw was <a href=\"https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712c",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.254,
      "summary_1line": "<p>It's wild that the first commit to OpenClaw was <a href=\"https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712c",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.472,
      "v2_global_score": 2.726
    },
    {
      "id": "731b24bba0459a2f",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "Sixteen Claude Agents Built a C Compiler Without Human Intervention... Almost",
      "url": "https://www.infoq.com/news/2026/02/claude-built-c-compiler/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/claude-built-c-compiler/en/headerimage/claude-built-c-compiler-1771067001094.jpeg\" /><p>In an effort to probe the limits of autonomous software development Anthropic used sixteen Claude Opus 4.6 AI agents to build a Rust-based C compiler from scratch. Working in parallel on a shared repository, the agents coordinated their changes and ultimately produced a compiler capable of building the Linux 6.9 kernel across x86, ARM, and RISC-V, as well as many other open-source projects.</p> <i>By Sergio De Simone</i>",
      "published": "Sat, 14 Feb 2026 12:00:00 GMT",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.333,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.483,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "<img src=\"https://res.infoq.com/news/2026/02/claude-built-c-compiler/en/headerimage/claude-built-c-compiler-1771067001094.jpeg\" /><p>In an e",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.2,
      "summary_1line": "<img src=\"https://res.infoq.com/news/2026/02/claude-built-c-compiler/en/headerimage/claude-built-c-compiler-1771067001094.jpeg\" /><p>In an e",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.472,
      "v2_global_score": 2.672
    },
    {
      "id": "504929088957b0e2",
      "source": "huggingface_blog",
      "source_weight": 1.1,
      "title": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
      "url": "https://huggingface.co/blog/openenv-turing",
      "summary": "",
      "published": "Thu, 12 Feb 2026 00:00:00 GMT",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.395,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.495,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.4,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 2.299,
      "summary_1line": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.326,
      "v2_global_score": 2.625
    },
    {
      "id": "c5ef81aef2a2ccc1",
      "source": "openai_blog",
      "source_weight": 2.0,
      "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
      "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt",
      "summary": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT to help organizations defend against prompt injection and AI-driven data exfiltration.",
      "published": "Fri, 13 Feb 2026 10:00:00 GMT",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.054,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.054,
      "llm_label_source": "llm",
      "llm_category": "release",
      "llm_summary_1line": "OpenAI adds Lockdown Mode and risk labeling to ChatGPT for org security against prompt injection.",
      "llm_why_1line": "Security feature for ChatGPT users; minimal relevance to coding agents, harness patterns, or delivery automation.",
      "v2_llm_score": 1.6,
      "v2_source_bias": 0.1,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.391,
      "summary_1line": "OpenAI adds Lockdown Mode and risk labeling to ChatGPT for org security against prompt injection.",
      "why_it_matters": "Security feature for ChatGPT users; minimal relevance to coding agents, harness patterns, or delivery automation.",
      "v2_slot_priority": 0.578,
      "v2_global_score": 1.969
    },
    {
      "id": "a76c02cb3f5a6457",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Claude For Financial Services",
      "url": "https://www.anthropic.com/news/claude-for-financial-services",
      "summary": "",
      "published": "2026-02-13T15:35:49.000Z",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.068,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.868,
      "llm_label_source": "llm",
      "llm_category": "release",
      "llm_summary_1line": "Anthropic announces Claude availability for financial services sector.",
      "llm_why_1line": "Generic vertical positioning; lacks technical depth on agentic coding, evals, or production automation patterns.",
      "v2_llm_score": 1.15,
      "v2_source_bias": 0.06,
      "v2_topical_bias": 0.0,
      "v2_final_score": 0.994,
      "summary_1line": "Anthropic announces Claude availability for financial services sector.",
      "why_it_matters": "Generic vertical positioning; lacks technical depth on agentic coding, evals, or production automation patterns.",
      "v2_slot_priority": 0.578,
      "v2_global_score": 1.572
    },
    {
      "id": "fe7dbebc903fef59",
      "source": "anthropic_research",
      "source_weight": 1.4,
      "title": "India Brief Economic Index",
      "url": "https://www.anthropic.com/research/india-brief-economic-index",
      "summary": "",
      "published": "2026-02-16T07:23:21.000Z",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "research_watch",
      "freshness": 0.995,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.395,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "India Brief Economic Index",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.4,
      "v2_topical_bias": 0.0,
      "v2_final_score": 2.249,
      "summary_1line": "India Brief Economic Index",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.326,
      "v2_global_score": 2.575
    },
    {
      "id": "5140244ba501d641",
      "source": "infoq_ai_ml",
      "source_weight": 1.15,
      "title": "From Paging to Postmortem: Google Cloud SREs on Using Gemini CLI for Outage Response",
      "url": "https://www.infoq.com/news/2026/02/google-sre-gemini-cli-outage/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=AI%2C+ML+%26+Data+Engineering",
      "summary": "<img src=\"https://res.infoq.com/news/2026/02/google-sre-gemini-cli-outage/en/headerimage/generatedHeaderImage-1770021438197.jpg\" /><p>A recent article by Google Cloud SREs describes how they use the AI-powered Gemini CLI internally to resolve real-world outages. This approach improves reliability in critical infrastructure operations and reduces incident response time by integrating intelligent reasoning directly into the terminal-based operational tools.</p> <i>By Renato Losio</i>",
      "published": "Sat, 14 Feb 2026 11:32:00 GMT",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.329,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.479,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "<img src=\"https://res.infoq.com/news/2026/02/google-sre-gemini-cli-outage/en/headerimage/generatedHeaderImage-1770021438197.jpg\" /><p>A rece",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.08,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.999,
      "summary_1line": "<img src=\"https://res.infoq.com/news/2026/02/google-sre-gemini-cli-outage/en/headerimage/generatedHeaderImage-1770021438197.jpg\" /><p>A rece",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.472,
      "v2_global_score": 2.471
    },
    {
      "id": "d7c065e7fd03c9f2",
      "source": "latent_space",
      "source_weight": 1.2,
      "title": "[AINews] Why OpenAI Should Build Slack",
      "url": "https://www.latent.space/p/ainews-why-openai-should-build-slack",
      "summary": "a quiet day lets us answer a Sam Altman question: what should he build next?",
      "published": "Sat, 14 Feb 2026 07:48:54 GMT",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "practitioner_analysis",
      "freshness": 0.3,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.5,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "a quiet day lets us answer a Sam Altman question: what should he build next?",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.2,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.915,
      "summary_1line": "a quiet day lets us answer a Sam Altman question: what should he build next?",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.472,
      "v2_global_score": 2.387
    },
    {
      "id": "393019c2d406463f",
      "source": "openai_codex_releases",
      "source_weight": 2.2,
      "title": "0.100.0",
      "url": "https://github.com/openai/codex/releases/tag/rust-v0.100.0",
      "summary": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state across tool calls, with optional runtime path overrides. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Added support for multiple simultaneous rate limits across the protocol, backend client, and TUI status surfaces. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a>)</li>\n<li>Reintroduced app-server websocket transport with a split inbound/outbound architecture, plus connection-aware thread resume subscriptions. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a>)</li>\n<li>Added memory management slash commands in the TUI (<code>/m_update</code>, <code>/m_drop</code>) and expanded memory-read/metrics plumbing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a>)</li>\n<li>Enabled Apps SDK apps in ChatGPT connector handling. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a>)</li>\n<li>Promoted sandbox capabilities on both Linux and Windows, and introduced a new <code>ReadOnlyAccess</code> policy shape for configurable read access. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a>)</li>\n</ul>\n<h2>Bug Fixes</h2>\n<ul>\n<li>Fixed websocket incremental output duplication, prevented appends after <code>response.completed</code>, and treated <code>response.incomplete</code> as an error path. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a>)</li>\n<li>Improved websocket session stability by continuing ping handling when idle and suppressing noisy first-retry errors during quick reconnects. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a>)</li>\n<li>Fixed stale thread entries by dropping missing rollout files and cleaning stale DB metadata during thread listing. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a>)</li>\n<li>Fixed Windows multi-line paste reliability in terminals (especially VS Code integrated terminal) by increasing paste burst timing tolerance. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a>)</li>\n<li>Fixed incorrect inheritance of <code>limit_name</code> when merging partial rate-limit updates. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a>)</li>\n<li>Reduced repeated skill parse-error spam during active edits by increasing file-watcher debounce from 1s to 10s. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>Added JS REPL documentation and config/schema guidance for enabling and configuring the feature. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a>)</li>\n<li>Updated app-server websocket transport documentation in the app-server README. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a>)</li>\n</ul>\n<h2>Chores</h2>\n<ul>\n<li>Split <code>codex-common</code> into focused <code>codex-utils-*</code> crates to simplify dependency boundaries across Rust workspace components. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a>)</li>\n<li>Improved Rust release pipeline throughput and reliability for Windows and musl targets, including parallel Windows builds and musl link fixes. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a>, <a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a>)</li>\n<li>Prevented GitHub release asset upload collisions by excluding duplicate <code>cargo-timing.html</code> artifacts. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a>)</li>\n</ul>\n<h2>Changelog</h2>\n<p>Full Changelog: <a class=\"commit-link\" href=\"https://github.com/openai/codex/compare/rust-v0.99.0...rust-v0.100.0\"><tt>rust-v0.99.0...rust-v0.100.0</tt></a></p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11383\">#11383</a> Do not resend output items in incremental websockets connections <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11246\">#11246</a> chore: persist turn_id in rollout session and make turn_id uuid based <a class=\"user-mention notranslate\" href=\"https://github.com/celia-oai\">@celia-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11260\">#11260</a> feat: support multiple rate limits <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11412\">#11412</a> tui: show non-file layer content in /debug-config <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11405\">#11405</a> Remove <code>test-support</code> feature from <code>codex-core</code> and replace it with explicit test toggles <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11428\">#11428</a> fix: flaky test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11429\">#11429</a> feat: improve thread listing <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11422\">#11422</a> feat: split codex-common into smaller utils crates <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11439\">#11439</a> feat: new memory prompts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11305\">#11305</a> Cache cloud requirements <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11452\">#11452</a> nit: increase max raw memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11455\">#11455</a> feat: close mem agent after consolidation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11454\">#11454</a> fix: optional schema of memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11449\">#11449</a> feat: set policy for phase 2 memory <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11420\">#11420</a> chore: rename disable_websockets -&gt; websockets_disabled <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11402\">#11402</a> Do not attempt to append after response.completed <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11462\">#11462</a> clean: memory rollout recorder <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11381\">#11381</a> feat(core): promote Linux bubblewrap sandbox to Experimental <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11389\">#11389</a> Extract <code>codex-config</code> from <code>codex-core</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11370\">#11370</a> Reapply \"Add app-server transport layer with websocket support\" <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11470\">#11470</a> feat: panic if Constrained does not support Disabled <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11475\">#11475</a> feat: remove \"cargo check individual crates\" from CI <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11459\">#11459</a> feat: memory read path <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11471\">#11471</a> chore: clean rollout extraction in memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/9348\">#9348</a> fix(tui): increase paste burst char interval on Windows to 30ms <a class=\"user-mention notranslate\" href=\"https://github.com/yuvrajangadsingh\">@yuvrajangadsingh</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11464\">#11464</a> chore: sub-agent never ask for approval <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11414\">#11414</a> Linkify feedback link <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11480\">#11480</a> chore: update mem prompt <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11485\">#11485</a> fix: Constrained import <a class=\"user-mention notranslate\" href=\"https://github.com/owenlin0\">@owenlin0</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11341\">#11341</a> Promote Windows Sandbox <a class=\"user-mention notranslate\" href=\"https://github.com/iceweasel-oai\">@iceweasel-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/10674\">#10674</a> Add feature-gated freeform js_repl core runtime <a class=\"user-mention notranslate\" href=\"https://github.com/fjord-oai\">@fjord-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11419\">#11419</a> refactor: codex app-server ThreadState <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11413\">#11413</a> Pump pings <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11488\">#11488</a> feat: use more powerful machines for building Windows releases <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11479\">#11479</a> nit: memory truncation <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11494\">#11494</a> Increased file watcher debounce duration from 1s to 10s <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11335\">#11335</a> Add AfterToolUse hook <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11500\">#11500</a> feat: build windows support binaries in parallel <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11290\">#11290</a> chore(tui) Simplify /status Permissions <a class=\"user-mention notranslate\" href=\"https://github.com/dylan-hurd-oai\">@dylan-hurd-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11503\">#11503</a> Make codex-sdk depend on openai/codex <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11474\">#11474</a> app-server: thread resume subscriptions <a class=\"user-mention notranslate\" href=\"https://github.com/maxj-oai\">@maxj-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11277\">#11277</a> Added seatbelt policy rule to allow os.cpus <a class=\"user-mention notranslate\" href=\"https://github.com/etraut-openai\">@etraut-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11506\">#11506</a> chore: inject originator/residency headers to ws client <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11497\">#11497</a> Hydrate previous model across resume/fork/rollback/task start <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11513\">#11513</a> feat: try to fix bugs I saw in the wild in the resource parsing logic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11509\">#11509</a> Consolidate search_tool feature into apps <a class=\"user-mention notranslate\" href=\"https://github.com/apanasenko-oai\">@apanasenko-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11388\">#11388</a> change model cap to server overload <a class=\"user-mention notranslate\" href=\"https://github.com/willwang-openai\">@willwang-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11504\">#11504</a> Pre-sampling compact with previous model context <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11516\">#11516</a> Clamp auto-compact limit to context window <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11520\">#11520</a> Update context window after model switch <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11519\">#11519</a> Use slug in tui <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11522\">#11522</a> fix: add --test_verbose_timeout_warnings to bazel.yml <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11526\">#11526</a> fix: remove errant Cargo.lock files <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11521\">#11521</a> test(app-server): stabilize app/list thread feature-flag test by using file-backed MCP OAuth creds <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11387\">#11387</a> feat: make sandbox read access configurable with <code>ReadOnlyAccess</code> <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11486\">#11486</a> [apps] Allow Apps SDK apps. <a class=\"user-mention notranslate\" href=\"https://github.com/mzeng-openai\">@mzeng-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11532\">#11532</a> fix compilation <a class=\"user-mention notranslate\" href=\"https://github.com/sayan-oai\">@sayan-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11531\">#11531</a> Teach codex to test itself <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11540\">#11540</a> ci: remove actions/cache from rust release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11542\">#11542</a> ci(windows): use DotSlash for zstd in rust-release-windows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11498\">#11498</a> build(linux-sandbox): always compile vendored bubblewrap on Linux; remove CODEX_BWRAP_ENABLE_FFI <a class=\"user-mention notranslate\" href=\"https://github.com/viyatb-oai\">@viyatb-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11545\">#11545</a> fix: make project_doc skill-render tests deterministic <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11543\">#11543</a> ci: capture cargo timings in Rust CI and release workflows <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11539\">#11539</a> Bump rmcp to 0.15 <a class=\"user-mention notranslate\" href=\"https://github.com/gpeal\">@gpeal</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11548\">#11548</a> Hide the first websocket retry <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11551\">#11551</a> Add logs to model cache <a class=\"user-mention notranslate\" href=\"https://github.com/aibrahim-oai\">@aibrahim-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11556\">#11556</a> Fix rust-release failures in musl linking and release asset upload <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11558\">#11558</a> Handle response.incomplete <a class=\"user-mention notranslate\" href=\"https://github.com/pakrym-oai\">@pakrym-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11557\">#11557</a> fix: stop inheriting rate-limit limit_name <a class=\"user-mention notranslate\" href=\"https://github.com/xl-openai\">@xl-openai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11564\">#11564</a> rust-release: exclude cargo-timing.html from release assets <a class=\"user-mention notranslate\" href=\"https://github.com/bolinfest\">@bolinfest</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11546\">#11546</a> fix: update memory writing prompt <a class=\"user-mention notranslate\" href=\"https://github.com/zuxin-oai\">@zuxin-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11448\">#11448</a> Fix test flake <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11569\">#11569</a> feat: mem slash commands <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11573\">#11573</a> Fix flaky pre_sampling_compact switch test <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11571\">#11571</a> feat: mem drop cot <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11572\">#11572</a> Ensure list_threads drops stale rollout files <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11575\">#11575</a> fix: db stuff mem <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11581\">#11581</a> nit: upgrade DB version <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11577\">#11577</a> feat: truncate with model infos <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11590\">#11590</a> chore: clean consts <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11593\">#11593</a> feat: metrics to memories <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11579\">#11579</a> Fix config test on macOS <a class=\"user-mention notranslate\" href=\"https://github.com/gt-oai\">@gt-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11600\">#11600</a> feat: add sanitizer to redact secrets <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n<li><a class=\"issue-link js-issue-link\" href=\"https://github.com/openai/codex/pull/11609\">#11609</a> chore: drop mcp validation of dynamic tools <a class=\"user-mention notranslate\" href=\"https://github.com/jif-oai\">@jif-oai</a></li>\n</ul>",
      "published": "2026-02-12T18:30:23Z",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.217,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.417,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state ac",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.35,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 1.91,
      "summary_1line": "<h2>New Features</h2>\n<ul>\n<li>Added an experimental, feature-gated JavaScript REPL runtime (<code>js_repl</code>) that can persist state ac",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.346,
      "v2_global_score": 2.256
    },
    {
      "id": "f70833bd2f581c75",
      "source": "claude_code_releases",
      "source_weight": 2.2,
      "title": "v2.1.41",
      "url": "https://github.com/anthropics/claude-code/releases/tag/v2.1.41",
      "summary": "<h2>What's changed</h2>\n<ul>\n<li>Fixed AWS auth refresh hanging indefinitely by adding a 3-minute timeout</li>\n<li>Added <code>claude auth login</code>, <code>claude auth status</code>, and <code>claude auth logout</code> CLI subcommands</li>\n<li>Added Windows ARM64 (win32-arm64) native binary support</li>\n<li>Improved <code>/rename</code> to auto-generate session name from conversation context when called without arguments</li>\n<li>Improved narrow terminal layout for prompt footer</li>\n<li>Fixed file resolution failing for @-mentions with anchor fragments (e.g., <code>@README.md#installation</code>)</li>\n<li>Fixed FileReadTool blocking the process on FIFOs, <code>/dev/stdin</code>, and large files</li>\n<li>Fixed background task notifications not being delivered in streaming Agent SDK mode</li>\n<li>Fixed cursor jumping to end on each keystroke in classifier rule input</li>\n<li>Fixed markdown link display text being dropped for raw URL</li>\n<li>Fixed auto-compact failure error notifications being shown to users</li>\n<li>Fixed permission wait time being included in subagent elapsed time display</li>\n<li>Fixed proactive ticks firing while in plan mode</li>\n<li>Fixed clear stale permission rules when settings change on disk</li>\n<li>Fixed hook blocking errors showing stderr content in UI</li>\n</ul>",
      "published": "2026-02-13T06:08:49Z",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.267,
      "source_reliability": 1.0,
      "v2_prefilter_score": 3.467,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "<h2>What's changed</h2>\n<ul>\n<li>Fixed AWS auth refresh hanging indefinitely by adding a 3-minute timeout</li>\n<li>Added <code>claude auth l",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.0,
      "v2_topical_bias": 0.2,
      "v2_final_score": 1.68,
      "summary_1line": "<h2>What's changed</h2>\n<ul>\n<li>Fixed AWS auth refresh hanging indefinitely by adding a 3-minute timeout</li>\n<li>Added <code>claude auth l",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.346,
      "v2_global_score": 2.026
    },
    {
      "id": "df23a683c09c1437",
      "source": "langgraph_releases",
      "source_weight": 0.95,
      "title": "langgraph-sdk==0.3.6",
      "url": "https://github.com/langchain-ai/langgraph/releases/tag/sdk%3D%3D0.3.6",
      "summary": "<p>Changes since sdk==0.3.5</p>\n<ul>\n<li>release(sdk-py): 0.3.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6805\">#6805</a>)</li>\n<li>chore: update to add prune method (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6804\">#6804</a>)</li>\n<li>chore: Re-organize client files. (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/langgraph/pull/6787\">#6787</a>)</li>\n</ul>",
      "published": "2026-02-14T19:46:16Z",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "agent_tooling_releases",
      "freshness": 0.524,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.474,
      "llm_label_source": "heuristic",
      "llm_category": "platform",
      "llm_summary_1line": "<p>Changes since sdk==0.3.5</p>\n<ul>\n<li>release(sdk-py): 0.3.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/l",
      "llm_why_1line": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_llm_score": 2.0,
      "v2_source_bias": 0.06,
      "v2_topical_bias": 0.0,
      "v2_final_score": 1.617,
      "summary_1line": "<p>Changes since sdk==0.3.5</p>\n<ul>\n<li>release(sdk-py): 0.3.6 (<a class=\"issue-link js-issue-link\" href=\"https://github.com/langchain-ai/l",
      "why_it_matters": "Potential relevance to AI platform engineering; verify practical impact.",
      "v2_slot_priority": 0.346,
      "v2_global_score": 1.963
    },
    {
      "id": "b91259f7d1a90da4",
      "source": "anthropic_newsroom",
      "source_weight": 1.8,
      "title": "Anthropic Codepath Partnership",
      "url": "https://www.anthropic.com/news/anthropic-codepath-partnership",
      "summary": "",
      "published": "2026-02-13T16:19:50.000Z",
      "collected_at": "2026-02-16T07:59:44.376533+00:00",
      "v2_slot": "frontier_official",
      "freshness": 0.07,
      "source_reliability": 1.0,
      "v2_prefilter_score": 2.87,
      "llm_label_source": "llm",
      "llm_category": "release",
      "llm_summary_1line": "Anthropic partners with Codepath on coding education; limited technical depth for platform engineers.",
      "llm_why_1line": "Partnership announcement lacks concrete harness, eval, or delivery automation insights for engineers.",
      "v2_llm_score": 1.35,
      "v2_source_bias": 0.06,
      "v2_topical_bias": -0.2,
      "v2_final_score": 0.954,
      "summary_1line": "Anthropic partners with Codepath on coding education; limited technical depth for platform engineers.",
      "why_it_matters": "Partnership announcement lacks concrete harness, eval, or delivery automation insights for engineers.",
      "v2_slot_priority": 0.578,
      "v2_global_score": 1.532
    }
  ]
}